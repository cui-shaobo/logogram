{"Type": "conference", "Year": "1988", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Neural Control of Sensory Acquisition", "Title": "The Vestibulo-Ocular Reflex", "Abstract": "We present a new hypothesis that the cerebellum plays a key role in ac(cid:173) tively controlling the acquisition of sensory infonnation by the nervous  system.  In this paper we explore this idea by examining the function of  a  simple  cerebellar-related  behavior,  the  vestibula-ocular  reflex  or  VOR, in  which  eye movements  are generated to minimize image slip  on  the  retina  during  rapid  head  movements.  Considering  this  system  from  the point of view of statistical estimation theory, our results  sug(cid:173) gest that the transfer function of the VOR, often regarded as a static or  slowly  modifiable  feature  of the  system,  should  actually  be  continu(cid:173) ously and rapidly changed during head movements. We further suggest  that these changes are under the direct control of the cerebellar cortex  and propose experiments to test this hypothesis."}
{"Type": "conference", "Year": "1988", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "The Boltzmann Perceptron Network", "Title": "A Multi-Layered Feed-Forward Network Equivalent to the Boltzmann Machine", "Abstract": "The  concept  of  the  stochastic  Boltzmann  machine  (BM)  is  auractive  for  decision  making  and  pattern  classification  purposes  since  the  probability  of  attaining  the network  states  is a  function  of the network energy.  Hence,  the  probability of attaining particular energy minima  may be associated  with  the  probabilities  of  making  certain  decisions  (or  classifications).  However,  because of its stochastic  nature,  the complexity of the BM is fairly  high and  therefore  such  networks  are  not  very  likely  to  be  used  in  practice.  In  this  paper  we  suggest  a  way  to  alleviate  this  drawback  by  converting  the  sto(cid:173) chastic  BM into  a  deterministic  network  which  we  call  the  Boltzmann  Per(cid:173) ceptron  Network  (BPN).  The BPN is functionally  equivalent  to  the  BM but  has  a  feed-forward  structure  and  low  complexity.  No annealing  is required.  The  conditions  under  which  such  a  convmion  is  feasible  are  given.  A  learning  algorithm  for  the  BPN based  on  the  conjugate  gradient  method  is  also provided which is somewhat akin  to the backpropagation algorithm."}
{"Type": "conference", "Year": "1988", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Skeletonization", "Title": "A Technique for Trimming the Fat from a Network via Relevance Assessment", "Abstract": "This paper proposes a means of using the knowledge in a network to  determine the functionality or relevance of individual units, both for  the purpose of understanding the network's behavior and improving its  performance. The basic idea is to iteratively train the network to a cer(cid:173) tain performance criterion, compute a measure of relevance that identi(cid:173) fies which input or hidden units are most critical to performance, and  automatically trim the least relevant units. This skeletonization tech(cid:173) nique can be used to simplify networks by eliminating units that con(cid:173) vey redundant information; to improve learning performance by first  learning with spare hidden units and then trimming the unnecessary  ones away, thereby constraining generalization; and to understand the  behavior of networks in terms of minimal \"rules.\""}
{"Type": "conference", "Year": "1988", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Linear Learning", "Title": "Landscapes and Algorithms", "Abstract": "Abstract Unavailable"}
{"Type": "conference", "Year": "1988", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "GENESIS", "Title": "A System for Simulating Neural Networks", "Abstract": "intended  for  use"}
{"Type": "conference", "Year": "1988", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "ALVINN", "Title": "An Autonomous Land Vehicle in a Neural Network", "Abstract": "ALVINN (Autonomous Land Vehicle In a Neural Network) is a 3-layer  back-propagation network designed for the task of road following. Cur(cid:173) rently ALVINN takes images from a camera and a laser range finder as input  and produces as output the direction the vehicle should travel in order to  follow the road. Training has been conducted using simulated road images.  Successful tests on the Carnegie Mellon autonomous navigation test vehicle  indicate that the network can effectively follow real roads under certain field  conditions. The representation developed to perfOIm the task differs dra(cid:173) matically when the networlc is trained under various conditions, suggesting  the possibility of a novel adaptive autonomous navigation system capable of  tailoring its processing to the conditions at hand."}
{"Type": "conference", "Year": "1988", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "GEMINI", "Title": "Gradient Estimation Through Matrix Inversion After Noise Injection", "Abstract": "Learning procedures that measure how random perturbations of unit ac(cid:173) tivities correlate with changes in reinforcement are inefficient but simple  to implement in hardware. Procedures like back-propagation (Rumelhart,  Hinton and Williams, 1986) which compute how changes in activities af(cid:173) fect the output error are much more efficient, but require more complex  hardware. GEMINI is a hybrid procedure for multilayer networks, which  shares many of the implementation advantages of correlational reinforce(cid:173) ment procedures but is more efficient. GEMINI injects noise only at the  first hidden layer and measures the resultant effect on the output error.  A linear network associated with each hidden layer iteratively inverts the  matrix which relates the noise to the error change, thereby obtaining  the error-derivatives. No back-propagation is involved, thus allowing un(cid:173) known non-linearities in the system. Two simulations demonstrate the  effectiveness of GEMINI."}
{"Type": "conference", "Year": "1988", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Speech Recognition", "Title": "Statistical and Neural Information Processing Approaches", "Abstract": "Abstract Unavailable"}
{"Type": "conference", "Year": "1988", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Models of Ocular Dominance Column Formation", "Title": "Analytical and Computational Results", "Abstract": "cal model  for  formation  of ocular  dominance  columns  in  mammalian  visual  cortex.  The  model  provides  a  com(cid:173) mon framework  in  which a  variety  of activity-dependent  biological machanisms can be studied.  Analytic and com(cid:173) putational  results  together  now  reveal  the  following:  if  inputs  specific  to  each eye  are  locally  correlated  in  their  firing,  and are not  anticorrelated within an arbor radius,  monocular  cells  will  robustly  form  and  be  organized  by  intra-cortical  interactions  into  columns.  Broader  corre(cid:173) lations  withln  each  eye,  or anti-correlations  between the  eyes, create a  more purely monocular cortex; positive cor(cid:173) relation  over  an  arbor  radius  yields  an  almost  perfectly  monocular cortex.  Most features of the model can be un(cid:173) derstood  analytically  through  decomposition  into  eigen(cid:173) functions and linear stability analysis.  This allows predic(cid:173) tion of the widths of the columns and other features from  measurable biological parameters."}
{"Type": "conference", "Year": "1988", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Further Explorations in Visually-Guided Reaching", "Title": "Making MURPHY Smarter", "Abstract": "MURPHY  is  a  vision-based  kinematic  controller  and  path  planner  based  on  a  connectionist  architecture,  and  implemented  with  a  video  camera and  Rhino XR-series robot  arm.  Imitative of the layout of sen(cid:173) sory  and motor maps in  cerebral cortex,  MURPHY'S internal representa(cid:173) tions  consist of four  coarse-coded populations of simple units represent(cid:173) ing both static and  dynamic aspects of the sensory-motor environment.  In previously reported work [4],  MURPHY first  learned a direct kinematic  model of his  camera-arm system during  a  period  of extended  practice,  and  then  used  this  \"mental  model\"  to  heuristically  guide  his  hand  to  unobstructed  visual  targets.  MURPHY  has  since  been  extended  in  two  ways:  First, he  now  learns the inverse differential-kinematics of his  arm  in  addition to ordinary direct  kinematics, which  allows  him to push  his  hand  directly towards  a  visual  target  without  the need  for  search.  Sec(cid:173) ondly,  he now  deals with the much more difficult problem of reaching in  the presence of obstacles."}
{"Type": "conference", "Year": "1988", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Scaling and Generalization in Neural Networks", "Title": "A Case Study", "Abstract": "The  issues  of scaling  and  generalization  have  emerged  as  key  issues  in  current studies of supervised learning from examples in neural networks.  Questions such  as  how  many  training  patterns  and  training  cycles  are  needed for  a problem of a given size  and difficulty,  how  to represent the  inllUh  and how  to choose useful training exemplars,  are of considerable  theoretical  and  practical  importance.  Several  intuitive  rules  of thumb  have been obtained from empirical studies, but as yet there are few  rig(cid:173) orous  results.  In  this  paper we  summarize  a  study Qf generalization in  the simplest possible case-perceptron networks learning linearly separa(cid:173) ble  functions.  The  task  chosen  was  the majority function  (i.e.  return  a  1  if a  majority  of the  input  units  are  on),  a  predicate  with  a  num(cid:173) ber  of useful  properties.  We  find  that  many  aspects  of.generalization  in  multilayer  networks  learning  large,  difficult  tasks  are  reproduced  in  this simple domain, in which  concrete numerical results and even some  analytic understanding can be achieved."}
