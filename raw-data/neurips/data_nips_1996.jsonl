{"Type": "conference", "Year": "1996", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "ARTEX", "Title": "A Self-organizing Architecture for Classifying Image Regions", "Abstract": "A self-organizing architecture is  developed for  image region  classi(cid:173) fication.  The system  consists  of a  preprocessor  that utilizes multi(cid:173) scale filtering, competition, cooperation, and diffusion to compute a  vector  of image boundary  and surface  properties,  notably  texture  and  brightness  properties.  This  vector  inputs  to  a  system  that  incrementally  learns  noisy  multidimensional  mappings  and  their  probabilities.  The  architecture  is  applied  to  difficult  real-world  image  classification  problems,  including  classification  of synthet(cid:173) ic  aperture  radar  and  natural  texture  images,  and  outperforms a  recent state-of-the-art system at classifying natural textures."}
{"Type": "conference", "Year": "1996", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "3D Object Recognition", "Title": "A Model of View-Tuned Neurons", "Abstract": "In  1990  Poggio and  Edelman proposed  a  view-based  model of ob(cid:173) ject recognition that accounts for several psychophysical properties  of certain  recognition  tasks.  The model predicted  the existence  of  view-tuned  and view-invariant units,  that were  later found  by  Lo(cid:173) gothetis  et  al.  (Logothetis  et  al.,  1995)  in  IT  cortex  of monkeys  trained  with views  of specific  paperclip  objects.  The model,  how(cid:173) ever,  does  not specify  the inputs to the view-tuned  units and their  internal  organization.  In  this  paper  we  propose  a  model of these  view-tuned  units  that  is  consistent  with  physiological  data from  single cell  responses."}
{"Type": "conference", "Year": "1996", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Learning Appearance Based Models", "Title": "Mixtures of Second Moment Experts", "Abstract": "This paper describes a new technique for object recognition based on learning  appearance  models.  The  image is  decomposed  into  local  regions  which  are  described  by  a  new  texture  representation  called  \"Generalized  Second  Mo(cid:173) ments\"  that are  derived  from the  output of multiscale,  multiorientation filter  banks.  Class-characteristic local texture features and their global composition  is learned by a hierarchical mixture of experts architecture (Jordan &  Jacobs).  The  technique  is  applied  to  a  vehicle  database  consisting  of 5  general  car  categories  (Sedan,  Van  with back-doors, Van  without back-doors, old Sedan,  and Volkswagen Bug).  This  is  a difficult problem with considerable in-class  variation.  The new  technique has  a 6.5% misclassification rate, compared to  eigen-images which give  17.4% misclassification rate,  and nearest neighbors  which give  15 .7% misclassification rate."}
{"Type": "conference", "Year": "1996", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "NeuroScale", "Title": "Novel Topographic Feature Extraction using RBF Networks", "Abstract": "E  = 2: 2:(d~p - dqp)2,"}
{"Type": "conference", "Year": "1996", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "MIMIC", "Title": "Finding Optima by Estimating Probability Densities", "Abstract": "In  many optimization problems,  the structure of solutions reflects  complex relationships between  the different  input parameters.  For  example, experience may tell us  that certain parameters are closely  related  and  should  not  be  explored  independently.  Similarly, ex(cid:173) perience  may establish  that  a  subset  of parameters must  take  on  particular  values.  Any  search  of the  cost  landscape  should  take  advantage of these relationships.  We  present  MIMIC,  a framework  in which  we  analyze the global structure of the optimization land(cid:173) scape.  A  novel  and  efficient  algorithm for  the  estimation of this  structure is  derived.  We  use  knowledge of this structure to guide a  randomized search  through the solution space  and,  in  turn,  to re(cid:173) fine our estimate ofthe structure.  Our technique obtains significant  speed  gains over  other randomized optimization procedures."}
{"Type": "conference", "Year": "1996", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "GTM", "Title": "A Principled Alternative to the Self-Organizing Map", "Abstract": "The  Self-Organizing Map  (SOM)  algorithm  has  been  extensively  studied and has  been  applied with considerable success to a  wide  variety of problems.  However, the algorithm is derived from heuris(cid:173) tic  ideas  and  this  leads  to a  number of significant  limitations.  In  this  paper,  we  consider  the  problem  of  modelling  the  probabil(cid:173) ity  density  of  data  in  a  space  of several  dimensions  in  terms  of  a  smaller number of latent,  or hidden,  variables.  We  introduce  a  novel form  of latent variable model,  which  we  call the GTM algo(cid:173) rithm (for  Generative  Topographic  Mapping),  which allows general  non-linear  transformations  from  latent  space  to  data  space,  and  which  is  trained  using  the  EM  (expectation-maximization)  algo(cid:173) rithm.  Our approach overcomes the limitations of the SOM, while  introducing no significant disadvantages.  We demonstrate the per(cid:173) formance of the GTM algorithm on simulated data from flow  diag(cid:173) nostics for  a  multi-phase oil pipeline."}
{"Type": "conference", "Year": "1996", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Bangs, Clicks, Snaps, Thuds and Whacks", "Title": "An Architecture for Acoustic Transient Processing", "Abstract": "We propose  a  neuromorphic  architecture  for  real-time  processing  of \nacoustic transients in analog VLSI.  We show how judicious normalization \nof a time-frequency signal allows an elegant and robust implementation \nof a correlation algorithm. The algorithm uses binary multiplexing instead \nof analog-analog  multiplication.  This  removes  the  need  for  analog \nstorage and analog-multiplication.  Simulations  show that the resulting \nalgorithm has the same out-of-sample classification performance (-93% \ncorrect) as a baseline template-matching algorithm."}
{"Type": "conference", "Year": "1996", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Promoting Poor Features to Supervisors", "Title": "Some Inputs Work Better as Outputs", "Abstract": "In supervised  learning there is  usually  a  clear  distinction  between  inputs  and  outputs - inputs are  what  you  will  measure,  outputs  are  what  you  will  predict  from  those  measurements.  This  paper  shows  that the distinction between  inputs  and  outputs is  not this  simple.  Some  features  are  more  useful  as  extra  outputs  than  as  inputs.  By using a feature  as  an output we  get  more than just the  case  values but can. learn a  mapping from  the other inputs to that  feature.  For many features  this mapping may be more useful  than  the  feature  value  itself.  We  present  two  regression  problems  and  one  classification problem  where  performance improves  if features  that  could  have  been  used  as  inputs  are  used  as  extra  outputs  instead.  This result is  surprising since  a feature used  as  an output  is  not  used  during testing."}
{"Type": "conference", "Year": "1996", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "An Architectural Mechanism for Direction-tuned Cortical Simple Cells", "Title": "The Role of Mutual Inhibition", "Abstract": "A  linear  architectural  model  of cortical  simple cells  is  presented.  The  model  evidences  how  mutual  inhibition,  occurring  through  synaptic  coupling  functions  asymmetrically  distributed  in  space,  can be a possible basis for  a wide variety of spatio-temporal simple  cell  response properties, including direction selectivity and velocity  tuning.  While  spatial  asymmetries  are  included  explicitly  in  the  structure of the inhibitory interconnections, temporal asymmetries  originate  from  the  specific  mutual  inhibition  scheme  considered.  Extensive simulations supporting the model are reported."}
{"Type": "conference", "Year": "1996", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Complex-Cell Responses Derived from Center-Surround Inputs", "Title": "The Surprising Power of Intradendritic Computation", "Abstract": "B.  W.  Mel,  D.  L.  Ruderman and K.  A. Archie"}
{"Type": "conference", "Year": "1996", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Dynamic Features for Visual Speechreading", "Title": "A Systematic Comparison", "Abstract": "Humans  use  visual as  well  as  auditory speech signals to recognize  spoken words.  A variety of systems have been investigated for per(cid:173) forming  this  task.  The main  purpose of this  research  was  to sys(cid:173) tematically compare the performance of a range of dynamic visual  features  on  a  speechreading  task.  We  have  found  that  normal(cid:173) ization  of images  to eliminate  variation  due  to  translation,  scale,  and planar rotation  yielded  substantial  improvements  in  general(cid:173) ization performance regardless of the visual representation used.  In  addition,  the dynamic  information  in  the difference  between  suc(cid:173) cessive  frames  yielded  better performance than optical-flow  based  approaches, and compression by local low-pass filtering worked sur(cid:173) prisingly better than global principal components analysis  (PCA).  These results are examined and possible explanations are explored."}
{"Type": "conference", "Year": "1996", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Selective Integration", "Title": "A Model for Disparity Estimation", "Abstract": "Selective Integration: A Model for Disparity Estimation"}
{"Type": "conference", "Year": "1996", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "The Neurothermostat", "Title": "Predictive Optimal Control of Residential Heating Systems", "Abstract": "1  TEMPERATURE REGULATION AS  AN OPTIMAL"}
{"Type": "conference", "Year": "1996", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Regression with Input-Dependent Noise", "Title": "A Bayesian Treatment", "Abstract": "In  most  treatments  of the  regression  problem  it  is  assumed  that  the distribution of target data can be described by a  deterministic  function  of the inputs, together with additive Gaussian noise hav(cid:173) ing constant variance.  The use of maximum likelihood to train such  models then  corresponds to the minimization of a  sum-of-squares  error function.  In  many applications a more realistic model would  allow  the  noise  variance  itself to  depend  on  the  input  variables.  However, the use of maximum likelihood to train such models would  give highly biased  results.  In  this paper we  show  how  a  Bayesian  treatment  can  allow  for  an  input-dependent  variance  while  over(cid:173) coming the bias of maximum likelihood."}
{"Type": "conference", "Year": "1996", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Multilayer Neural Networks", "Title": "One or Two Hidden Layers?", "Abstract": "We study the number of hidden layers required by a multilayer neu(cid:173) ral network with threshold  units to compute a function  f  from n d  to {O, I}.  In dimension d  =  2,  Gibson  characterized  the functions  computable with just one hidden layer, under  the assumption that  there  is no  \"multiple intersection  point\"  and that f  is only defined  on a compact set.  We consider the restriction of f  to the neighbor(cid:173) hood of a  multiple intersection  point or of infinity,  and give  neces(cid:173) sary  and sufficient  conditions for  it  to  be  locally computable with  one  hidden  layer.  We  show  that  adding  these  conditions  to  Gib(cid:173) son's  assumptions  is  not  sufficient  to  ensure  global  computability  with one hidden layer,  by exhibiting a new  non-local configuration,  the  \"critical cycle\",  which  implies  that f  is  not  computable  with  one  hidden  layer."}
{"Type": "conference", "Year": "1996", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Online Learning from Finite Training Sets", "Title": "An Analytical Case Study", "Abstract": "We  analyse  online  learning  from  finite  training  sets  at  non(cid:173) infinitesimal  learning  rates  TJ.  By  an  extension  of statistical  me(cid:173) chanics  methods,  we  obtain  exact  results  for  the  time-dependent  generalization  error  of  a  linear  network  with  a  large  number  of  weights  N.  We  find,  for  example,  that  for  small training  sets  of  size  p  ~ N,  larger  learning rates  can  be  used  without compromis(cid:173) ing  asymptotic  generalization  performance  or  convergence  speed.  Encouragingly,  for  optimal  settings  of  TJ  (and,  less  importantly,  weight decay ,)  at given final  learning time, the generalization per(cid:173) formance of online learning is  essentially  as good as  that of offline  learning."}
{"Type": "conference", "Year": "1996", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "ARC-LH", "Title": "A New Adaptive Resampling Algorithm for Improving ANN Classifiers", "Abstract": "We introduce arc-Ih,  a new algorithm for improvement of ANN clas(cid:173) sifier  performance,  which  measures  the  importance of patterns  by  aggregated network output errors.  On several  artificial benchmark  problems,  this  algorithm compares favorably  with other  resample  and combine techniques."}
{"Type": "conference", "Year": "1996", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Estimating Equivalent Kernels for Neural Networks", "Title": "A Data Perturbation Approach", "Abstract": "We  describe  the  notion  of  \"equivalent  kernels\"  and  suggest  that  this  provides a framework  for comparing different classes of regression models,  including  neural  networks  and  both  parametric  and  non-parametric  statistical techniques.  Unfortunately,  standard techniques break down  when  faced with models, such as neural networks,  in which there is more than one  \"layer\" of adjustable parameters.  We propose an algorithm which overcomes  this limitation,  estimating the equivalent kernels for  neural network models  using  a  data  perturbation approach.  Experimental  results  indicate  that  the  networks do  not  use  the  maximum possible  number of degrees of freedom,  that  these  can  be  controlled  using  regularisation  techniques  and  that  the  equivalent kernels learnt by the network vary both  in \"size\"  and in \"shape\"  in different regions of the input space."}
{"Type": "conference", "Year": "1996", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Size of Multilayer Networks for Exact Learning", "Title": "Analytic Approach", "Abstract": "This  article  presents  a  new  result  about  the  size  of a  multilayer  neural network computing real outputs for exact learning of a finite  set of real samples.  The architecture of the network is feedforward,  with  one  hidden  layer  and several  outputs.  Starting from  a  fixed  training set,  we  consider  the  network  as  a  function  of its  weights.  We  derive,  for  a  wide family  of transfer  functions,  a  lower  and  an  upper  bound  on  the  number  of hidden  units  for  exact  learning,  given  the size  of the  dataset  and  the  dimensions of the  input and  output spaces."}
{"Type": "conference", "Year": "1996", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Maximum Likelihood Blind Source Separation", "Title": "A Context-Sensitive Generalization of ICA", "Abstract": "Xi(t) = 2: 2: aji(r)8j(t - r) = 2: aji * 8j"}
