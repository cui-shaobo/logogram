{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Emergence of Multiplication in a Biophysical Model of a Wide-Field Visual Neuron for Computing Object Approaches", "Title": "Dynamics, Peaks, & Fits", "Abstract": "Many species show avoidance reactions in response to looming object approaches.  In locusts, the corresponding escape behavior correlates with the activity  of the lobula giant movement detector (LGMD) neuron.  During an object approach,  its firing rate was reported to gradually increase until a peak is reached,  and then it declines quickly.  The $\\eta$-function predicts that the LGMD activity  is a product between an exponential function of angular size $\\exp(-\\Theta)$ and  angular velocity $\\dot{\\Theta}$, and that peak activity is reached before time-to-contact  (ttc).  The $\\eta$-function has become the prevailing LGMD model because it  reproduces many experimental observations, and even experimental evidence for  the multiplicative operation was reported.  Several inconsistencies remain  unresolved, though.  Here we address these issues with a new model ($\\psi$-model),  which explicitly connects $\\Theta$ and $\\dot{\\Theta}$ to biophysical quantities.  The $\\psi$-model avoids biophysical problems associated with implementing  $\\exp(\\cdot)$, implements the multiplicative operation of $\\eta$ via divisive  inhibition, and explains why activity peaks could occur after ttc.  It consistently  predicts response features of the LGMD, and provides excellent fits to published  experimental data, with goodness of fit measures comparable to corresponding  fits with the $\\eta$-function."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "SpaRCS", "Title": "Recovering low-rank and sparse matrices from compressive measurements", "Abstract": "We consider the problem of recovering a matrix $\\mathbf{M}$ that is the sum of a low-rank matrix $\\mathbf{L}$ and a sparse matrix $\\mathbf{S}$ from a small set of linear measurements of the form $\\mathbf{y} = \\mathcal{A}(\\mathbf{M}) = \\mathcal{A}({\\bf L}+{\\bf S})$.  This model subsumes three important classes of signal recovery problems:  compressive sensing, affine rank minimization, and robust principal component analysis.  We propose a natural optimization problem for signal recovery under this model and develop a new greedy algorithm called SpaRCS to solve it.  SpaRCS inherits a number of desirable properties from the state-of-the-art CoSaMP and ADMiRA algorithms, including exponential convergence and efficient implementation.  Simulation results with video compressive sensing, hyperspectral imaging, and robust matrix completion data sets demonstrate both the accuracy and efficacy of the algorithm."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Multilinear Subspace Regression", "Title": "An Orthogonal Tensor Decomposition Approach", "Abstract": "A multilinear subspace regression model based on so called latent variable decomposition is introduced. Unlike standard regression methods which typically employ matrix (2D) data representations followed by vector subspace transformations, the proposed approach uses tensor subspace transformations to model common latent variables across both the independent and dependent data. The proposed approach aims to maximize the correlation between the so derived latent variables and is shown to be suitable for the prediction of multidimensional dependent data from multidimensional independent data, where for the estimation of the latent variables we introduce an algorithm based on Multilinear Singular Value Decomposition (MSVD) on a specially defined cross-covariance tensor. It is next shown that in this way we are also able to unify the existing Partial Least Squares (PLS) and N-way PLS regression algorithms within the same framework. Simulations on benchmark synthetic data confirm the advantages of the proposed approach, in terms of its predictive ability and robustness, especially for small sample sizes. The potential of the proposed technique is further illustrated on a real world task of the decoding of human intracranial electrocorticogram (ECoG) from a simultaneously recorded scalp electroencephalograph (EEG)."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "PiCoDes", "Title": "Learning a Compact Code for Novel-Category Recognition", "Abstract": "We introduce PiCoDes: a very compact image descriptor which nevertheless allows high performance on object category recognition. In particular, we address novel-category recognition: the task of defining indexing structures and image representations which enable a large collection of images to be searched for an object category that was not known when the index was built. Instead, the training images defining the category are supplied at query time. We explicitly learn descriptors of a given length (from as small as 16 bytes per image) which have good object-recognition performance. In contrast to previous work in the domain of object recognition, we do not choose an arbitrary intermediate representation, but explicitly learn short codes. In contrast to previous approaches to learn compact codes, we optimize explicitly for (an upper bound on) classification performance. Optimization directly for binary features is difficult and nonconvex, but we present an alternation scheme and convex upper bound which demonstrate excellent performance in practice. PiCoDes of 256 bytes match the accuracy of the current best known classifier for the Caltech256 benchmark, but they decrease the database storage size by a factor of 100 and speed-up the training and testing of novel classes by orders of magnitude."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Hogwild!", "Title": "A Lock-Free Approach to Parallelizing Stochastic Gradient Descent", "Abstract": "Stochastic Gradient Descent (SGD) is a popular algorithm that can achieve state-of-the-art performance on a variety of machine  learning tasks.  Several researchers have recently proposed schemes  to parallelize SGD, but all require  performance-destroying memory locking and synchronization. This work aims to show using novel theoretical analysis, algorithms,  and implementation that SGD can be implemented without any locking. We present an update scheme called Hogwild which allows processors access to shared memory with the possibility of overwriting each other's work. We show that when the associated optimization problem is sparse, meaning most gradient updates only modify small parts of the decision variable, then Hogwild achieves a nearly optimal rate of convergence.  We demonstrate experimentally that Hogwild outperforms alternative schemes that use locking by an order of magnitude."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Two is better than one", "Title": "distinct roles for familiarity and recollection in retrieving palimpsest memories", "Abstract": "Storing a new pattern in a palimpsest memory system comes at the cost of interfering with the memory traces of previously stored items. Knowing the age of a pattern thus becomes critical for recalling it faithfully. This implies that there should be a tight coupling between estimates of age, as a form of familiarity, and the neural dynamics of recollection, something which current theories omit. Using a normative model of autoassociative memory, we show that a dual memory system, consisting of two interacting modules for familiarity and recollection, has best performance for both recollection and recognition. This finding provides a new window onto actively contentious psychological and neural aspects of recognition memory."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Multiclass Boosting", "Title": "Theory and Algorithms", "Abstract": "The problem of  multiclass boosting is considered. A new framework,based on multi-dimensional codewords and predictors is introduced. The optimal set of codewords is derived, and a margin enforcing loss proposed. The resulting risk is minimized by gradient descent on a multidimensional functional space. Two algorithms are proposed: 1) CD-MCBoost, based on coordinate descent, updates one predictor component at a time, 2) GD-MCBoost, based on gradient descent, updates all components jointly. The algorithms differ in the weak learners that they support but are both shown to be 1) Bayes consistent, 2) margin enforcing, and 3) convergent to the global minimum of the risk. They also reduce to AdaBoost when there are only two classes. Experiments show that both methods outperform previous multiclass boosting approaches on a number of datasets."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Trace Lasso", "Title": "a trace norm regularization for correlated designs", "Abstract": "Using the $\\ell_1$-norm to regularize the estimation of  the parameter vector of a linear model leads to an unstable estimator when covariates are highly correlated. In this paper, we introduce a new penalty function which takes into account the correlation of the design matrix to stabilize the estimation. This norm, called the trace Lasso, uses the trace norm of the selected covariates, which is a convex surrogate of their rank, as the criterion of model complexity. We analyze the properties of our norm, describe an optimization algorithm based on reweighted least-squares, and illustrate the behavior of this norm on synthetic data, showing that it is more adapted to strong correlations than competing methods such as the elastic net."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "RTRMC", "Title": "A Riemannian trust-region method for low-rank matrix completion", "Abstract": "We consider large matrices of low rank. We address the problem of recovering such matrices when most of the entries are unknown. Matrix completion finds applications in recommender systems. In this setting, the rows of the matrix may correspond to items and the columns may correspond to users. The known entries are the ratings given by users to some items. The aim is to predict the unobserved ratings. This problem is commonly stated in a constrained optimization framework. We follow an approach that exploits the geometry of the low-rank constraint to recast the problem as an unconstrained optimization problem on the Grassmann manifold. We then apply first- and second-order Riemannian trust-region methods to solve it. The cost of each iteration is linear in the number of known entries. Our methods, RTRMC 1 and 2, outperform state-of-the-art algorithms on a wide range of problem instances."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "High-Dimensional Graphical Model Selection", "Title": "Tractable Graph Families and Necessary Conditions", "Abstract": "We consider the problem of Ising and Gaussian graphical model selection given n i.i.d. samples from the model. We propose an efficient threshold-based algorithm   for structure estimation based known as  conditional mutual information test. This simple local algorithm    requires only low-order statistics of the data and decides    whether  two nodes   are neighbors in the unknown graph. Under some transparent assumptions, we establish that the proposed algorithm is structurally consistent (or sparsistent)  when the number of samples scales as n= Omega(J{min}^{-4} log p), where p is the number of nodes and J{min} is the minimum edge potential.  We also prove novel non-asymptotic necessary conditions for graphical model selection."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "ShareBoost", "Title": "Efficient multiclass learning with feature sharing", "Abstract": "Multiclass prediction is the problem of classifying an object into a    relevant target class.  We consider the problem of learning a    multiclass predictor that uses only few features, and in particular,    the number of used features should increase sub-linearly with the    number of possible classes. This implies that features should be    shared by several classes. We describe and analyze the ShareBoost    algorithm for learning a multiclass predictor that uses few shared    features. We prove that ShareBoost efficiently finds a predictor    that uses few shared features (if such a predictor exists) and that    it has a small generalization error. We also describe how to use    ShareBoost for learning a non-linear predictor that has a fast    evaluation time. In a series of experiments with natural data sets    we demonstrate the benefits of ShareBoost and evaluate its success    relatively to other state-of-the-art approaches."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Fast and Balanced", "Title": "Efficient Label Tree Learning for Large Scale Object Recognition", "Abstract": "We present a novel approach to efficiently learn a label tree for large scale classification with many classes. The key contribution of the approach is a technique to simultaneously determine the structure of the tree and learn the classifiers for each node in the tree. This approach also allows fine grained control over the efficiency vs accuracy trade-off in designing a label tree, leading to more balanced trees. Experiments are performed on large scale image classification with 10184 classes and 9 million images. We demonstrate significant improvements in test accuracy and efficiency with less training time and more balanced trees compared to the previous state of the art by Bengio et al."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Im2Text", "Title": "Describing Images Using 1 Million Captioned Photographs", "Abstract": "We develop and demonstrate automatic image description methods using a large captioned photo collection.  One contribution is our technique for the automatic collection of this new dataset -- performing a huge number of Flickr queries and then filtering the noisy results down to 1 million images with associated visually relevant captions.  Such a collection allows us to approach the extremely challenging problem of description generation using relatively simple non-parametric methods and produces surprisingly effective results. We also develop methods incorporating many state of the art, but fairly noisy, estimates of image content to produce even more pleasing results. Finally we introduce a new objective performance measure for image captioning."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Beating SGD", "Title": "Learning SVMs in Sublinear Time", "Abstract": "We present an optimization approach for linear SVMs based on a stochastic primal-dual approach, where the primal step is akin to an importance-weighted SGD, and the dual step is a stochastic update on the importance weights.  This yields an optimization method with a sublinear dependence on the training set size, and the first method for learning linear SVMs with runtime less then the size of the training set required for learning!"}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Online Learning", "Title": "Stochastic, Constrained, and Smoothed Adversaries", "Abstract": "Learning theory has largely focused on two main learning scenarios: the classical statistical setting where instances are drawn i.i.d. from a fixed distribution, and the adversarial scenario whereby at every time step the worst instance is revealed to the player. It can be argued that in the real world neither of these assumptions is reasonable. We define the minimax value of a game where the adversary is restricted in his moves, capturing stochastic and non-stochastic assumptions on data. Building on the sequential symmetrization approach, we define a notion of distribution-dependent Rademacher complexity for the spectrum of problems ranging from i.i.d. to worst-case. The bounds let us immediately deduce variation-type bounds. We study a smoothed online learning scenario and show that exponentially small amount of noise can make function classes with infinite Littlestone dimension learnable."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Learning in Hilbert vs. Banach Spaces", "Title": "A Measure Embedding Viewpoint", "Abstract": "The goal of this paper is to investigate the advantages and disadvantages of learning in Banach spaces over Hilbert spaces. While many works have been carried out in generalizing Hilbert methods to Banach spaces, in this paper, we consider the simple problem of learning a Parzen window classifier in a reproducing kernel Banach space (RKBS)---which is closely related to the notion of embedding probability measures into an RKBS---in order to carefully understand its pros and cons over the Hilbert space classifier. We show that while this generalization yields richer distance measures on probabilities compared to its Hilbert space counterpart, it however suffers from serious computational drawback limiting its practical applicability, which therefore demonstrates the need for developing efficient learning algorithms in Banach spaces."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "See the Tree Through the Lines", "Title": "The Shazoo Algorithm", "Abstract": "Predicting the nodes of a given graph is a fascinating   theoretical problem with applications in several domains.   Since graph sparsification via spanning trees   retains enough information while making the task much easier,   trees are an important special case of this problem.   Although it is known how to predict the nodes of an unweighted tree   in a nearly optimal way, in the weighted case a fully satisfactory   algorithm is not available yet. We fill this hole and introduce an efficient node predictor,   Shazoo, which is nearly optimal on any weighted tree. Moreover, we show that Shazoo can   be viewed as a common nontrivial generalization of both previous approaches for   unweighted trees and weighted lines.   Experiments on real-world datasets confirm that Shazoo performs well in that   it fully exploits the structure of the input tree,   and gets very close to (and sometimes better than)   less scalable energy minimization methods."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Directed Graph Embedding", "Title": "an Algorithm based on Continuous Limits of Laplacian-type Operators", "Abstract": "This paper considers the problem of embedding directed graphs in Euclidean space while retaining directional information. We model the observed graph as a sample from a manifold endowed with a vector field, and we design an algo- rithm that separates and recovers the features of this process: the geometry of the manifold, the data density and the vector field. The algorithm is motivated by our analysis of Laplacian-type operators and their continuous limit as generators of diffusions on a manifold. We illustrate the recovery algorithm on both artificially constructed and real data."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "High-dimensional regression with noisy and missing data", "Title": "Provable guarantees with non-convexity", "Abstract": "Although the standard formulations of prediction problems involve fully-observed and noiseless data drawn in an i.i.d. manner, many  applications involve noisy and/or missing data, possibly involving dependencies. We study these issues in the context of high-dimensional  sparse linear regression, and propose novel estimators for the cases of noisy, missing, and/or dependent data. Many standard approaches to noisy or missing data, such as those using the EM algorithm, lead to optimization problems that are inherently non-convex, and it is difficult to establish theoretical guarantees on practical algorithms. While our approach also involves optimizing non-convex programs, we are able to both analyze the statistical error associated with any global optimum, and prove that a simple projected gradient descent algorithm will converge in polynomial time to a small neighborhood of the set of global minimizers. On the statistical side, we provide non-asymptotic bounds that hold with high probability for the cases of noisy, missing, and/or dependent data. On the computational side, we prove that under the same types of conditions required for statistical consistency, the projected gradient descent algorithm will converge at geometric rates to a near-global minimizer. We illustrate these theoretical predictions with simulations, showing agreement with the predicted scalings."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Active dendrites", "Title": "adaptation to spike-based communication", "Abstract": "Computational analyses of dendritic computations often assume stationary inputs to neurons, ignoring the pulsatile nature of spike-based communication between neurons and the moment-to-moment fluctuations caused by such spiking inputs. Conversely, circuit computations with spiking neurons are usually formalized without regard to the rich nonlinear nature of dendritic processing. Here we address the computational challenge faced by neurons that compute and represent analogue quantities but communicate with digital spikes, and show that reliable computation of even purely linear functions of inputs can require the interplay of strongly nonlinear subunits within the postsynaptic dendritic tree. Our theory predicts a matching of dendritic nonlinearities and synaptic weight distributions to the joint statistics of presynaptic inputs. This approach suggests normative roles for some puzzling forms of nonlinear dendritic dynamics and plasticity."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "EigenNet", "Title": "A Bayesian hybrid of generative and conditional models for sparse learning", "Abstract": "For many real-world applications, we often need to select correlated variables---such as genetic variations and imaging features associated with Alzheimer's disease---in a high dimensional space. The correlation between variables presents a challenge to classical variable selection methods. To address this challenge, the elastic net has been developed and successfully applied to many applications. Despite its great success, the elastic net does not exploit the correlation information embedded in the data to select correlated variables. To overcome this limitation, we present a novel hybrid model, EigenNet, that uses the eigenstructures of data to guide variable selection. Specifically, it integrates a sparse conditional classification model with a generative model capturing variable correlations in a principled Bayesian framework. We develop an efficient active-set algorithm to estimate the model via evidence maximization. Experiments on synthetic data and imaging genetics data demonstrated the superior predictive performance of the EigenNet over the lasso, the elastic net, and the automatic relevance determination."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Manifold Precis", "Title": "An Annealing Technique for Diverse Sampling of Manifolds", "Abstract": "In this paper, we consider the 'Precis' problem of sampling K representative yet diverse data points from a large dataset. This problem arises frequently in applications such as video and document summarization, exploratory data analysis, and pre-filtering. We formulate a general theory which encompasses not just traditional techniques devised for vector spaces, but also non-Euclidean manifolds, thereby enabling these techniques to shapes, human activities, textures and many other image and video based datasets. We propose intrinsic manifold measures for measuring the quality of a selection of points with respect to their representative power, and their diversity. We then propose efficient algorithms to optimize the cost function using a novel annealing-based iterative alternation algorithm. The proposed formulation is applicable to manifolds of known geometry as well as to manifolds whose geometry needs to be estimated from samples. Experimental results show the strength and generality of the proposed approach."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "$\\theta$-MRF", "Title": "Capturing Spatial and Semantic Structure in the Parameters for Scene Understanding", "Abstract": "For most scene understanding tasks (such as object detection or depth estimation), the classifiers need to consider contextual information in addition to the local features. We can capture such contextual information by taking as input the features/attributes from all the regions in the image. However, this contextual dependence also varies with the spatial location of the region of interest, and we therefore need a different set of parameters for each spatial location. This results in a very large number of parameters. In this work, we model the independence properties between the parameters for each location and for each task, by defining a Markov Random Field (MRF) over the parameters. In particular, two sets of parameters are encouraged to have similar values if they are spatially close or semantically close. Our method is, in principle, complementary to other ways of capturing context such as the ones that use a graphical model over the labels instead. In extensive evaluation over two different settings, of multi-class object detection and of multiple scene understanding tasks (scene categorization, depth estimation, geometric labeling), our method beats the state-of-the-art methods in all the four tasks."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Maximum Covariance Unfolding ", "Title": "Manifold Learning for Bimodal Data", "Abstract": "We propose maximum covariance unfolding (MCU), a manifold learning algorithm for simultaneous dimensionality reduction of data from different input modalities.   Given high dimensional inputs from two different but naturally aligned sources, MCU computes a common low dimensional embedding that maximizes the cross-modal (inter-source) correlations while preserving the local (intra-source) distances.  In this paper, we explore two applications of MCU.  First we use MCU to analyze EEG-fMRI data, where an important goal is to visualize the fMRI voxels that are most strongly correlated with changes in EEG traces.  To perform this visualization, we augment MCU with an additional step for metric learning in the high dimensional voxel space.  Second, we use MCU to perform cross-modal retrieval of matched image and text samples from Wikipedia.  To manage large applications of MCU, we develop a fast implementation based on ideas from spectral graph theory.  These ideas transform the original problem for MCU, one of semidefinite programming, into a simpler problem in semidefinite quadratic linear programming."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "From Bandits to Experts", "Title": "On the Value of Side-Observations", "Abstract": "We consider an adversarial online learning setting where a decision maker can choose an action in every stage of the game. In addition to observing the reward of the chosen action, the decision maker gets side observations on the reward he would have obtained had he chosen some of the other actions. The observation structure is encoded as a graph, where node i is linked to  node j if sampling i provides information on the reward of j. This setting naturally interpolates between the well-known ``experts'' setting, where the decision maker can view all rewards, and the multi-armed bandits setting, where the decision maker can only view the reward of the chosen action. We develop practical algorithms with provable regret guarantees, which depend on non-trivial graph-theoretic properties of the information feedback structure. We also provide partially-matching lower bounds."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Hierarchical Matching Pursuit for Image Classification", "Title": "Architecture and Fast Algorithms", "Abstract": "Extracting good representations from images is essential for many computer vision tasks. In this paper, we propose hierarchical matching pursuit (HMP), which builds a feature hierarchy layer-by-layer using an efficient matching pursuit encoder. It includes three modules: batch (tree) orthogonal matching pursuit, spatial pyramid max pooling, and contrast normalization. We investigate the architecture of HMP, and show that all three components are critical for good performance. To speed up the orthogonal matching pursuit, we propose a batch tree orthogonal matching pursuit that is particularly suitable to encode a large number of observations that share the same large dictionary. HMP is scalable and can efficiently handle full-size images. In addition, HMP enables linear support vector machines (SVM) to match the performance of nonlinear SVM while being scalable to large datasets. We compare HMP with many state-of-the-art algorithms including convolutional deep belief networks, SIFT based single layer sparse coding, and kernel based feature learning. HMP consistently yields superior accuracy on three types of image classification problems: object recognition (Caltech-101), scene recognition (MIT-Scene), and static event recognition (UIUC-Sports)."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "How Do Humans Teach", "Title": "On Curriculum Learning and Teaching Dimension", "Abstract": "We study the empirical strategies that humans follow as they teach a target concept with a simple 1D threshold to a robot.  Previous studies of computational teaching, particularly the teaching dimension model and the curriculum learning principle, offer contradictory predictions on what optimal strategy the teacher should follow in this teaching task. We show through behavioral studies that humans employ three distinct teaching strategies, one of which is consistent with the curriculum learning principle, and propose a novel theoretical framework as a potential explanation for this strategy. This framework, which assumes a teaching goal of minimizing the learner's expected generalization error at each iteration, extends the standard teaching dimension model and offers a theoretical justification for curriculum learning."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "TD_gamma", "Title": "Re-evaluating Complex Backups in Temporal Difference Learning", "Abstract": "We show that the lambda-return target used in the TD(lambda) family of algorithms is the maximum likelihood estimator for a specific model of how the variance of an n-step return estimate increases with n. We introduce the gamma-return estimator, an alternative target based on a more accurate model of variance, which defines the TDgamma family of complex-backup temporal difference learning algorithms. We derive TDgamma, the gamma-return equivalent of the original TD(lambda) algorithm, which eliminates the lambda parameter but can only perform updates at the end of an episode and requires time and space proportional to the episode length. We then derive a second algorithm, TDgamma(C), with a capacity parameter C. TDgamma(C) requires C times more time and memory than TD(lambda) and is incremental and online. We show that TDgamma outperforms TD(lambda) for any setting of lambda on 4 out of 5 benchmark domains, and that TDgamma(C) performs as well as or better than TD_gamma for intermediate settings of C."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Newtron", "Title": "an Efficient Bandit algorithm for Online Multiclass Prediction", "Abstract": "We present an efficient algorithm for the problem of online multiclass prediction with bandit feedback in the fully adversarial setting. We measure its regret with respect to the log-loss defined in \\cite{AbernethyR09}, which is parameterized by a scalar (\\alpha). We prove that the regret of \\newtron is (O(\\log T)) when (\\alpha) is a constant that does not vary with horizon (T), and at most (O(T^{2/3})) if (\\alpha) is allowed to increase to infinity with (T). For (\\alpha) = (O(\\log T)), the regret is bounded by (O(\\sqrt{T})), thus solving the open problem of \\cite{KST08, AbernethyR09}. Our algorithm is based on a novel application of the online Newton method \\cite{HAK07}. We test our algorithm and show it to perform well in experiments, even when (\\alpha) is a small constant."}
