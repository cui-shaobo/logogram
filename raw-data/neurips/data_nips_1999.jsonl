{"Type": "conference", "Year": "1999", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "v-Arc", "Title": "Ensemble Learning in the Presence of Outliers", "Abstract": "AdaBoost and other ensemble methods have successfully  been ap(cid:173) plied  to a  number  of classification  tasks,  seemingly  defying  prob(cid:173) lems of overfitting.  AdaBoost performs gradient descent in an error  function  with  respect  to the margin,  asymptotically concentrating  on  the  patterns which  are  hardest to learn.  For  very  noisy  prob(cid:173) lems,  however,  this  can  be  disadvantageous.  Indeed,  theoretical  analysis has shown that the margin distribution,  as opposed to just  the minimal margin, plays a crucial role in understanding this phe(cid:173) nomenon.  Loosely  speaking,  some  outliers  should  be  tolerated  if  this  has  the  benefit  of substantially  increasing  the  margin  on  the  remaining points.  We  propose a  new  boosting algorithm which  al(cid:173) lows for  the possibility of a  pre-specified fraction of points to lie  in  the margin area Or even on the wrong side of the decision boundary."}
{"Type": "conference", "Year": "1999", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Image Recognition in Context", "Title": "Application to Microscopic Urinalysis", "Abstract": "i  =  1, ... N.  With  each  object  we  associate  a  Consider  a  set  of  N  objects  Ti ,  class  label  Ci  that  is  a  member  of  a  label  set  n  =  {1 , ... , D} .  Each  object  Ti  is  characterized  by  a  set  of  measurements  Xi  E  R P,  which  we  call  a  feature  vec(cid:173) tor.  Many  techniques  [1][2][4J[6}  incorporate  context  by  conditioning  the  posterior  probability  of objects'  identities  on  the joint features  of all  accompanying objects.  i.e .•  P(Cl, C2,··· , cNlxl , . . . , XN). and then maximizing it with respectto Cl, C2, . .. , CN . It can  be  shown  thatp(cl,c2, . . . ,cNlxl, . . . ,xN)  ex  p(cllxl) ... p(CNlxN)  (~ci\"\"'(N\\ given  certain reasonable assumptions."}
{"Type": "conference", "Year": "1999", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Better Generative Models for Sequential Data Problems", "Title": "Bidirectional Recurrent Mixture Density Networks", "Abstract": "This  paper  describes  bidirectional  recurrent  mixture  density  net(cid:173) works,  which  can  model  multi-modal  distributions  of  the  type  P(Xt Iyf)  and  P(Xt lXI, X2 , ... ,Xt-l, yf) without  any  explicit  as(cid:173) sumptions  about  the  use  of context .  These  expressions  occur  fre(cid:173) quently  in  pattern  recognition  problems  with  sequential  data,  for  example  in  speech  recognition.  Experiments  show  that  the  pro(cid:173) posed generative models give  a higher likelihood on test data com(cid:173) pared to a  traditional modeling approach, indicating that they can  summarize  the statistical  properties of the data better."}
{"Type": "conference", "Year": "1999", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Understanding Stepwise Generalization of Support Vector Machines", "Title": "a Toy Model", "Abstract": "In this  article  we  study the effects  of introducing structure in the  input distribution of the data to be learnt by a  simple perceptron.  We  determine the learning curves within the framework of Statis(cid:173) tical  Mechanics.  Stepwise  generalization  occurs  as  a  function  of  the number of examples when the distribution of patterns is highly  anisotropic.  Although  extremely simple,  the model  seems  to cap(cid:173) ture  the  relevant  features  of  a  class  of  Support  Vector  Machines  which was  recently shown to present this behavior."}
{"Type": "conference", "Year": "1999", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Recurrent Cortical Competition", "Title": "Strengthen or Weaken?", "Abstract": "We investigate the short term .dynamics of the recurrent competition and  neural  activity in the primary visual cortex in terms of information pro(cid:173) cessing and in the context of orientation selectivity.  We propose that af(cid:173) ter stimulus onset, the strength of the recurrent excitation decreases due  to  fast  synaptic depression.  As  a consequence,  the network shifts from  an  initially highly  nonlinear to a  more  linear operating  regime.  Sharp  orientation tuning is established in the first highly competitive phase.  In  the second and less competitive phase, precise signaling of multiple ori(cid:173) entations and long range modulation, e.g.,  by intra- and inter-areal con(cid:173) nections becomes possible (surround effects).  Thus the network first ex(cid:173) tracts  the  salient  features  from  the  stimulus,  and  then  starts  to  process  the details.  We  show  that  this  signal  processing  strategy  is  optimal  if  the neurons have limited bandwidth and their objective is to transmit the  maximum amount of information in any time interval beginning with the  stimulus onset."}
{"Type": "conference", "Year": "1999", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Learning Informative Statistics", "Title": "A Nonparametnic Approach", "Abstract": "We discuss an information theoretic approach for categorizing and mod(cid:173) eling dynamic processes. The approach can learn a compact and informa(cid:173) tive statistic which summarizes past states to predict future observations.  Furthermore, the  uncertainty of the prediction is characterized nonpara(cid:173) metrically by a joint density over the learned statistic and present obser(cid:173) vation.  We  discuss the  application of the technique to both noise driven  dynamical systems and random processes sampled from a density which  is conditioned on the past. In the first case we show results in which both  the  dynamics of random walk and the  statistics of the  driving noise  are  captured.  In the second case we  present results in  which a summarizing  statistic  is  learned  on  noisy  random telegraph  waves  with  differing de(cid:173) pendencies on past states.  In both cases the algorithm yields a principled  approach for discriminating processes with differing dynamics and/or de(cid:173) pendencies.  The method is  grounded in  ideas  from  information theory  and nonparametric statistics."}
{"Type": "conference", "Year": "1999", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "From Coexpression to Coregulation", "Title": "An Approach to Inferring Transcriptional Regulation among Gene Classes from Large-Scale Expression Data", "Abstract": "the  circuit  parameter  sets"}
{"Type": "conference", "Year": "1999", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Data Visualization and Feature Selection", "Title": "New Algorithms for Nongaussian Data", "Abstract": "Data  visualization  and  feature  selection  methods  are  proposed  based on the )oint mutual information and ICA.  The visualization  methods can find  many good 2-D  projections for  high dimensional  data interpretation,  which  cannot be easily found by  the other ex(cid:173) isting methods.  The new  variable selection  method is found  to be  better in eliminating redundancy in the inputs than other methods  based  on  simple mutual information.  The efficacy  of the  methods  is illustrated on a radar signal analysis problem to find  2-D viewing  coordinates for  data visualization and to select  inputs for  a  neural  network  classifier.  Keywords:  feature  selection,  joint mutual information,  ICA,  vi(cid:173) sualization, classification."}
{"Type": "conference", "Year": "1999", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Learning the Similarity of Documents", "Title": "An Information-Geometric Approach to Document Retrieval and Categorization", "Abstract": "The  project  pursued  in  this  paper  is  to  develop  from  first  information-geometric  principles  a  general  method  for  learning  the  similarity  between  text  documents.  Each  individual  docu(cid:173) ment  is  modeled  as  a  memoryless  information source.  Based  on  a  latent  class  decomposition of the  term-document  matrix, a  low(cid:173) dimensional  (curved)  multinomial subfamily is  learned.  From this  model a  canonical similarity function - known as the Fisher  kernel  - is  derived.  Our  approach  can  be  applied  for  unsupervised  and  supervised  learning problems alike.  This in  particular covers inter(cid:173) esting  cases  where  both,  labeled and  unlabeled  data are  available.  Experiments in  automated indexing and text categorization verify  the advantages of the proposed  method."}
{"Type": "conference", "Year": "1999", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Constructing Heterogeneous Committees Using Input Feature Grouping", "Title": "Application to Economic Forecasting", "Abstract": "The  committee  approach  has  been  proposed  for  reducing  model  uncertainty  and  improving  generalization  performance.  The  ad(cid:173) vantage of committees depends  on  (1)  the performance of individ(cid:173) ual members  and  (2)  the correlational structure of errors between  members.  This paper presents an input grouping technique for  de(cid:173) signing a  heterogeneous  committee.  With  this  technique,  all input  variables are first grouped based on their mutual information.  Sta(cid:173) tistically  similar  variables  are  assigned  to  the  same  group.  Each  member's  input  set  is  then  formed  by  input  variables  extracted  from different groups.  Our designed  committees have less error cor(cid:173) relation between its members, since each member observes different  input variable combinations.  The individual member's feature sets  contain less redundant information, because highly correlated vari(cid:173) ables will not be combined together.  The member feature sets con(cid:173) tain almost complete information, since each set contains a feature  from  each  information group.  An  empirical study for  a  noisy  and  nonstationary  economic  forecasting  problem  shows  that  commit(cid:173) tees constructed by our proposed technique outperform committees  formed using several existing techniques."}
{"Type": "conference", "Year": "1999", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Audio Vision", "Title": "Using Audio-Visual Synchrony to Locate Sounds", "Abstract": "Psychophysical and physiological evidence shows that sound local(cid:173) ization of acoustic signals is strongly influenced by  their synchrony  with visual signals.  This effect,  known as ventriloquism, is at work  when  sound  coming  from  the  side  of a  TV  set  feels  as  if it  were  coming  from  the  mouth  of the  actors.  The  ventriloquism  effect  suggests that there is  important information about sound location  encoded in  the synchrony between the audio and video signals.  In  spite  of  this  evidence,  audiovisual  synchrony  is  rarely  used  as  a  source of information  in  computer  vision  tasks.  In  this  paper  we  explore the use  of audio  visual  synchrony to locate sound sources.  We developed a system that searches for regions of the visual land(cid:173) scape that correlate highly with the acoustic signals and tags them  as  likely  to contain an acoustic  source.  We  discuss  our experience  implementing the system,  present results on a  speaker localization  task and discuss potential applications of the approach."}
{"Type": "conference", "Year": "1999", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "The Parallel Problems Server", "Title": "an Interactive Tool for Large Scale Machine Learning", "Abstract": "Imagine that you wish to classify data consisting of tens of thousands of ex(cid:173) amples residing in a twenty thousand dimensional space.  How can one ap(cid:173) ply standard machine learning algorithms? We describe the Parallel Prob(cid:173) lems Server (PPServer) and MATLAB*P.  In tandem they  allow users  of  networked computers to work transparently on large data sets from within  Matlab.  This  work is motivated by the desire to  bring the many  benefits  of scientific computing algorithms and computational power to  machine  learning researchers.  We  demonstrate the usefulness  of the system on  a number of tasks.  For  example,  we perform independent components analysis on very large text  corpora consisting  of tens  of thousands of documents,  making  minimal  changes  to  the original Bell  and  Sejnowski Matlab source  (Bell  and  Se(cid:173) jnowski,  1995).  Applying ML techniques to data previously beyond their  reach leads to interesting analyses of both data and algorithms."}
