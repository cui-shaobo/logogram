{"Type": "conference", "Year": "2005", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Inference with Minimal Communication", "Title": "a Decision-Theoretic Variational Approach", "Abstract": "Given a directed graphical model with binary-valued hidden nodes and real-valued noisy observations, consider deciding upon the maximum a-posteriori (MAP) or the maximum posterior-marginal (MPM) assignment under the restriction that each node broadcasts only to its children exactly one single-bit message. We present a variational formulation, viewing the processing rules local to all nodes as degrees-of-freedom, that minimizes the loss in expected (MAP or MPM) performance subject to such online communication constraints. The approach leads to a novel message-passing algorithm to be executed offline, or before observations are realized, which mitigates the performance loss by iteratively coupling all rules in a manner implicitly driven by global statistics. We also provide (i) illustrative examples, (ii) assumptions that guarantee convergence and efficiency and (iii) connections to active research areas."}
{"Type": "conference", "Year": "2005", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Non-Gaussian Component Analysis", "Title": "a Semi-parametric Framework for Linear Dimension Reduction", "Abstract": "We propose a new linear method for dimension reduction to identify nonGaussian components in high dimensional data. Our method, NGCA (non-Gaussian component analysis), uses a very general semi-parametric framework. In contrast to existing projection methods we define what is uninteresting (Gaussian): by projecting out uninterestingness, we can estimate the relevant non-Gaussian subspace. We show that the estimation error of finding the non-Gaussian components tends to zero at a parametric rate. Once NGCA components are identified and extracted, various tasks can be applied in the data analysis process, like data visualization, clustering, denoising or classification. A numerical study demonstrates the usefulness of our method."}
{"Type": "conference", "Year": "2005", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "How fast to work", "Title": "Response vigor, motivation and tonic dopamine", "Abstract": "Reinforcement learning models have long promised to unify computa- tional, psychological and neural accounts of appetitively conditioned be- havior. However, the bulk of data on animal conditioning comes from free-operant experiments measuring how fast animals will work for rein- forcement. Existing reinforcement learning (RL) models are silent about these tasks, because they lack any notion of vigor. They thus fail to ad- dress the simple observation that hungrier animals will work harder for food, as well as stranger facts such as their sometimes greater produc- tivity even when working for irrelevant outcomes such as water. Here, we develop an RL framework for free-operant behavior, suggesting that subjects choose how vigorously to perform selected actions by optimally balancing the costs and beneﬁts of quick responding. Motivational states such as hunger shift these factors, skewing the tradeoff. This accounts normatively for the effects of motivation on response rates, as well as many other classic ﬁndings. Finally, we suggest that tonic levels of dopamine may be involved in the computation linking motivational state to optimal responding, thereby explaining the complex vigor-related ef- fects of pharmacological manipulation of dopamine."}
{"Type": "conference", "Year": "2005", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "On the Accuracy of Bounded Rationality", "Title": "How Far from Optimal Is Fast and Frugal?", "Abstract": "Fast and frugal heuristics are well studied models of bounded rationality. Psychological research has proposed the take-the-best heuristic as a successful strategy in decision making with limited resources. Take-thebest searches for a sufficiently good ordering of cues (features) in a task where objects are to be compared lexicographically. We investigate the complexity of the problem of approximating optimal cue permutations for lexicographic strategies. We show that no efficient algorithm can approximate the optimum to within any constant factor, if P = NP. We further consider a greedy approach for building lexicographic strategies and derive tight bounds for the performance ratio of a new and simple algorithm. This algorithm is proven to perform better than take-the-best."}
{"Type": "conference", "Year": "2005", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Two view learning", "Title": "SVM-2K, Theory and Practice", "Abstract": "Kernel methods make it relatively easy to define complex highdimensional feature spaces. This raises the question of how we can identify the relevant subspaces for a particular learning task. When two views of the same phenomenon are available kernel Canonical Correlation Analysis (KCCA) has been shown to be an effective preprocessing step that can improve the performance of classification algorithms such as the Support Vector Machine (SVM). This paper takes this observation to its logical conclusion and proposes a method that combines this two stage learning (KCCA followed by SVM) into a single optimisation termed SVM-2K. We present both experimental and theoretical analysis of the approach showing encouraging results and insights."}
{"Type": "conference", "Year": "2005", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Top-Down Control of Visual Attention", "Title": "A Rational Account", "Abstract": "Theories of visual attention commonly posit that early parallel processes extract con- spicuous features such as color contrast and motion from the visual field. These features are then combined into a saliency map, and attention is directed to the most salient regions first. Top-down attentional control is achieved by modulating the contribution of different feature types to the saliency map. A key source of data concerning attentional control comes from behavioral studies in which the effect of recent experience is exam- ined as individuals repeatedly perform a perceptual discrimination task (e.g., “what shape is the odd-colored object?”). The robust finding is that repetition of features of recent trials (e.g., target color) facilitates performance. We view this facilitation as an adaptation to the statistical structure of the environment. We propose a probabilistic model of the environment that is updated after each trial. Under the assumption that attentional control operates so as to make performance more efficient for more likely environmental states, we obtain parsimonious explanations for data from four different experiments. Further, our model provides a rational explanation for why the influence of past experience on attentional control is short lived."}
{"Type": "conference", "Year": "2005", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Using ``epitomes'' to model genetic diversity", "Title": "Rational design of HIV vaccine cocktails", "Abstract": "We introduce a new model of genetic diversity which summarizes a large input dataset into an epitome, a short sequence or a small set of short sequences of probability distributions capturing many overlapping subsequences from the dataset. The epitome as a representation has already been used in modeling real-valued signals, such as images and audio. The discrete sequence model we introduce in this paper targets applications in genetics, from multiple alignment to recombination and mutation inference. In our experiments, we concentrate on modeling the diversity of HIV where the epitome emerges as a natural model for producing relatively small vaccines covering a large number of immune system targets known as epitopes. Our experiments show that the epitome includes more epitopes than other vaccine designs of similar length, including cocktails of consensus strains, phylogenetic tree centers, and observed strains. We also discuss epitome designs that take into account uncertainty about Tcell cross reactivity and epitope presentation. In our experiments, we find that vaccine optimization is fairly robust to these uncertainties."}
{"Type": "conference", "Year": "2005", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Ideal Observers for Detecting Motion", "Title": "Correspondence Noise", "Abstract": "We derive a Bayesian Ideal Observer (BIO) for detecting motion and solving the correspondence problem. We obtain Barlow and Tripathy’s classic model as an approximation. Our psychophysical experiments show that the trends of human performance are similar to the Bayesian Ideal, but overall human performance is far worse. We investigate ways to degrade the Bayesian Ideal but show that even extreme degradations do not approach human performance. Instead we propose that humans perform motion tasks using generic, general purpose, models of motion. We perform more psychophysical experiments which are consistent with humans using a Slow-and-Smooth model and which rule out an alterna- tive model using Slowness."}
{"Type": "conference", "Year": "2005", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Learning in Silicon", "Title": "Timing is Everything", "Abstract": "We hypothesize that the hippocampus achieves its precise spike timing (about 10ms) through plasticity enhanced phase-coding (PEP). The source of hippocampal timing preci- sion in the presence of variability (and noise) remains unexplained. Synaptic plasticity can compensate for variability in excitability if it increases excitatory synaptic input to neurons in inverse proportion to their excitabilities. Recasting this in a phase-coding framework, we desire a learning rule that increases excitatory synaptic input to neurons directly related to their phases. Neurons that lag require additional synaptic input, whereas neurons that lead"}
{"Type": "conference", "Year": "2005", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Rate Distortion Codes in Sensor Networks", "Title": "A System-level Analysis", "Abstract": "This paper provides a system-level analysis of a scalable distributed sens- ing model for networked sensors. In our system model, a data center ac- quires data from a bunch of L sensors which each independently encode their noisy observations of an original binary sequence, and transmit their encoded data sequences to the data center at a combined rate R, which is limited. Supposing that the sensors use independent LDGM rate dis- tortion codes, we show that the system performance can be evaluated for any given ﬁnite R when the number of sensors L goes to inﬁnity. The analysis shows how the optimal strategy for the distributed sensing prob- lem changes at critical values of the data rate R or the noise level."}
{"Type": "conference", "Year": "2005", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Beyond Pair-Based STDP", "Title": "a Phenomenological Rule for Spike Triplet and Frequency Effects", "Abstract": "While classical experiments on spike-timing dependent plasticity analyzed synaptic changes as a function of the timing of pairs of pre- and postsynaptic spikes, more recent experiments also point to the effect of spike triplets. Here we develop a mathematical framework that allows us to characterize timing based learning rules. Moreover, we identify a candidate learning rule with five variables (and 5 free parameters) that captures a variety of experimental data, including the dependence of potentiation and depression upon pre- and postsynaptic firing frequencies. The relation to the Bienenstock-Cooper-Munro rule as well as to some timing-based rules is discussed."}
{"Type": "conference", "Year": "2005", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Estimating the wrong Markov random field", "Title": "Benefits in the computation-limited setting", "Abstract": "Consider the problem of joint parameter estimation and prediction in a Markov random field: i.e., the model parameters are estimated on the basis of an initial set of data, and then the fitted model is used to perform prediction (e.g., smoothing, denoising, interpolation) on a new noisy observation. Working in the computation-limited setting, we analyze a joint method in which the same convex variational relaxation is used to construct an M-estimator for fitting parameters, and to perform approximate marginalization for the prediction step. The key result of this paper is that in the computation-limited setting, using an inconsistent parameter estimator (i.e., an estimator that returns the \"wrong\" model even in the infinite data limit) is provably beneficial, since the resulting errors can partially compensate for errors made by using an approximate prediction technique. En route to this result, we analyze the asymptotic properties of M-estimators based on convex variational relaxations, and establish a Lipschitz stability property that holds for a broad class of variational methods. We show that joint estimation/prediction based on the reweighted sum-product algorithm substantially outperforms a commonly used heuristic based on ordinary sum-product. 1 Keywords: Markov random fields; variational method; message-passing algorithms; sum-product; belief propagation; parameter estimation; learning."}
{"Type": "conference", "Year": "2005", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Analyzing Coupled Brain Sources", "Title": "Distinguishing True from Spurious Interaction", "Abstract": "When trying to understand the brain, it is of fundamental importance to analyse (e.g. from EEG/MEG measurements) what parts of the cortex interact with each other in order to infer more accurate models of brain activity. Common techniques like Blind Source Separation (BSS) can estimate brain sources and single out artifacts by using the underlying assumption of source signal independence. However, physiologically interesting brain sources typically interact, so BSS will--by construction-- fail to characterize them properly. Noting that there are truly interacting sources and signals that only seemingly interact due to effects of volume conduction, this work aims to contribute by distinguishing these effects. For this a new BSS technique is proposed that uses anti-symmetrized cross-correlation matrices and subsequent diagonalization. The resulting decomposition consists of the truly interacting brain sources and suppresses any spurious interaction stemming from volume conduction. Our new concept of interacting source analysis (ISA) is successfully demonstrated on MEG data."}
{"Type": "conference", "Year": "2005", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "CMOL CrossNets", "Title": "Possible Neuromorphic Nanoelectronic Circuits", "Abstract": "Recent  results  [1,  2]  indicate  that  the  current  VLSI  paradigm  based  on  CMOS  technology  can  be  hardly  extended  beyond  the  10-nm  frontier:  in  this  range  the  sensitivity  of  parameters  (most  importantly,  the  gate  voltage  threshold)  of  silicon  field-effect  transistors  to  inevitable  fabrication  spreads  grows  exponentially.  This  sensitivity will probably send the fabrication facilities costs skyrocketing, and may  lead to the end of Moore’s Law some time during the next decade.   There  is  a  growing  consensus  that  the  impending  Moore’s  Law  crisis  may  be  preempted by a radical paradigm shift from the purely CMOS technology to hybrid  CMOS/nanodevice circuits, e.g., those of “CMOL” variety (Fig. 1). Such circuits (see,  e.g., Ref. 3 for their recent review) would combine a level of advanced CMOS devices  fabricated by the lithographic patterning, and two-layer nanowire crossbar formed,  e.g.,  by  nanoimprint,  with  nanowires  connected  by  simple,  similar,  two-terminal  nanodevices at each crosspoint. For such devices, molecular single-electron latching  switches [4] are presently the leading candidates, in particular because they may be  fabricated using the self-assembled monolayer (SAM) technique which already gave  reproducible results for simpler molecular devices [5]."}
{"Type": "conference", "Year": "2005", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "The Forgetron", "Title": "A Kernel-Based Perceptron on a Fixed Budget", "Abstract": "The Perceptron algorithm, despite its simplicity, often performs well on online classification tasks. The Perceptron becomes especially effective when it is used in conjunction with kernels. However, a common difficulty encountered when implementing kernel-based online algorithms is the amount of memory required to store the online hypothesis, which may grow unboundedly. In this paper we present and analyze the Forgetron algorithm for kernel-based online learning on a fixed memory budget. To our knowledge, this is the first online learning algorithm which, on one hand, maintains a strict limit on the number of examples it stores while, on the other hand, entertains a relative mistake bound. In addition to the formal results, we also present experiments with real datasets which underscore the merits of our approach."}
{"Type": "conference", "Year": "2005", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Rodeo", "Title": "Sparse Nonparametric Regression in High Dimensions", "Abstract": "We present a method for nonparametric regression that performs bandwidth selection and variable selection simultaneously. The approach is based on the technique of incrementally decreasing the bandwidth in directions where the gradient of the estimator with respect to bandwidth is large. When the unknown function satisfies a sparsity condition, our approach avoids the curse of dimensionality, achieving the optimal minimax rate of convergence, up to logarithmic factors, as if the relevant variables were known in advance. The method--called rodeo (regularization of derivative expectation operator)--conducts a sequence of hypothesis tests, and is easy to implement. A modified version that replaces hard with soft thresholding effectively solves a sequence of lasso problems."}
{"Type": "conference", "Year": "2005", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Hot Coupling", "Title": "A Particle Approach to Inference and Normalization on Pairwise Undirected Graphs", "Abstract": "This paper presents a new sampling algorithm for approximating func- tions of variables representable as undirected graphical models of arbi- trary connectivity with pairwise potentials, as well as for estimating the notoriously dif(cid:2)cult partition function of the graph. The algorithm (cid:2)ts into the framework of sequential Monte Carlo methods rather than the more widely used MCMC, and relies on constructing a sequence of in- termediate distributions which get closer to the desired one. While the idea of using (cid:147)tempered(cid:148) proposals is known, we construct a novel se- quence of target distributions where, rather than dropping a global tem- perature parameter, we sequentially couple individual pairs of variables that are, initially, sampled exactly from a spanning tree of the variables. We present experimental results on inference and estimation of the parti- tion function for sparse and densely-connected graphs."}
{"Type": "conference", "Year": "2005", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Spectral Bounds for Sparse PCA", "Title": "Exact and Greedy Algorithms", "Abstract": "Sparse PCA seeks approximate sparse \"eigenvectors\" whose projections capture the maximal variance of data. As a cardinality-constrained and non-convex optimization problem, it is NP-hard and is encountered in a wide range of applied fields, from bio-informatics to finance. Recent progress has focused mainly on continuous approximation and convex relaxation of the hard cardinality constraint. In contrast, we consider an alternative discrete spectral formulation based on variational eigenvalue bounds and provide an effective greedy strategy as well as provably optimal solutions using branch-and-bound search. Moreover, the exact methodology used reveals a simple renormalization step that improves approximate solutions obtained by any continuous method. The resulting performance gain of discrete algorithms is demonstrated on real-world benchmark data and in extensive Monte Carlo evaluation trials."}
{"Type": "conference", "Year": "2005", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Beyond Gaussian Processes", "Title": "On the Distributions of Infinite Networks", "Abstract": "A general analysis of the limiting distribution of neural network functions is performed, with emphasis on non-Gaussian limits. We show that with i.i.d. symmetric stable output weights, and more generally with weights distributed from the normal domain of attraction of a stable variable, that the neural functions converge in distribution to stable processes. Conditions are also investigated under which Gaussian limits do occur when the weights are independent but not identically distributed. Some particularly tractable classes of stable distributions are examined, and the possibility of learning with such processes."}
{"Type": "conference", "Year": "2005", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Oblivious Equilibrium", "Title": "A Mean Field Approximation for Large-Scale Dynamic Games", "Abstract": "We propose a mean-ﬁeld approximation that dramatically reduces the computational complexity of solving stochastic dynamic games. We pro- vide conditions that guarantee our method approximates an equilibrium as the number of agents grow. We then derive a performance bound to assess how well the approximation performs for any given number of agents. We apply our method to an important class of problems in ap- plied microeconomics. We show with numerical experiments that we are able to greatly expand the set of economic problems that can be analyzed computationally."}
