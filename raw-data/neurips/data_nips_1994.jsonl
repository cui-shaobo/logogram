{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Learning with Preknowledge", "Title": "Clustering with Point and Graph Matching Distance Measures", "Abstract": "Prior constraints are imposed upon a learning problem in the form  of distance measures. Prototypical 2-D point sets and graphs are  learned by clustering with point matching and graph matching dis(cid:173) tance measures. The point matching distance measure is approx.  invariant under affine transformations - translation, rotation, scale  and shear - and permutations. It operates between noisy images  with missing and spurious points. The graph matching distance  measure operates on weighted graphs and is invariant under per(cid:173) mutations. Learning is formulated as an optimization problem .  Large objectives so formulated ('\" million variables) are efficiently  minimized using a combination of optimization techniques - alge(cid:173) braic transformations, iterative projective scaling, clocked objec(cid:173) tives, and deterministic annealing."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Using a Saliency Map for Active Spatial Selective Attention", "Title": "Implementation & Initial Results", "Abstract": "In many vision based tasks, the ability to focus attention on the important  portions of a scene is crucial for good performance on the tasks. In this paper  we present a simple method of achieving spatial selective attention through  the use of a saliency map. The saliency map indicates which regions of the  input retina are important for performing the task. The saliency map is cre(cid:173) ated through predictive auto-encoding. The performance of this method is  demonstrated on two simple tasks which have multiple very strong distract(cid:173) ing features in the input retina. Architectural extensions and application  directions for this model are presented."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "A Connectionist Technique for Accelerated Textual Input", "Title": "Letting a Network Do the Typing", "Abstract": "Each year people spend a huge amount of time typing. The text people type  typically contains a tremendous amount of redundancy due to predictable  word  usage  patterns  and  the  text's  structure.  This  paper  describes  a  neural network system call AutoTypist that monitors a person's typing and  predicts what will be entered  next.  AutoTypist displays the most likely  subsequent word to the typist, who can accept it with a single keystroke,  instead of typing it in its entirety.  The multi-layer perceptron at the heart  of Auto'JYpist adapts its predictions of likely subsequent text to the user's  word usage pattern,  and to the characteristics of the text currently being  typed.  Increases in typing speed of 2-3% when typing English prose and  10-20% when typing C code have been demonstrated using the system,  suggesting a potential time savings of more than 20 hours per user per year.  In addition to increasing typing speed, AutoTypist reduces the number of  keystrokes a user must type by a similar amount (2-3% for English,  10- 20% for computer programs).  This keystroke savings has the potential to  significantly reduce the frequency  and severity of repeated stress injuries  caused by typing, which are the most common injury suffered in today's  office environment."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "The Ni1000", "Title": "High Speed Parallel VLSI for Implementing Multilayer Perceptrons", "Abstract": "1"}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "JPMAX", "Title": "Learning to Recognize Moving Objects as a Model-fitting Problem", "Abstract": "934"}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "The Electrotonic Transformation", "Title": "a Tool for Relating Neuronal Form to Function", "Abstract": "The spatial distribution and time course of electrical signals in neurons  have important theoretical and practical consequences. Because it is  difficult to infer how neuronal form affects electrical signaling, we  have developed a quantitative yet intuitive approach to the analysis of  electrotonus. This approach transforms the architecture of the cell  from anatomical to electrotonic space, using the logarithm of voltage  attenuation as the distance metric. We describe the theory behind this  approach and illustrate its use."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Predictive Coding with Neural Nets", "Title": "Application to Text Compression", "Abstract": "To compress text files,  a neural predictor network  P  is used to ap(cid:173) proximate the conditional probability distribution of possible  \"next  characters\",  given  n  previous  characters.  P's outputs are fed  into  standard coding algorithms that generate short codes for characters  with  high  predicted  probability and  long  codes  for  highly  unpre(cid:173) dictable  characters.  Tested  on  short  German  newspaper  articles,  our method outperforms widely used  Lempel-Ziv algorithms (used  in  UNIX  functions such  as  \"compress\"  and  \"gzip\")."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Computational Structure of coordinate transformations", "Title": "A generalization study", "Abstract": "One  of the fundamental properties  that both neural  networks  and  the  central  nervous  system share is  the  ability to learn  and gener(cid:173) alize  from examples.  While this  property  has  been  studied  exten(cid:173) sively  in  the  neural  network  literature  it  has  not  been  thoroughly  explored in human perceptual and motor learning.  We have chosen  a  coordinate  transformation  system-the  visuomotor  map  which  transforms visual coordinates into motor coordinates-to study the  generalization effects  of learning new  input-output  pairs.  Using  a  paradigm of computer  controlled  altered  visual  feedback,  we  have  studied  the  generalization  of the  visuomotor  map  subsequent  to  both local  and context-dependent  remappings.  A  local  remapping  of one  or  two  input-output  pairs  induced  a  significant  global,  yet  decaying,  change  in  the  visuomotor map, suggesting  a  representa(cid:173) tion for  the  map composed of units  with large functional  receptive  fields.  Our study of context-dependent  remappings indicated that  a  single  point  in  visual  space  can  be  mapped to  two different  fin(cid:173) ger  locations  depending  on  a  context  variable-the starting  point  of  the  movement.  Furthermore,  as  the  context  is  varied  there  is  a  gradual  shift  between  the  two  remappings,  consistent  with  two  visuomotor  modules  being  learned  and  gated  smoothly  with  the  context."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Associative Decorrelation Dynamics", "Title": "A Theory of Self-Organization and Optimization in Feedback Networks", "Abstract": "This  paper  outlines  a  dynamic  theory  of  development  and  adap(cid:173) tation  in  neural  networks  with  feedback  connections.  Given  in(cid:173) put ensemble,  the  connections  change in strength according  to  an  associative  learning  rule  and  approach  a  stable  state  where  the  neuronal  outputs  are  decorrelated .  We  apply  this  theory  to  pri(cid:173) mary  visual  cortex and  examine  the implications of the  dynamical  decorrelation  of the  activities  of orientation  selective  cells  by  the  intracortical connections.  The theory gives a  unified  and quantita(cid:173) tive explanation of the  psychophysical experiments on  orientation  contrast and orientation adaptation.  Using only one parameter, we  achieve  good  agreements  between  the  theoretical  predictions  and  the experimental data."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "SARDNET", "Title": "A Self-Organizing Feature Map for Sequences", "Abstract": "A  self-organizing  neural  network  for  sequence  classification  called  SARDNET is described  and analyzed experimentally.  SARDNET  extends the Kohonen  Feature Map architecture  with activation re(cid:173) tention  and  decay  in  order  to  create  unique  distributed  response  patterns for different sequences.  SARDNET yields extremely dense  yet descriptive representations of sequential input in very few  train(cid:173) ing  iterations.  The  network  has  proven  successful  on  mapping ar(cid:173) bitrary sequences  of binary and real numbers,  as well  as  phonemic  representations  of  English  words.  Potential  applications  include  isolated  spoken  word  recognition  and  cognitive  science  models  of  sequence  processing."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Learning direction in global motion", "Title": "two classes of psychophysically-motivated models", "Abstract": "Perceptual learning is  defined  as  fast  improvement  in  performance and  retention  of the  learned  ability  over  a  period  of time.  In a  set  of psy(cid:173) chophysical experiments  we  demonstrated  that  perceptual learning oc(cid:173) curs for the discrimination of direction in stochastic motion stimuli.  Here  we  model  this  learning  using  two  approaches:  a clustering  model  that  learns  to  accommodate  the motion  noise,  and an averaging  model  that  learns to  ignore the noise.  Simulations of the models show  performance  similar to the psychophysical results."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Forward dynamic models in human motor control", "Title": "Psychophysical evidence", "Abstract": "Based  on  computational  principles,  with  as  yet  no  direct  experi(cid:173) mental  validation,  it  has  been  proposed  that  the  central  nervous  system  (CNS)  uses  an internal model to simulate the  dynamic be(cid:173) havior of the motor system in planning, control and learning (Sut(cid:173) ton  and  Barto,  1981;  Ito,  1984;  Kawato  et  aI.,  1987;  Jordan  and  Rumelhart,  1992;  Miall et aI.,  1993).  We  present  experimental re(cid:173) sults  and simulations based on a  novel  approach  that investigates  the temporal propagation of errors  in the sensorimotor integration  process.  Our results  provide  direct  support for  the  existence  of an  internal model."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Patterns of damage in neural networks", "Title": "The effects of lesion area, shape and number", "Abstract": "Eytan  Ruppin,  James A.  Reggia"}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Glove-TalkII", "Title": "Mapping Hand Gestures to Speech Using Neural Networks", "Abstract": "S.  Sidney  Fe Is,  Geoffrey  Hinton"}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "New Algorithms for 2D and 3D Point Matching", "Title": "Pose Estimation and Correspondence", "Abstract": "A  fundamental  open  problem  in  computer  vision-determining  pose  and  correspondence  between  two  sets  of  points  in  space(cid:173) is  solved with a novel, robust  and easily implementable algorithm.  The  technique  works  on  noisy  point sets  that  may be  of unequal  sizes  and  may  differ  by  non-rigid  transformations.  A  2D  varia(cid:173) tion  calculates  the  pose  between  point  sets  related  by  an  affine  transformation-translation, rotation, scale and shear.  A 3D to 3D  variation calculates translation and rotation.  An objective describ(cid:173) ing  the  problem is  derived  from  Mean field  theory.  The objective  is  minimized with clocked  (EM-like)  dynamics.  Experiments with  both  handwritten  and  synthetic  data provide  empirical  evidence  for  the method."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Morphogenesis of the Lateral Geniculate Nucleus", "Title": "How Singularities Affect Global Structure", "Abstract": "The macaque lateral geniculate nucleus (LGN) exhibits an intricate  lamination pattern, which changes midway through the nucleus at a  point coincident with small gaps due to the blind spot in the retina.  We  present a  three-dimensional model of morphogenesis in  which  local cell interactions cause a  wave  of development of neuronal re(cid:173) ceptive fields  to propagate through the nucleus  and establish two  distinct lamination patterns.  We  examine the interactions between  the wave  and the localized singularities due  to the gaps,  and find  that the gaps induce the change in lamination pattern.  We  explore  critical factors which  determine general LGN organization."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Recurrent Networks", "Title": "Second Order Properties and Pruning", "Abstract": "x(t + 1) = Yo(t + 1) = L WojYj(t)  + Wthres,o"}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Generalization in Reinforcement Learning", "Title": "Safely Approximating the Value Function", "Abstract": "A  straightforward  approach  to  the  curse  of  dimensionality  in  re(cid:173) inforcement  learning  and  dynamic  programming  is  to  replace  the  lookup table with a generalizing function approximator such as a neu(cid:173) ral net.  Although this has been successful in the domain of backgam(cid:173) mon,  there  is  no  guarantee  of convergence.  In  this  paper,  we  show  that the combination of dynamic programming and function approx(cid:173) imation  is  not  robust,  and  in  even  very  benign  cases,  may produce  an  entirely  wrong  policy.  We  then  introduce  Grow-Support,  a  new  algorithm which is safe from divergence yet can still reap the benefits  of successful  generalization ."}
