{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Learning Spatio-Temporal Planning from a Dynamic Programming Teacher", "Title": "Feed-Forward Neurocontrol for Moving Obstacle Avoidance", "Abstract": "e-mail:  gerald@nero.uni-bonn.de"}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Hidden Markov Models in Molecular Biology", "Title": "New Algorithms and Applications", "Abstract": "*and Division  of Biology,  California  Institute of Technology.  t and Department of Psychology,  Stanford University."}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Adaptive Stimulus Representations", "Title": "A Computational Theory of Hippocampal-Region Function", "Abstract": "We present a theory of cortico-hippocampal interaction in discrimination learning. The  hippocampal region is presumed to form new stimulus representations which facilitate  learning by enhancing the discriminability of predictive stimuli and compressing  stimulus-stimulus redundancies. The cortical and cerebellar regions, which are the sites  of long-term memory. may acquire these new representations but are not assumed to be  capable of forming new representations themselves.  Instantiated as a connectionist  model. this theory accounts for a wide range of trial-level classical conditioning  phenomena in normal (intact) and hippocampal-Iesioned animals. It also makes several  novel predictions which remain to be investigated empirically. The theory implies that  the hippocampal region is involved in even the simplest learning tasks; although  hippocampal-Iesioned animals may be able to use other strategies to learn these tasks. the  theory predicts that they will show consistently different patterns of transfer and  generalization when the task demands change."}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Intersecting regions", "Title": "The Key to combinatorial structure in hidden unit space", "Abstract": "In multi-layer networks, regions of hidden unit space can be identified with classes of  equivalent outputs. For example, Elman (1989) showed that the hidden unit patterns for  words in simple grammatical sentences cluster into regions, with similar patterns  representing similar grammatical entities. For example, different tokens of the same word  are clustered tightly, indicating that they are represented within a small region. These  regions can be grouped into larger regions, reflecting a hierarchical structure. The largest"}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Planar Hidden Markov Modeling", "Title": "From Speech to Optical Character Recognition", "Abstract": "Tbe  PHMM  approach  was  evaluated  using  a  set  of  isolated  band-written  digits.  An  overall  digit  recognition  accuracy  of  95%  was  acbieved.  An  analysis of the results showed  that even in  the  simple case of recognition  of  isolated  characters,  the  elimination  of  elastic  distortions  enhances  the  performance Significantly. We expect that the advantage of this approach  will  be  even  more  such  as  connected  writing  recognition/spotting,  for  whicb  there  is  no  known  high  accuracy  method  of  recognition."}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Perceiving Complex Visual Scenes", "Title": "An Oscillator Neural Network Model that Integrates Selective Attention, Perceptual Organisation, and Invariant Recognition", "Abstract": "the 'where(cid:173)"}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Second order derivatives for network pruning", "Title": "Optimal Brain Surgeon", "Abstract": "We investigate the use of information from all second order derivatives of the error  function to perfonn network pruning (i.e., removing unimportant weights from a trained  network) in order to improve generalization, simplify networks, reduce hardware or  storage requirements, increase the speed of further training, and in some cases enable rule  extraction. Our method, Optimal Brain Surgeon (OBS), is Significantly better than  magnitude-based methods and Optimal Brain Damage [Le Cun, Denker and Sol1a, 1990],  which often remove the wrong weights. OBS permits the pruning of more weights than  other methods (for the same error on the training set), and thus yields better  generalization on test data. Crucial to OBS is a recursion relation for calculating the  inverse Hessian matrix H-I from training data and structural information of the net. OBS  permits a 90%, a 76%, and a 62% reduction in weights over backpropagation with weighL  decay on three benchmark MONK's problems [Thrun et aI., 1991]. Of OBS, Optimal  Brain Damage, and magnitude-based methods, only OBS deletes the correct weights from  a trained XOR network in every case. Finally, whereas Sejnowski and Rosenberg [1987J  used 18,000 weights in their NETtalk network, we used OBS to prune a network to just  1560 weights, yielding better generalization."}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "On-Line Estimation of the Optimal Value Function", "Title": "HJB- Estimators", "Abstract": "The complete effect of a control action Uk at a given time step t/.; is clouded by  the fact that the state history depends on the control actions taken after time  step tk' So the effect of a control action over all future time must be monitored.  Hence, choice of control must inevitably involve knowledge of the future history  of the state trajectory. In other words, the optimal control sequence can not be  determined until after the fact. Of course, standard optimal control theory supplies  an optimal control sequence to this problem for a variety of performance criteria.  Roughly, there are two approaches of interest: solving the two-point boundary value"}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Memory-Based Reinforcement Learning", "Title": "Efficient Computation with Prioritized Sweeping", "Abstract": "The paper introduces a memory-based technique, prioritized 6weeping, which is used  both for  stochastic  prediction and  reinforcement  learning.  A fuller  version  of this  paper is in preparation [Moore and Atkeson, 1992].  Consider the  500 state Markov  system depicted in Figure 1.  The system has sixteen absorbing states,  depicted by  white and black circles.  The prediction problem is  to estimate, for  every state, the  long-term  probability  that  it will  terminate  in  a  white,  rather  than  black,  circle.  The data available to the learner is a sequence of observed state transitions.  Let us  consider two existing methods along  with prioritized sweeping."}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Weight Space Probability Densities in Stochastic Learning", "Title": "I. Dynamics and Equilibria", "Abstract": "where w E 7£m  is  the vector of m  weights,  /-l  is  the learning rate, H[.]  E 7£m  is  the  update function,  and  x(n)  is  the exemplar  (input or  input/target pair)  presented"}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "A Formal Model of the Insect Olfactory Macroglomerulus", "Title": "Simulations and Analytic Results", "Abstract": "It  is  known  from  biological  data  that  the  response  patterns  of  interneurons  in  the olfactory  macroglomerulus  (MGC) of insects are of  central importance for the coding of the olfactory signal. We propose an  analytically  tractable  model  of the  MGC  which allows us  to  relate  the  distribution of response patterns to the architecture of the network."}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Learning to See Where and What", "Title": "Training a Net to Make Saccades and Recognize Handwritten Characters", "Abstract": "The Saccade system takes a cue from the ballistic and corrective saccades (eye movements)  of natural vision systems.  Natural saccades make it possible to efficiently move from one  informative area to another by jumping.  The eye  typically  initiates a ballistic saccade to"}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Summed Weight Neuron Perturbation", "Title": "An O(N) Improvement Over Weight Perturbation", "Abstract": "The algorithm presented performs gradient descent on the weight space  of an Artificial Neural Network (ANN), using a finite difference to  approximate the gradient The method is novel in that it achieves a com(cid:173) putational complexity similar to that of Node Perturbation, O(N3), but  does not require access to the activity of hidden or internal neurons.  This is possible due to a stochastic relation between perturbations at the  weights and the neurons of an ANN. The algorithm is also similar to  Weight Perturbation in that it is optimal in terms of hardware require(cid:173) ments when used for the training ofVLSI implementations of ANN's."}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Attractor Neural Networks with Local Inhibition", "Title": "from Statistical Physics to a Digitial Programmable Integrated Circuit", "Abstract": "Networks with local inhibition are shown to have enhanced compu(cid:173) tational performance with respect to the classical Hopfield-like net(cid:173) works. In particular the critical capacity of the network is increased  as well as its capability to store correlated patterns. Chaotic dy(cid:173) namic behaviour (exponentially long transients) of the devices in(cid:173) dicates the overloading of the associative memory. An implementa(cid:173) tion based on a programmable logic device is here presented. A 16  neurons circuit is implemented whit a XILINK 4020 device. The  peculiarity of this solution is the possibility to change parts of the  project (weights, transfer function or the whole architecture) with  a simple software download of the configuration into the XILINK  chip."}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Metamorphosis Networks", "Title": "An Alternative to Constructive Models", "Abstract": "Given a set oft raining examples, determining the appropriate num(cid:173) ber  of  free  parameters  is  a  challenging  problem.  Constructive  learning algorithms attempt to solve this problem automatically by  adding  hidden  units,  and  therefore  free  parameters,  during  learn(cid:173) ing.  We  explore  an  alternative  class  of algorithms-called  meta(cid:173) morphosis  algorithms-in  which  the  number  of units  is  fixed,  but  the number of free  parameters gradually increases  during learning.  The architecture we investigate is composed of RBF units on a lat(cid:173) tice,  which  imposes  flexible  constraints  on  the  parameters  of the  network.  Virtues  of this  approach  include  variable  subset  selec(cid:173) tion,  robust  parameter  selection,  multiresolution  processing,  and  interpolation  of sparse training  data."}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Weight Space Probability Densities in Stochastic Learning", "Title": "II. Transients and Basin Hopping Times", "Abstract": "Despite the recent application of convergence theorems from stochastic approxima(cid:173) tion  theory to  neural  network  learning  (Oja 1982,  White  1989)  there remain  out(cid:173) standing questions about the search dynamics in stochastic learning.  For example,  the convergence theorems do  not  tell  us  to which  of several  optima the algorithm"}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Performance Through Consistency", "Title": "MS-TDNN's for Large Vocabulary Continuous Speech Recognition", "Abstract": "Connectionist Rpeech recognition systems are often handicapped by  an  inconsistency  between  training and  testing  criteria.  This  prob(cid:173) lem  is  addressed  by  the  Multi-State  Time Delay  Neural  Network  (MS-TDNN), a hierarchical phonf'mp and word classifier which uses  DTW  to  modulate  its  connectivit.y  pattern,  and  which  is  directly  trained  on  word-level  targets.  The  consistent  use  of word  accu(cid:173) racy  as  a  criterion  during  bot.h  t.raining  and  testing  leads  to  very  high  system  performance,  even  wif II  limited  training  dat.a.  Until  now,  the  MS-TDN N  has  been  appli('d  primarily  to  small  vocabu(cid:173) lary  recognition  and  word  spotting  tasks.  In  this  papf'f  we  apply  the architecture to large vocabulary continuous speech recognition,  and  demonstrate  that  our  MS-TDNN  outperforms  all  ot,hf'r  sys(cid:173) tems  that  have  been  tested  on  tht'  eMU  Conference  Registration  database."}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Unsmearing Visual Motion", "Title": "Development of Long-Range Horizontal Intrinsic Connections", "Abstract": "Human VlSlon systems integrate information nonlocally, across long  spatial ranges.  For example, a moving stimulus appears smeared  when viewed briefly (30 ms), yet sharp when viewed for a longer  exposure (100 ms) (Burr, 1980). This suggests that visual systems  combine information along a trajectory that matches the motion of  the stimulus. Our self-organizing neural network model shows how  developmental exposure to moving stimuli can direct the formation of  horizontal trajectory-specific motion integration pathways that unsmear  representations of moving stimuli. These results account for Burr's data  and can potentially also model ot.her phenomena, such as visual inertia."}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "The Power of Approximating", "Title": "a Comparison of Activation Functions", "Abstract": "We compare activation functions in terms of the approximation  power of their feedforward nets. We consider the case of analog as  well as boolean input."}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Transient Signal Detection with Neural Networks", "Title": "The Search for the Desired Signal", "Abstract": "Matched  filtering  has  been  one  of the  most  powerful  techniques  employed for transient detection. Here we will show that a dynamic  neural  network  outperforms  the  conventional  approach.  When  the  artificial neural  network (ANN) is  trained with supervised learning  schemes  there  is  a  need  to  supply  the  desired  signal  for  all  time,  although  we  are  only  interested  in  detecting  the  transient.  In  this  paper  we  also  show  the  effects  on  the  detection  agreement  of  different strategies to construct the desired signal. The extension of  the  Bayes  decision  rule  (011  desired  signal),  optimal  in  static  classification,  performs  worse  than  desired  signals  constructed by  random noise or prediction during the background."}
