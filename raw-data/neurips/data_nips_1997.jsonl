{"Type": "conference", "Year": "1997", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "A Revolution", "Title": "Belief Propagation in Graphs with Cycles", "Abstract": "producing  the  real-valued  channel  output  vector  y  =  (Y!, ... ,YN).  The  decoder  must  then use  this  received  vector  to  make  a  guess  U at the  original  information  vector.  The  probability  P\" (e)  of  bit  error  is  minimized  by  choosing  the  Uk  that  maximizes  P(ukly)  for  k  = 1, ... , K.  The  rate  K/N of a  code  is  the  number  of  information  bits  communicated  per  codeword  bit.  We  will  consider  rate  ~ 1/2  systems in this paper,  where  N  ==  2K.  The simplest rate 1/2 encoder duplicates each information hit:  X2k-l  =  X2k  =  Uk,  k  =  1, ... , K.  The optimal decoder for this repetition code  simply averages together  pairs of noisy channel outputs and then applies  a threshold:"}
{"Type": "conference", "Year": "1997", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "A General Purpose Image Processing Chip", "Title": "Orientation Detection", "Abstract": "The  generalization  ability  of a  neural  network  can  sometimes  be  improved dramatically by regularization.  To  analyze the improve(cid:173) ment  one  needs  more  refined  results  than  the  asymptotic  distri(cid:173) bution  of  the  weight  vector.  Here  we  study  the  simple  case  of  one-dimensional  linear  regression  under  quadratic  regularization,  i.e.,  ridge  regression.  We  study  the  random  design,  misspecified  case, where we derive expansions for  the optimal regularization pa(cid:173) rameter and  the ensuing improvement.  It is  possible  to construct  examples  where it  is  best to use no regularization."}
{"Type": "conference", "Year": "1997", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Serial Order in Reading Aloud", "Title": "Connectionist Models and Neighborhood Structure", "Abstract": "If globally high dimensional data has locally only low dimensional distribu(cid:173) tions,  it is  advantageous to perform a local dimensionality reduction before  further processing the data.  In this paper we examine several techniques for  local  dimensionality reduction  in the  context of locally weighted linear re(cid:173) gression.  As possible candidates, we derive local versions of factor analysis  regression, principle component regression, principle component regression  on joint distributions, and partial least squares regression. After outlining the  statistical bases of these  methods,  we perform Monte Carlo  simulations to  evaluate  their  robustness  with  respect  to  violations  of their  statistical  as(cid:173) sumptions.  One  surprising  outcome  is  that  locally  weighted  partial  least  squares  regression offers the best average results,  thus outperforming even  factor analysis, the theoretically most appealing of our candidate techniques."}
{"Type": "conference", "Year": "1997", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Automated Aircraft Recovery via Reinforcement Learning", "Title": "Initial Experiments", "Abstract": "Automated Aircraft Recovery via Reinforcement Learning"}
{"Type": "conference", "Year": "1997", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Receptive Field Formation in Natural Scene Environments", "Title": "Comparison of Single Cell Learning Rules", "Abstract": "B.  S.  Blais, N.  Intrator, H.  Shouval and L  N.  Cooper"}
{"Type": "conference", "Year": "1997", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Generalization in Decision Trees and DNF", "Title": "Does Size Matter?", "Abstract": "M.  Golea,  P Bartlett,  W.  S. Lee and L  Mason"}
{"Type": "conference", "Year": "1997", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Asymptotic Theory for Regularization", "Title": "One-Dimensional Linear Case", "Abstract": "The  generalization  ability  of a  neural  network  can  sometimes  be  improved dramatically by regularization.  To  analyze the improve(cid:173) ment  one  needs  more  refined  results  than  the  asymptotic  distri(cid:173) bution  of  the  weight  vector.  Here  we  study  the  simple  case  of  one-dimensional  linear  regression  under  quadratic  regularization,  i.e.,  ridge  regression.  We  study  the  random  design,  misspecified  case, where we derive expansions for  the optimal regularization pa(cid:173) rameter and  the ensuing improvement.  It is  possible  to construct  examples  where it  is  best to use no regularization."}
{"Type": "conference", "Year": "1997", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Instabilities in Eye Movement Control", "Title": "A Model of Periodic Alternating Nystagmus", "Abstract": "Nystagmus is  a pattern of eye movement characterized by  smooth rota(cid:173) tions  of the eye in one direction  and  rapid  rotations  in  the opposite di(cid:173) rection that reset eye position.  Periodic alternating nystagmus (PAN) is  a form  of uncontrollable  nystagmus  that  has  been  described  as  an  un(cid:173) stable but amplitude-limited oscillation.  PAN has been observed previ(cid:173) ously  only  in  subjects  with  vestibulo-cerebellar damage.  We describe  results in  which  PAN can be produced  in normal  subjects by prolonged  rotation in darkness.  We propose a new model  in  which the neural  cir(cid:173) cuits  that control eye movement are  inherently  unstable,  but  this  insta(cid:173) bility  is  kept  in  check  under  normal  circumstances  by  the  cerebellum.  Circumstances  which  alter  this  cerebellar  restraint,  such  as  vestibulo(cid:173) cerebellar damage or plasticity  due  to  rotation  in  darkness,  can  lead  to  PAN."}
{"Type": "conference", "Year": "1997", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Radial Basis Functions", "Title": "A Bayesian Treatment", "Abstract": "Bayesian methods have been successfully applied to regression and  classification  problems  in  multi-layer  perceptrons.  We  present  a  novel  application of Bayesian  techniques to Radial Basis Function  networks  by  developing a  Gaussian approximation to the posterior  distribution  which,  for  fixed  basis  function  widths,  is  analytic  in  the  parameters.  The setting of regularization  constants  by  cross(cid:173) validation is  wasteful  as  only  a  single optimal  parameter estimate  is  retained.  We  treat this  issue  by  assigning  prior distributions to  these constants, which are then adapted in light of the data under  a  simple re-estimation formula."}
{"Type": "conference", "Year": "1997", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Synaptic Transmission", "Title": "An Information-Theoretic Perspective", "Abstract": "Here  we  analyze  synaptic  transmission  from  an  infonnation-theoretic  perspective. We derive c1osed-fonn expressions for the lower-bounds on  the capacity of a simple model of a cortical synapse under two explicit  coding paradigms.  Under the \"signal estimation\" paradigm, we assume  the signal to be encoded in the mean firing rate of a Poisson neuron.  The  perfonnance of an optimal linear estimator of the  signal  then  provides  a lower bound on the  capacity for signal estimation.  Under the  \"signal  detection\" paradigm, the presence or absence of the signal has to be de(cid:173) tected.  Perfonnance of the optimal spike detector allows us to compute  a lower bound on the capacity for signal detection.  We  find  that single  synapses (for empirically measured parameter values) transmit infonna(cid:173) tion poorly but  significant  improvement can be  achieved  with  a  small  amount of redundancy."}
{"Type": "conference", "Year": "1997", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Just One View", "Title": "Invariances in Inferotemporal Cell Tuning", "Abstract": "216"}
{"Type": "conference", "Year": "1997", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Using Expectation to Guide Processing", "Title": "A Study of Three Real-World Applications", "Abstract": "In many real world tasks, only a small fraction of the available inputs are important  at any  particular time. This paper presents a method for ascertaining the relevance  of inputs  by  exploiting  temporal  coherence  and  predictability.  The  method  pro(cid:173) posed in this paper dynamically allocates relevance to inputs by using expectations  of their  future  values.  As  a  model  of the  task  is  learned,  the  model  is  simulta(cid:173) neously extended to create task-specific predictions of the future values of inputs.  Inputs which are either not relevant, and therefore not accounted for in the model,  or those which contain noise, will not be predicted accurately. These inputs can be  de-emphasized, and, in turn, a new, improved, model of the task created. The tech(cid:173) niques  presented  in  this  paper  have  yielded  significant  improvements  for  the  vision-based  autonomous control of a land vehicle,  vision-based hand tracking in  cluttered scenes, and the detection of faults in the etching of semiconductor wafers."}
{"Type": "conference", "Year": "1997", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "On Parallel versus Serial Processing", "Title": "A Computational Study of Visual Search", "Abstract": "On Parallel versus Serial Processing: A  Computational Study a/Visual Search"}
{"Type": "conference", "Year": "1997", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "S-Map", "Title": "A Network with a Simple Self-Organization Algorithm for Generative Topographic Mappings", "Abstract": "The S-Map is a network with a simple learning algorithm that com(cid:173) bines  the  self-organization  capability  of the  Self-Organizing  Map  (SOM)  and the probabilistic interpretability of the Generative To(cid:173) pographic  Mapping  (GTM).  The  simulations  suggest  that  the  S(cid:173) Map algorithm has a  stronger tendency  to self-organize from  ran(cid:173) dom  initial  configuration  than  the  GTM.  The  S-Map  algorithm  can  be  further  simplified  to employ  pure  Hebbian  learning,  with(cid:173) out changing the qualitative behaviour of the network."}
{"Type": "conference", "Year": "1997", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Linear Concepts and Hidden Variables", "Title": "An Empirical Study", "Abstract": "We  learn  CIA  with  two  techniques:  the  standard  EM  algorithm,  and  a  new  algorithm we develop based on covariances.  We  compare these, in a controlled  fashion, against an algorithm (a version of Winnow) that attempts to find a good  linear classifier directly.  Our conclusions help delimit the fragility  of using the  CIA model for classification: once the data departs from this model, performance  quickly degrades and drops below that of the directly-learned linear classifier."}
{"Type": "conference", "Year": "1997", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Regression with Input-dependent Noise", "Title": "A Gaussian Process Treatment", "Abstract": "Cy(x(i),xU» =vyexp (-~ tWYl(x~i) _x~j»2) + Jy 8(i,j)"}
{"Type": "conference", "Year": "1997", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Learning Human-like Knowledge by Singular Value Decomposition", "Title": "A Progress Report", "Abstract": "Singular value decomposition  (SVD)  can  be  viewed  as  a  method  for  unsupervised training of a network that associates two  classes  of events  reciprocally by linear connections  through  a single  hidden  layer.  SVD  was used to learn and represent relations among  very large numbers  of  words  (20k-60k)  and  very  large numbers  of natural  text  passages  (lk- 70k)  in  which  they  occurred.  The  result  was  100-350  dimensional  \"semantic spaces\" in which any trained or newly aibl word or passage  could be represented as  a vector,  and  similarities  were measured by  the  cosine  of  the  contained  angle  between  vectors.  Good  accmacy  in  simulating  human judgments and behaviors has  been  demonstrated  by  performance  on  multiple-choice  vocabulary  and  domain  knowledge  tests, emulation of expert essay evaluations,  and  in  several other ways.  Examples are also given of how the kind of knowledge extracted by  this  method can be applied."}
{"Type": "conference", "Year": "1997", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Toward a Single-Cell Account for Binocular Disparity Tuning", "Title": "An Energy Model May Be Hiding in Your Dendrites", "Abstract": "Converging  evidence  has  shown  that  human  object  recognition  depends  on  familiarity  with  the  images  of  an  object.  Further,  the  greater  the  similarity  between  objects,  the  stronger  is  the  dependence  on  object  appearance,  and  the  more  important  two(cid:173) dimensional (2D) image information becomes.  These findings,  how(cid:173) ever, do not rule out the use of 3D structural information in recog(cid:173) nition,  and  the  degree  to  which  3D  information  is  used  in  visual  memory is an important issue.  Liu, Knill, & Kersten (1995) showed  that  any model  that  is  restricted  to  rotations  in  the  image  plane  of independent  2D  templates  could not  account for  human perfor(cid:173) mance in discriminating novel object views.  We now present results  from models of generalized radial basis functions  (GRBF), 2D near(cid:173) est  neighbor  matching that  allows  2D  affine  transformations,  and  a Bayesian statistical estimator that integrates over all possible 2D  affine  transformations.  The  performance  of the  human  observers  relative  to  each  of  the  models  is  better for  the  novel  views  than  for  the familiar  template views,  suggesting that humans generalize  better to novel  views  from  template views.  The  Bayesian estima(cid:173) tor yields the optimal performance with  2D  affine  transformations  and  independent  2D  templates.  Therefore,  models  of  2D  affine  matching  operations  with  independent  2D  templates  are unlikely  to account for  human recognition performance."}
{"Type": "conference", "Year": "1997", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "MELONET I", "Title": "Neural Nets for Inventing Baroque-Style Chorale Variations", "Abstract": "MELONET I is a multi-scale neural network system producing  baroque-style melodic variations. Given a melody, the system in(cid:173) vents a four-part chorale harmonization and a variation of any  chorale voice, after being trained on music pieces of composers like  J. S. Bach and J . Pachelbel. Unlike earlier approaches to the learn(cid:173) ing of melodic structure, the system is able to learn and reproduce  high-order structure like harmonic, motif and phrase structure in  melodic sequences. This is achieved by using mutually interacting  feedforward networks operating at different time scales, in combi(cid:173) nation with Kohonen networks to classify and recognize musical  structure. The results are chorale partitas in the style of J. Pachel(cid:173) bel. Their quality has been judged by experts to be comparable to  improvisations invented by an experienced human organist."}
{"Type": "conference", "Year": "1997", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Ensemble and Modular Approaches for Face Detection", "Title": "A Comparison", "Abstract": "threshold  used  to adjust the sensitivity of the model."}
