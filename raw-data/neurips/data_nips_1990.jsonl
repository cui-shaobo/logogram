{"Type": "conference", "Year": "1990", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Closed-Form Inversion of Backpropagation Networks", "Title": "Theory and Optimization Issues", "Abstract": "We describe a closed-form technique for mapping the output of a trained  backpropagation network int.o input activity space. The mapping is an in(cid:173) verse mapping in the sense that, when the image of the mapping in input  activity space is propagat.ed forward through the normal network dynam(cid:173) ics, it reproduces the output used to generate that image. When more  than one such inverse mappings exist, our inverse ma.pping is special in  that it has no projection onto the nullspace of the activation flow opera(cid:173) tor for the entire network. An important by-product of our calculation,  when more than one invel'se mappings exist, is an orthogonal basis set of  a significant portion of the activation flow operator nullspace. This basis  set can be used to obtain an alternate inverse mapping that is optimized  for a particular rea.l-world application."}
{"Type": "conference", "Year": "1990", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Comparison of three classification techniques", "Title": "CART, C4.5 and Multi-Layer Perceptrons", "Abstract": "In this paper, after some introductory remarks into the classification prob(cid:173) lem as considered in various research communities, and some discussions  concerning some of the reasons for ascertaining the performances of the  three chosen algorithms, viz., CART (Classification and Regression Tree),  C4.5 (one of the more recent versions of a popular induction tree tech(cid:173) nique known as ID3), and a multi-layer perceptron (MLP), it is proposed  to compare the performances of these algorithms under two criteria: classi(cid:173) fication and generalisation. It is found that, in general, the MLP has better  classification and generalisation accuracies compared with the other two  algorithms."}
{"Type": "conference", "Year": "1990", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "VLSI Implementations of Learning and Memory Systems", "Title": "A Review", "Abstract": "Today most neural models are already implemented in silicon VLSI, in the form of pro(cid:173) grams running on general purpose digital von Neumann computers. These machines  are available at low cost and are highly flexible. Their flexibility results from the ease  with which their programs can be changed. Maximizing flexibility, however, usually  results in reduced performance. A program will often have to specify several simple op-"}
{"Type": "conference", "Year": "1990", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Kohonen Networks and Clustering", "Title": "Comparative Performance in Color Clustering", "Abstract": "The problem of color clustering is defined and shown to be a problem of  assigning a large number (hundreds of thousands) of 3-vectors to a  small number (256) of clusters. Finding those clusters in such a way that  they best represent a full color image using only 256 distinct colors is a  burdensome computational problem. In this paper, the problem is solved  using \"classical\" techniques -- k-means clustering, vector quantization  (which turns out to be the same thing in this application), competitive  learning, and Kohonen self-organizing feature maps. Quality of the  result is judged subjectively by how much the pseudo-color result  resembles the true color image, by RMS quantization error, and by run  time. The Kohonen map provides the best solution."}
{"Type": "conference", "Year": "1990", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Extensions of a Theory of Networks for Approximation and Learning", "Title": "Outliers and Negative Examples", "Abstract": "Learning an input-output mapping from a set of examples can be regarded  as synthesizing an approximation of a multi-dimensional function. From  this point of view, this form of learning is closely related to regularization  theory, and we have previously shown (Poggio and Girosi, 1990a, 1990b)  the equivalence between reglilari~at.ioll and a. class of three-layer networks  that we call regularization networks. In this note, we ext.end the theory  by introducing ways of"}
{"Type": "conference", "Year": "1990", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "EMPATH", "Title": "Face, Emotion, and Gender Recognition Using Holons", "Abstract": "The  dimens~onali~y of a  set Off  160 1:~ :a:~s ~~·.10 .  female  subjects  IS  reduced  ........ .  network  The extracted features do not correspond to  in previ~us face  recognition systems (KaR· na~e, 19~;)y' ......••.•..  f~tures we  call  holons.  The  hol.ons  are fV~~ t~!  ..... .  ..  ......\\  d'  tances  between  facial  elements."}
{"Type": "conference", "Year": "1990", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Second Order Properties of Error Surfaces", "Title": "Learning Time and Generalization", "Abstract": "The learning time of a simple neural network model is obtained through an  analytic computation of the eigenvalue spectrum for the Hessian matrix,  which describes the second order properties of the cost function in the  space of coupling coefficients. The form of the eigenvalue distribution  suggests new techniques for accelerating the learning process, and provides  a theoretical justification for the choice of centered versus biased state  variables."}
{"Type": "conference", "Year": "1990", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "ALCOVE", "Title": "A Connectionist Model of Human Category Learning", "Abstract": "ALCOVE  is  a  connectionist  model  of human  category  learning  that  fits  a  broad spectrum of human learning data.  Its architecture is  based on well(cid:173) established  psychological  theory,  and  is  related  to  networks  using  radial  basis functions.  From the perspective of cognitive psychology,  ALCOVE can  be construed as a combination of exemplar-based representation and error(cid:173) driven  learning.  From the perspective of connectionism,  it can  be seen  as  incorporating constraints into back-propagation networks  appropriate for  modelling human learning."}
{"Type": "conference", "Year": "1990", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Spherical Units as Dynamic Consequential Regions", "Title": "Implications for Attention, Competition and Categorization", "Abstract": "• Also a member of Cognitive Science Laboratory, Princeton University, Princeton, NJ 08544"}
{"Type": "conference", "Year": "1990", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Direct memory access using two cues", "Title": "Finding the intersection of sets in a connectionist model", "Abstract": "For lack of alternative models, search and decision processes have provided the  dominant paradigm for human memory access using two or more cues, despite  evidence against search as an access process (Humphreys, Wiles & Bain, 1990).  We present an alternative process to search, based on calculating the intersection  of sets of targets activated by two or more cues. Two methods of computing  the intersection are presented, one using information about the possible targets,  the other constraining the cue-target strengths in the memory matrix. Analysis  using orthogonal vectors to represent the cues and targets demonstrates the  competence of both processes, and simulations using sparse distributed  representations demonstrate the performance of the latter process for tasks  involving 2 and 3 cues."}
{"Type": "conference", "Year": "1990", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Flight Control in the Dragonfly", "Title": "A Neurobiological Simulation", "Abstract": "Neural network simulations of the dragonfly flight neurocontrol system  have  been  developed  to  understand  how  this  insect  uses  complex,  unsteady  aerodynamics.  The  simulation  networks  account  for  the  ganglionic  spatial  distribution  of  cells  as  well  as  the  physiologic  operating range and the stochastic cellular fIring history of each neuron.  In  addition  the  motor  neuron  firing  patterns,  \"flight  command  sequences\", were utilized. Simulation training was targeted against both  the  cellular  and  flight  motor  neuron  firing  patterns.  The  trained  networks  accurately  resynthesized  the  intraganglionic  cellular firing  patterns. These in  tum controlled the  motor neuron fIring patterns that  drive  wing  musculature  during  flight.  Such  networks  provide  both  neurobiological analysis tools and fIrst  generation controls for  the  use  of \"unsteady\" aerodynamics."}
{"Type": "conference", "Year": "1990", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Evolution and Learning in Neural Networks", "Title": "The Number and Distribution of Learning Trials Affect the Rate of Evolution", "Abstract": "Learning can increase the rate of evolution of a population of  biological organisms (the Baldwin effect). Our simulations  show that in a population of artificial neural networks  solving a pattern recognition problem, no learning or too  much learning leads to slow evolution of the genes whereas  an intermediate amount is optimal. Moreover, for a given  total number of training presentations, fastest evoution  occurs if different individuals within each generation receive  different numbers of presentations, rather than equal  numbers. Because genetic algorithms (GAs) help avoid  local minima in energy functions, our hybrid learning-GA  systems can be applied successfully to complex, high(cid:173) dimensional pattern recognition problems."}
{"Type": "conference", "Year": "1990", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "SEXNET", "Title": "A Neural Network Identifies Sex From Human Faces", "Abstract": "Sex identification in animals has biological importance. Humans are good  at making this determination visually, but machines have not matched  this ability. A neural network was trained to discriminate sex in human  faces, and performed as well as humans on a set of 90 exemplars. Images  sampled at 30x30 were compressed using a 900x40x900 fully-connected  back-propagation network; activities of hidden units served as input to a  back-propagation \"SexNet\" trained to produce values of 1 for male and  o for female faces. The network's average error rate of 8.1% compared  favorably to humans, who averaged 11.6%. Some SexNet errors mimicked  those of humans."}
{"Type": "conference", "Year": "1990", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "From Speech Recognition to Spoken Language Understanding", "Title": "The Development of the MIT SUMMIT and VOYAGER Systems", "Abstract": "Spoken language is one of the most natural, efficient, flexible, and econom(cid:173) ical means of communication among humans. As computers play an ever  increasing role in our lives, it is important that we address the issue of  providing a graceful human-machine interface through spoken language.  In this paper, we will describe our recent efforts in moving beyond the  scope of speech recognition into the realm of spoken-language understand(cid:173) ing. Specifically, we report on the development of an urban navigation and  exploration system called VOYAGER, an application which we have used as  a basis for performing research in spoken-language understanding."}
{"Type": "conference", "Year": "1990", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "RecNorm", "Title": "Simultaneous Normalisation and Classification applied to Speech Recognition", "Abstract": "A particular form of neural network is described, which has terminals  for acoustic patterns, class labels and speaker parameters. A method of  training this network to \"tune in\" the speaker parameters to a particular  speaker is outlined, based on a trick for converting a supervised network  to an unsupervised mode. We describe experiments using this approach  in isolated word recognition based on whole-word hidden Markov models.  The results indicate an improvement over speaker-independent perfor(cid:173) mance and, for unlabelled data, a performance close to that achieved on  labelled data."}
{"Type": "conference", "Year": "1990", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Analog Computation at a Critical Point", "Title": "A Novel Function for Neuronal Oscillations?", "Abstract": "\\Ve show that a simple spin system bia.sed at its critical point can en(cid:173) code spatial characteristics of external signals, sHch as the dimensions of  \"objects\" in the visual field. in the temporal correlation functions of indi(cid:173) vidual spins. Qualit.ative arguments suggest that regularly firing neurons  should be described by a planar spin of unit lengt.h. and such XY models  exhibit critical dynamics over a broad range of parameters. \\Ve show how  to extract these spins from spike trains and then mea'3ure t.he interaction  Hamilt.onian using simulations of small dusters of cells. Static correla(cid:173) tions among spike trains obtained from simulations of large arrays of cells  are in agreement with the predictions from these Hamiltonians, and dy(cid:173) namic correlat.ions display the predicted encoding of spatial information.  \\Ve suggest that this novel representation of object dinwnsions in temporal  correlations may be relevant t.o recent experiment.s on oscillatory neural  firing in the visual cortex."}
{"Type": "conference", "Year": "1990", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "The Devil and the Network", "Title": "What Sparsity Implies to Robustness and Memory", "Abstract": "1 Well, maybe an imp."}
{"Type": "conference", "Year": "1990", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Development and Spatial Structure of Cortical Feature Maps", "Title": "A Model Study", "Abstract": "Feature selective cells in  the primary visual cortex of several species are or(cid:173) ganized in hierarchical topographic maps of stimulus features like  \"position  in  visual  space\",  \"orientation\"  and\" ocular  dominance\".  In  order  to  un(cid:173) derstand and describe their spatial structure and their development, we  in(cid:173) vestigate a self-organizing neural network model based on the feature map  algorithm.  The  model  explains  map  formation  as  a  dimension-reducing  mapping  from  a  high-dimensional  feature  space  onto  a  two-dimensional  lattice,  such  that \"similarity\"  between features  (or feature  combinations)  is  translated  into  \"spatial  proximity\"  between  the  corresponding  feature  selective cells.  The model is  able to reproduce several aspects of the spatial  structure of cortical maps  in  the visual  cortex."}
{"Type": "conference", "Year": "1990", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Optimal Sampling of Natural Images", "Title": "A Design Principle for the Visual System", "Abstract": "We formulate the problem of optimizing the sampling of natural images  using an array of linear filters. Optimization of information capacity is  constrained by the noise levels of the individual channels and by a penalty  for the construction of long-range interconnections in the array. At low  signal-to-noise ratios the optimal filter characteristics correspond to bound  states of a Schrodinger equation in which the signal spectrum plays the  role of the potential. The resulting optimal filters are remarkably similar  to those observed in the mammalian visual cortex and the retinal ganglion  cells of lower vertebrates. The observed scale invariance of natural images  plays an essential role in this construction."}
{"Type": "conference", "Year": "1990", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "The Tempo 2 Algorithm", "Title": "Adjusting Time-Delays By Supervised Learning", "Abstract": "In this work we describe a new method that adjusts time-delays and the widths of  time-windows in artificial neural networks automatically.  The input of the units  are weighted by a gaussian input-window over time which allows the learning  rules for the delays and widths to be derived in the same way as it is used for the  weights.  Our results on a phoneme classification task compare well with results  obtained with the TDNN by Waibel et al., which was manually optimized for the  same task."}
