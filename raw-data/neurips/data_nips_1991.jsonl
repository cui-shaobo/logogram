{"Type": "conference", "Year": "1991", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Node Splitting", "Title": "A Constructive Algorithm for Feed-Forward Neural Networks", "Abstract": "A constructive algorithm is proposed for feed-forward neural networks,  which uses node-splitting in the hidden layers to build large networks from  smaller ones. The small network forms an approximate model of a set of  training data, and the split creates a larger more powerful network which is  initialised with the approximate solution already found. The insufficiency  of the smaller network in modelling the system which generated the data  leads to oscillation in those hidden nodes whose weight vectors cover re(cid:173) gions in the input space where more detail is required in the model. These  nodes are identified and split in two using principal component analysis,  allowing the new nodes t.o cover the two main modes of each oscillating  vector. Nodes are selected for splitting using principal component analysis  on the oscillating weight vectors, or by examining the Hessian matrix of  second derivatives of the network error with respect to the weight.s. The  second derivat.ive method can also be applied to the input layer, where it  provides a useful indication of t.he relative import.ances of parameters for  the classification t.ask. Node splitting in a standard Multi Layer Percep(cid:173) t.ron is equivalent to introducing a hinge in the decision boundary to allow  more detail to be learned. Initial results were promising, but further eval(cid:173) uation indicates that the long range effects of decision boundaries cause  the new nodes to slip back to the old node position, and nothing is gained.  This problem does not occur in networks of localised receptive fields such  as radial basis functions or gaussian mixtures, where the t.echnique appears  to work well."}
{"Type": "conference", "Year": "1991", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Principled Architecture Selection for Neural Networks", "Title": "Application to Corporate Bond Rating Prediction", "Abstract": "684"}
{"Type": "conference", "Year": "1991", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Learning in the Vestibular System", "Title": "Simulations of Vestibular Compensation Using Recurrent Back-Propagation", "Abstract": "Vestibular compensation is one of the oldest and most well studied paradigms in motor  learning.  Although it is neurophysiologically well described, the adaptive mechanisms  underlying  vestibular  compensation,  and  its  effects  on  the  dynamics  of vestibular  responses,  are still poorly understood.  The purpose of this study  is to gain insight into  the  compensatory  process  by  simulating  it  as  learning  in  a  recurrent  neural  network  model of the vestibulo-ocular reflex (VOR)."}
{"Type": "conference", "Year": "1991", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Repeat Until Bored", "Title": "A Pattern Selection Strategy", "Abstract": "An alternative to the typical technique of selecting  training examples  independently from a fixed distribution is fonnulated and analyzed, in  which the current example is presented repeatedly until the error for that  item is reduced to  some criterion value,  ~; then,  another  item  is ran(cid:173) domly selected.  The convergence time can be dramatically increased or  decreased by this heuristic, depending on the task, and is very sensitive  to the value of ~."}
{"Type": "conference", "Year": "1991", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Against Edges", "Title": "Function Approximation with Multiple Support Maps", "Abstract": "Networks for reconstructing a sparse or noisy function often use an edge  field to segment the function into homogeneous regions, This approach  assumes that these regions do not overlap or have disjoint parts, which is  often false. For example, images which contain regions split by an occlud(cid:173) ing object can't be properly reconstructed using this type of network. We  have developed a network that overcomes these limitations, using support  maps to represent the segmentation of a signal. In our approach, the sup(cid:173) port of each region in the signal is explicitly represented. Results from  an initial implementation demonstrate that this method can reconstruct  images and motion sequences which contain complicated occlusion."}
{"Type": "conference", "Year": "1991", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Locomotion in a Lower Vertebrate", "Title": "Studies of the Cellular Basis of Rhythmogenesis and Oscillator Coupling", "Abstract": "To test  whether  the  known  connectivies  of neurons  in  the lamprey spinal  cord are sufficient to account for locomotor rhythmogenesis, a  CCconnection(cid:173) ist\"  neural network simulation was done using identical cells connected ac(cid:173) cording to experimentally established  patterns.  It was  demonstrated  that  the  network  oscillates  in  a  stable  manner  with  the  same  phase  relation(cid:173) ships among the neurons as observed  in the lamprey.  The model was  then  used  to  explore  coupling  between  identical"}
{"Type": "conference", "Year": "1991", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Single Neuron Model", "Title": "Response to Weak Modulation in the Presence of Noise", "Abstract": "We consider a noisy bist.able single neuron model driven by a periodic  external modulation. The modulation introduces a correlated switching  between st.ates driven by the noise. The information flow through the sys(cid:173) tem from the modulation to the output switching events, leads to a succes(cid:173) sion of strong peaks in the power spectrum. The signal-to-noise ratio (SNR)  obtained from this power spectrum is a measure of the information content  in the neuron response . With increasing noise intensity, the SNR passes  t.hrough a maximum, an effect which has been called stochastic resonance.  We treat t.he problem wit.hin the framework of a recently developed approx(cid:173) imate theory, valid in the limits of weak noise intensity, weak periodic forc(cid:173) ing and low forcing frequency. A comparison of the results of this theory  with those obtained from a linear syst.em FFT is also presented ."}
{"Type": "conference", "Year": "1991", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Self-organization in real neurons", "Title": "Anti-Hebb in 'Channel Space'?", "Abstract": "Ion channels are the dynamical systems of the nervous system. Their  distribution within the membrane governs not only communication of in(cid:173) formation between neurons, but also how that information is integrated  within the cell. Here, an argument is presented for an 'anti-Hebbian' rule  for changing the distribution of voltage-dependent ion channels in order  to flatten voltage curvatures in dendrites. Simulations show that this rule  can account for the self-organisation of dynamical receptive field properties  such as resonance and direction selectivity. It also creates the conditions  for the faithful conduction within the cell of signals to which the cell has  been exposed. Various possible cellular implementations of such a learn(cid:173) ing rule are proposed, including activity-dependent migration of channel  proteins in the plane of the membrane."}
{"Type": "conference", "Year": "1991", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "JANUS", "Title": "Speech-to-Speech Translation Using Connectionist and Non-Connectionist Techniques", "Abstract": "â€¢ Also with University of Karlsruhe, Karlsruhe. Germany.  1N\"ow with Alliant Techsystems Research and Technology Center. Hopkins. Minnesota."}
{"Type": "conference", "Year": "1991", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "VISIT", "Title": "A Neural Model of Covert Visual Attention", "Abstract": "Visual attention is the ability to dynamically restrict processing to a subset  of the visual field.  Researchers  have long argued that such a  mechanism is  necessary  to efficiently perform many intermediate level visual tasks.  This  paper describes  VISIT,  a  novel  neural  network  model  of visual attention.  The current system models the search for target objects in scenes  contain(cid:173) ing  multiple distractors.  This  is  a  natural  task  for  people,  it  is  studied  extensively by psychologists,  and it requires  attention.  The network's be(cid:173) havior  closely  matches  the  known  psychophysical  data  on  visual  search  and visual  attention.  VISIT also  matches much of the  physiological data  on attention and provides a  novel view  of the functionality of a  number of  visual areas.  This paper concentrates  on  the  biological plausibility of the  model and its relationship to the primary visual cortex, pulvinar, superior  colliculus and posterior  parietal areas."}
{"Type": "conference", "Year": "1991", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Operators and curried functions", "Title": "Training and analysis of simple recurrent networks", "Abstract": "We present a framework for programming tbe bidden unit representations of  simple recurrent networks based on the use of hint units (additional targets at  the output layer). We present two ways of analysing a network trained within  this framework: Input patterns act as operators on the information encoded by  the context units; symmetrically, patterns of activation over tbe context units  act as curried functions of the input sequences. Simulations demonstrate that a  network can learn to represent three different functions simultaneously and  canonical discriminant analysis is used to investigate bow operators and curried  functions are represented in the space of bidden unit activations."}
{"Type": "conference", "Year": "1991", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Benchmarking Feed-Forward Neural Networks", "Title": "Models and Measures", "Abstract": "Existing metrics for the learning performance of feed-forward neural networks do  not provide a satisfactory basis for comparison because the choice of the training  epoch limit can determine the results of the comparison.  I propose new metrics  which  have  the  desirable property of being  independent of the  training epoch  limit.  The efficiency measures  the yield of correct networks in proportion to the  training effort expended.  The optimal epoch limit provides the greatest efficiency.  The learning performance is modelled statistically, and asymptotic performance  is estimated.  Implementation details may be found in (Harney,  1992)."}
{"Type": "conference", "Year": "1991", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Models Wanted", "Title": "Must Fit Dimensions of Sleep and Dreaming", "Abstract": "During waking and sleep, the brain and mind undergo a  tightly linked and  precisely  specified  set  of changes  in state.  At  the  level  of neurons,  this  process  has  been  modeled  by  variations  of Volterra-Lotka  equations  for  cyclic fluctuations of brainstem cell populations.  However, neural network  models based upon rapidly developing knowledge ofthe specific population  connectivities  and  their  differential responses  to drugs  have  not  yet  been  developed.  Furthermore, only  the  most  preliminary  attempts  have  been  made  to model across states.  Some  of our own attempts  to link rapid eye  movement (REM) sleep neurophysiology and dream cognition using neural  network approaches  are summarized in this  paper."}
{"Type": "conference", "Year": "1991", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Neural Control for Rolling Mills", "Title": "Incorporating Domain Theories to Overcome Data Deficiency", "Abstract": "In  a  Bayesian  framework,  we  give  a  principled  account  of how  domain(cid:173) specific prior knowledge such  as imperfect analytic domain theories can be  optimally  incorporated  into  networks  of locally-tuned  units:  by  choosing  a  specific  architecture  and  by  applying  a  specific  training  regimen.  Our  method  proved  successful  in  overcoming  the  data  deficiency  problem  in  a  large-scale  application  to  devise  a  neural  control  for  a  hot  line  rolling  mill.  It  achieves  in  this  application  significantly  higher  accuracy  than  optimally-tuned standard  algorithms such  as  sigmoidal backpropagation,  and  outperforms the state-of-the-art solution."}
{"Type": "conference", "Year": "1991", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "HARMONET", "Title": "A Neural Net for Harmonizing Chorales in the Style of J. S. Bach", "Abstract": "HARMONET, a system employing connectionist networks for music pro(cid:173) cessing, is presented. After being trained on some dozen Bach chorales  using error backpropagation, the system is capable of producing four-part  chorales in the style of J .s.Bach, given a one-part melody. Our system  solves a musical real-world problem on a performance level appropriate  for musical practice. HARMONET's power is based on (a) a new coding  scheme capturing musically relevant information and (b) the integration of  backpropagation and symbolic algorithms in a hierarchical system, com(cid:173) bining the advantages of both."}
{"Type": "conference", "Year": "1991", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Network generalization for production", "Title": "Learning and producing styled letterforms", "Abstract": "We designed and trained a connectionist network to generate  letterfonns in a new font given just a few exemplars from  that font. During learning. our network constructed a  distributed internal representation of fonts as well as letters.  despite the fact that each training instance exemplified both a  font and a letter. It was necessary to have separate but  interconnected hidden units for \" letter\" and \"font\"  representations - several alternative architectures were not  successful."}
{"Type": "conference", "Year": "1991", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Retinogeniculate Development", "Title": "The Role of Competition and Correlated Retinal Activity", "Abstract": "During visual development, projections from retinal ganglion cells  (RGCs) to the lateral geniculate nucleus (LGN) in cat are refined to  produce ocular dominance layering and precise topographic mapping.  Normal development depends upon activity in RGCs, suggesting a key  role for activity-dependent synaptic plasticity. Recent experiments on  prenatal retina show that during early development, \"waves\" of activity  pass across RGCs (Meister, et aI., 1991). We provide the first  simulations to demonstrate that such retinal waves, in conjunction with  Hebbian synaptic competition and early arrival of contralateral axons,  can account for observed patterns of retinogeniculate projections in  normal and experimentally-treated animals."}
{"Type": "conference", "Year": "1991", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Time-Warping Network", "Title": "A Hybrid Framework for Speech Recognition", "Abstract": "recognition  system.  and  we  propose"}
{"Type": "conference", "Year": "1991", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "The Effective Number of Parameters", "Title": "An Analysis of Generalization and Regularization in Nonlinear Learning Systems", "Abstract": "lCPE and Peff(>\")  were previously  introduced  in  Moody  (1991)."}
{"Type": "conference", "Year": "1991", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Gradient Descent", "Title": "Second Order Momentum and Saturating Error", "Abstract": "Batch gradient descent, ~w(t) = -7JdE/dw(t) , conver~es to a minimum  of quadratic form with a time constant no better than '4Amax/ Amin where  Amin and Amax are the minimum and maximum eigenvalues of the Hessian  matrix of E with respect to w.  It was recently shown that adding a  momentum term ~w(t) = -7JdE/dw(t) + Q'~w(t - 1) improves this to  ~ VAmax/ Amin, although only in the batch case. Here we show that second(cid:173) order momentum, ~w(t) = -7JdE/dw(t) + Q'~w(t -1) + (3~w(t - 2), can  lower this no further. We then regard gradient descent with momentum  as a dynamic system and explore a non quadratic error surface, showing  that saturation of the error accounts for a variety of effects observed in  simulations and justifies some popular heuristics."}
{"Type": "conference", "Year": "1991", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Interpretation of Artificial Neural Networks", "Title": "Mapping Knowledge-Based Neural Networks into Rules", "Abstract": "We propose and empirically evaluate a method for the extraction of expert(cid:173) comprehensible rules from trained neural networks. Our method operates in  the context of a three-step process for learning that uses rule-based domain  knowledge in combination with neural networks. Empirical tests using real(cid:173) worlds problems from molecular biology show that the rules our method extracts  from trained neural networks: closely reproduce the accuracy of the network  from which they came, are superior to the rules derived by a learning system that  directly refines symbolic rules, and are expert-comprehensible."}
{"Type": "conference", "Year": "1991", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "The Clusteron", "Title": "Toward a Simple Abstraction for a Complex Neuron", "Abstract": "Are  single  neocortical  neurons  as  powerful as  multi-layered  networks?  A  recent  compartmental modeling study  has shown  that voltage-dependent  membrane  nonlinearities  present  in  a  complex dendritic  tree  can  provide  a  virtual layer of local  nonlinear processing  elements between synaptic in(cid:173) puts  and  the  final  output  at  the  cell  body,  analogous  to  a  hidden  layer  in  a  multi-layer  network.  In  this  paper,  an  abstract  model  neuron  is  in(cid:173) troduced,  called  a  clusteron,  which  incorporates  aspects  of the  dendritic  \"cluster-sensitivity\"  phenomenon seen  in these  detailed  biophysical mod(cid:173) eling  studies.  It is  shown,  using  a  clusteron,  that  a  Hebb-type  learning  rule  can  be  used  to  extract  higher-order  statistics  from  a  set  of  train(cid:173) ing patterns,  by manipulating the spatial ordering of synaptic connections  onto  the  dendritic  tree.  The  potential neurobiological  relevance  of these  higher-order statistics for  nonlinear pattern discrimination is then studied  within a  full  compartmental model  of a  neocortical  pyramidal cell,  using  a  training set  of 1000 high-dimensional sparse random patterns."}
{"Type": "conference", "Year": "1991", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Reverse TDNN", "Title": "An Architecture For Trajectory Generation", "Abstract": "The backpropagation algorithm can be used for both recognition and gen(cid:173) eration of time trajectories. When used as a recognizer, it has been shown  that the performance of a network can be greatly improved by adding  structure to the architecture. The same is true in trajectory generation.  In particular a new architecture corresponding to a \"reversed\" TDNN is  proposed. Results show dramatic improvement of performance in the gen(cid:173) eration of hand-written characters. A combination of TDNN and reversed  TDNN for compact encoding is also suggested."}
