{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Spatial Organization of Neural Networks", "Title": "A Probabilistic Modeling Approach", "Abstract": "The  aim  of  this  paper  is  to  explore  the  spatial  organization  of  neural  networks  under  Markovian  assumptions,  in  what  concerns  the be(cid:173) haviour  of  individual  cells  and  the  interconnection  mechanism.  Space(cid:173) organizational  properties  of  neural  nets  are  very  relevant  in  image  modeling  and  pattern  analysis,  where  spatial  computations  on  stocha(cid:173) stic  two-dimensional  image  fields  are  involved.  As  a  first  approach  we  develop  a  random  neural  network  model,  based  upon  simple  probabi(cid:173) listic  assumptions,  whose  organization  is  studied  by  means  of  dis(cid:173) crete-event  simulation.  We  then  investigate  the  possibility  of  ap(cid:173) proXimating  the  random  network's  behaviour  by  using  an  analytical  ap(cid:173) proach  originating  from  the  theory  of  general  product-form  queueing  networks.  The  neural  network  is  described  by  an  open  network  of  no(cid:173) des,  in  which  customers  moving  from  node  to  node  represent  stimula(cid:173) tions  and  connections  between  nodes  are  expressed  in  terms  of  sui(cid:173) tably  selected  routing  probabilities.  We  obtain  the  solution  of  the  model  under  different  disciplines  affecting  the  time  spent  by  a  sti(cid:173) mulation  at  each  node  visited.  Results  concerning  the  distribution  of  excitation  in  the  network  as  a  function  of  network  topology  and  external  stimulation  arrival  pattern  are  compared  with  measures  ob(cid:173) tained  from  the  simulation  and  validate  the  approach  followed."}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Optimization with Artificial Neural Network Systems", "Title": "A Mapping Principle and a Comparison to Gradient Based Methods", "Abstract": "equations  associated with artificial  neural  networks  are  presented.  A comparison is  made  to  optim(cid:173) ization using gradient-search methods.  The perfonnance  measure  is  the  settling time  from  an  initial  state  to  a  target  state.  A  simple  analytical  example  illustrates  a situation  where  dynamical  systems  representing  artificial  neural  network  methods  would  settle  faster  than  those  representing  gradient(cid:173) search.  Settling  time  was  investigated  for  a  more  complicated  optimization  problem  using  com(cid:173) puter  simulations.  The  problem  was  a  simplified  version  of a problem  in  medical  imaging:  deter(cid:173) mining  loci  of cerebral  activity  from  electromagnetic  measurements  at  the  scalp.  The  simulations  showed  that  gradient  based  systems  typically  settled  50  to  100  times  faster  than  systems  based  on  current neural  network optimization methods."}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "A Computer Simulation of Cerebral Neocortex", "Title": "Computational Capabilities of Nonlinear Neural Networks", "Abstract": "Abstract Unavailable"}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Cycles", "Title": "A Simulation Tool for Studying Cyclic Neural Networks", "Abstract": "networks containing cyclic connection paths with  the aid of a  powerful graphics(cid:173) based interface.  Numerous cycles have been studied, including cycles with one or  more activation points, non-interruptible cycles, cycles with variable path lengths,  and interacting cycles.  The final  class,  interacting cycles,  is  important due to its  ability to implement time-dependent goal processing in neural networks."}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "HOW THE CATFISH TRACKS ITS PREY", "Title": "AN INTERACTIVE \"PIPELINED\" PROCESSING SYSTEM MAY DIRECT FORAGING VIA RETICULOSPINAL NEURONS", "Abstract": "of"}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "MURPHY", "Title": "A Robot that Learns by Doing", "Abstract": "MURPHY consists of a camera looking at a robot arm, with a connectionist network  architecture situated in between. By moving its arm through a small, representative  sample of the 1 billion possible joint configurations, MURPHY learns the relationships,  backwards and forwards, between the positions of its joints and the state of its visual field.  MURPHY can use its internal model in the forward direction to \"envision\" sequences  of actions for planning purposes, such as in grabbing a visually presented object, or in  the reverse direction to \"imitate\", with its arm, autonomous activity in its visual field.  Furthermore, by taking explicit advantage of continuity in the mappings between visual  space and joint space, MURPHY is able to learn non-linear mappings with only a single  layer of modifiable weights."}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "An Artificial Neural Network for Spatio-Temporal Bipolar Patterns", "Title": "Application to Phoneme Classification", "Abstract": "An  artificial  neural  network  is  developed  to  recognize  spatio-temporal  bipolar patterns  associatively.  The  function  of a formal  neuron is  generalized by  replacing  multiplication  with  convolution,  weights  with  transfer  functions,  and  thresholding  with  nonlinear  transform  following  adaptation.  The Hebbian  learn(cid:173) ing  rule  and  the  delta  learning  rule  are  generalized  accordingly,  resulting  in  the  learning  of weights  and  delays.  The  neural  network  which  was  first  developed  for  spatial  patterns  was  thus  generalized  for  spatio-temporal  patterns.  It  was  tested  using  a  set  of bipolar input patterns  derived from  speech  signals,  showing  robust classification of 30 model phonemes."}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Teaching Artificial Neural Systems to Drive", "Title": "Manual Training Techniques for Autonomous Systems", "Abstract": "Abstract Unavailable"}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Neural Networks for Template Matching", "Title": "Application to Real-Time Classification of the Action Potentials of Real Neurons", "Abstract": "extracellulary  sampled  neural  signals  (i .e.  action  potentials)  recorded  from  the  brains  of ex(cid:173) perimental animals.  In  most  neurophysiology  laboratories this classification  task  is  simplified  by  limiting  investigations  to  single,  electrically  well-isolated  neurons recorded  one  at  a  time.  However, for those interested in sampling the activities of many single neurons simultaneously,  waveform  classification  becomes  a  serious  concern.  In  this  paper  we  describe  and  constrast  three  approaches  to  this  problem  each  designed  not  only  to  recognize  isolated  neural  events,  but also  to separately classify temporally overlapping events in real time.  First we  present two  formulations  of  waveform  classification  using  a  neural network  template  matching  approach.  These  two  formulations  are  then  compared  to  a  simple  template  matching  implementation.  Analysis with real neural signals reveals  that simple template matching is  a  better solution to  this  problem  than either neural network  approach."}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Minkowski-r Back-Propagation", "Title": "Learning in Connectionist Models with Non-Euclidian Error Signals", "Abstract": "Many connectionist learning models are implemented using a gradient descent  in a least squares error function of the output and teacher signal.  The present model  Fneralizes. in particular. back-propagation [1]  by using Minkowski-r power metrics.  For  small  r's  a  \"city-block\"  error  metric  is  approximated  and  for  large  r's  the  \"maximum\" or \"supremum\"  metric is  approached.  while  for r=2  the  standard  back(cid:173) propagation  model  results.  An  implementation  of Minkowski-r back-propagation  is  described.  and  several  experiments  are  done  which  show  that  different values  of r  may be desirable for various purposes. Different r values may be appropriate for the  reduction  of  the  effects  of outliers  (noise).  modeling  the  input  space  with  more  compact clusters. or modeling  the statistics of a particular domain more naturally or  in a way that may be more perceptually or psychologically meaningful (e.g. speech or  vision)."}
