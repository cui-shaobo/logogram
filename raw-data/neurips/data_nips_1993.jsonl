{"Type": "conference", "Year": "1993", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Hoeffding Races", "Title": "Accelerating Model Selection Search for Classification and Function Approximation", "Abstract": "Selecting  a good  model of a set  of input points by  cross  validation  is  a  computationally intensive  process,  especially  if the  number of  possible  models  or  the  number  of training  points  is  high.  Tech(cid:173) niques  such  as  gradient  descent  are  helpful  in  searching  through  the space of models,  but problems such  as  local minima, and more  importantly, lack  of a  distance  metric  between  various  models re(cid:173) duce  the applicability of these search  methods.  Hoeffding  Races  is  a  technique  for  finding  a  good  model  for  the  data by  quickly  dis(cid:173) carding bad models, and concentrating the computational effort  at  differentiating between  the better  ones.  This paper focuses  on  the  special  case  of leave-one-out  cross  validation  applied  to  memory(cid:173) based  learning  algorithms,  but  we  also  argue  that  it is  applicable  to any  class  of model selection  problems."}
{"Type": "conference", "Year": "1993", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Putting It All Together", "Title": "Methods for Combining Neural Networks", "Abstract": "Abstract Unavailable"}
{"Type": "conference", "Year": "1993", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Learning Curves", "Title": "Asymptotic Values and Rate of Convergence", "Abstract": "1"}
{"Type": "conference", "Year": "1993", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "How to Describe Neuronal Activity", "Title": "Spikes, Rates, or Assemblies?", "Abstract": "What  is  the  'correct'  theoretical  description  of neuronal  activity?  The  analysis  of the  dynamics  of a  globally  connected  network  of  spiking neurons  (the Spike Response  Model)  shows  that a  descrip(cid:173) tion  by  mean firing  rates  is  possible  only if active  neurons  fire  in(cid:173) coherently.  If firing  occurs  coherently  or  with spatio-temporal cor(cid:173) relations,  the  spike  structure of the neural  code  becomes  relevant.  Alternatively, neurons can  be gathered into local or distributed en(cid:173) sembles or 'assemblies'.  A description  based on the mean ensemble  activity is,  in principle, possible but the interaction  between  differ(cid:173) ent assemblies becomes highly nonlinear.  A description with spikes  should  therefore  be  preferred."}
{"Type": "conference", "Year": "1993", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Robot Learning", "Title": "Exploration and Continuous Domains", "Abstract": "Abstract Unavailable"}
{"Type": "conference", "Year": "1993", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Catastrophic interference in connectionist networks", "Title": "Can It Be predicted, can It be prevented?", "Abstract": "Abstract Unavailable"}
{"Type": "conference", "Year": "1993", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Identifying Fault-Prone Software Modules Using Feed-Forward Networks", "Title": "A Case Study", "Abstract": "Functional complexity of a software module can be measured in  terms of static complexity metrics of the program text. Classify(cid:173) ing software modules, based on their static complexity measures,  into different fault-prone categories is a difficult problem in soft(cid:173) ware engineering. This research investigates the applicability of  neural network classifiers for identifying fault-prone software mod(cid:173) ules using a data set from a commercial software system. A pre(cid:173) liminary empirical comparison is performed between a minimum  distance based Gaussian classifier, a perceptron classifier and a  multilayer layer feed-forward network classifier constructed using  a modified Cascade-Correlation algorithm. The modified version  of the Cascade-Correlation algorithm constrains the growth of the  network size by incorporating a cross-validation check during the  output layer training phase. Our preliminary results suggest that  a multilayer feed-forward network can be used as a tool for iden(cid:173) tifying fault-prone software modules early during the development  cycle. Other issues such as representation of software metrics and  selection of a proper training samples are also discussed."}
{"Type": "conference", "Year": "1993", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "The \"Softmax\" Nonlinearity", "Title": "Derivation Using Statistical Mechanics and Useful Properties as a Multiterminal Analog Circuit Element", "Abstract": "We use mean-field theory methods from Statistical Mechanics to  derive the \"softmax\" nonlinearity from the discontinuous winner(cid:173) take-all (WTA) mapping. We give two simple ways of implementing  \"soft max\" as a multiterminal network element. One of these has a  number of important network-theoretic properties. It is a recipro(cid:173) cal, passive, incrementally passive, nonlinear, resistive multitermi(cid:173) nal element with a content function having the form of information(cid:173) theoretic entropy. These properties should enable one to use this  element in nonlinear RC networks with such other reciprocal el(cid:173) ements as resistive fuses and constraint boxes to implement very  high speed analog optimization algorithms using a minimum of  hardware."}
{"Type": "conference", "Year": "1993", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Fool's Gold", "Title": "Extracting Finite State Machines from Recurrent Network Dynamics", "Abstract": "Several recurrent networks have been proposed as representations for the  task of formal language learning. After training a recurrent network rec(cid:173) ognize a formal  language or predict the next symbol of a sequence, the  next logical step is to  understand the information processing carried out  by  the  network.  Some researchers have  begun  to  extracting  finite  state  machines from the internal state trajectories of their recurrent networks.  This  paper describes  how  sensitivity  to  initial  conditions  and  discrete  measurements can trick these extraction methods to return illusory finite  state descriptions."}
{"Type": "conference", "Year": "1993", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Statistics of Natural Images", "Title": "Scaling in the Woods", "Abstract": "In  order  to  best  understand  a  visual  system  one  should  attempt  to characterize  the natural images it processes.  We  gather  images  from the woods and find that these scenes possess an ensemble scale  invariance.  Further,  they  are  highly  non-Gaussian,  and  this  non(cid:173) Gaussian  character  cannot  be  removed  through local  linear filter(cid:173) ing.  We find  that including a simple  \"gain control\"  nonlinearity in  the filtering  process  makes the filter  output quite Gaussian, mean(cid:173) ing information is maximized at fixed  channel variance.  Finally, we  use  the measured power spectrum to place  an upper bound on the  information conveyed about natural scenes by an array of receptors."}
{"Type": "conference", "Year": "1993", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "GDS", "Title": "Gradient Descent Generation of Symbolic Classification Rules", "Abstract": "Imagine you have designed a neural network that successfully learns  a complex classification task.  What are the relevant input features  the classifier relies on and how  are these features  combined to pro(cid:173) duce  the  classification  decisions?  There  are  applications  where  a  deeper  insight  into  the  structure  of an  adaptive  system  and  thus  into the underlying classification problem may well be as important  as  the  system's  performance  characteristics,  e.g.  in  economics  or  medicine.  GDSi  is  a  backpropagation-based  training scheme  that  produces networks  transformable into an equivalent and concise  set  of IF-THEN rules.  This is achieved by  imposing penalty terms  on the network parameters that adapt the network to the expressive  power of this class of rules.  Thus during training we simultaneously  minimize classification  and  transformation error.  Some real-world  tasks  demonstrate the viability of our approach."}
{"Type": "conference", "Year": "1993", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Packet Routing in Dynamically Changing Networks", "Title": "A Reinforcement Learning Approach", "Abstract": "This  paper  describes  the  Q-routing  algorithm for  packet  routing,  in  which  a  reinforcement  learning  module  is  embedded  into  each  node  of a  switching  network.  Only  local  communication  is  used  by each node to keep  accurate statistics on which routing decisions  lead  to  minimal  delivery  times.  In  simple  experiments  involving  a  36-node,  irregularly  connected  network,  Q-routing  proves  supe(cid:173) rior  to  a  nonadaptive  algorithm  based  on  precomputed  shortest  paths  and  is  able  to route  efficiently  even  when  critical  aspects  of  the  simulation,  such  as  the  network  load,  are  allowed  to  vary  dy(cid:173) namically.  The  paper  concludes  with  a  discussion  of the  tradeoff  between  discovering  shortcuts  and  maintaining stable policies."}
{"Type": "conference", "Year": "1993", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Bayesian Backprop in Action", "Title": "Pruning, Committees, Error Bars and an Application to Spectroscopy", "Abstract": "If several theories  account  for  a phenomenon  we  should  prefer the  simplest  which  describes  the  data  sufficiently  well."}
{"Type": "conference", "Year": "1993", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "What Does the Hippocampus Compute?", "Title": "A Precis of the 1993 NIPS Workshop", "Abstract": "Abstract Unavailable"}
{"Type": "conference", "Year": "1993", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Credit Assignment through Time", "Title": "Alternatives to Backpropagation", "Abstract": "Learning  to  recognize  or  predict  sequences  using  long-term  con(cid:173) text  has  many  applications.  However,  practical  and  theoretical  problems  are  found  in  training  recurrent  neural  networks  to  per(cid:173) form tasks in which input/output dependencies span long intervals.  Starting from a mathematical analysis of the  problem, we  consider  and compare alternative algorithms and  architectures  on  tasks for  which the span of the input/output dependencies can be controlled.  Results  on the new  algorithms show  performance qualitatively su(cid:173) perior  to that obtained with backpropagation."}
{"Type": "conference", "Year": "1993", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Learning Complex Boolean Functions", "Title": "Algorithms and Applications", "Abstract": "The most commonly used neural network models are not well suited  to direct digital implementations because each node needs to per(cid:173) form a large number of operations between floating point values.  Fortunately, the ability to learn from examples and to generalize is  not restricted to networks ofthis type. Indeed, networks where each  node implements a simple Boolean function (Boolean networks) can  be designed in such a way as to exhibit similar properties. Two  algorithms that generate Boolean networks from examples are pre(cid:173) sented. The results show that these algorithms generalize very  well in a class of problems that accept compact Boolean network  descriptions. The techniques described are general and can be ap(cid:173) plied to tasks that are not known to have that characteristic. Two  examples of applications are presented: image reconstruction and  hand-written character recognition."}
{"Type": "conference", "Year": "1993", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Odor Processing in the Bee", "Title": "A Preliminary Study of the Role of Central Input to the Antennal Lobe", "Abstract": "Based  on  precise  anatomical  data  of the  bee's  olfactory  system,  we  propose an investigation of the possible mechanisms of modulation and  control between the two levels of olfactory  information processing:  the  antennallobe glomeruli  and  the  mushroom  bodies.  We use  simplified  neurons,  but realistic  architecture.  As  a  first  conclusion,  we  postulate  that the feature extraction performed by the antennallobe (glomeruli and  interneurons)  necessitates  central  input from  the  mushroom bodies for  fine  tuning. The central input thus  facilitates  the evolution from fuzzy  olfactory images in the glomerular layer towards more focussed  images  upon odor presentation."}
{"Type": "conference", "Year": "1993", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Lipreading by neural networks", "Title": "Visual preprocessing, learning, and sensory integration", "Abstract": "We have developed visual preprocessing algorithms for extracting  phonologically relevant features from the grayscale video image of  a speaker, to provide speaker-independent inputs for an automat(cid:173) ic lipreading (\"speechreading\") system. Visual features such as  mouth open/closed, tongue visible/not-visible, teeth visible/not(cid:173) visible, and several shape descriptors of the mouth and its motion  are all rapidly computable in a manner quite insensitive to lighting  conditions. We formed a hybrid speechreading system consisting  of two time delay neural networks (video and acoustic) and inte(cid:173) grated their responses by means of independent opinion pooling  - the Bayesian optimal method given conditional independence,  which seems to hold for our data. This hybrid system had an er(cid:173) ror rate 25% lower than that of the acoustic subsystem alone on a  five-utterance speaker-independent task, indicating that video can  be used to improve speech recognition."}
{"Type": "conference", "Year": "1993", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Optimal Brain Surgeon", "Title": "Extensions and performance comparisons", "Abstract": "We  extend  Optimal  Brain  Surgeon  (OBS)  - to  allow  for  general  error  mea(cid:173) method  for  pruning  networks  - sures, and explore a reduced computational and storage implemen(cid:173) tation  via  a  dominant  eigenspace  decomposition.  Simulations  on  nonlinear,  noisy  pattern  classification  problems  reveal  that  OBS  does  lead  to  improved  generalization,  and  performs  favorably  in  comparison with Optimal Brain Damage (OBD).  We  find  that the  required  retraining  steps  in  OBD  may  lead  to inferior  generaliza(cid:173) tion, a result that can be interpreted as due to injecting noise back  into the system.  A common technique is to stop training of a large  network at the minimum validation error.  We  found  that the test  error  could  be  reduced  even  further  by  means  of  OBS  (but  not  OBD)  pruning.  Our  results justify the t  ~ 0  approximation  used  in  OBS  and  indicate  why  retraining  in  a  highly  pruned  network  may lead to inferior  performance."}
{"Type": "conference", "Year": "1993", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "WATTLE", "Title": "A Trainable Gain Analogue VLSI Neural Network", "Abstract": "This paper describes a low power analogue VLSI neural network  called Wattle. Wattle is a 10:6:4 three layer perceptron with multi(cid:173) plying DAC synapses and on chip switched capacitor neurons fabri(cid:173) cated in 1.2um CMOS. The on chip neurons facillitate variable gain  per neuron and lower energy/connection than for previous designs.  The intended application of this chip is Intra Cardiac Electrogram  classification as part of an implantable pacemaker / defibrillator sys(cid:173) tem. Measurements of t.he chip indicate that 10pJ per connection  is achievable as part of an integrated system. Wattle has been suc(cid:173) cessfully trained in loop on parity 4 and ICEG morphology classi(cid:173) fication problems."}
{"Type": "conference", "Year": "1993", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Tonal Music as a Componential Code", "Title": "Learning Temporal Relationships Between and Within Pitch and Timing Components", "Abstract": "This study explores the extent to which a network that learns the  temporal relationships within and between the component features of  Western tonal music can account for music theoretic and psychological  phenomena such as the tonal hierarchy and rhythmic expectancies.  Predicted and generated sequences were recorded as the representation of  a 153-note waltz melody was learnt by a predictive, recurrent network.  The network learned transitions and relations between and within pitch  and timing components: accent and duration values interacted in the  development of rhythmic and metric structures and, with training, the  network developed chordal expectancies in response to the activation of  individual tones. Analysis of the hidden unit representation revealed  that musical sequences are represented as transitions between states in  hidden unit space."}
{"Type": "conference", "Year": "1993", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Learning in Compositional Hierarchies", "Title": "Inducing the Structure of Objects from Data", "Abstract": "I  propose a learning algorithm for  learning hierarchical  models  for ob(cid:173) ject recognition.  The  model  architecture  is  a  compositional  hierarchy  that  represents  part-whole relationships:  parts  are  described  in  the lo(cid:173) cal  context of substructures  of the  object.  The  focus  of this  report  is  inducing  the  structure  of  learning  hierarchical  models  from  data,  i.e.  model  prototypes from  observed exemplars of an  object.  At each  node  in the hierarchy, a probability distribution governing its parameters must  be learned.  The connections between  nodes reflects  the structure of the  object.  The  formulation of substructures  is  encouraged  such  that  their  parts  become  conditionally  independent.  The  resulting  model  can  be  interpreted  as  a  Bayesian  Belief Network  and  also  is  in  many  respects  similar to the stochastic visual grammar described by Mjolsness."}
