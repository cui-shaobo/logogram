{"Type": "conference", "Year": "1998", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Probabilistic Modeling for Face Orientation Discrimination", "Title": "Learning from Labeled and Unlabeled Data", "Abstract": "This paper presents probabilistic modeling methods to solve the problem of dis(cid:173) criminating between five facial  orientations with  very  little labeled data.  Three  models are explored. The first model maintains no inter-pixel dependencies, the  second model is capable of modeling a set of arbitrary pair-wise dependencies,  and the last model allows  dependencies  only  between  neighboring pixels. We  show that for all three of these models, the accuracy of the learned models can  be greatly improved by  augmenting a small number of labeled training images  with  a  large  set of unlabeled  images using  Expectation-Maximization.  This  is  important because it is often difficult to obtain image labels, while many unla(cid:173) beled images  are  readily  available.  Through  a  large  set  of empirical  tests,  we  examine the benefits  of unlabeled data  for  each  of the  models.  By  using only  two randomly selected labeled examples per class, we can discriminate between  the five  facial orientations with an accuracy of 94%; with six labeled examples,  we achieve an accuracy of 98%."}
{"Type": "conference", "Year": "1998", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Boxlets", "Title": "A Fast Convolution Algorithm for Signal Processing and Neural Networks", "Abstract": "Signal  processing  and  pattern  recognition  algorithms make  exten(cid:173) sive  use  of convolution.  In  many cases, computational accuracy  is  not  as  important  as  computational  speed.  In  feature  extraction,  for  instance,  the  features  of interest  in  a  signal  are  usually  quite  distorted.  This form of noise justifies some level of quantization in  order  to  achieve  faster  feature  extraction .  Our  approach  consists  of approximating regions  of the  signal  with  low  degree  polynomi(cid:173) als,  and then differentiating the resulting signals in order to obtain  impulse functions  (or derivatives of impulse functions).  With this  representation,  convolution  becomes  extremely  simple and  can  be  implemented quite effectively.  The true  convolution can  be  recov(cid:173) ered  by  integrating  the  result  of  the  convolution.  This  method  yields  substantial speed  up  in feature  extraction  and is  applicable  to convolutional neural  networks."}
{"Type": "conference", "Year": "1998", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Convergence Rates of Algorithms for Visual Search", "Title": "Detecting Visual Contours", "Abstract": "This  paper  formulates  the  problem  of  visual  search  as  Bayesian  inference  and  defines  a  Bayesian  ensemble  of  problem  instances .  In  particular,  we  address  the  problem  of  the  detection  of  visual  contours  in  noise/clutter  by  optimizing  a  global  criterion  which  combines  local  intensity  and  geometry  information.  We  analyze  the  convergence  rates  of  A * search  algorithms  using  results  from  information theory to bound  the  probability of rare  events  within  the Bayesian ensemble.  This analysis determines characteristics of  the  domain ,  which  we  call  order  parameters,  that  determine  the  convergence  rates.  In  particular,  we  present  a  specific  admissible  A * algorithm with pruning which converges, with high probability,  with  expected  time  O(N)  in  the  size  of  the  problem.  In  addi(cid:173) tion,  we  briefly  summarize  extensions  of this  work  which  address  fundamental  limits  of target  contour  detectability  (Le.  algorithm  independent results)  and the use  of non-admissible heuristics."}
{"Type": "conference", "Year": "1998", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "On-Line Learning with Restricted Training Sets", "Title": "Exact Solution as Benchmark for General Theories", "Abstract": "O(ws(s log d+log(dqh/ s))) and O(ws((h/ s) log q) +log(dqh/ s)) are  upper bounds for  the VC-dimension of a  set of neural networks of  units  with  piecewise  polynomial  activation  functions,  where  s  is  the  depth  of  the  network,  h  is  the  number  of  hidden  units,  w  is  the  number  of  adjustable  parameters,  q  is  the  maximum  of  the  number of polynomial segments of the activation function, and d is  the  maximum degree  of  the polynomials;  also  n(wslog(dqh/s))  is  a  lower  bound  for  the VC-dimension  of such  a  network set,  which  are tight for  the cases  s =  8(h)  and  s is  constant.  For the special  case q =  1,  the VC-dimension is  8(ws log d)."}
{"Type": "conference", "Year": "1998", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Shrinking the Tube", "Title": "A New Support Vector Regression Algorithm", "Abstract": "A new algorithm for Support Vector regression is  described.  For a priori  chosen 1/,  it automatically adjusts a flexible tube of minimal radius to the  data such that  at most a fraction  1/  of the data points lie outside.  More(cid:173) over,  it  is  shown  how  to  use  parametric  tube  shapes  with  non-constant  radius. The algorithm is analysed theoretically and experimentally."}
{"Type": "conference", "Year": "1998", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Sparse Code Shrinkage", "Title": "Denoising by Nonlinear Maximum Likelihood Estimation", "Abstract": "Sparse  coding  is  a  method  for  finding  a  representation  of data in  which  each  of the  components of the representation is  only  rarely  significantly  active.  Such  a  representation is  closely  related  to re(cid:173) dundancy reduction and independent component analysis,  and has  some  neurophysiological  plausibility.  In  this  paper,  we  show  how  sparse coding can be used for denoising.  Using maximum likelihood  estimation of nongaussian variables corrupted by gaussian noise, we  show  how  to  apply a  shrinkage nonlinearity on  the components  of  sparse coding so  as  to reduce noise.  Furthermore,  we  show  how  to  choose the optimal sparse coding basis for  denoising.  Our method  is  closely  related  to  the  method  of wavelet  shrinkage,  but  has  the  important benefit over wavelet methods that both the features and  the shrinkage parameters are estimated directly from  the data."}
{"Type": "conference", "Year": "1998", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Maximum-Likelihood Continuity Mapping (MALCOM)", "Title": "An Alternative to HMMs", "Abstract": "We describe Maximum-Likelihood Continuity Mapping (MALCOM), an  alternative to  hidden Markov models (HMMs) for processing sequence  data such as speech.  While HMMs have a discrete \"hidden\" space con(cid:173) strained by  a fixed  finite-automaton architecture, MALCOM has a con(cid:173) tinuous hidden space-a continuity map-that is  constrained only  by  a  smoothness requirement on paths through the space.  MALCOM fits  into  the same probabilistic framework for speech recognition as  HMMs, but  it represents  a  more  realistic  model  of the  speech  production  process.  To  evaluate the extent to which MALCOM captures speech production  information, we  generated continuous speech continuity maps  for three  speakers  and  used  the  paths  through  them  to  predict measured  speech  articulator data.  The median  correlation  between  the MALCOM paths  obtained from  only the  speech  acoustics  and  articulator measurements  was 0.77 on an  independent test set not used  to train MALCOM or the  predictor.  This  unsupervised  model  achieved  correlations  over speak(cid:173) ers and articulators only 0.02 to 0.15 lower than those obtained using an  analogous supervised method which used articulatory measurements as  well as acoustics .."}
{"Type": "conference", "Year": "1998", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "DTs", "Title": "Dynamic Trees", "Abstract": "In  this  paper we  introduce a new  class of image models,  which  we  call  dynamic trees or  DTs.  A dynamic tree model specifies  a prior  over a  large number of trees, each one of which is  a tree-structured  belief  net  (TSBN).  Experiments  show  that  DTs  are  capable  of  generating images that are less blocky, and the models have better  translation  invariance properties  than  a  fixed,  \"balanced\"  TSBN.  We  also show that Simulated Annealing is effective at finding trees  which  have high  posterior probability."}
{"Type": "conference", "Year": "1998", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Unsupervised and Supervised Clustering", "Title": "The Mutual Information between Parameters and Observations", "Abstract": "Recent  works  in  parameter  estimation  and  neural  coding  have  demonstrated that optimal performance are related to the  mutual  information between parameters and data.  We  consider the mutual  information in  the case where  the dependency in the parameter (a  vector  8)  of  the  conditional  p.d.f.  of each  observation  (a  vector  0, is  through  the  scalar  product  8.~ only.  We  derive  bounds  and  asymptotic behaviour for  the mutual information and compare with  results  obtained on  the same model with the\" replica technique\" ."}
{"Type": "conference", "Year": "1998", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Perceiving without Learning", "Title": "From Spirals to Inside/Outside Relations", "Abstract": "As  a benchmark task,  the  spiral  problem  is  well  known  in  neural  net(cid:173) works.  Unlike  previous  work  that  emphasizes  learning,  we  approach  the  problem from  a generic perspective that does  not involve learning.  We  point out that the spiral problem is intrinsically connected to the in(cid:173) side/outside problem.  A  generic  solution  to  both problems is proposed  based on  oscillatory correlation using a time delay network.  Our simu(cid:173) lation results are qualitatively  consistent with  human performance,  and  we  interpret human limitations in  terms  of synchrony and  time  delays,  both biologically plausible.  As a special case, our network without time  delays can always distinguish these figures regardless of shape, position,  size, and orientation."}
{"Type": "conference", "Year": "1998", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Utilizing lime", "Title": "Asynchronous Binding", "Abstract": "Historically,  connectionist  systems  have  not excelled  at  represent(cid:173) ing and manipulating complex structures.  How  can a  system com(cid:173) posed  of simple  neuron-like  computing  elements  encode  complex  relations?  Recently,  researchers have begun to appreciate that rep(cid:173) resentations can extend in  both time and space.  Many researchers  have proposed that the synchronous firing of units can encode com(cid:173) plex  representations.  I  identify  the  limitations  of  this  approach  and present an  asynchronous model of binding that effectively rep(cid:173) resents  complex  structures.  The  asynchronous model  extends  the  synchronous approach.  I argue that our cognitive architecture uti(cid:173) lizes  a similar mechanism."}
