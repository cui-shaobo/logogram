{"Type": "conference", "Year": "2004", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "The power of feature clustering", "Title": "An application to object detection", "Abstract": "We give a fast rejection scheme that is based on image segments and          demonstrate it on the canonical example of face detection. However, in-          stead of focusing on the detection step we focus on the rejection step and          show that our method is simple and fast to be learned, thus making it          an excellent pre-processing step to accelerate standard machine learning          classifiers, such as neural-networks, Bayes classifiers or SVM. We de-          compose a collection of face images into regions of pixels with similar          behavior over the image set. The relationships between the mean and          variance of image segments are used to form a cascade of rejectors that          can reject over 99.8% of image patches, thus only a small fraction of the          image patches must be passed to a full-scale classifier. Moreover, the          training time for our method is much less than an hour, on a standard PC.          The shape of the features (i.e. image segments) we use is data-driven,          they are very cheap to compute and they form a very low dimensional          feature space in which exhaustive search for the best features is tractable."}
{"Type": "conference", "Year": "2004", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Kernel Projection Machine", "Title": "a New Tool for Pattern Recognition", "Abstract": "for every f  L2([0, 1]). Given a Mercer kernel k on [0, 1][0, 1], the regularization least square procedure proposes"}
{"Type": "conference", "Year": "2004", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Schema Learning", "Title": "Experience-Based Construction of Predictive Action Models", "Abstract": "Schema learning is a way to discover probabilistic, constructivist, pre- dictive action models (schemas) from experience. It includes meth- ods for ﬁnding and using hidden state to make predictions more accu- rate. We extend the original schema mechanism [1] to handle arbitrary discrete-valued sensors, improve the original learning criteria to handle POMDP domains, and better maintain hidden state by using schema pre- dictions. These extensions show large improvement over the original schema mechanism in several rewardless POMDPs, and achieve very low prediction error in a difﬁcult speech modeling task. Further, we compare extended schema learning to the recently introduced predictive state rep- resentations [2], and ﬁnd their predictions of next-step action effects to be approximately equal in accuracy. This work lays the foundation for a schema-based system of integrated learning and planning."}
{"Type": "conference", "Year": "2004", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Machine Learning Applied to Perception", "Title": "Decision Images for Gender Classification", "Abstract": "We study gender discrimination of human faces using a combination of psychophysical classiﬁcation and discrimination experiments together with methods from machine learning. We reduce the dimensionality of a set of face images using principal component analysis, and then train a set of linear classiﬁers on this reduced representation (linear support vec- tor machines (SVMs), relevance vector machines (RVMs), Fisher linear discriminant (FLD), and prototype (prot) classiﬁers) using human clas- siﬁcation data. Because we combine a linear preprocessor with linear classiﬁers, the entire system acts as a linear classiﬁer, allowing us to visu- alise the decision-image corresponding to the normal vector of the separ- ating hyperplanes (SH) of each classiﬁer. We predict that the female-to- maleness transition along the normal vector for classiﬁers closely mim- icking human classiﬁcation (SVM and RVM [1]) should be faster than the transition along any other direction. A psychophysical discrimina- tion experiment using the decision images as stimuli is consistent with this prediction."}
{"Type": "conference", "Year": "2004", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "A Topographic Support Vector Machine", "Title": "Classification Using Local Label Configurations", "Abstract": "The standard approach to the classification of objects is to consider the          examples as independent and identically distributed (iid). In many real          world settings, however, this assumption is not valid, because a topo-          graphical relationship exists between the objects. In this contribution we          consider the special case of image segmentation, where the objects are          pixels and where the underlying topography is a 2D regular rectangular          grid. We introduce a classification method which not only uses measured          vectorial feature information but also the label configuration within a to-          pographic neighborhood. Due to the resulting dependence between the          labels of neighboring pixels, a collective classification of a set of pixels          becomes necessary. We propose a new method called 'Topographic Sup-          port Vector Machine' (TSVM), which is based on a topographic kernel          and a self-consistent solution to the label assignment shown to be equiv-          alent to a recurrent neural network. The performance of the algorithm is          compared to a conventional SVM on a cell image segmentation task."}
{"Type": "conference", "Year": "2004", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "The Cerebellum Chip", "Title": "an Analog VLSI Implementation of a Cerebellar Model of Classical Conditioning", "Abstract": "We  present  a  biophysically  constrained  cerebellar  model  of                classical  conditioning,  implemented  using  a  neuromorphic  analog                VLSI (aVLSI) chip.  Like its biological counterpart, our cerebellar                model  is  able  to  control  adaptive  behavior  by  predicting  the                precise timing of events.  Here we describe the functionality of the                chip  and  present  its  learning  performance,  as  evaluated  in                simulated  conditioning  experiments  at  the  circuit  level  and  in                behavioral  experiments  using  a  mobile  robot.    We  show  that  this                aVLSI model supports the acquisition and extinction of adaptively                timed  conditioned  responses  under  real-world  conditions  with                ultra-low power consumption."}
{"Type": "conference", "Year": "2004", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Pictorial Structures for Molecular Modeling", "Title": "Interpreting Density Maps", "Abstract": "X-ray crystallography is currently the most common way protein  structures are elucidated. One of the most time-consuming steps in  the crystallographic process is interpretation of the electron density  map, a task that involves finding patterns in a three-dimensional  picture of a protein. This paper describes DEFT (DEFormable  Template), an algorithm using pictorial structures to build a  flexible protein model from the protein's amino-acid sequence.  Matching this pictorial structure into the density map is a way of  automating density-map interpretation. Also described are several  extensions to the pictorial structure matching algorithm necessary  for this automated interpretation. DEFT is tested on a set of  density maps ranging from 2 to 4Å resolution, producing root- mean-squared errors ranging from 1.38 to 1.84Å."}
{"Type": "conference", "Year": "2004", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "The Laplacian PDF Distance", "Title": "A Cost Function for Clustering in a Kernel Feature Space", "Abstract": "Acknowledgments.               This work was partially supported by NSF grant ECS- 0300340."}
{"Type": "conference", "Year": "2004", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Solitaire", "Title": "Man Versus Machine", "Abstract": "In this paper, we apply the rollout method to a version of solitaire, modeled as a deter- ministic Markov decision problem with over 52! states. Determinism drastically reduces computational requirements, making it possible to consider iterated rollouts1. With five iterations, a game, implemented in Java, takes about one hour and forty-five minutes on average on a SUN Blade 2000 machine with two 900MHz CPUs, and the probability of winning exceeds that of a human expert by about a factor of two. Our study represents an important contribution both to the study of the rollout method and to the study of solitaire."}
{"Type": "conference", "Year": "2004", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Similarity and Discrimination in Classical Conditioning", "Title": "A Latent Variable Account", "Abstract": "We propose a probabilistic, generative account of configural learning          phenomena in classical conditioning. Configural learning experiments          probe how animals discriminate and generalize between patterns of si-          multaneously presented stimuli (such as tones and lights) that are dif-          ferentially predictive of reinforcement. Previous models of these issues          have been successful more on a phenomenological than an explanatory          level: they reproduce experimental findings but, lacking formal founda-          tions, provide scant basis for understanding why animals behave as they          do. We present a theory that clarifies seemingly arbitrary aspects of pre-          vious models while also capturing a broader set of data. Key patterns          of data, e.g. concerning animals' readiness to distinguish patterns with          varying degrees of overlap, are shown to follow from statistical inference."}
{"Type": "conference", "Year": "2004", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Boosting on Manifolds", "Title": "Adaptive Regularization of Base Classifiers", "Abstract": "to determine the base errors. Figure 3(f) indicates that REGBOOST has a clear advantage here. REGBOOST is also far better than the semi-supervised algorithm proposed in [12] (their best test error using the same settings is 18%)."}
{"Type": "conference", "Year": "2004", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Reducing Spike Train Variability", "Title": "A Computational Theory Of Spike-Timing Dependent Plasticity", "Abstract": "We model in detail the experiment of Zhang et al. [12] (Figure 2a). In this exper- iment, a post neuron is identified that has two neurons projecting to it, call them the pre and the driver. The pre is subthreshold: it produces depolarization but no spike. The driver is suprathreshold: it induces a spike in the post. Plasticity of the pre-post synapse is measured as a function of the timing between pre and post spikes (tpre-post) by varying the timing between induced spikes in the pre and the driver (tpre-driver). This measurement yields the well-known STDP curve (Figure 1b).1 The experiment imposes several constraints on a simulation: The driver alone causes spiking > 70% of the time, the pre alone causes spiking < 10% of the time, synchronous firing of driver and pre cause LTP if and only if the post fires, and the time constants of the EPSPs--s and m in the sSRM--are in the range of 13ms and 1015ms respectively. These constraints remove many free parameters from our simulation. We do not explicitly model the two input cells; instead, we model the EPSPs they produce. The magnitude of these EPSPs are picked to satisfy the experimental constraints: the driver EPSP alone causes a spike in the post on 77.4% of trials, and the pre EPSP alone causes a spike on fewer than 0.1% of trials. Free parameters of the simulation are  and  in the spike-probability function ( can be folded into ), and the magnitude (us, u                                          ,  f ,                                        r     abs) and reset time constants ( s                                                                             r       r     abs). The dependent variable of the simulation is tpre-driver, and we measure the time of the post spike to determine tpre-post. We estimate the weight update for a given tpre-driver using Equation 8, approximating the integral by a summation over all time-discretized output responses consisting of 0, 1, or 2 spikes. Three or more spikes have a probability that is vanishingly small."}
{"Type": "conference", "Year": "2004", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Chemosensory Processing in a Spiking Model of the Olfactory Bulb", "Title": "Chemotopic Convergence and Center Surround Inhibition", "Abstract": "This paper presents a neuromorphic model of two olfactory signal-          processing primitives: chemotopic convergence of olfactory           receptor neurons, and center on-off surround lateral inhibition in           the olfactory bulb. A self-organizing model of receptor           convergence onto glomeruli is used to generate a spatially           organized map, an olfactory image. This map serves as input to a           lattice of spiking neurons with lateral connections. The dynamics           of this recurrent network transforms the initial olfactory image into           a spatio-temporal pattern that evolves and stabilizes into odor- and           intensity-coding attractors. The model is validated using           experimental data from an array of temperature-modulated gas           sensors.  Our results are consistent with recent neurobiological           findings on the antennal lobe of the honeybee and the locust."}
{"Type": "conference", "Year": "2004", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "VDCBPI", "Title": "an Approximate Scalable Algorithm for Large POMDPs", "Abstract": "The above tips work well when VDC is integrated with BPI. We believe they are sufficient to ensure proper integration of VDC with other POMDP algorithms, though we haven't verified this empirically."}
{"Type": "conference", "Year": "2004", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "A Generalized Bradley-Terry Model", "Title": "From Group Competition to Individual Skill", "Abstract": "The Bradley-Terry model for paired comparison has been popular in many areas. We propose a generalized version in which paired individual comparisons are extended to paired team comparisons. We introduce a simple algorithm with convergence proofs to solve the model and obtain individual skill. A useful application to multi-class probability estimates using error-correcting codes is demonstrated."}
{"Type": "conference", "Year": "2004", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Co-Validation", "Title": "Using Model Disagreement on Unlabeled Data to Validate Classification Algorithms", "Abstract": "In the context of binary classification, we define disagreement as a mea-          sure of how often two independently-trained models differ in their clas-          sification of unlabeled data. We explore the use of disagreement for error          estimation and model selection. We call the procedure co-validation,          since the two models effectively (in)validate one another by comparing          results on unlabeled data, which we assume is relatively cheap and plen-          tiful compared to labeled data. We show that per-instance disagreement          is an unbiased estimate of the variance of error for that instance. We also          show that disagreement provides a lower bound on the prediction (gen-          eralization) error, and a tight upper bound on the \"variance of prediction          error\", or the variance of the average error across instances, where vari-          ance is measured across training sets. We present experimental results on          several data sets exploring co-validation for error estimation and model          selection. The procedure is especially effective in active learning set-          tings, where training sets are not drawn at random and cross validation          overestimates error."}
{"Type": "conference", "Year": "2004", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Co-Training and Expansion", "Title": "Towards Bridging Theory and Practice", "Abstract": "Computer Science Dept. Carnegie Mellon Univ. Pittsburgh, PA 15213"}
{"Type": "conference", "Year": "2004", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Theory of localized synfire chain", "Title": "characteristic propagation speed of stable spike pattern", "Abstract": "Repeated spike patterns have often been taken as evidence for the synfire          chain, a phenomenon that a stable spike synchrony propagates through          a feedforward network. Inter-spike intervals which represent a repeated          spike pattern are influenced by the propagation speed of a spike packet.          However, the relation between the propagation speed and network struc-          ture is not well understood. While it is apparent that the propagation          speed depends on the excitatory synapse strength, it might also be related          to spike patterns. We analyze a feedforward network with Mexican-Hat-          type connectivity (FMH) using the Fokker-Planck equation. We show          that both a uniform and a localized spike packet are stable in the FMH          in a certain parameter region. We also demonstrate that the propagation          speed depends on the distinct firing patterns in the same network."}
{"Type": "conference", "Year": "2004", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "The Power of Selective Memory", "Title": "Self-Bounded Learning of Prediction Suffix Trees", "Abstract": "Prediction suffix trees (PST) provide a popular and effective tool for tasks          such as compression, classification, and language modeling. In this pa-          per we take a decision theoretic view of PSTs for the task of sequence          prediction. Generalizing the notion of margin to PSTs, we present an on-          line PST learning algorithm and derive a loss bound for it. The depth of          the PST generated by this algorithm scales linearly with the length of the          input. We then describe a self-bounded enhancement of our learning al-          gorithm which automatically grows a bounded-depth PST. We also prove          an analogous mistake-bound for the self-bounded algorithm. The result          is an efficient algorithm that neither relies on a-priori assumptions on the          shape or maximal depth of the target PST nor does it require any param-          eters. To our knowledge, this is the first provably-correct PST learning          algorithm which generates a bounded-depth PST while being competi-          tive with any fixed PST determined in hindsight."}
{"Type": "conference", "Year": "2004", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Spike Sorting", "Title": "Bayesian Clustering of Non-Stationary Data", "Abstract": "In some cases, validity of the automatic clustering can be assessed by checking functional properties associated with the underlying neurons. In Figure 3B we present such a valida- tion for a successfully tracked cluster."}
{"Type": "conference", "Year": "2004", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Blind One-microphone Speech Separation", "Title": "A Spectral Learning Approach", "Abstract": "We present an algorithm to perform blind, one-microphone speech sep-          aration. Our algorithm separates mixtures of speech without modeling          individual speakers. Instead, we formulate the problem of speech sep-          aration as a problem in segmenting the spectrogram of the signal into          two or more disjoint sets. We build feature sets for our segmenter using          classical cues from speech psychophysics. We then combine these fea-          tures into parameterized affinity matrices. We also take advantage of the          fact that we can generate training examples for segmentation by artifi-          cially superposing separately-recorded signals. Thus the parameters of          the affinity matrices can be tuned using recent work on learning spectral          clustering [1]. This yields an adaptive, speech-specific segmentation al-          gorithm that can successfully separate one-microphone speech mixtures."}
{"Type": "conference", "Year": "2004", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Parallel Support Vector Machines", "Title": "The Cascade SVM", "Abstract": "We describe an algorithm for support vector machines (SVM) that  can be parallelized efficiently and scales to very large problems with  hundreds of thousands of training vectors. Instead of analyzing the  whole training set in one optimization step, the data are split into  subsets and optimized separately with multiple SVMs. The partial  results are combined and filtered again in a ‘Cascade’ of SVMs, until  the global optimum is reached. The Cascade SVM can be spread over  multiple processors with minimal communication overhead and  requires far less memory, since the kernel matrices are much smaller  than for a regular SVM. Convergence to the global optimum is  guaranteed with multiple passes through the Cascade, but already a  single pass provides good generalization. A single pass is 5x – 10x  faster than a regular SVM for problems of 100,000 vectors when  implemented on a single processor. Parallel implementations on a  cluster of 16 processors were tested with over 1 million vectors  (2-class problems), converging in a day or two, while a regular SVM  never converged in over a week."}
{"Type": "conference", "Year": "2004", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "At the Edge of Chaos", "Title": "Real-time Computations and Self-Organized Criticality in Recurrent Neural Networks", "Abstract": "Acknowledgement                        This work was supported in part by the PASCAL project #IST-2002-506778 of the European Community."}
{"Type": "conference", "Year": "2004", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Sharing Clusters among Related Groups", "Title": "Hierarchical Dirichlet Processes", "Abstract": "We propose the hierarchical Dirichlet process (HDP), a nonparametric Bayesian model for clustering problems involving multiple groups of data. Each group of data is modeled with a mixture, with the number of components being open-ended and inferred automatically by the model. Further, components can be shared across groups, allowing dependencies across groups to be modeled effectively as well as conferring generaliza- tion to new groups. Such grouped clustering problems occur often in practice, e.g. in the problem of topic discovery in document corpora. We report experimental results on three text corpora showing the effective and superior performance of the HDP over previous models."}
