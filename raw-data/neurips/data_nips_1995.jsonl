{"Type": "conference", "Year": "1995", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Beating a Defender in Robotic Soccer", "Title": "Memory-Based Learning of a Continuous Function", "Abstract": "Learning how to adjust to an opponent's position is critical to  the success of having intelligent agents collaborating towards the  achievement of specific tasks in unfriendly environments. This pa(cid:173) per describes our work on a Memory-based technique for to choose  an action based on a continuous-valued state attribute indicating  the position of an opponent. We investigate the question of how an  agent performs in nondeterministic variations of the training situ(cid:173) ations. Our experiments indicate that when the random variations  fall within some bound of the initial training, the agent performs  better with some initial training rather than from a tabula-rasa."}
{"Type": "conference", "Year": "1995", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Learning with ensembles", "Title": "How overfitting can be useful", "Abstract": "We  study  the  characteristics  of learning  with ensembles.  Solving  exactly  the  simple  model  of an  ensemble  of linear  students,  we  find  surprisingly  rich  behaviour.  For  learning  in  large  ensembles,  it  is  advantageous  to  use  under-regularized  students,  which  actu(cid:173) ally  over-fit  the  training data.  Globally optimal performance  can  be  obtained by  choosing  the training set  sizes  of the students ap(cid:173) propriately.  For  smaller ensembles,  optimization of the  ensemble  weights  can yield significant improvements in ensemble generaliza(cid:173) tion  performance,  in particular if the  individual students  are  sub(cid:173) ject to noise in the training process.  Choosing students with a wide  range of regularization parameters makes this improvement robust  against changes in the unknown level of noise  in the training data."}
{"Type": "conference", "Year": "1995", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Tempering Backpropagation Networks", "Title": "Not All Weights are Created Equal", "Abstract": "Computational Neurobiology Lab  The Salk Institute for BioI. Studies  San Diego, CA 92186-5800, USA"}
{"Type": "conference", "Year": "1995", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "SPERT-II", "Title": "A Vector Microprocessor System and its Application to Large Problems in Backpropagation Training", "Abstract": "We  report  on  our  development  of a  high-performance  system  for  neural  network  and other signal  processing  applications.  We  have  designed  and  implemented  a  vector  microprocessor  and  pack(cid:173) aged  it  as  an  attached  processor  for  a  conventional  workstation.  We  present  performance  comparisons  with  commercial  worksta(cid:173) tions on neural  network  backpropagation training.  The SPERT-II  system  demonstrates  significant  speedups  over  extensively  hand(cid:173) optimization code running on  the workstations."}
{"Type": "conference", "Year": "1995", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Pruning with generalization based weight saliencies", "Title": "λOBD, λOBS", "Abstract": "Abstract Unavailable"}
{"Type": "conference", "Year": "1995", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Using the Future to \"Sort Out\" the Present", "Title": "Rankprop and Multitask Learning for Medical Risk Evaluation", "Abstract": "A patient visits the doctor; the doctor reviews  the patient's history,  asks questions, makes basic measurements (blood pressure,  .. . ), and  prescribes  tests  or  treatment .  The  prescribed  course  of action  is  based  on  an assessment of patient risk-patients at higher risk  are  given  more  and  faster  attention.  It is  also  sequential- it  is  too  expensive  to  immediately order  all  tests  which  might  later  be  of  value.  This  paper  presents  two  methods  that  together  improve  the  accuracy  of  backprop  nets  on  a  pneumonia  risk  assessment  problem  by  10-50%.  Rankprop improves on backpropagation  with  sum of squares error in ranking patients by risk.  Multitask learning  takes advantage of future lab tests  available in the training set,  but  not  available  in  practice  when  predictions  must  be  made.  Both  methods are broadly  applicable."}
{"Type": "conference", "Year": "1995", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Predictive Q-Routing", "Title": "A Memory-based Reinforcement Learning Approach to Adaptive Traffic Control", "Abstract": "In  this  paper,  we  propose  a  memory-based  Q-Iearning  algorithm  called  predictive  Q-routing  (PQ-routing)  for  adaptive  traffic  con(cid:173) trol.  We attempt to address two problems encountered in Q-routing  (Boyan  &  Littman,  1994),  namely,  the inability to fine-tune  rout(cid:173) ing policies under  low  network  load and  the  inability to learn  new  optimal  policies  under  decreasing  load  conditions.  Unlike  other  memory-based  reinforcement  learning  algorithms  in  which  mem(cid:173) ory  is  used  to  keep  past  experiences  to  increase  learning  speed,  PQ-routing  keeps  the  best  experiences  learned  and  reuses  them  by  predicting  the  traffic  trend.  The  effectiveness  of  PQ-routing  has been  verified  under various network  topologies and traffic con(cid:173) ditions.  Simulation  results  show  that  PQ-routing  is  superior  to  Q-routing in terms of both learning speed and  adaptability."}
{"Type": "conference", "Year": "1995", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "REMAP", "Title": "Recursive Estimation and Maximization of A Posteriori Probabilities - Application to Transition-Based Connectionist Speech Recognition", "Abstract": "In  this  paper,  we  introduce  REMAP,  an  approach for  the training  and estimation of posterior probabilities using a recursive algorithm  that is  reminiscent of the EM-based  Forward-Backward  (Liporace  1982)  algorithm  for  the  estimation  of sequence  likelihoods.  Al(cid:173) though  very  general,  the  method  is  developed  in  the  context  of a  statistical  model for  transition-based speech  recognition  using  Ar(cid:173) tificial  Neural  Networks  (ANN)  to  generate  probabilities for  Hid(cid:173) den  Markov  Models  (HMMs).  In  the  new  approach,  we  use  local  conditional posterior probabilities of transitions to estimate global  posterior  probabilities of word  sequences.  Although  we  still  use  ANNs  to  estimate  posterior  probabilities,  the  network  is  trained  with targets that are themselves estimates of local posterior proba(cid:173) bilities.  An  initial experimental result shows a significant decrease  in error-rate in  comparison to a  baseline system."}
{"Type": "conference", "Year": "1995", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Generalization in Reinforcement Learning", "Title": "Successful Examples Using Sparse Coarse Coding", "Abstract": "Generalization  in  Reinforcement Learning"}
{"Type": "conference", "Year": "1995", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "SEEMORE", "Title": "A View-Based Approach to 3-D Object Recognition Using Multiple Visual Cues", "Abstract": "A  neurally-inspired  visual  object  recognition  system  is  described  called  SEEMORE,  whose  goal  is  to  identify  common objects  from  a  large  known  set-independent  of  3-D  viewiag  angle,  distance,  and  non-rigid  distortion.  SEEMORE's  database  consists  of 100  ob(cid:173) jects  that  are  rigid  (shovel),  non-rigid  (telephone  cord),  articu(cid:173) lated  (book), statistical (shrubbery),  and complex (photographs of  scenes).  Recognition results  were  obtained using a  set of 102 color  and shape feature channels within a simple feedforward network ar(cid:173) chitecture.  In  response  to  a  test  set  of 600  novel  test  views  (6  of  each object)  presented individually in color video images, SEEMORE  identified  the  object correctly  97% of the time (chance is 1%)  using  a  nearest  neighbor  classifier.  Similar levels  of performance  were  obtained for  the  subset  of 15  non-rigid objects.  Generalization be(cid:173) havior reveals emergence  of striking natural category structure  not  explicit in  the  input feature  dimensions."}
{"Type": "conference", "Year": "1995", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "A Unified Learning Scheme", "Title": "Bayesian-Kullback Ying-Yang Machine", "Abstract": "A  Bayesian-Kullback  learning scheme,  called Ying-Yang  Machine,  is  proposed  based on  the  two  complement but equivalent Bayesian  representations  for  joint  density  and  their  Kullback  divergence.  Not  only  the  scheme  unifies  existing  major supervised  and  unsu(cid:173) pervised  learnings,  including  the  classical  maximum likelihood  or  least  square learning,  the  maximum information preservation,  the  EM  & em algorithm and information geometry, the recent  popular  Helmholtz  machine,  as  well  as  other  learning  methods  with  new  variants  and  new  results;  but  also  the  scheme  provides  a  number  of new  learning models."}
{"Type": "conference", "Year": "1995", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Temporal coding in the sub-millisecond range", "Title": "Model of barn owl auditory pathway", "Abstract": "Temporal Coding in  the Submillisecond Range:  Model of Bam Owl Auditory Pathway"}
{"Type": "conference", "Year": "1995", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Softassign versus Softmax", "Title": "Benchmarks in Combinatorial Optimization", "Abstract": "A  new  technique,  termed  soft assign,  is  applied  for  the  first  time  to  two  classic  combinatorial  optimization  problems,  the  travel(cid:173) ing  salesman  problem  and  graph  partitioning.  Soft assign ,  which  has emerged from  the recurrent  neural  network/statistical physics  framework, enforces  two-way  (assignment) constraints without the  use  of penalty  terms  in  the  energy  functions.  The  soft assign  can  also  be  generalized  from  two-way  winner-take-all  constraints  to  multiple membership constraints which are required for graph par(cid:173) titioning.  The  soft assign  technique  is  compared  to  the  softmax  (Potts  glass).  Within  the  statistical  physics  framework,  softmax  and a penalty term has been a widely used method for enforcing the  two-way constraints common within many combinatorial optimiza(cid:173) tion  problems.  The  benchmarks  present  evidence  that  soft assign  has clear advantages in accuracy, speed,  parallelizabilityand algo(cid:173) rithmic simplicity over softmax and a penalty term in optimization  problems with two-way constraints."}
{"Type": "conference", "Year": "1995", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "From Isolation to Cooperation", "Title": "An Alternative View of a System of Experts", "Abstract": "We introduce a constructive,  incremental learning system for regression  problems that models data by means of locally linear experts.  In contrast  to  other approaches,  the  experts  are  trained  independently  and  do  not  compete for data during learning.  Only when a prediction for  a query  is  required  do  the  experts  cooperate  by  blending  their  individual  predic(cid:173) tions.  Each expert is trained by  minimizing  a penalized local cross vali(cid:173) dation error using second order methods. In this way, an expert is able to  find a local distance metric by adjusting the size and  shape of the recep(cid:173) tive field in which its predictions are valid, and also to detect relevant in(cid:173) put features  by  adjusting  its bias  on  the  importance of individual input  dimensions. We derive asymptotic results for our method. In a variety of  simulations the properties of the algorithm are demonstrated with respect  to  interference,  learning  speed,  prediction  accuracy,  feature  detection,  and task oriented incremental learning."}
{"Type": "conference", "Year": "1995", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Correlated Neuronal Response", "Title": "Time Scales and Mechanisms", "Abstract": "We  have  analyzed the relationship between  correlated spike  count  and the peak in the cross-correlation of spike trains for  pairs of si(cid:173) multaneously recorded neurons  from  a  previous  study of area MT  in  the  macaque  monkey  (Zohary  et  al.,  1994).  We  conclude  that  common  input,  responsible  for  creating peaks  on  the order of ten  milliseconds  wide  in  the  spike  train  cross-correlograms  (CCGs),  is  also  responsible  for  creating  the  correlation  in  spike  count  ob(cid:173) served  at  the  two  second  time  scale  of  the  trial.  We  argue  that  both common excitation and  inhibition  may  play significant  roles  in establishing this  correlation."}
{"Type": "conference", "Year": "1995", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Control of Selective Visual Attention", "Title": "Modeling the \"Where\" Pathway", "Abstract": "We  then  (section  2.3)  address  the  question  of the  integration  of  the  input  in  the  \"saliency map,\"  a  topographically organized map which codes for  the instantaneous  conspicuity  of the  different  parts of the visual field ."}
