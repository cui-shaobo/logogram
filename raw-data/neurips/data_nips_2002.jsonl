{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Nonparametric Representation of Policies and Value Functions", "Title": "A Trajectory-Based Approach", "Abstract": "A longstanding goal of reinforcement learning is to develop non- parametric representations of policies and value functions that support rapid learning without suffering from interference or the curse of di- mensionality. We have developed a trajectory-based approach, in which policies and value functions are represented nonparametrically along tra- jectories. These trajectories, policies, and value functions are updated as the value function becomes more accurate or as a model of the task is up- dated. We have applied this approach to periodic tasks such as hopping and walking, which required handling discount factors and discontinu- ities in the task dynamics, and using function approximation to represent value functions at discontinuities. We also describe extensions of the ap- proach to make the policies more robust to modeling error and sensor noise."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Automatic Derivation of Statistical Algorithms", "Title": "The EM Family and Beyond", "Abstract": "14 max pr(x| phi,mu,sigma\u0001 ) wrt  phi,mu,sigma\u0001 ; \u0005\u0007\u0006 inference task: maximize the conditional probability pr\u0002\u0004\u0003 rameters \u0003"}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Going Metric", "Title": "Denoising Pairwise Data", "Abstract": "Pairwise  data in  empirical  sciences  typically  violate  metricity,  ei(cid:173) ther  due  to  noise  or  due  to  fallible  estimates,  and  therefore  are  hard  to  analyze  by  conventional  machine  learning  technology.  In  this  paper  we  therefore  study  ways  to  work  around  this  problem.  First,  we  present  an  alternative  embedding  to  multi-dimensional  scaling  (MDS)  that  allows  us  to  apply  a  variety  of classical  ma(cid:173) chine learning and signal processing algorithms.  The class of pair(cid:173) wise grouping algorithms which share the shift-invariance property  is  statistically  invariant  under  this  embedding  procedure,  leading  to  identical assignments  of objects to clusters.  Based on this  new  vectorial  representation,  denoising  methods  are  applied  in  a  sec(cid:173) ond  step.  Both steps  provide a  theoretically  well  controlled setup  to  translate  from  pairwise  data  to  the  respective  denoised  met(cid:173) ric  representation.  We  demonstrate the practical usefulness of our  theoretical  reasoning by  discovering structure in  protein sequence  data bases, visibly improving performance upon existing automatic  methods."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Spikernels", "Title": "Embedding Spiking Neurons in Inner-Product Spaces", "Abstract": "Inner-product operators, often referred to as kernels in statistical learning, de- ﬁne a mapping from some input space into a feature space. The focus of this paper is the construction of biologically-motivated kernels for cortical ac- tivities. The kernels we derive, termed Spikernels, map spike count sequences into an abstract vector space in which we can perform various prediction tasks. We discuss in detail the derivation of Spikernels and describe an efﬁcient al- gorithm for computing their value on any two sequences of neural population spike counts. We demonstrate the merits of our modeling approach using the Spikernel and various standard kernels for the task of predicting hand move- ment velocities from cortical recordings. In all of our experiments all the ker- nels we tested outperform the standard scalar product used in regression with the Spikernel consistently achieving the best performance."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "The RA Scanner", "Title": "Prediction of Rheumatoid Joint Inflammation Based on Laser Imaging", "Abstract": "We describe the RA scanner, a novel system for the examination of pa- tients suffering from rheumatoid arthritis. The RA scanner is based on a novel laser-based imaging technique which is sensitive to the optical characteristics of ﬁnger joint tissue. Based on the laser images, ﬁnger joints are classiﬁed according to whether the inﬂammatory status has improved or worsened. To perform the classiﬁcation task, various lin- ear and kernel-based systems were implemented and their performances were compared. Special emphasis was put on measures to reliably per- form parameter tuning and evaluation, since only a very small data set was available. Based on the results presented in this paper, it was con- cluded that the RA scanner permits a reliable classiﬁcation of patholog- ical ﬁnger joints, thus paving the way for a further development from prototype to product stage."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Shape Recipes", "Title": "Scene Representations that Refer to the Image", "Abstract": "The goal of low-level vision is to estimate an underlying scene, given an observed image. Real-world scenes (eg, albedos or shapes) can be very complex, conventionally requiring high dimensional representations which are hard to estimate and store. We propose a low-dimensional rep- resentation, called a scene recipe, that relies on the image itself to de- scribe the complex scene conﬁgurations. Shape recipes are an example: these are the regression coefﬁcients that predict the bandpassed shape from image data. We describe the beneﬁts of this representation, and show two uses illustrating their properties: (1) we improve stereo shape estimates by learning shape recipes at low resolution and applying them at full resolution; (2) Shape recipes implicitly contain information about lighting and materials and we use them for material segmentation."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Hidden Markov Model of Cortical Synaptic Plasticity", "Title": "Derivation of the Learning Rule", "Abstract": "Cortical synaptic plasticity depends on the relative timing of pre- and postsynaptic spikes and also on the temporal pattern of presynaptic spikes and of postsynaptic spikes. We study the hypothesis that cortical synap- tic plasticity does not associate individual spikes, but rather whole ﬁr- ing episodes, and depends only on when these episodes start and how long they last, but as little as possible on the timing of individual spikes. Here we present the mathematical background for such a study. Stan- dard methods from hidden Markov models are used to deﬁne what “ﬁr- ing episodes” are. Estimating the probability of being in such an episode requires not only the knowledge of past spikes, but also of future spikes. We show how to construct a causal learning rule, which depends only on past spikes, but associates pre- and postsynaptic ﬁring episodes as if it also knew future spikes. We also show that this learning rule agrees with some features of synaptic plasticity in superﬁcial layers of rat visual cortex (Froemke and Dan, Nature 416:433, 2002)."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Rate Distortion Function in the Spin Glass State", "Title": "A Toy Model", "Abstract": "We applied statistical mechanics to an inverse problem of linear mapping to investigate the physics of optimal lossy compressions. We used the replica symmetry breaking technique with a toy model to demonstrate Shannon’s result. The rate distortion function, which is widely known as the theoretical limit of the compression with a ﬁdelity criterion, is derived. Numerical study shows that sparse constructions of the model provide suboptimal compressions."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Real Time Voice Processing with Audiovisual Feedback", "Title": "Toward Autonomous Agents with Perfect Pitch", "Abstract": "We have implemented a real time front end for detecting voiced speech and estimating its fundamental frequency. The front end performs the signal processing for voice-driven agents that attend to the pitch contours of human speech and provide continuous audiovisual feedback. The al- gorithm we use for pitch tracking has several distinguishing features: it makes no use of FFTs or autocorrelation at the pitch period; it updates the pitch incrementally on a sample-by-sample basis; it avoids peak picking and does not require interpolation in time or frequency to obtain high res- olution estimates; and it works reliably over a four octave range, in real time, without the need for postprocessing to produce smooth contours. The algorithm is based on two simple ideas in neural computation: the introduction of a purposeful nonlinearity, and the error signal of a least squares ﬁt. The pitch tracker is used in two real time multimedia applica- tions: a voice-to-MIDI player that synthesizes electronic music from vo- calized melodies, and an audiovisual Karaoke machine with multimodal feedback. Both applications run on a laptop and display the user’s pitch scrolling across the screen as he or she sings into the computer."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Ranking with Large Margin Principle", "Title": "Two Approaches", "Abstract": "We discuss the problem of ranking k instances with the use of a \"large  margin\" principle. We introduce two main approaches: the first is the  \"fixed margin\" policy in which the margin of the closest neighboring  classes is being maximized - which turns out to be a direct generaliza(cid:173) tion of SVM to ranking learning. The second approach allows for k - 1  different margins where the sum of margins is maximized. This approach  is shown to reduce to lI-SVM when the number of classes k = 2. Both  approaches are optimal in size of 21 where I is the total number of training  examples. Experiments performed on visual classification and \"collab(cid:173) orative filtering\" show that both approaches outperform existing ordinal  regression algorithms applied for ranking and multi-class SVM applied  to general multi-class classification."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Categorization Under Complexity", "Title": "A Unified MDL Account of Human Learning of Regular and Irregular Categories", "Abstract": "A number of different principles have been advanced to explain the manner in which hu(cid:173) mans learn to categorize objects. It has been variously suggested that the underlying prin(cid:173) ciple might be the similarity structure of objects [1], the manipulability of decision bound~ aries [2], or Bayesian inference [3][4]. While many of these theories are mathematically well-grounded and have been successful in explaining a range of experimental findings, they have commonly only been tested on a narrow collection of concept types similar to the simple unimodal categories of Figure 1(a-e)."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Maximally Informative Dimensions", "Title": "Analyzing Neural Responses to Natural Signals", "Abstract": "Abstract Unavailable"}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Expected and Unexpected Uncertainty", "Title": "ACh and NE in the Neocortex", "Abstract": "Inference and adaptation in noisy and changing, rich sensory environ- ments are rife with a variety of speciﬁc sorts of variability. Experimental and theoretical studies suggest that these different forms of variability play different behavioral, neural and computational roles, and may be reported by different (notably neuromodulatory) systems. Here, we re- ﬁne our previous theory of acetylcholine’s role in cortical inference in the (oxymoronic) terms of expected uncertainty, and advocate a theory for norepinephrine in terms of unexpected uncertainty. We suggest that norepinephrine reports the radical divergence of bottom-up inputs from prevailing top-down interpretations, to inﬂuence inference and plasticity. We illustrate this proposal using an adaptive factor analysis model."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Feature Selection and Classification on Matrix Data", "Title": "From Large Margins to Small Covering Numbers", "Abstract": "We investigate the problem of learning a classiﬂcation task for datasets which are described by matrices. Rows and columns of these matrices correspond to objects, where row and column ob- jects may belong to diﬁerent sets, and the entries in the matrix express the relationships between them. We interpret the matrix el- ements as being produced by an unknown kernel which operates on object pairs and we show that - under mild assumptions - these ker- nels correspond to dot products in some (unknown) feature space. Minimizing a bound for the generalization error of a linear classi- ﬂer which has been obtained using covering numbers we derive an objective function for model selection according to the principle of structural risk minimization. The new objective function has the advantage that it allows the analysis of matrices which are not pos- itive deﬂnite, and not even symmetric or square. We then consider the case that row objects are interpreted as features. We suggest an additional constraint, which imposes sparseness on the row objects and show, that the method can then be used for feature selection. Finally, we apply this method to data obtained from DNA microar- rays, where \\column\" objects correspond to samples, \\row\" objects correspond to genes and matrix elements correspond to expression levels. Benchmarks are conducted using standard one-gene classiﬂ- cation and support vector machines and K-nearest neighbors after standard feature selection. Our new method extracts a sparse set of genes and provides superior classiﬂcation results."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "A Digital Antennal Lobe for Pattern Equalization", "Title": "Analysis and Design", "Abstract": "Re-mapping  patterns  in  order  to  equalize  their  distribution  may  greatly  simplify  both the  structure  and  the  training of classifiers.  Here,  the  properties  of one  such  map  obtained  by  running  a  few  steps of discrete-time dynamical system are explored.  The system  is  called  'Digital  Antennal  Lobe'  (DAL)  because  it  is  inspired  by  recent studies of the antennallobe, a structure in the olfactory sys(cid:173) tem  of the  grasshopper.  The  pattern-spreading  properties  of the  DAL  as  well  as  its  average behavior  as  a  function  of its  (few)  de(cid:173) sign  parameters are analyzed by extending previous results of Van  Vreeswijk and Sompolinsky.  Furthermore, a technique for  adapting  the  parameters of the  initial  design  in  order  to  obtain  opportune  noise-rejection behavior is suggested.  Our results are demonstrated  with a  number of simulations."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "VIBES", "Title": "A Variational Inference Engine for Bayesian Networks", "Abstract": "In recent years variational methods have become a popular tool for approximate inference and learning in a wide variety of proba- bilistic models. For each new application, however, it is currently necessary (cid:12)rst to derive the variational update equations, and then to implement them in application-speci(cid:12)c code. Each of these steps is both time consuming and error prone. In this paper we describe a general purpose inference engine called VIBES (‘Variational Infer- ence for Bayesian Networks’) which allows a wide variety of proba- bilistic models to be implemented and solved variationally without recourse to coding. New models are speci(cid:12)ed either through a simple script or via a graphical interface analogous to a drawing package. VIBES then automatically generates and solves the vari- ational equations. We illustrate the power and (cid:13)exibility of VIBES using examples from Bayesian mixture modelling."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "How to Combine Color and Shape Information for 3D Object Recognition", "Title": "Kernels do the Trick", "Abstract": "This paper presents a  kernel method that allows to combine  color  and shape information for  appearance-based object recognition.  It  doesn't require to define a new common representation, but use the  power of kernels to combine different representations together in an  effective manner.  These results are achieved using results of statis(cid:173) tical mechanics of spin glasses combined with Markov random fields  via kernel functions.  Experiments show  an increase in recognition  rate up  to 5.92%  with respect to conventional strategies."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Learning About Multiple Objects in Images", "Title": "Factorial Learning without Factorial Search", "Abstract": "We consider data which are images containing views of multiple objects. Our task is to learn about each of the objects present in the images. This task can be approached as a factorial learning problem, where each image must be explained by instantiating a model for each of the objects present with the correct instantiation parameters. A major problem with learning a factorial model is that as the number of objects increases, there is a combinatorial explosion of the number of conﬁgurations that need to be considered. We develop a method to extract object models sequentially from the data by making use of a robust statistical method, thus avoid- ing the combinatorial explosion, and present results showing successful extraction of objects from real images."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Fast Sparse Gaussian Process Methods", "Title": "The Informative Vector Machine", "Abstract": "We present a framework for sparse Gaussian process (GP) methods which uses forward selection with criteria based on information- theoretic principles, previously suggested for active learning. Our goal is not only to learn d{sparse predictors (which can be evalu- ated in O(d) rather than O(n), d (cid:28) n, n the number of training points), but also to perform training under strong restrictions on time and memory requirements. The scaling of our method is at most O(n (cid:1) d2), and in large real-world classi(cid:12)cation experiments we show that it can match prediction performance of the popular support vector machine (SVM), yet can be signi(cid:12)cantly faster in training. In contrast to the SVM, our approximation produces esti- mates of predictive probabilities (‘error bars’), allows for Bayesian model selection and is less complex in implementation."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Kernel-Based Extraction of Slow Features", "Title": "Complex Cells Learn Disparity and Translation Invariance from Natural Images", "Abstract": "F  _  V  _  L.i Yi 2  L.i Yi"}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Coulomb Classifiers", "Title": "Generalizing Support Vector Machines via an Analogy to Electrostatic Systems", "Abstract": "We introduce a family of classiﬂers based on a physical analogy to an electrostatic system of charged conductors. The family, called Coulomb classiﬂers, includes the two best-known support-vector machines (SVMs), the ”{SVM and the C{SVM. In the electrostat- ics analogy, a training example corresponds to a charged conductor at a given location in space, the classiﬂcation function corresponds to the electrostatic potential function, and the training objective function corresponds to the Coulomb energy. The electrostatic framework provides not only a novel interpretation of existing algo- rithms and their interrelationships, but it suggests a variety of new methods for SVMs including kernels that bridge the gap between polynomial and radial-basis functions, objective functions that do not require positive-deﬂnite kernels, regularization techniques that allow for the construction of an optimal classiﬂer in Minkowski space. Based on the framework, we propose novel SVMs and per- form simulation studies to show that they are comparable or su- perior to standard SVMs. The experiments include classiﬂcation tasks on data which are represented in terms of their pairwise prox- imities, where a Coulomb Classiﬂer outperformed standard SVMs."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Minimax Differential Dynamic Programming", "Title": "An Application to Robust Biped Walking", "Abstract": "We developed a robust control policy design method in high-dimensional state space by using differential dynamic programming with a minimax criterion. As an example, we applied our method to a simulated ﬁve link biped robot. The results show lower joint torques from the optimal con- trol policy compared to a hand-tuned PD servo controller. Results also show that the simulated biped robot can successfully walk with unknown disturbances that cause controllers generated by standard differential dy- namic programming and the hand-tuned PD servo to fail. Learning to compensate for modeling error and previously unknown disturbances in conjunction with robust control design is also demonstrated."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Improving Transfer Rates in Brain Computer Interfacing", "Title": "A Case Study", "Abstract": "In this paper we present results of a study on brain computer interfacing. We adopted an approach of Farwell & Donchin [4], which we tried to improve in several aspects. The main objective was to improve the trans- fer rates based on ofﬂine analysis of EEG-data but within a more realistic setup closer to an online realization than in the original studies. The ob- jective was achieved along two different tracks: on the one hand we used state-of-the-art machine learning techniques for signal classiﬁcation and on the other hand we augmented the data space by using more electrodes for the interface. For the classiﬁcation task we utilized SVMs and, as mo- tivated by recent ﬁndings on the learning of discriminative densities, we accumulated the values of the classiﬁcation function in order to combine several classiﬁcations, which ﬁnally lead to signiﬁcantly improved rates as compared with techniques applied in the original work. In combina- tion with the data space augmentation, we achieved competitive transfer rates at an average of 50.5 bits/min and with a maximum of 84.7 bits/min."}
