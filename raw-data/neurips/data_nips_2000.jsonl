{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Overfitting in Neural Nets", "Title": "Backpropagation, Conjugate Gradient, and Early Stopping", "Abstract": "The conventional wisdom is that backprop nets with excess hidden units  generalize  poorly.  We  show  that  nets  with  excess  capacity  generalize  well when  trained with backprop and early  stopping.  Experiments  sug(cid:173) gest two reasons for this:  1) Overfitting can vary significantly in different  regions of the model.  Excess capacity allows better fit to regions of high  non-linearity,  and  backprop often  avoids  overfitting  the  regions  of low  non-linearity.  2)  Regardless  of size,  nets  learn  task  subcomponents  in  similar sequence.  Big nets pass  through stages  similar to those learned  by  smaller nets.  Early  stopping can  stop training the large net  when  it  generalizes  comparably  to  a  smaller net.  We  also  show  that conjugate  gradient can yield worse generalization because it overfits regions of low  non-linearity when learning to fit regions of high non-linearity."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "A PAC-Bayesian Margin Bound for Linear Classifiers", "Title": "Why SVMs work", "Abstract": "to a  vanishing com(cid:173)"}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "APRICODD", "Title": "Approximate Policy Construction Using Decision Diagrams", "Abstract": "We propose a method of approximate dynamic programming for Markov  decision processes (MDPs) using algebraic decision diagrams  (ADDs).  We produce near-optimal value functions and policies with much lower  time  and  space  requirements  than  exact  dynamic  programming.  Our  method reduces  the  sizes  of the  intermediate value functions  generated  during value iteration by replacing the values at the terminals of the ADD  with  ranges  of values.  Our method is  demonstrated on  a class  of large  MDPs (with up to 34 billion states), and we compare the results with the  optimal value functions."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Accumulator Networks", "Title": "Suitors of Local Probability Propagation", "Abstract": "One  way  to  approximate  inference  in  richly-connected  graphical  models  is  to  apply  the  sum-product  algorithm  (a.k.a.  probabil(cid:173) ity propagation algorithm), while  ignoring the fact  that the graph  has cycles.  The sum-product  algorithm can  be directly applied in  Gaussian networks  and in  graphs for  coding,  but for  many condi(cid:173) tional probability functions  - including the sigmoid function  - di(cid:173) rect  application of the sum-product  algorithm is  not possible.  We  introduce  \"accumulator networks\"  that  have low  local  complexity  (but exponential global complexity) so  the sum-product algorithm  can be directly applied.  In an accumulator network, the probability  of a child given its parents is  computed by accumulating the inputs  from the parents in a Markov chain or more generally a tree.  After  giving  expressions  for  inference  and  learning  in  accumulator  net(cid:173) works,  we  give  results  on the  \"bars problem\"  and on the problem  of extracting translated, overlapping faces  from  an image."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Beyond Maximum Likelihood and Density Estimation", "Title": "A Sample-Based Criterion for Unsupervised Learning of Complex Models", "Abstract": "The goal of many unsupervised learning procedures is to bring two  probability  distributions  into  alignment.  Generative  models  such  as  Gaussian mixtures and Boltzmann machines can be cast in this  light,  as  can recoding models  such  as ICA  and projection pursuit.  We propose a novel sample-based error measure for these classes of  models, which applies even in situations where maximum likelihood  (ML)  and  probability  density  estimation-based  formulations  can(cid:173) not be applied,  e.g.,  models that are nonlinear  or have intractable  posteriors.  Furthermore,  our  sample-based  error  measure  avoids  the difficulties of approximating a  density function.  We  prove that  with  an unconstrained  model,  (1)  our approach  converges  on  the  correct solution as the number of samples goes to infinity,  and  (2)  the expected solution of our approach in the generative framework  is  the  ML  solution.  Finally,  we  evaluate our approach via simula(cid:173) tions of linear and nonlinear models  on  mixture of Gaussians and  ICA problems.  The experiments show the broad applicability and  generality of our approach."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "The Manhattan World Assumption", "Title": "Regularities in Scene Statistics which Enable Bayesian Inference", "Abstract": "Preliminary work by the authors made use of the so-called  \"Man(cid:173) hattan  world\"  assumption  about  the  scene  statistics  of  city  and  indoor scenes.  This assumption stated that such  scenes were built  on a  cartesian grid which led to regularities in the image edge gra(cid:173) dient  statistics.  In this paper we  explore the general applicability  of this  assumption  and show  that,  surprisingly,  it holds  in a  large  variety of less structured environments including rural scenes.  This  enables us, from  a single image, to determine the orientation of the  viewer relative to the scene structure and also to detect target ob(cid:173) jects  which  are  not  aligned  with  the  grid.  These  inferences  are  performed  using  a  Bayesian  model  with  probability  distributions  (e.g.  on the image gradient statistics)  learnt from  real data."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Discovering Hidden Variables", "Title": "A Structure-Based Approach", "Abstract": "A serious problem in learning probabilistic models is the presence of hid(cid:173) den variables. These variables are not observed, yet interact with several  of the observed variables.  As  such,  they induce seemingly complex de(cid:173) pendencies  among  the  latter.  In  recent years,  much  attention  has  been  devoted to  the  development of algorithms for  learning parameters,  and  in  some cases  structure, in  the presence of hidden variables.  In this  pa(cid:173) per,  we  address  the  related  problem of detecting  hidden  variables  that  interact with the observed variables.  This problem is of interest both for  improving our understanding of the domain and as a preliminary step that  guides the learning procedure towards promising models.  A very natural  approach is  to  search for \"structural  signatures\" of hidden variables - substructures in the learned network that tend to  suggest the presence of  a hidden variable.  We  make this  basic  idea concrete, and  show  how  to  integrate it with structure-search algorithms.  We evaluate this method on  several synthetic and real-life datasets, and show that it performs surpris(cid:173) ingly well."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "FaceSync", "Title": "A Linear Operator for Measuring Synchronization of Video Facial Images and Audio Tracks", "Abstract": "This paper describes an algorithm, FaceSync, that measures the degree of synchronization  between the video image of a face and the associated audio signal. We can do this task by  synthesizing the talking face,  using techniques  such  as Video  Rewrite  [1], and then com(cid:173) paring the synthesized video with the test video. That process, however, is expensive. Our  solution finds  a linear operator that, when applied to the audio and video signals, generates  an  audio-video-synchronization-error  signal.  The  linear  operator  gathers  information  from throughout the image and thus allows us to do the computation inexpensively.  Hershey and Movellan  [2]  describe an approach based on measuring the mutual informa(cid:173) tion between the audio signal and individual pixels in the  video. The correlation between  the audio signal, x, and one pixel in the image y,  is  given by Pearson's correlation,  r.  The  mutual information between these two  variables is  given by f(x,y)  = -1/2  log(l-?). They  create movies that show the regions of the video that have high correlation with the audio;"}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Bayes Networks on Ice", "Title": "Robotic Search for Antarctic Meteorites", "Abstract": "A  Bayes  network  based  classifier  for  distinguishing  terrestrial  rocks  from  meteorites  is  implemented  onboard  the  Nomad  robot.  Equipped with a camera,  spectrometer and eddy current sensor, this  robot searched the  ice  sheets of Antarctica and autonomously made  the first robotic identification of a meteorite, in January 2000 at the  Elephant Moraine.  This paper discusses  rock classification from  a  robotic platform, and describes the system onboard Nomad."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Shape Context", "Title": "A New Descriptor for Shape Matching and Object Recognition", "Abstract": "We  develop  an  approach  to  object  recognition  based  on  match(cid:173) ing shapes and using a resulting measure of similarity in a  nearest  neighbor  classifier.  The  key  algorithmic  problem  here  is  that  of  finding  pointwise  correspondences  between  an  image shape  and  a  stored  prototype  shape.  We  introduce  a  new  shape  descriptor,  the  shape  context,  which  makes  this  possible,  using  a  simple  and  robust algorithm.  The shape context at a point captures the distri(cid:173) bution over relative positions of other shape points and thus sum(cid:173) marizes  global  shape in  a  rich,  local  descriptor.  We  demonstrate  that  shape  contexts  greatly  simplify  recovery  of correspondences  between points of two given shapes.  Once shapes are aligned, shape  contexts are used to define a robust score for measuring shape sim(cid:173) ilarity.  We  have  used  this  score  in  a  nearest-neighbor  classifier  for  recognition of hand written  digits  as  well  as  3D  objects,  using  exactly  the  same  distance  function.  On  the  benchmark  MNIST  dataset  of handwritten  digits,  this  yields  an  error  rate  of 0.63%,  outperforming other published techniques."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Kernel-Based Reinforcement Learning in Average-Cost Problems", "Title": "An Application to Optimal Portfolio Choice", "Abstract": "Many  approaches  to  reinforcement  learning  combine  neural  net(cid:173) works  or other  parametric function  approximators with  a  form  of  temporal-difference  learning  to  estimate  the  value  function  of  a  Markov  Decision  Process.  A  significant disadvantage of those  pro(cid:173) cedures  is that the resulting learning algorithms are frequently un(cid:173) stable.  In  this  work,  we  present  a  new,  kernel-based  approach  to  reinforcement learning which overcomes this difficulty and provably  converges  to a  unique solution.  By contrast to existing algorithms,  our  method  can  also  be  shown  to  be  consistent  in  the  sense  that  its  costs  converge  to  the  optimal costs  asymptotically.  Our  focus  is  on learning in  an average-cost  framework  and on a  practical  ap(cid:173) plication to  the optimal portfolio choice problem."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Interactive Parts Model", "Title": "An Application to Recognition of On-line Cursive Script", "Abstract": "In  this  work,  we  introduce  an  Interactive  Parts  (IP)  model as  an  alternative  to  Hidden  Markov  Models  (HMMs).  We  tested  both  models  on  a  database of on-line  cursive  script.  We  show  that  im(cid:173) plementations of HMMs  and the IP model, in which  all letters  are  assumed  to have  the same average width, give  comparable results.  However , in contrast to  HMMs,  the  IP model can handle duration  modeling without  an increase  in  computational complexity."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Feature Correspondence", "Title": "A Markov Chain Monte Carlo Approach", "Abstract": "When  trying  to  recover  3D  structure  from  a  set  of  images,  the  most difficult  problem is  establishing  the correspondence  between  the measurements.  Most existing  approaches  assume that features  can be tracked across frames,  whereas methods that exploit rigidity  constraints to facilitate matching do so only under restricted  cam(cid:173) era  motion.  In  this  paper  we  propose  a  Bayesian  approach  that  avoids  the  brittleness  associated  with  singling out  one  \"best\"  cor(cid:173) respondence,  and instead consider the distribution over all possible  correspondences.  We  treat  both  a  fully  Bayesian  approach  that  yields  a  posterior  distribution,  and  a  MAP  approach  that  makes  use of EM  to maximize this posterior.  We show how  Markov chain  Monte  Carlo methods can  be  used  to implement these  techniques  in practice,  and present  experimental results on real  data."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Temporally Dependent Plasticity", "Title": "An Information Theoretic Account", "Abstract": "The paradigm of Hebbian learning has recently received a novel in(cid:173) terpretation with the discovery of synaptic plasticity that depends  on the relative timing of pre and post  synaptic spikes.  This paper  derives a temporally dependent learning rule from the basic princi(cid:173) ple of mutual information maximization and studies its relation to  the experimentally observed  plasticity.  We  find  that  a  supervised  spike-dependent learning rule sharing similar structure with the ex(cid:173) perimentally observed plasticity increases mutual information to a  stable near  optimal  level.  Moreover,  the  analysis  reveals  how  the  temporal structure of time-dependent learning rules is  determined  by the temporal filter  applied by neurons over their inputs.  These  results suggest experimental prediction as to the dependency of the  learning rule on neuronal biophysical parameters"}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Learning Curves for Gaussian Processes Regression", "Title": "A Framework for Good Approximations", "Abstract": "Based  on  a  statistical mechanics  approach,  we  develop  a  method  for approximately computing average case learning curves for Gaus(cid:173) sian  process  regression  models.  The  approximation  works  well  in  the large  sample size  limit  and for  arbitrary dimensionality  of the  input space.  We explain how the approximation can be systemati(cid:173) cally improved and argue that similar techniques can be applied to  general likelihood models."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Periodic Component Analysis", "Title": "An Eigenvalue Method for Representing Periodic Structure in Speech", "Abstract": "An  eigenvalue  method  is  developed  for  analyzing  periodic  structure in  speech.  Signals are analyzed by  a matrix diagonalization reminiscent of  methods  for principal component analysis  (PCA)  and  independent com(cid:173) ponent analysis (ICA).  Our method-called periodic component analysis  (1l\"CA)-uses  constructive interference to  enhance periodic  components  of the frequency  spectrum and  destructive interference  to  cancel  noise.  The front end emulates important aspects of auditory processing, such as  cochlear filtering, nonlinear compression, and insensitivity to phase, with  the  aim  of approaching the robustness of human  listeners.  The  method  avoids the inefficiencies of autocorrelation at the pitch period:  it does not  require  long  delay  lines,  and  it correlates  signals  at  a clock rate  on  the  order  of the  actual pitch,  as  opposed  to  the  original  sampling rate.  We  derive its cost function and present some experimental results."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Learning Continuous Distributions", "Title": "Simulations With Field Theoretic Priors", "Abstract": "that a particular density Q(x) gave rise to these data is given by  P[Q(x)] rr~1 Q(Xi)"}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Divisive and Subtractive Mask Effects", "Title": "Linking Psychophysics and Biophysics", "Abstract": "We describe an analogy between psychophysically measured effects  in  contrast  masking,  and  the  behavior  of a  simple  integrate-and(cid:173) fire  neuron  that  receives  time-modulated  inhibition.  In  the  psy(cid:173) chophysical experiments, we tested observers ability to discriminate  contrasts of peripheral  Gabor patches  in  the  presence  of collinear  Gabor flankers.  The data reveal a complex interaction pattern that  we  account  for  by  assuming  that  flankers  provide  divisive  inhibi(cid:173) tion  to  the  target  unit  for  low  target  contrasts,  but  provide  sub(cid:173) tractive  inhibition  to  the  target  unit  for  higher  target  contrasts.  A similar switch from divisive to subtractive inhibition is  observed  in  an integrate-and-fire unit that receives  inhibition  modulated in  time such that the cell spends part of the time in  a high-inhibition  state  and  part  of  the  time  in  a  low-inhibition  state.  The  simi(cid:173) larity  between  the effects  suggests  that  one  may  cause  the  other.  The biophysical model makes testable predictions for  physiological  single-cell  recordings."}
