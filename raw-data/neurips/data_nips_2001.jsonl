{"Type": "conference", "Year": "2001", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Playing is believing", "Title": "The role of beliefs in multi-agent learning", "Abstract": "We propose a new classiﬁcation for multi-agent learning algorithms, with each league of players characterized by both their possible strategies and possible beliefs. Using this classiﬁcation, we review the optimality of ex- isting algorithms, including the case of interleague play. We propose an incremental improvement to the existing algorithms that seems to achieve average payoffs that are at least the Nash equilibrium payoffs in the long- run against fair opponents."}
{"Type": "conference", "Year": "2001", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Probabilistic principles in unsupervised learning of visual structure", "Title": "human data and a model", "Abstract": "To ﬁnd out how the representations of structured visual objects depend on the co-occurrence statistics of their constituents, we exposed subjects to a set of composite images with tight control exerted over (1) the condi- tional probabilities of the constituent fragments, and (2) the value of Bar- low’s criterion of “suspicious coincidence” (the ratio of joint probability to the product of marginals). We then compared the part veriﬁcation re- sponse times for various probe/target combinations before and after the exposure. For composite probes, the speedup was much larger for tar- gets that contained pairs of fragments perfectly predictive of each other, compared to those that did not. This effect was modulated by the sig- niﬁcance of their co-occurrence as estimated by Barlow’s criterion. For lone-fragment probes, the speedup in all conditions was generally lower than for composites. These results shed light on the brain’s strategies for unsupervised acquisition of structural information in vision."}
{"Type": "conference", "Year": "2001", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Prodding the ROC Curve", "Title": "Constrained Optimization of Classifier Performance", "Abstract": "An ROC curve (Green & Swets, 1966) can be used to visualize the discriminative performance of a two-alternative classiﬁer that outputs class posteriors. To explain the ROC curve, a classiﬁer can be thought of as making a positive/negative judgement as to whether an input is a member of some class. Two different accuracy measures can be obtained from the classiﬁer: the accuracy of correctly identifying an input as a member of the class (a correct acceptance or CA), and the accuracy of correctly identifying an input as a nonmember of the class (a correct rejection or CR). To evaluate the CA and CR rates, it is necessary to pick a threshold above which the classiﬁer’s probability estimate is inter- preted as an “accept,” and below which is interpreted as a “reject”—call this the criterion. The ROC curve plots CA against CR rates for various criteria (Figure 1a). Note that as the threshold is lowered, the CA rate increases and the CR rate decreases. For a criterion of 1, the CA rate approaches 0 and the CR rate 1; for a criterion of 0, the CA rate approaches 1"}
{"Type": "conference", "Year": "2001", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Estimating Car Insurance Premia", "Title": "a Case Study in High-Dimensional Data Inference", "Abstract": "Estimating insurance premia from data is a difficult regression problem for several reasons: the large number of variables, many of which are .discrete, and the very peculiar shape of the noise distri(cid:173) bution, asymmetric with fat tails, with a large majority zeros and a few unreliable and very large values. We compare several machine learning methods for estimating insurance premia, and test them on a large data base of car insurance policies. We find that func(cid:173) tion approximation methods that do not optimize a squared loss, like Support Vector Machines regression, do not work well in this context. Compared methods include decision trees and generalized linear models. The best results are obtained with a mixture of experts, which better identifies the least and most risky contracts, and allows to reduce the median premium by charging more to the most risky customers."}
{"Type": "conference", "Year": "2001", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Classifying Single Trial EEG", "Title": "Towards Brain Computer Interfacing", "Abstract": "Driven by the progress in the ﬁeld of single-trial analysis of EEG, there is a growing interest in brain computer interfaces (BCIs), i.e., systems that enable human subjects to control a computer only by means of their brain signals. In a pseudo-online simulation our BCI detects upcoming ﬁnger movements in a natural keyboard typing condition and predicts their lat- erality. This can be done on average 100–230 ms before the respective key is actually pressed, i.e., long before the onset of EMG. Our approach is appealing for its short response time and high classiﬁcation accuracy (>96%) in a binary decision where no human training is involved. We compare discriminative classiﬁers like Support Vector Machines (SVMs) and different variants of Fisher Discriminant that possess favorable reg- ularization properties for dealing with high noise cases (inter-trial vari- ablity)."}
{"Type": "conference", "Year": "2001", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "A Model of the Phonological Loop", "Title": "Generalization and Binding", "Abstract": "We present a  neural network model that shows how the prefrontal  cortex, interacting with the basal ganglia, can maintain a sequence  of  phonological  information  in  activation-based  working  memory  (i.e.,  the  phonological  loop).  The  primary function  of this  phono(cid:173) logical  loop  may  be  to  transiently  encode  arbitrary  bindings  of  information  necessary  for  tasks  - the  combinatorial  expressive  power  of language  enables  very  flexible  binding  of essentially  ar(cid:173) bitrary  pieces  of information.  Our  model  takes  advantage  of the  closed-class nature of phonemes, which allows  different  neural rep(cid:173) resentations of all possible phonemes at each sequential position to  be encoded.  To  make this work,  we  suggest that the basal ganglia  provide a  region-specific update signal that allocates phonemes to  the appropriate sequential coding slot.  To  demonstrate that flexi(cid:173) ble,  arbitrary binding of novel  sequences can be supported by this  mechanism,  we  show  that  the  model  can  generalize  to  novel  se(cid:173) quences after  moderate amounts of training."}
{"Type": "conference", "Year": "2001", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Linking Motor Learning to Function Approximation", "Title": "Learning in an Unlearnable Force Field", "Abstract": "velocity-dependent force field (such as the fields we use), the IM must be able to encode velocity in order to anticipate the upcoming force. We hoped that the e#ect of errors in one direction on subsequent movements in other directions would give information about the width of the elements which the IM used in encoding velocity. For example, if the basis elements were narrow, then movements in a given direction would result in little or no change in performance in neighboring directions. Wide basis elements would mean appropriately larger e#ects. We hypothesized that an estimate of the width of the basis elements could be cal- culated by fitting the time sequence of errors to a set of equations representing a dynamical system. The dynamical system assumed that error in a movement resulted from a di#erence between the IM's approximation and the actual environ- ment, an assumption that has recently been corroborated [3]. The error in turn changed the IM, a#ecting subsequent movements:"}
{"Type": "conference", "Year": "2001", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Modularity in the motor system", "Title": "decomposition of muscle patterns as combinations of time-varying synergies", "Abstract": "The question of whether the nervous system produces movement through the combination of a few discrete elements has long been central to the study of motor control. Muscle synergies, i.e. coordinated patterns of muscle activity, have been proposed as possible building blocks. Here we propose a model based on combinations of muscle synergies with a spe- ciﬁc amplitude and temporal structure. Time-varying synergies provide a realistic basis for the decomposition of the complex patterns observed in natural behaviors. To extract time-varying synergies from simultane- ous recording of EMG activity we developed an algorithm which extends existing non-negative matrix factorization techniques."}
{"Type": "conference", "Year": "2001", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "On Discriminative vs. Generative Classifiers", "Title": "A comparison of logistic regression and naive Bayes", "Abstract": "We  compare discriminative  and  generative learning as  typified  by  logistic regression and naive Bayes.  We show,  contrary to a widely(cid:173) held  belief  that  discriminative  classifiers  are  almost  always  to  be  preferred,  that  there  can  often  be  two  distinct  regimes  of  per(cid:173) formance  as  the  training  set  size  is  increased,  one  in  which  each  algorithm  does  better.  This  stems  from  the  observation- which  is  borne  out  in  repeated  experiments- that  while  discriminative  learning has lower asymptotic error, a generative classifier may also  approach its  (higher)  asymptotic error  much faster."}
{"Type": "conference", "Year": "2001", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "On Spectral Clustering", "Title": "Analysis and an algorithm", "Abstract": "Despite many empirical successes of spectral  clustering  methods(cid:173) algorithms  that  cluster  points  using  eigenvectors  of  matrices  de(cid:173) rived  from  the  data- there  are  several  unresolved  issues.  First,  there  are  a  wide  variety  of  algorithms  that  use  the  eigenvectors  in  slightly  different  ways.  Second,  many of these  algorithms  have  no  proof that  they  will  actually  compute  a  reasonable  clustering.  In  this  paper,  we  present  a  simple  spectral  clustering  algorithm  that can be implemented using a  few  lines  of Matlab.  Using  tools  from  matrix  perturbation  theory,  we  analyze  the  algorithm,  and  give  conditions  under  which  it  can  be  expected  to  do  well.  We  also  show  surprisingly  good  experimental  results  on  a  number  of  challenging clustering problems."}
{"Type": "conference", "Year": "2001", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Cobot", "Title": "A Social Reinforcement Learning Agent", "Abstract": "We report on the use of reinforcement learning with Cobot, a software agent residing in the well-known online community LambdaMOO. Our initial work on Cobot (Isbell et al.2000) provided him with the ability to collect social statistics and report them to users. Here we describe an application of RL allowing Cobot to take proactive actions in this complex social environment, and adapt behavior from multiple sources of human reward. After 5 months of training, and 3171 reward and punishment events from 254 different LambdaMOO users, Cobot learned nontrivial preferences for a number of users, modiﬁng his behavior based on his current state. Here we describe LambdaMOO and the state and action spaces of Cobot, and report the statistical results of the learning experiment."}
{"Type": "conference", "Year": "2001", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Relative Density Nets", "Title": "A New Way to Combine Backpropagation with HMM's", "Abstract": "Logistic units in the first  hidden layer of a  feedforward neural net(cid:173) work  compute  the  relative  probability  of a  data point  under  two  Gaussians.  This  leads  us  to  consider  substituting  other  density  models.  We  present  an architecture for  performing discriminative  learning of Hidden Markov Models using a  network of many small  HMM's.  Experiments on speech  data show it  to be superior to the  standard method of discriminatively training HMM's."}
{"Type": "conference", "Year": "2001", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "The Intelligent surfer", "Title": "Probabilistic Combination of Link and Content Information in PageRank", "Abstract": "The PageRank algorithm, used in the Google search engine, greatly  improves the results of Web search by taking into account the link  structure  of  the  Web.  PageRank  assigns  to  a  page  a  score  propor- tional to the number of times a random surfer would visit that page,  if  it  surfed  indefinitely  from  page  to  page,  following  all  outlinks  from  a  page  with  equal  probability.  We  propose  to  improve  Page- Rank  by  using  a  more  intelligent  surfer,  one  that  is  guided  by  a  probabilistic model of the relevance of a page to a query. Efficient  execution  of  our  algorithm  at  query  time  is  made  possible  by  pre- computing  at  crawl  time  (and  thus  once  for  all  queries)  the  neces- sary  terms.  Experiments  on  two  large  subsets  of  the  Web  indicate  that  our  algorithm  significantly  outperforms  PageRank  in  the  (hu- man-rated)  quality  of the pages returned, while remaining efficient  enough to be used in today’s large search engines."}
{"Type": "conference", "Year": "2001", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "KLD-Sampling", "Title": "Adaptive Particle Filters", "Abstract": "Over the last years, particle ﬁlters have been applied with great success to a variety of state estimation problems. We present a statistical approach to increasing the efﬁciency of particle ﬁlters by adapting the size of sample sets on-the-ﬂy. The key idea of the KLD-sampling method is to bound the approximation error introduced by the sample-based representation of the particle ﬁlter. The name KLD-sampling is due to the fact that we measure the approximation error by the Kullback-Leibler distance. Our adaptation approach chooses a small number of samples if the density is focused on a small part of the state space, and it chooses a large number of samples if the state uncertainty is high. Both the implementation and computation overhead of this approach are small. Extensive experiments using mobile robot localization as a test application show that our approach yields drastic improvements over particle ﬁlters with ﬁxed sample set sizes and over a previously introduced adaptation technique."}
{"Type": "conference", "Year": "2001", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "Product Analysis", "Title": "Learning to Model Observations as Products of Hidden Variables", "Abstract": "Factor analysis  and principal  components  analysis  can be  used  to  model linear relationships between observed variables  and linearly  map  high-dimensional  data to  a  lower-dimensional  hidden  space.  In  factor  analysis,  the  observations  are  modeled  as  a  linear  com(cid:173) bination  of normally  distributed  hidden  variables.  We  describe  a  nonlinear  generalization of factor  analysis,  called  \"product analy(cid:173) sis\",  that  models  the  observed  variables  as  a  linear  combination  of products  of normally  distributed  hidden  variables.  Just  as  fac(cid:173) tor  analysis  can  be  viewed  as  unsupervised  linear  regression  on  unobserved,  normally  distributed  hidden  variables,  product  anal(cid:173) ysis  can  be  viewed  as  unsupervised  linear  regression  on  products  of unobserved,  normally  distributed  hidden  variables.  The  map(cid:173) ping  between  the  data  and  the  hidden  space  is  nonlinear,  so  we  use  an  approximate variational  technique  for  inference  and learn(cid:173) ing.  Since  product  analysis  is  a  generalization  of factor  analysis,  product  analysis  always  finds  a  higher  data likelihood than factor  analysis.  We  give results on pattern recognition  and illumination(cid:173) invariant image clustering."}
{"Type": "conference", "Year": "2001", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "EM-DD", "Title": "An Improved Multiple-Instance Learning Technique", "Abstract": "We  present  a  new  multiple-instance  (MI)  learning technique  (EM(cid:173) DD)  that  combines  EM  with  the  diverse  density  (DD)  algorithm.  EM-DD is a general-purpose MI algorithm that can be applied with  boolean  or  real-value  labels  and  makes  real-value  predictions.  On  the boolean Musk benchmarks, the EM-DD algorithm without any  tuning  significantly  outperforms  all  previous  algorithms.  EM-DD  is  relatively  insensitive to the number of relevant  attributes  in  the  data set  and  scales  up  well  to  large  bag  sizes.  Furthermore,  EM(cid:173) DD  provides  a  new  framework  for  MI  learning,  in  which  the  MI  problem  is  converted  to  a  single-instance  setting  by  using  EM  to  estimate the instance  responsible  for  the  label of the bag."}
{"Type": "conference", "Year": "2001", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "MIME", "Title": "Mutual Information Minimization and Entropy Maximization for Bayesian Belief Propagation", "Abstract": "Bayesian belief propagation in graphical models has been recently shown to have very close ties to inference methods based in statis- tical physics. After Yedidia et al. demonstrated that belief prop- agation (cid:12)xed points correspond to extrema of the so-called Bethe free energy, Yuille derived a double loop algorithm that is guar- anteed to converge to a local minimum of the Bethe free energy. Yuille’s algorithm is based on a certain decomposition of the Bethe free energy and he mentions that other decompositions are possi- ble and may even be fruitful. In the present work, we begin with the Bethe free energy and show that it has a principled interpre- tation as pairwise mutual information minimization and marginal entropy maximization (MIME). Next, we construct a family of free energy functions from a spectrum of decompositions of the original Bethe free energy. For each free energy in this family, we develop a new algorithm that is guaranteed to converge to a local min- imum. Preliminary computer simulations are in agreement with this theoretical development."}
{"Type": "conference", "Year": "2001", "Area": "AI", "Where": "NeurIPS", "Abbreviation": "The g Factor", "Title": "Relating Distributions on Features to Distributions on Images", "Abstract": "We  describe  the  g-factor,  which  relates  probability  distributions  on image features  to  distributions on the  images  themselves.  The  g-factor  depends  only  on  our choice  of features  and  lattice  quanti(cid:173) zation and is  independent of the training image data.  We  illustrate  the importance  of the g-factor by analyzing how the parameters of  Markov Random Field (i.e.  Gibbs or log-linear) probability models  of images are learned from data by maximum likelihood estimation.  In particular, we study homogeneous MRF models which learn im(cid:173) age distributions in terms of clique potentials corresponding to fea(cid:173) ture  histogram  statistics  (d.  Minimax  Entropy  Learning  (MEL)  by  Zhu,  Wu  and  Mumford  1997  [11]) .  We  first  use  our  analysis  of the  g-factor  to  determine  when  the  clique  potentials  decouple  for  different  features .  Second,  we  show  that  clique  potentials  can  be  computed  analytically  by  approximating  the  g-factor.  Third,  we  demonstrate a  connection between this  approximation and the  Generalized Iterative Scaling algorithm (GIS),  due to Darroch and  Ratcliff  1972  [2],  for  calculating  potentials.  This  connection  en(cid:173) ables  us  to  use  GIS  to  improve  our  multinomial  approximation,  using  Bethe-Kikuchi[8]  approximations to simplify  the  GIS  proce(cid:173) dure.  We  support our analysis by computer simulations."}
