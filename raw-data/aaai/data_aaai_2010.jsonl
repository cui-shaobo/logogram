{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "A Cross-Entropy Method that Optimizes Partially Decomposable Problems", "Title": "A New Way to Interpret NMR Spectra", "Abstract": "Some real-world problems are partially decomposable, in that they can be decomposed into a set of coupled sub- problems, that are each relatively easy to solve. However, when these sub-problem share some common variables, it is not sufficient to simply solve each sub-problem in isolation. We develop a technology for such problems, and use it to address the challenge of finding the concentrations of the chemicals that appear in a complex mixture, based on its one-dimensional 1H Nuclear Magnetic Resonance (NMR) spectrum. As each chemical involves clusters of spatially localized peaks, this requires finding the shifts for the clusters and the concentrations of the chemicals, that collectively pro- duce the best match to the observed NMR spectrum. Here, each sub-problem requires finding the chemical concentrations and cluster shifts that can appear within a limited spectrum range; these are coupled as these limited regions can share many chemicals, and so must agree on the concentrations and cluster shifts of the common chemicals. This task motivates CEED: a novel extension to the Cross-Entropy stochastic optimization method constructed to address such partially decomposable problems. Our experimental results in the NMR task show that our CEED system is superior to other well-known optimization methods, and indeed produces the best-known results in this important, real-world application."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "UserRec", "Title": "A User Recommendation Framework in Social Tagging Systems", "Abstract": "Social tagging systems have emerged as an effective way for users to annotate and share objects on the Web. However, with the growth of social tagging systems, users are easily overwhelmed by the large amount of data and it is very difficult for users to dig out information that he/she is interested in. Though the tagging system has provided interest-based social network features to enable the user to keep track of other users' tagging activities, there is still no automatic and effective way for the user to discover other users with common interests. In this paper, we propose a User Recommendation (UserRec) framework for user interest modeling and interest-based user recommendation, aiming to boost information sharing among users with similar interests. Our work brings three major contributions to the research community: (1) we propose a tag-graph based community detection method to model the users' personal interests, which are further represented by discrete topic distributions; (2) the similarity values between users' topic distributions are measured by Kullback-Leibler divergence (KL-divergence), and the similarity values are further used to perform interest-based user recommendation; and (3) by analyzing users' roles in a tagging system, we find users' roles in a tagging system are similar to Web pages in the Internet. Experiments on tagging dataset of Web pages (Yahoo!~Delicious) show that UserRec outperforms other state-of-the-art recommender system approaches."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "PR + RQ  ≈ PQ", "Title": "Transliteration Mining Using Bridge Language", "Abstract": "We address the problem of mining name transliterations from comparable corpora in languages P and Q in the following resource-poor scenario:Parallel names in PQ are not available for training. Parallel names in PR and RQ are available for training.We propose a novel solution for the problem by computing a common geometric feature space for P,Q and R where name transliterations are mapped to similar vectors. We employ Canonical Correlation Analysis (CCA) to compute the common geometric feature space using only parallel names in PR and RQ and without requiring parallel names in  PQ. We test our algorithm on data sets in several languages and show that it gives results comparable to the state-of-the-art transliteration mining algorithms that use parallel names in PQ for training."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "Visual Contextual Advertising", "Title": "Bringing Textual Advertisements to Images", "Abstract": "Advertising in the case of textual Web pages has been studied extensively by many researchers. However, with the increasing amount of multimedia data such as image, audio and video on the Web, the need for recommending advertisement for the multimedia data is becoming a reality. In this paper, we address the novel problem of visual contextual advertising, which is to directly advertise when users are viewing images which do not have any surrounding text. A key challenging issue of visual contextual advertising is that images and advertisements are usually represented in image space and word space respectively, which are quite different with each other inherently. As a result, existing methods for Web page advertising are inapplicable since they represent both Web pages and advertisement in the same word space. In order to solve the problem, we propose to exploit the social Web to link these two feature spaces together. In particular, we present a unified generative model to integrate advertisements, words and images. Specifically, our solution combines two parts in a principled approach: First, we transform images from a image feature space to a word space utilizing the knowledge from images with annotations from social Web. Then, a language model based approach is applied to estimate the relevance between transformed images and advertisements. Moreover, in this model, the probability of recommending an advertisement can be inferred efficiently given an image, which enables potential applications to online advertising."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "GTPA", "Title": "A Generative Model For Online Mentor-Apprentice Networks", "Abstract": "There is a large body of work on the evolution of graphs in various domains, which shows that many real graphs evolve in a similar manner. In this paper we study a novel type of network formed by mentor-apprentice relationships in a massively multiplayer online role playing game. We observe that some of the static and dynamic laws which have been observed in many other real world networks are not observed in this network. Consequently well known graph generators like Preferential Attachment, Forest Fire, Butterfly, RTM, etc., cannot be applied to such mentoring networks. We propose a novel generative model to generate networks with the characteristics of mentoring networks."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "Ad Hoc Autonomous Agent Teams", "Title": "Collaboration without Pre-Coordination", "Abstract": "As autonomous agents proliferate in the real world, both in software and robotic settings, they will increasingly need to band together for cooperative activities with previously unfamiliar teammates. In such ad hoc team settings, team strategies cannot be developed a priori.  Rather, an agent must be prepared to cooperate with many types of teammates: it must collaborate without pre-coordination. This paper challenges the AI community to develop theory and to implement prototypes of ad hoc team agents.  It defines the concept of ad hoc team agents, specifies an evaluation paradigm, and provides examples of possible theoretical and empirical approaches to challenge.  The goal is to encourage progress towards this ambitious, newly realistic, and increasingly important research goal."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "Searching Without a Heuristic", "Title": "Efficient Use of Abstraction", "Abstract": "In problem domains where an informative heuristic evaluation function is not known or not easily computed, abstraction can be used to derive admissible heuristic values.  Optimal path lengths in the abstracted problem are consistent heuristic estimates for the original problem.  Pattern databases are the traditional method of creating such heuristics, but they exhaustively compute costs for all abstract states and are thus usually appropriate only when all instances share the same single goal state.  Hierarchical heuristic search algorithms address these shortcomings by searching for paths in the abstract space on an as-needed basis.  However, existing hierarchical algorithms search less efficiently than pattern database constructors:  abstract nodes may be expanded many times during the course of a base-level search.  We present a novel hierarchical heuristic search algorithm, called Switchback, that uses an alternating direction of search to avoid abstract node re-expansions.  This algorithm is simple to implement and demonstrates superior performance to existing hierarchical heuristic search algorithms on several standard benchmarks."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "Hydra", "Title": "Automatically Configuring Algorithms for Portfolio-Based Selection", "Abstract": "The AI community has achieved great success in designing high-performance algorithms for hard combinatorial problems, given both considerable domain knowledge and considerable effort by human experts. Two influential methods aim to automate this process: automated algorithm configuration and portfolio-based algorithm selection. The former has the advantage of requiring virtually no domain knowledge, but produces only a single solver; the latter exploits per-instance variation, but requires a set of relatively uncorrelated candidate solvers. Here, we introduce Hydra, a novel technique for combining these two methods, thereby realizing the benefits of both. Hydra automatically builds a set of solvers with complementary strengths by iteratively configuring new algorithms. It is primarily intended for use in problem domains for which an adequate set of candidate solvers does not already exist. Nevertheless, we tested Hydra on a widely studied domain, stochastic local search algorithms for SAT, in order to characterize its performance against a well-established and highly competitive baseline. We found that Hydra consistently achieved major improvements over the best existing individual algorithms, and always at least roughly matched — and indeed often exceeded — the performance of the best portfolios of these algorithms."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "Lazy Theta*", "Title": "Any-Angle Path Planning and Path Length Analysis in 3D", "Abstract": "Grids with blocked and unblocked cells are often used to represent continuous 2D and 3D environments in robotics and video games. The shortest paths formed by the edges of 8-neighbor 2D grids can be up to 8% longer than the shortest paths in the continuous environment. Theta* typically finds much shorter paths than that by propagating information along graph edges (to achieve short runtimes) without constraining paths to be formed by graph edges (to find short \"any-angle\" paths). We show in this paper that the shortest paths formed by the edges of 26-neighbor 3D grids can be 13% longer than the shortest paths in the continuous environment, which highlights the need for smart path planning algorithms in 3D.  Theta* can be applied to 3D grids in a straight-forward manner, but it performs a line-of-sight check for each unexpanded visible neighbor of each expanded vertex and thus it performs many more line-of-sight checks per expanded vertex on a 26-neighbor 3D grid than on an 8-neighbor 2D grid. We therefore introduce Lazy Theta*, a variant of Theta* which uses lazy evaluation to perform only one line-of-sight check per expanded vertex (but with slightly more expanded vertices).  We show experimentally that Lazy Theta* finds paths faster than Theta* on 26-neighbor 3D grids, with one order of magnitude fewer line-of-sight checks and without an increase in path length."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "EWLS", "Title": "A New Local Search for Minimum Vertex Cover", "Abstract": "A number of algorithms have been proposed for the Minimum Vertex  Cover problem. However, they are far from satisfactory, especially  on hard instances. In this paper, we introduce Edge Weighting Local  Search (EWLS), a new local search algorithm for the Minimum Vertex  Cover problem. EWLS is based on the idea of extending a partial  vertex cover into a vertex cover. A key point of EWLS is to find a  vertex set that provides a tight upper bound on the size of the  minimum vertex cover. To this purpose, EWLS employs an iterated  local search procedure, using an edge weighting scheme which updates  edge weights when stuck in local optima. Moreover, some  sophisticated search strategies have been taken to improve the  quality of local optima. Experimental results on the broadly used  DIMACS benchmark show that EWLS is competitive with the current best  heuristic algorithms, and outperforms them on hard instances.  Furthermore, on a suite of difficult benchmarks, EWLS delivers the  best results and sets a new record on the largest instance."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "Learning Methods to Generate Good Plans", "Title": "Integrating HTN Learning and Reinforcement Learning", "Abstract": "We consider how to learn Hierarchical Task Networks (HTNs) for planning problems in which both the quality of solution plans generated by the HTNs and the speed at which those plans are found is important.  We describe an integration of HTN Learning with Reinforcement Learning to both learn methods by analyzing semantic annotations on tasks and to produce estimates of the expected values of the learned methods by performing Monte Carlo updates.  We performed an experiment in which plan quality was inversely related to plan length.  In two planning domains, we evaluated the planning performance of the learned methods in comparison to two state-of-the-art satisficing classical planners, FastForward and SGPlan6, and one optimal planner, HSP*.  The results demonstrate that a greedy HTN planner using the learned methods was able to generate higher quality solutions than SGPlan6 in both domains and FastForward in one.  Our planner, FastForward, and SGPlan6 ran in similar time, while HSP* was exponentially slower."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "Supporting Wilderness Search and Rescue with Integrated Intelligence", "Title": "Autonomy and Information at the Right Time and the Right Place", "Abstract": "Current practice in Wilderness Search and Rescue (WiSAR) is analogous to an intelligent system designed to gather and analyze information to find missing persons in remote areas. The system consists of multiple parts - various tools for information management (maps, GPS, etc) distributed across personnel with different skills and responsibilities. Introducing a camera-equipped mini-UAV into this task requires autonomy and information technology that itself is an integrated intelligent system to be used by a sub-team that must be integrated into the overall intelligent system. In this paper, we identify key elements of the integration challenges along two dimensions: (a) attributes of intelligent system and (b) scale, meaning individual or group. We then present component technology that offload or supplement many responsibilities to autonomous systems, and finally describe how autonomy and information are integrated into user interfaces to better support distributed search across time and space. The integrated system was demoed for Utah County Search and Rescue personnel. A real searcher flew the UAV after minimal training and successfully located the simulated missing person in a wilderness area."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "Integrating a Closed World Planner with an Open World Robot", "Title": "A Case Study", "Abstract": "In this paper, we present an integrated planning and robotic architecture that actively directs an agent engaged in an urban search and rescue (USAR) scenario. We describe three salient features that comprise the planning component of this system, namely (1) the ability to plan in a world open with respect to objects, (2) execution monitoring and replanning abilities, and (3) handling soft goals, and detail the interaction of these parts in representing and solving the USAR scenario at hand. We show that though insufficient in an individual capacity, the integration of this trio of features is sufficient to solve the scenario that we present. We test our system with an example problem that involves soft and hard goals, as well as goal deadlines and action costs, and show that the planner is capable of incorporating sensing actions and execution monitoring in order to produce goal-fulfilling plans that maximize the net benefit accrued."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "Collaborative Filtering Meets Mobile Recommendation", "Title": "A User-Centered Approach", "Abstract": "With the increasing popularity of location tracking services such as GPS, more and more mobile data are being accumulated. Based on such data, a potentially useful service is to make timely and targeted recommendations for users on places where they might be interested to go and activities that they are likely to conduct. For example, a user arriving in Beijing might wonder where to visit and what she can do around the Forbidden City. A key challenge for such recommendation problems is that the data we have on each individual user might be very limited, while to make useful and accurate recommendations, we need extensive annotated location and activity information from user trace data. In this paper, we present a new approach, known as user-centered collaborative location and activity filtering (UCLAF), to pull many users’ data together and apply collaborative filtering to find like-minded users and like-patterned activities at different locations. We model the userlocation- activity relations with a tensor representation, and propose a regularized tensor and matrix decomposition solution which can better address the sparse data problem in mobile information retrieval. We empirically evaluate UCLAF using a real-world GPS dataset collected from 164 users over 2.5 years, and showed that our system can outperform several state-of-the-art solutions to the problem."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "Beyond Equilibrium", "Title": "Predicting Human Behavior in Normal-Form Games", "Abstract": "It is standard in multiagent settings to assume that agents will adopt Nash equilibrium strategies.  However, studies in experimental economics demonstrate that Nash equilibrium is a poor description of human players' initial behavior in normal-form games.  In this paper, we consider a wide range of widely-studied models from behavioral game theory. For what we believe is the first time, we evaluate each of these models in a meta-analysis, taking as our data set large-scale and publicly-available experimental data from the literature.  We then propose modifications to the best-performing model that we believe make it more suitable for practical prediction of initial play by humans in normal-form games."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "Dynamic Auction", "Title": "A Tractable Auction Procedure", "Abstract": "Dynamic auctions are trading mechanisms for discovering market-clearing prices and efficient allocations based on price adjustment processes. This paper studies the computational issues of dynamic auctions for selling multiple indivisible items. Although the decision problem of efficient allocations in a dynamic auction in general is intractable, it can be solved in polynomial time if the economy under consideration satisfies the condition of Gross Substitutes and Complements, which is known as the most general condition that guarantees the existence of Walrasian equilibrium. We propose a polynomial algorithm that can be used to find efficient allocations and introduce a double-direction auction procedure to discover a Walrasian equilibrium in polynomial time."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "Bypassing Combinatorial Protections", "Title": "Polynomial-Time Algorithms for Single-Peaked Electorates", "Abstract": "For many election systems, bribery (and related) attacks have been shown NP-hard using constructions on combinatorially rich structures such as partitions and covers. It is important to learn how robust these hardness protection results are, in order to find whether they can be relied on in practice. This paper shows that for voters who follow the most central political-science model of electorates — single-peaked preferences — those protections vanish. By using single-peaked preferences to simplify combinatorial covering challenges, we show that NP-hard bribery problems — including those for Kemeny and Llull elections- — fall to polynomial time. By using single-peaked preferences to simplify combinatorial partition challenges, we show that NP-hard partition-of-voters problems fall to polynomial time. We furthermore show that for single-peaked electorates, the winner problems for Dodgson and Kemeny elections, though Θ2p-complete in the general case, fall to polynomial time. And we completely classify the complexity of weighted coalition manipulation for scoring protocols in single-peaked electorates."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "Possible Winners when New Candidates Are Added", "Title": "The Case of Scoring Rules", "Abstract": "In some voting situations, some new candidates may show up in the course of the process. In this case, we may want to determine which of the initial candidates are possible winners, given that a fixed number k of new candidates will be added. Focusing on scoring rules, we give complexity results for the above possible winner problem."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "Stackelberg Voting Games", "Title": "Computational Aspects and Paradoxes", "Abstract": "We consider settings in which voters vote in sequence, each voter knows the votes of the earlier voters and the preferences of the later voters, and voters are strategic. This can be modeled as an extensive-form game of perfect information, which we call a Stackelberg voting game.  We first propose a dynamic-programming algorithm for finding the backward-induction outcome for any Stackelberg voting game when the rule is anonymous; this algorithm is efficient if the number of alternatives is no more than a constant. We show how to use compilation functions to further reduce the time and space requirements.  Our main theoretical results are paradoxes for the backward-induction outcomes of Stackelberg voting games. We show that for any n ≥ 5 and any voting rule that satisfies nonimposition and with a low domination index, there exists a profile consisting of n voters, such that the backward-induction outcome is ranked somewhere in the bottom two positions in almost every voter’s preferences. Moreover, this outcome loses all but one of its pairwise elections. Furthermore, we show that many common voting rules have a very low (= 1) domination index, including all majority-consistent voting rules. For the plurality and nomination rules, we show even stronger paradoxes.  Finally, using our dynamic-programming algorithm, we run simulations to compare the backward-induction outcome of the Stackelberg voting game to the winner when voters vote truthfully, for the plurality and veto rules. Surprisingly, our experimental results suggest that on average, more voters prefer the backward-induction outcome."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "Security Games with Arbitrary Schedules", "Title": "A Branch and Price Approach", "Abstract": "Security games, and important class of Stackelberg games, are used in deployed decision-support tools in use by LAX police and the Federal Air Marshals Service. The algorithms used to solve these games find optimal randomized schedules to allocate security resources for infrastructure protection. Unfortunately, the state of the art algorithms either fail to scale or to provide a correct solution for large problems with arbitrary scheduling constraints. We introduce ASPEN, a branch-and-price approach that overcomes these limitations based on two key contributions: (i) A column-generation approach that exploits a novel network flow representation, avoiding a combinatorial explosion of schedule allocations; (ii) A branch-and-bound algorithm that generates bounds via a fast algorithm for solving security games with relaxed scheduling constraints. ASPEN is the first known method for efficiently solving massive security games with arbitrary schedules."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "Urban Security", "Title": "Game-Theoretic Resource Allocation in Networked Domains", "Abstract": "Law enforcement agencies frequently must allocate limited resources to protect targets embedded in a network, such as important buildings in a city road network. Since intelligent attackers may observe and exploit patterns in the allocation, it is crucial that the allocations be randomized. We cast this problem as an attacker-defender Stackelberg game: the defender’s goal is to obtain an optimal mixed strategy for allocating resources. The defender’s strategy space is exponential in the number of resources, and the attacker’s exponential in the network size. Existing algorithms are therefore useless for all but the smallest networks.  We present a solution approach based on two key ideas: (i) A polynomial-sized game model obtained via an approximation of the strategy space, solved efficiently using a linear program; (ii) Two efficient techniques that map solutions from the approximate game to the original, with proofs of correctness  under certain assumptions. We present in-depth experimental results, including an evaluation on part of the Mumbai road network."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "Trust Models and Con-Man Agents", "Title": "From Mathematical to Empirical Analysis", "Abstract": "Recent work has demonstrated that several trust and reputation models can be exploited by malicious agents with cyclical behaviour. In each cycle, the malicious agent with cyclical behaviour first regains a high trust value after a number of cooperations and then abuses its gained trust by engaging in a bad transaction. Using a game theoretic formulation, Salehi-Abari and White have proposed the AER model that is resistant to exploitation by cyclical behaviour. Their simulation results imply that FIRE, Regret, and a model due to Yu and Singh, can always be exploited with an appropriate value for the period of cyclical behaviour. Furthermore, their results demonstrate that this is not so for the proposed adaptive scheme. This paper provides a mathematical analysis of the properties of five trust models when faced with cyclical behaviour of malicious agents. Three main results are proven. First, malicious agents can always select a cycle period that allows them to exploit the four models of FIRE, Regret, Probabilistic models, and  Yu and Singh indefinitely. Second, malicious agents cannot select a single, finite cycle period that allows them to exploit the AER model forever. Finally,  the number of cooperations required to achieve a given trust value increases monotonically with each cycle.  In addition to the mathematical analysis, this paper empirically shows how malicious agents can use the theorems proven in this paper to mount efficient attacks on trust models."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "Enhancing ASP by Functions", "Title": "Decidable Classes and Implementation Techniques", "Abstract": "This paper summarizes our line of research about the introduction of function symbols (functions) in Answer Set Programming (ASP) – a powerful language for knowledge representation and reasoning. The undecidability of reasoning on ASP with functions, implied that functions were subject to severe restrictions or disallowed at all, drastically limiting ASP applicability. We overcame most of the technical difficulties preventing this introduction, and we singled out a highly expressive class of programs with functions (FG-programs), allowing the (possibly recursive) use of function terms in the full ASP language with disjunction and negation. Reasoning on FG-programs is decidable, and they can express any computable function (causing membership in this class to be semi-decidable). We singled out also FD-programs, a subset of FG-programs which are effectively recognizable, while keeping the computability of reasoning. We implemented all results into the DLV system, thus obtaining an ASP system allowing to encode any computable function in a rich and fully declarative KRR language, ensuring termination on every FG program. Finally, we singled out the class of DFRP programs, where decidability of reasoning is guaranteed and Prolog-like functions are allowed."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "Computationally Feasible Automated Mechanism Design", "Title": "General Approach and Case Studies", "Abstract": "In many multiagent settings, a decision must be made based on the preferences of multiple agents, and agents may lie about their preferences if this is to their benefit. In mechanism design, the goal is to design procedures (mechanisms) for making the decision that work in spite of such strategic behavior, usually by making untruthful behavior suboptimal. In automated mechanism design, the idea is to computationally search through the space of feasible mechanisms, rather than to design them analytically by hand. Unfortunately, the most straightforward approach to automated mechanism design does not scale to large instances, because it requires searching over a very large space of possible functions. In this paper, we describe an approach to automated mechanism design that is computationally feasible. Instead of optimizing over all feasible mechanisms, we carefully choose a parameterized subfamily of mechanisms. Then we optimize over mechanisms within this family, and analyze whether and to what extent the resulting mechanism is suboptimal outside the subfamily. We demonstrate the usefulness of our approach with two case studies."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "CAO", "Title": "A Fully Automatic Emoticon Analysis System", "Abstract": "This paper presents CAO, a system for affect analysis of emoticons. Emoticons are strings of symbols widely used in text-based online communication to convey emotions. It extracts emoticons from input and determines specific emotions they express. Firstly, by matching the extracted emoticons to a raw emoticon database, containing over ten thousand emoticon samples extracted from the Web and annotated automatically. The emoticons for which emotion types could not be determined using only this database, are automatically divided into semantic areas representing \"mouths\" or \"eyes,\" based on the theory of kinesics. The areas are automatically annotated according to their co-occurrence in the database. The annotation is firstly based on the eye-mouth-eye triplet, and if no such triplet is found, all semantic areas are estimated separately. This provides the system coverage exceeding 3 million possibilities. The evaluation, performed on both training and test sets, confirmed the system's capability to sufficiently detect and extract any emoticon, analyze its semantic structure and estimate the potential emotion types expressed. The system achieved nearly ideal scores, outperforming existing emoticon analysis systems."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "Community-Guided Learning", "Title": "Exploiting Mobile Sensor Users to Model Human Behavior", "Abstract": "Modeling human behavior requires vast quantities of accurately labeled training data, but for ubiquitous people-aware applications such data is rarely attainable. Even researchers make mistakes when labeling data, and consistent, reliable labels from low-commitment users are rare. In particular, users may give identical labels to activities with characteristically different signatures (e.g., labeling eating at home or at a restaurant as \"dinner\") or may give different labels to the same context (e.g., \"work\" vs. \"office\"). In this scenario, labels are unreliable but nonetheless contain valuable information for classification. To facilitate learning in such unconstrained labeling scenarios, we propose Community-Guided Learning (CGL), a framework that allows existing classifiers to learn robustly from unreliably-labeled user-submitted data. CGL exploits the underlying structure in the data and the unconstrained labels to intelligently group crowd-sourced data. We demonstrate how to use similarity measures to determine when and how to split and merge contributions from different labeled categories and present experimental results that demonstrate the effectiveness of our framework."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "g-Planner", "Title": "Real-time Motion Planning and Global Navigation using GPUs", "Abstract": "We present novel randomized algorithms for solving global motion planning problems that exploit the computational capabilities of many-core GPUs. Our approach uses thread and data parallelism to achieve high performance for all components of sample-based algorithms, including random sampling, nearest neighbor computation, local planning, collision queries and graph search. The approach can efficiently solve both the multi-query and single-query versions of the problem and obtain considerable speedups over prior CPU-based algorithms. We demonstrate the efficiency of our algorithms by applying them to a number of 6DOF planning benchmarks in 3D environments. Overall, this is the first algorithm that can perform real-time motion planning and global navigation using commodity hardware."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "Planning in Dynamic Environments", "Title": "Extending HTNs with Nonlinear Continuous Effects", "Abstract": "Planning in dynamic continuous environments requires reasoning about nonlinear continuous effects, which previous Hierarchical Task Network (HTN) planners do not support. In this paper, we extend an existing HTN planner with a new state projection algorithm. To our knowledge, this is the first HTN planner that can reason about nonlinear continuous effects. We use a wait action to instruct this planner to consider continuous effects in a given state. We also introduce a new planning domain to demonstrate the benefits of planning with nonlinear continuous effects. We compare our approach with a linear continuous effects planner and a discrete effects HTN planner on a benchmark domain, which reveals that its additional costs are largely mitigated by domain knowledge. Finally, we present an initial application of this algorithm in a practical domain, a Navy training simulation, illustrating the utility of this approach for planning in dynamic continuous environments."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "To Max or Not to Max", "Title": "Online Learning for Speeding Up Optimal Planning", "Abstract": "It is well known that there cannot be a single \"best\" heuristic for optimal planning in general. One way of overcoming this is by combining admissible heuristics (e.g. by using their maximum), which requires computing numerous heuristic estimates at each state.  However, there is a tradeoff between the time spent on  computing these heuristic estimates for each state, and the  time saved by reducing the number of expanded states.  We present a novel method that reduces the cost of combining admissible heuristics for optimal search, while maintaining its benefits.  Based on an idealized search space model, we formulate a decision rule for choosing the best heuristic to compute at each state. We then present an active online learning approach  for that decision rule, and employ the learned model to  decide which heuristic to compute at each state.  We evaluate this technique empirically, and show that it substantially outperforms each of the individual heuristics that were used, as well as their regular maximum."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "Multi-Agent Plan Recognition", "Title": "Formalization and Algorithms", "Abstract": "Multi-Agent Plan Recognition (MAPR) seeks to identify the dynamic team structures and team behaviors from the observations of the activity-sequences of a set of intelligent agents, based on a library of known team-activities (plan library). It has important applications in analyzing data from automated monitoring, surveillance, and intelligence analysis in general. In this paper, we formalize MAPR using a basic model that explicates the cost of abduction in single agent plan recognition by \"flattening\" or decompressing the (usually compact, hierarchical) plan library. We show that single-agent plan recognition with a decompressed library can be solved in time polynomial in the input size, while it is known that with a compressed (by partial ordering constraints) library it is NP-complete. This leads to an important insight: that although the compactness of the plan library plays an important role in the hardness of single-agent plan recognition (as recognized in the existing literature), that is not the case with multiple agents. We show, for the first time, that MAPR is NP-complete even when the (multi-agent) plan library is fully decompressed. As with previous solution approaches, we break the problem into two stages: hypothesis generation and hypothesis search. We show that Knuth's ``Algorithm X'' (with the efficient ``dancing links'' representation) is particularly suited for our model, and can be adapted to perform a branch and bound search for the second stage, in this model. We show empirically that this new approach leads to significant pruning of the hypothesis space in MAPR."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "PUMA", "Title": "Planning Under Uncertainty with Macro-Actions", "Abstract": "Planning in large, partially observable domains is challenging, especially when a long-horizon lookahead is necessary to obtain a good policy. Traditional POMDP planners that plan a different potential action for each future observation can be prohibitively expensive when planning many steps ahead. An efficient solution for planning far into the future in fully observable domains is to use temporally-extended sequences of actions, or \"macro-actions.\" In this paper, we present a POMDP algorithm for planning under uncertainty with macro-actions (PUMA) that automatically constructs and evaluates open-loop macro-actions within forward-search planning, where the planner branches on observations only at the end of each macro-action.  Additionally, we show how to incrementally refine the plan over time, resulting in an anytime algorithm that provably converges to an epsilon-optimal policy. In experiments on several large POMDP problems which require a long horizon lookahead, PUMA outperforms existing state-of-the art solvers."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "SixthSense", "Title": "Fast and Reliable Recognition of Dead Ends in MDPs", "Abstract": "The results of the latest International Probabilistic Planning Competition (IPPC-2008) indicate that the presence of dead ends, states with no trajectory to the goal, makes MDPs hard for modern probabilistic planners. Implicit dead ends, states with executable actions but no path to the goal, are particularly challenging; existing MDP solvers spend much time and memory identifying these states.  As a first attempt to address this issue, we propose a machine learning algorithm called SIXTHSENSE. SIXTHSENSE helps existing MDP solvers by finding nogoods, conjunctions of literals whose truth in a state implies that the state is a dead end. Importantly, our learned nogoods are sound, and hence the states they identify are true dead ends. SIXTHSENSE is very fast, needs little training data, and takes only a small fraction of total planning time. While IPPC problems may have millions of dead ends, they may typically be represented with only a dozen or two no-goods. Thus, nogood learning efficiently produces a quick and reliable means for dead-end recognition. Our experiments show that the nogoods found by SIXTHSENSE routinely reduce planning space and time on IPPC domains, enabling some planners to solve problems they could not previously handle."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "DTProbLog", "Title": "A Decision-Theoretic Probabilistic Prolog", "Abstract": "We introduce DTProbLog, a decision-theoretic extension of Prolog and its probabilistic variant ProbLog. DTProbLog is a simple but expressive probabilistic programming language that allows the modeling of a wide variety of domains, such as viral marketing. In DTProbLog, the utility of a strategy (a particular choice of actions) is defined as the expected reward for its execution in the presence of probabilistic effects. The key contribution of this paper is the introduction of exact, as well as approximate, solvers to compute the optimal strategy for a DTProbLog program and the decision problem it represents, by making use of binary and algebraic decision diagrams.  We also report on experimental results that show the effectiveness and the practical usefulness of the approach."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "The Model-Based Approach to Autonomous Behavior", "Title": "A Personal View", "Abstract": "The selection of the action to do next is one of the central problems faced by autonomous agents.  In AI, three  approaches have been used to address this problem:  the programming-based approach, where the agent controller is given by the programmer, the learning-based approach, where the controller is induced from experience via a learning algorithm,  and the model-based approach, where the controller is derived  from a  model of the problem. Planning in AI is best conceived as the model-based approach to action selection. The models represent the initial situation, actions, sensors, and goals.  The main challenge in planning is computational, as all the models, whether accommodating  feedback and  uncertainty or not,  are intractable in the worst case. In this article, I review some of  the models considered in current planning research,  the progress achieved in solving these models, and some of the open problems."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "Multi-Label Classification", "Title": "Inconsistency and Class Balanced K-Nearest Neighbor", "Abstract": "Many existing approaches employ one-vs-rest method to decompose a multi-label classification problem into a set of 2- class classification problems, one for each class. This method is valid in traditional single-label classification, it, however, incurs training inconsistency in multi-label classification, because in the latter a data point could belong to more than one class. In order to deal with this problem, in this work, we further develop classicalK-Nearest Neighbor classifier and propose a novel Class Balanced K-Nearest Neighbor approach for multi-label classification by emphasizing balanced usage of data from all the classes. In addition, we also propose a Class Balanced Linear Discriminant Analysis approach to address high-dimensional multi-label input data. Promising experimental results on three broadly used multi-label data sets demonstrate the effectiveness of our approach."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "Genome Rearrangement", "Title": "A Planning Approach", "Abstract": "Evolutionary trees of species can be reconstructed by pairwise comparison of their entire genomes. Such a comparison can be quantified by determining the number of events that change the order of genes in a genome. Earlier Erdem and Tillier formulated the pairwise comparison of entire genomes as the problem of planning rearrangement events that transform one genome to the other. We reformulate this problem as a planning problem to extend its applicability to genomes with multiple copies of genes and with unequal gene content, and illustrate its applicability and effectiveness on three real datasets: mitochondrial genomes of Metazoa, chloroplast genomes of Campanulaceae, chloroplast genomes of various land plants and green algae."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "Combining Human Reasoning and Machine Computation", "Title": "Towards a Memetic Network Solution to Satisfiability", "Abstract": "We propose a framework where humans and computers can collaborate seamlessly to solve problems. We do so by developing and applying a network model, namely Memenets, where human knowledge and reasoning are combined with machine computation to achieve problem-solving. The development of a Memenet is done in three steps: first, we simulate a machine-only network, as previous results have shown that memenets are efficient problem-solvers. Then, we perform an experiment with human agents organized in a online network. This allows us to investigate human behavior while solving problems in a social network and to postulate principles of agent communication in Memenets. These postulates describe an initial theory of how human-computer interaction functions inside social networks. In the third stage, postulates of step two allow one to combine human and machine computation to propose an integrated Memenet-based problem-solving computing model."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "Semantic Search in Linked Data", "Title": "Opportunities and Challenges", "Abstract": "In this abstract, we compare semantic search (in the RDF model) with keyword search (in the relational model), and illustrate how these two search paradigms are different. This comparison addresses the following questions: (1) What can semantic search achieve that keyword search can not (in terms of behavior)? (2) Why is it difficult to simulate semantic search, using keyword search on the relational data model? We use the term keyword search, when the search is performed on data stored in the relational data model, as in traditional relational databases, and an example of keyword search in databases is [Hri02]. We use the term semantic search, when the search is performed on data stored in the RDF data model. Note that when the data is modeled in RDF, it inherently contains explicit typed relations or semantics, and hence the use of the term “semantic search.” Let us begin with an example, to illustrate the differences between semantic search and keyword search."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "AI-Based Software Defect Predictors", "Title": "Applications and Benefits in a Case Study", "Abstract": "Software defect prediction aims to reduce software testing efforts by guiding testers through the defect-prone sections of software systems. Defect predictors are widely used in organizations to predict defects in order to save time and effort as an alternative to other techniques such as manual code reviews. The application of a defect prediction model in a real-life setting is difficult because it requires software metrics and defect data from past projects to predict the defect-proneness of new projects. It is, on the other hand, very practical because it is easy to apply, can detect defects using less time and reduces the testing effort. We have built a learning-based defect prediction model for a telecommunication company during a period of one year. In this study, we have briefly explained our model, presented its pay-off and described how we have implemented the model in the company. Furthermore, we have compared the performance of our model with that of another testing strategy applied in a pilot project that implemented a new process called Team Software Process (TSP). Our results show that defect predictors can be used as supportive tools during a new process implementation, predict 75% of code defects, and decrease the testing time compared with 25% of the code defects detected through more labor-intensive strategies such as code reviews and formal checklists."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "Sketch Worksheets", "Title": "A Sketch-Based Educational Software System", "Abstract": "Intelligent tutoring systems and learning environments can provide important benefits for education, but few have been developed for heavily spatial domains. One bottleneck has been the lack of rich models of visual and conceptual processing in sketch understanding, so that what students draw can be interpreted in a human-like way. This paper describes Sketch Worksheets, a form of sketch-based educational software that mimics aspects of pencil and paper worksheets commonly found in classrooms, but provides on-the-spot feedback and support for richer off-line assessments. The basic architecture of sketch worksheets is described, including an authoring environment that allows non-developers to create them and a coach that uses analogy to compare student and instructor sketches as a means to provide feedback. A pilot experiment where sketch worksheets were used successfully in a college geoscience class in Fall 2009 is summarized to show the potential of the idea."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "Reinforcement Learning for Closed-Loop Propofol Anesthesia", "Title": "A Human Volunteer Study", "Abstract": "Research has demonstrated the efficacy of closed-loop control of anesthesia using the bispectral index (BIS) of the electroencephalogram as the controlled variable, and the development of model-based, patient-adaptive systems has considerably improved anesthetic control. To further explore the use of model-based control in anesthesia, we investigated the application of reinforcement learning (RL) in the delivery of patient-specific, propofol-induced hypnosis in human volunteers. When compared to published performance metrics, RL control demonstrated accuracy and stability, indicating that further, more rigorous clinical study is warranted."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "Sentiment Extraction", "Title": "Integrating Statistical Parsing, Semantic Analysis, and Common Sense Reasoning", "Abstract": "Much of the ongoing explosion of digital content is in the form of text. This content is a virtual gold-mine of information that can inform a range of social, governmental, and business decisions. For example, using content available on blogs and social networking sites businesses can find out what its customers are saying about their products and services. In the digital age where customer is king, the business value of ascertaining consumer sentiment cannot be overstated. People express sentiments in myriad ways. At times, they use simple, direct assertions, but most often they use sentences involving comparisons, conjunctions expressing multiple and possibly opposing sentiments about multiple features and entities,and pronominal references whose resolution requires discourse level context. Frequently people use abbreviations, slang, SMSese, idioms and metaphors. Understanding the latter also requires common sense reasoning. In this paper, we present iSEE, a fully implemented sentiment extraction engine, which makes use of statistical methods, classical NLU techniques, common sense reasoning, and probabilistic inference to extract entity and feature specific sentiment from complex sentences and dialog. Most of the components of iSEE are domain independent and the system can be generalized to new domains by simply adding domain relevant lexicons."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "Ambulatory Energy Expenditure Estimation", "Title": "A Machine Learning Approach", "Abstract": "This paper presents a machine learning approach for accurate estimation of energy expenditure using a fusion of accelerometer and heart rate sensing. To address short comings in existing off-the-shelf solutions, we designed Jog Falls, an end to end system for weight management in collaboration with physicians in India. This system is meant to enable people to accurately monitor their energy expenditure and intake and make educated tradeoffs to reach their weight goals. In this paper we describe the sensing components of Jog Falls and focus on the energy expenditure estimation algorithm. We present results from controlled experiments in the lab, as well results from a 15 participant user study over a period of 63 days. We show how our algorithm mitigates many of the issues in existing solutions and yields more accurate results."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "Agent-Based Decision Support", "Title": "A Case-Study on DSL Access Networks", "Abstract": "Network management is a complex task involving various challenges, such as the heterogeneity of the infrastructure or the information flood caused by billions of log messages from different systems and operated by different organizational units. All of these messages and systems may contain information relevant to other operational units. For example, in order to ensure reliable DSL connections for IPTV customers, optimal customer traffic path assignments for the current network state and traffic demands need to be evaluated. Currently reassignments are only manually performed during routine maintenance or as a response to reported problems. In this paper we present a decision support system for this task. In addition, the system predicts future possible demands and allows reconfigurations of a DSL access network before congestions may occur."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "Gaudii", "Title": "An Automated Graphic Design Expert System", "Abstract": "Graphic design is the process of creating graphics to meet specific commercial needs based on knowledge of layout principles and esthetic concepts. This is usually an iterative trial and error process which requires a lot of time even for expert designers. This expert knowledge can be modelled, represented and used by a computer to perform design activities. This paper describes a novel approach named Gaudii (standing for \"Intelligent Automated Graphic Design Generator\") which utilizes principles and techniques known from the fields of Evolutionary Computation and Fuzzy Logic to automatically obtain design elements. Experimental results that demonstrate the potential of the proposed approach are presented in the area of poster design."}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "Designing the Finch", "Title": "Creating a Robot Aligned to Computer Science Concepts", "Abstract": "We present a new robot platform, the Finch, that was designed to align with the learning goals and concepts taught in introductory computer science courses. The Finch was developed in the context of the CSbots program, the goal of which is to improve retention and learning in computer science courses through the use of robots and other physically embodied hardware. This paper concentrates on design constraints that were determined in earlier CSbots studies and how those constraints were instantiated by the Finch. We also present some preliminary results from pilot studies in which Finch robots were used in CS1 and CS2 classes"}
{"Type": "conference", "Year": "2010", "Area": "AI", "Where": "AAAI", "Abbreviation": "The Tekkotsu “Crew”", "Title": "Teaching Robot Programming at a Higher Level", "Abstract": "The Tekkotsu \"crew\" is a collection of interacting software components designed to relieve a programmer of much of the burden of specifying low-level robot behaviors. Using this abstract approach to robot programming we can teach beginning roboticists to develop interesting robot applications with relatively little effort."}
