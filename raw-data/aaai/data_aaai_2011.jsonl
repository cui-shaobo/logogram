{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "Hekateros", "Title": "A Desktop 5 Degree-of-Freedom Robot Arm for the Small-Scale Manipulation Robot Chess Challenge", "Abstract": "RoadNarrows has entered the “desktop-size” 5 degree of freedom robot arm, Hekateros, in the Small-Scale Manipulator Challenge, at the 2011 AAAI conference. Hekateros is utilizing a variety of sensors and software to successfully perceive and manipulate the chess gam"}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "Lego Plays Chess", "Title": "A Low-Cost, Low-Complexity Approach to Intelligent Robotics", "Abstract": "The design and implementation of a robotic chess agent is described. Shallow Blue, a competitor in the AAAI 2011 Small Scale Manipulation Challenge, is constructed with low-cost components including Lego NXT bricks and is programmed using Java and Lejos."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "Conjunctive Representations in Contingent Planning", "Title": "Prime Implicates Versus Minimal CNF Formula", "Abstract": "This paper compares in depth the effectiveness of two conjunctive belief state representations in contingent planning: prime implicates and minimal CNF, a compact form of CNF formulae, which were initially proposed in conformant planning research (To et al. 2010a; 2010b). Similar to the development of the contingent planner CNFct for minimal CNF (To et al. 2011b), the present paper extends the progression function for the prime implicate representation in (To et al. 2010b) for computing successor belief states in the presence of incomplete information to handle non-deterministic and sensing actions required in contingent planning. The idea was instantiated in a new contingent planner, called PIct, using the same AND/OR search algorithm and heuristic function as those for CNFct. The experiments show that, like CNFct, PIct performs very well in a wide range of benchmarks. The study investigates the advantages and disadvantages of the two planners and identifies the properties of each representation method that affect the performance."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "Toward Learning to Solve Insertion Tasks", "Title": "A Developmental Approach Using Exploratory Behaviors and Proprioception", "Abstract": "This paper describes an approach to solving insertion tasks by a robot that uses exploratory behaviors and proprioceptive feedback. The approach was inspired by the developmental progression of insertion abilities in both chimpanzees and humans (Hayashi et al. 2006). Before mastering insertions, the infants of the two species undergo a stage where they only press objects against other objects without releasing them. Our goal was to emulate this developmental stage on a robot to see if it may lead to simpler representations for insertion tasks. Experiments were performed using a shapesorter puzzle with three different blocks and holes."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "Time Complexity of Iterative-Deepening A*", "Title": "The Informativeness Pathology (Abstract)", "Abstract": "Korf, Reid, and Edelkamp launched a line of research aimed at predicting how many nodes IDA* will expand with a given depth bound. This paper advances this line of research in three ways. First, we identify a source of prediction error that has hitherto been overlooked. We call it the \"discretization effect.\" Second, we disprove the intuitively appealing idea that a \"more informed\" prediction system cannot make worse predictions than a ``less informed'' one. More informed systems are more susceptible to the discretization effect, and in our experiments the more informed system makes poorer predictions. Our third contribution is a method, called \"Epsilon-truncation,\" which makes a prediction system less informed, in a carefully chosen way, so as to improve its predictions by reducing the discretization effect. In our experiments Epsilon-truncation improved predictions substantially."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "Multiple-Instance Learning", "Title": "Multiple Feature Selection on Instance Representation", "Abstract": "In multiple-Instance Learning (MIL), training class labels are attached to sets of bags composed of unlabeled instances, and the goal is to deal with classification of bags. Most previous MIL algorithms, which tackle classification problems, consider each instance as a represented feature. Although the algorithms work well in some prediction problems, considering diverse features to represent an instance may provide more significant information for learning task. Moreover, since each instance may be mapped into diverse feature spaces, encountering a large number of irrelevant or redundant features is inevitable. In this paper, we propose a method to select relevant instances and concurrently consider multiple features for each instance, which is termed as MIL-MFS. MIL-MFS is based on multiple kernel learning (MKL), and it iteratively selects the fusing multiple features for classifier training. Experimental results show that the MIL-MFS combined with multiple kernel learning can significantly improve the classification performance."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "Towards Large-Scale Collaborative Planning", "Title": "Answering High-Level Search Queries Using Human Computation", "Abstract": "Behind every search query is a high-level mission that the user wants to accomplish.  While current search engines can often provide relevant information in response to well-specified queries, they place the heavy burden of making a plan for achieving a mission on the user. We take the alternative approach of tackling users' high-level missions directly by introducing a human computation system that generates simple plans, by decomposing a mission into goals and retrieving search results tailored to each goal. Results show that our system is able to provide users with diverse, actionable search results and useful roadmaps for accomplishing their missions."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "SemRec", "Title": "A Semantic Enhancement Framework for Tag Based Recommendation", "Abstract": "Collaborative tagging services provided by various social web sites become popular means to mark web resources for different purposes such as categorization, expression of a preference and so on. However, the tags are of syntactic nature, in a free style and do not reflect semantics, resulting in the problems of redundancy, ambiguity and less semantics. Current tag-based recommender systems mainly take the explicit structural information among users, resources and tags into consideration, while neglecting the important implicit semantic relationships hidden in tagging data. In this study, we propose a Semantic Enhancement Recommendation strategy (SemRec), based on both structural information and semantic information through a unified fusion model. Extensive experiments conducted on two real datasets demonstarte the effectiveness of our approaches."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "Cross-Language Latent Relational Search", "Title": "Mapping Knowledge across Languages", "Abstract": "Latent relational search (LRS) is a novel approach for mapping knowledge across two domains. Given a source domain knowledge concerning the Moon, \"The Moon is a satellite of the Earth,\" one can form a question {(Moon, Earth), (Ganymede, ?)} to query an LRS engine for new knowledge in the target domain concerning the Ganymede. An LRS engine relies on some supporting sentences such as ``Ganymede is a natural satellite of Jupiter.'' to retrieve and rank \"Jupiter\" as the first answer. This paper proposes cross-language latent relational search (CLRS) to extend the knowledge mapping capability of LRS from cross-domain knowledge mapping to cross-domain and cross-language knowledge mapping. In CLRS, the supporting sentences for the source pair might be in a different language with that of the target pair. We represent the relation between two entities in an entity pair by lexical patterns of the context surrounding the two entities. We then propose a novel hybrid lexical pattern clustering algorithm to capture the semantic similarity between paraphrased lexical patterns across languages. Experiments on Japanese-English datasets show that the proposed method achieves an MRR of 0.579 for CLRS task, which is comparable to the MRR of an existing monolingual LRS engine."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "CCRank", "Title": "Parallel Learning to Rank with Cooperative Coevolution", "Abstract": "We propose CCRank, the first parallel algorithm for learning to rank, targeting simultaneous improvement in learning accuracy and efficiency. CCRank is based on cooperative coevolution (CC), a divide-and-conquer framework that has demonstrated high promise in function optimization for problems with large search space and complex structures. Moreover, CC naturally allows parallelization of sub-solutions to the decomposed subproblems, which can substantially boost learning efficiency. With CCRank, we investigate parallel CC in the context of learning to rank. Extensive experiments on benchmarks in comparison with the state-of-the-art algorithms show that CCRank gains in both accuracy and efficiency."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "The Steiner Multigraph Problem", "Title": "Wildlife Corridor Design for Multiple Species", "Abstract": "The conservation of wildlife corridors between existing habitat preserves is important for combating the effects of habitat loss and fragmentation facing species of concern.  We introduce the Steiner Multigraph Problem to model the problem of minimum-cost wildlife corridor design for multiple species with different landscape requirements.  This problem can also model other analogous settings in wireless and social networks.  As a generalization of Steiner forest, the goal is to find a minimum-cost subgraph that connects multiple sets of terminals.  In contrast to Steiner forest, each set of terminals can only be connected via a subset of the nodes.  Generalizing Steiner forest in this way makes the problem NP-hard even when restricted to two pairs of terminals.  However, we show that if the node subsets have a nested structure, the problem admits a fixed-parameter tractable algorithm in the number of terminals.  We successfully test exact and heuristic solution approaches on a wildlife corridor instance for wolverines and lynx in western Montana, showing that though the problem is computationally hard, heuristics perform well, and provably optimal solutions can still be obtained."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "Green Driver", "Title": "AI in a Microcosm", "Abstract": "The Green Driver app is a dynamic routing application for GPS-enabled smartphones. Green Driver combines client GPS data with real-time traffic light information provided by cities to determine optimal routes in response to driver route requests. Routes are optimized with respect to travel time, with the intention of saving the driver both time and fuel, and rerouting can occur if warranted. During a routing session, client phones communicate with a centralized server that both collects GPS data and processes route requests. All relevant data are anonymized and saved to databases for analysis; statistics are calculated from the aggregate data and fed back to the routing engine to improve future routing. Analyses can also be performed to discern driver trends: where do drivers tend to go, how long do they stay, when and where does traffic congestion occur, and so on. The system uses a number of techniques from the field of artificial intelligence. We apply a variant of A* search for solving the stochastic shortest path problem in order to find optimal driving routes through a network of roads given light-status information. We also use dynamic  programming and hidden Markov models to determine the progress of a driver through a network of roads from GPS data and light-status data. The Green Driver system is currently deployed for testing in Eugene, Oregon, and is scheduled for large-scale deployment in Portland, Oregon, in Spring 2011."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "Verifying Intervention Policies to Counter Infection Propagation over Networks", "Title": "A Model Checking Approach", "Abstract": "Spread of infections (diseases, ideas, etc.) in a networkcan be modeled as the evolution of states of nodes ina graph as a function of the states of their neighbors.Given an initial configuration of a network in which asubset of the nodes have been infected, and an infectionpropagation function that specifies how the states ofthe nodes evolve over time, we show how to use modelchecking to identify, verify, and evaluate the effectivenessof intervention policies for containing the propagationof infection over such networks."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "Block A*", "Title": "Database-Driven Search with Applications in Any-Angle Path-Planning", "Abstract": "We present three new ideas for grid-based path-planning algorithms that improve the search speed and quality of the paths found. First, we introduce a new type of database, the Local Distance Database (LDDB), that contains distances between boundary points of a local neighborhood. Second, an LDDB based algorithm is introduced, called Block A*, that calculates the optimal path between start and goal locations given the local distances stored in the LDDB. Third, our experimental results for any-angle path planning in a wide variety of test domains, including real game maps, show that Block A* is faster than both A* and the previously best grid-based any-angle search algorithm, Theta*."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "On the Complexity of BDDs for State Space Search", "Title": "A Case Study in Connect Four", "Abstract": "Symbolic search using BDDs usually saves huge amounts of memory, while in some domains its savings are moderate at best. It is an open problem to determine if BDDs work well for a certain domain. Motivated by finding evidences for BDD growths for state space search, in this paper we are concerned with symbolic search in the domain of Connect Four. We prove that there is a variable ordering for which the set of all possible states – when continuing after a terminal state has been reached – can be represented by polynomial sized BDDs, whereas the termination criterion leads to an exponential number of nodes in the BDD given any variable ordering."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "Contextually-Based Utility", "Title": "An Appraisal-Based Approach at Modeling Framing and Decisions", "Abstract": "Creating accurate computational models of human decision making is a vital step towards the realization of socially intelligent systems capable of both predicting and simulating human behavior. In modeling human decision making, a key factor is the psychological phenomenon known as \"framing\", in which the preferences of a decision maker change in response to contextual changes in decision problems. Existing approaches treat framing as a one-dimensional contextual influence based on the perception of outcomes as either gains or losses. However, empirical studies have shown that framing effects are much more multifaceted than one-dimensional views of framing suggest. To address this limitation, we propose an integrative approach to modeling framing which combines the psychological principles of cognitive appraisal theories and decision-theoretic notions of utility and probability. We show that this approach allows for both the identification and computation of the salient contextual factors in a decision as well as modeling how they ultimately affect the decision process. Furthermore, we show that our multi-dimensional, appraisal-based approach can account for framing effects identified in the empirical literature which cannot be addressed by one-dimensional theories, thereby promising more accurate models of human behavior."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "Analogical Dialogue Acts", "Title": "Supporting Learning by Reading Analogies in Instructional Texts", "Abstract": "Analogy is heavily used in instructional texts.  We introduce the concept of analogical dialogue acts (ADAs), which represent the roles utterances play in instructional analogies.  We describe a catalog of such acts, based on ideas from structure-mapping theory.  We focus on the operations that these acts lead to while understanding instructional texts, using the Structure-Mapping Engine (SME) and dynamic case construction in a computational model.  We test this model on a small corpus of instructional analogies expressed in simplified English, which were understood via a semi-automatic natural language system using analogical dialogue acts.  The model enabled a system to answer questions after understanding the analogies that it was not able to answer without them."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "CosTriage", "Title": "A Cost-Aware Triage Algorithm for Bug Reporting Systems", "Abstract": "Who can fix this bug? is an important question in bug triage to \"accurately\" assign developers to bug reports. To address this question, recent research treats it as a optimizing recommendation accuracy problem and proposes a solution that is essentially an instance of content-based recommendation (CBR). However, CBR is well-known to cause over-specialization, recommending only the types of bugs that each developer has solved before. This problem is critical in practice, as some experienced developers could be overloaded, and this would slow the bug fixing process. In this paper, we take two directions to address this problem: First,we reformulate the problem as an optimization problem of both accuracy and cost. Second, we adopt a content-boosted collaborative filtering (CBCF), combining an existing CBR with a collaborative filtering recommender (CF), which enhances the recommendationquality of either approach alone. However, unlike general recommendation scenarios, bug fix history is extremely sparse. Due to the nature of bug fixes, one bug is fixed by only one developer, which makes it challenging to pursue the above two directions. To address this challenge, we develop a topic-model to reduce the sparseness and enhance the quality of CBCF. Our experimental evaluation shows that our solution reduces the cost efficiently by 30% without seriously compromising accuracy."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "Transportability of Causal and Statistical Relations", "Title": "A Formal Approach", "Abstract": "We address the problem of transferring information learned from experiments to a different environment, in which only passive observations can be collected.   We introduce a formal representation called \"selection diagrams\" for expressing knowledge about differences and commonalities between environments  and, using this representation, we derive procedures for deciding whether effects in the target environment can be inferred from experiments conducted elsewhere. When the answer is affirmative, the procedures identify the set of experiments and observations that need be conducted to license the transport. We further discuss how transportability analysis can guide the transfer of knowledge in non-experimental learning to minimize re-measurement cost and improve prediction power."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "Preferred Explanations", "Title": "Theory and Generation via Planning", "Abstract": "In this paper we examine the general problem of generating preferred explanations for observed behavior with respect to a model of the behavior of a dynamical system. This problem arises in a diversity of applications including diagnosis of dynamical systems and activity recognition. We provide a logical characterization of the notion of an explanation. To generate explanations we identify and exploit a correspondence between explanation generation and planning. The determination of good explanations requires additional domain-specific knowledge which we represent as preferences over explanations. The nature of explanations requires us to formulate preferences in a somewhat retrodictive fashion by utilizing Past Linear Temporal Logic. We propose methods for exploiting these somewhat unique preferences effectively within state-of-the-art planners and illustrate the feasibility of generating (preferred) explanations via planning."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "Learning in Repeated Games with Minimal Information", "Title": "The Effects of Learning Bias", "Abstract": "Automated agents for electricity markets, social networks, and other distributed networks must repeatedly interact with other intelligent agents, often without observing associates' actions or payoffs (i.e., minimal information).  Given this reality, our goal is to create algorithms that learn effectively in repeated games played with minimal information.  As in other applications of machine learning, the success of a learning algorithm in repeated games depends on its learning bias.  To better understand what learning biases are most successful, we analyze the learning biases of previously published multi-agent learning (MAL) algorithms.  We then describe a new algorithm that adapts a successful learning bias from the literature to minimal information environments.  Finally, we compare the performance of this algorithm with ten other algorithms in repeated games played with minimal information."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "M-Unit EigenAnt", "Title": "An Ant Algorithm to Find the M Best Solutions", "Abstract": "In this paper, we shed light on how powerful congestion control based on local interactions may be obtained. We show how ants can use repellent pheromones and incorporate the effect of crowding to avoid traffic congestion on the optimal path. Based on these interactions, we propose an ant algorithm, the M-unit EigenAnt algorithm, that leads to the selection of the M shortest paths. The ratio of selection of each of these paths is also optimal and regulated by an optimal amount of pheromone on each of them. To the best of our knowledge, the M-unit EigenAnt algorithm is the first antalgorithm that explicitly ensures the selection of the M shortest paths and regulates the amount of  pheromone on them such that it is asymptotically optimal. In fact, it is in contrast with most ant algorithms that aim to discover just a single best path. We provide its convergence analysis and show that the steady state distribution of pheromone aligns with the eigenvectors of the cost matrix, and thus is related to its measure of quality. We also provide analysis to show that this property ensues even when the food is moved or path lengths change during foraging. We show that this behavior is robust in the presence of fluctuations and quickly reflects the change in the M optimal solutions. This makes it suitable for not only distributed applications butalso dynamic ones as well. Finally, we provide simulation results for the convergence to the optimal solution under different initial biases, dynamism in lengths of paths, and discovery of new paths."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "Mean Field Inference in Dependency Networks", "Title": "An Empirical Study", "Abstract": "Dependency networks are a compelling alternative to Bayesian networks for learning joint probability distributions from data and using them to compute probabilities.  A dependency network consists of a set of conditional probability distributions, each representing the probability of a single variable given its Markov blanket.  Running Gibbs sampling with these conditional distributions produces a joint distribution that can be used to answer queries, but suffers from the traditional slowness of sampling-based inference.  In this paper, we observe that the mean field update equation can be applied to dependency networks, even though the conditional probability distributions may be inconsistent with each other.  In experiments with learning and inference on 12 datasets, we demonstrate that mean field inference in dependency networks offers similar accuracy to Gibbs sampling but with orders of magnitude improvements in speed.  Compared to Bayesian networks learned on the same data, dependency networks offer higher accuracy at greater amounts of evidence.  Furthermore, mean field inference is consistently more accurate in dependency networks than in Bayesian networks learned on the same data."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "OASIS", "Title": "Online Active Semi-Supervised Learning", "Abstract": "We consider a learning setting of importance to large scale machine learning: potentially unlimited data arrives sequentially, but only a small fraction of it is labeled. The learner cannot store the data; it should learn from both labeled and unlabeled data, and it may also request labels for some of the unlabeled items. This setting is frequently encountered in real-world applications and has the characteristics of online, semi-supervised, and active learning. Yet previous learning models fail to consider these characteristics jointly. We present OASIS, a Bayesian model for this learning setting. The main contributions of the model include the novel integration of a semi-supervised likelihood function, a sequential Monte Carlo scheme for efficient online Bayesian updating, and a posterior-reduction criterion for active learning. Encouraging results on both synthetic and real-world optical character recognition data demonstrate the synergy of these characteristics in OASIS."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "Transfer Latent Semantic Learning", "Title": "Microblog Mining with Less Supervision", "Abstract": "The increasing volume of information generated on micro-blogging sites such as Twitter raises several challenges to traditional text mining techniques. First, most texts from those sites are abbreviated due to the constraints of limited characters in one post; second, the input usually comes in streams of large-volumes. Therefore, it is of significant importance to develop effective and efficient representations of abbreviated texts for better filtering and mining. In this paper, we introduce a novel transfer learning approach, namely transfer latent semantic learning, that utilizes a large number of related tagged documents with rich information from other sources (source domain) to help build a robust latent semantic space for the abbreviated texts (target domain). This is achieved by simultaneously minimizing the document reconstruction error and the classification error of the labeled examples from the source domain by building a classifier with hinge loss in the latent semantic space. We demonstrate the effectiveness of our method by applying them to the task of classifying and tagging abbreviated texts. Experimental results on both synthetic datasets and real application datasets, including Reuters-21578 and Twitter data, suggest substantial improvements using our approach over existing ones."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "Linear Discriminant Analysis", "Title": "New Formulations and Overfit Analysis", "Abstract": "In this paper, we will present a unified view for LDA. We will (1) emphasize that standard LDA solutions are not unique, (2) propose several new LDA formulations: St-orthonormal LDA, Sw-orthonormal LDA and orthogonal LDA which have unique solutions, and (3) show that with St-orthonormal LDA and Sw-orthonormal LDA formulations, solutions to all four major LDA objective functions are identical. Furthermore, we perform an indepth analysis to show that the LDA sometimes performs poorly due to over-fitting, i.e., it picks up PCA dimensions with small eigenvalues. From this analysis, we propose a stable LDA which uses PCA first to reduce to a small PCA subspace and do LDA in the subspace."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "Markov Logic Sets", "Title": "Towards Lifted Information Retrieval Using PageRank and Label Propagation", "Abstract": "Inspired by “GoogleTM Sets” and Bayesian sets, we consider the problem of retrieving complex objects and relations among them, i.e., ground atoms from a logical concept, given a query consisting of a few atoms from that concept. We formulate this as a within-network relational learning problem using few labels only and describe an algorithm that ranks atoms using a score based on random walks with restart (RWR): the probability that a random surfer hits an atom starting from the query atoms. Specifically, we compute an initial ranking using personalized PageRank. Then, we find paths of atoms that are connected via their arguments, variablize the ground atoms in each path, in order to create features for the query. These features are used to re-personalize the original RWR and to finally compute the set completion, based on Label Propagation. Moreover, we exploit that RWR techniques can naturally be lifted and show that lifted inference for label propagation is possible. We evaluate our algorithm on a realworld relational dataset by finding completions of sets of objects describing the Roman city of Pompeii. We compare to Bayesian sets and show that our approach gives very reasonable set completions."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "Human Spatial Relational Reasoning", "Title": "Processing Demands, Representations, and Cognitive Model", "Abstract": "Empirical findings indicate that humans draw infer- ences about spatial arrangements by constructing and manipulating mental models which are internal representations of objects and relations in spatial working memory. Central to the Mental Model Theory (MMT), is the assumption that the human reasoning process can be divided into three phases: (i) Mental model construction, (ii) model inspection, and (iii) model validation. The MMT can be formalized with respect to a computational model, connecting the reasoning process to operations on mental model representations. In this respect a computational model has been implemented in the cognitive architecture ACT-R capable of explaining human reasoning difficulty by the number of model operations. The presented ACT-R model allows simulation of psychological findings about spatial reasoning problems from a previous study that investigated conventional behavioral data such as response times and error rates in the context of certain mental model construction principles."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "Quantity Makes Quality", "Title": "Learning with Partial Views", "Abstract": "In many real world applications, the number of examples to learn from is plentiful, but we can only obtain limited information on each individual example. We study the possibilities of efficient, provably correct, large-scale learning in such settings. The main theme we would like to establish is that large amounts of examples can compensate for the lack of full information on each individual example. The type of partial information we consider can be due to inherent noise or from constraints on the type of interaction with the data source. In particular, we describe and analyze algorithms for budgeted learning, in which the learner can only view a few attributes of each training example, and algorithms for learning kernel-based predictors, when  individual examples are corrupted by random noise."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "Recommendation Sets and Choice Queries", "Title": "There Is No Exploration/Exploitation Tradeoff!", "Abstract": "Utility elicitation is an important component of many applications, such as decision support systems and recommender systems. Such systems query users about their preferences and offer recommendations based on the system's belief about the user's utility function. We analyze the connection between the problem of generating optimal recommendation sets and the problem of generating optimal choice queries, considering both Bayesian and regret-based elicitation. Our results show that, somewhat surprisingly, under very general circumstances, the optimal recommendation set coincides with the optimal query."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "Global Seismic Monitoring", "Title": "A Bayesian Approach", "Abstract": "The automated processing of multiple seismic signals to detect and localize seismic events is a central tool in both geophysics and nuclear treaty verification. This paper reports on a project, begun in 2009, to reformulate this problem in a Bayesian framework. A Bayesian seismic monitoring system, NET-VISA, has been built comprising a spatial event prior and generative models of event transmission and detection, as well as an inference algorithm. Applied in the context of the International Monitoring System (IMS), a global sensor network developed for the Comprehensive Nuclear-Test-Ban Treaty (CTBT), NET-VISA achieves a reduction of around 50% in the number of missed events compared to the currently deployed system. It also finds events that are missed even by the human analysts who post-process the IMS output."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "Lossy Conservative Update (LCU) Sketch", "Title": "Succinct Approximate Count Storage", "Abstract": "In this paper, we propose a variant of the conservativeupdate Count-Min sketch to further reduce the overestimation error incurred. Inspired by ideas from lossy counting, we divide a stream of items into multiple windows, and decrement certain counts in the sketch at window boundaries. We refer to this approach as a lossy conservative update (LCU). The reduction in overestimation error of counts comes at the cost of introducing under-estimation error in counts. However, in our intrinsic evaluations, we show that the reduction in overestimation is much greater than the under-estimation error introduced by our method LCU. We apply our LCU framework to scale distributional similarity computations to web-scale corpora. We show that this technique is more efficient in terms of memory, and time, and more robust than conservative update with Count-Min (CU) sketch on this task."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "WikiSimple", "Title": "Automatic Simplification of Wikipedia Articles", "Abstract": "Text simplification aims to rewrite text into simpler versions and thus make information accessible to a broader audience (e.g., non-native speakers, children, and individuals with language impairments). In this paper, we propose a model that simplifies documents automatically while selecting their most important content and rewriting them in a simpler style. We learn content selection rules from same-topic Wikipedia articles written in the main encyclopedia and its Simple English variant. We also use the revision histories of Simple Wikipedia articles to learn a quasi-synchronous grammar of simplification rewrite rules. Based on an integer linear programming formulation, we develop a joint model where preferences based on content and style are optimized simultaneously. Experiments on simplifying main Wikipedia articles show that our method significantly reduces the reading difficulty, while still capturing the important content."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "DISCO", "Title": "Describing Images Using Scene Contexts and Objects", "Abstract": "In this paper, we propose a bottom-up approach to generating short descriptive sentences from images, to enhance scene understanding. We demonstrate automatic methods for mapping the visual content in an image to natural spoken or written language. We also introduce a human-in-the-loop evaluation strategy that quantitatively captures the meaningfulness of the generated sentences. We recorded a correctness rate of 60.34% when human users were asked to judge the meaningfulness of the sentences generated from relatively challenging images. Also, our automatic methods compared well with the state-of-the-art techniques for the related computer vision tasks."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "NewsFinder", "Title": "Automating an Artificial Intelligence News Service", "Abstract": "NewsFinder automates the steps involved in finding, select- ing and publishing news stories that meet subjective judgments of relevance and interest to the Artificial Intelligence community. NewsFinder combines a broad search with AI-specific filters and incorporates a learning program whose judgment of interestingness of stories can be trained by feedback from readers. Since August, 2010, the program has been used to operate the AI in the News service that is part of the AAAI AI Topics site."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "The Glass Infrastructure", "Title": "Using Common Sense to Create a Dynamic, Place-Based Social Information System", "Abstract": "Most organizations have a wealth of knowledge about themselves available online, but little for a visitor to interact with on-site. At the MIT Media Lab, we have designed and deployed a novel intelligent signage system, the Glass Infrastructure (GI) that enables small groups of users to physically interact with this data and to discover the latent connections between people, projects, and ideas. The displays are built on an adaptive, unsupervised model of the organization developed using dimensionality reduction and common sense knowledge which automatically classifies and organizes the information.The GI is currently in daily use at the lab. We discuss the AI models development, the integration of AI into an HCI interface, and the use of the GI during the labs peak visitor periods. We show that the GI is used repeatedly by lab visitors and provides a window into the workings of the organization."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "Learning by Demonstration Technology for Military Planning and Decision Making", "Title": "A Deployment Story", "Abstract": "Learning by demonstration technology has long held the promise to empower non-programmers to customize and extend software. We describe the deployment of a learning by demonstration capability to support user creation of automated procedures in a collaborative planning environment that is used widely by the U.S. Army. This technology, which has been in operational use since the summer of 2010, has helped to reduce user workloads by automating repetitive and time-consuming tasks. The technology has also provided the unexpected benefit of enabling standardization of products and processes."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "Monitoring Entities in an Uncertain World", "Title": "Entity Resolution and Referential Integrit", "Abstract": "This paper describes a system to help intelligence analysts track and analyze information being published in multiple sources, particularly open sources on the Web. The system integrates technology for Web harvesting, natural language extraction, and network analytics, and allows analysts to view and explore the results via a Web application. One of the difficult problems we address is the entity resolution problem, which occurs when there are multiple, differing ways to refer to the same entity. The problem is particularly complex when noisy data is being aggregated over time, there is no clean master list of entities, and the entities under investigation are intentionally being deceptive. Our system must not only perform entity resolution with noisy data, but must also gracefully recover when entity resolution mistakes are subsequently corrected. We present a case study in arms trafficking that illustrates the issues, and describe how they are addressed."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "Abductive Inference for Combat", "Title": "Using SCARE-S2 to Find High-Value Targets in Afghanistan", "Abstract": "Recently, geospatial abduction was introduced by the authors in (Shakarian, Subrahmanian, and Sapino 2010) as a way to infer unobserved geographic phenomena from a set of known observations and constraints between the two. In this paper, we introduce the SCARE- S2 software tool which applies geospatial abduction to the environment of Afghanistan. Unlike previous work, where we looked for small weapon caches supporting local attacks, here we look for insurgent high-value targets (HVT’s), supporting insurgent operations in two provinces. These HVT’s include the locations of insurgent leaders and major supply depots. Applying this method of inference to Afghanistan introduces several practical issues not addressed in previous work. Namely, we are conducting inference in a much larger area (24, 940 sq km as compared to 675 sq km in previous work), on more varied terrain, and must consider the influence of many local tribes. We address all of these problems and evaluate our software on 6 months of real-world counter-insurgency data. We show that we are able to abduce regions of a relatively small area (on average, under 100 sq km and each containing, on aver- age, 4.8 villages) that are more dense with HVT’s (35× more than the overall area considered)."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "Accelerating the Discovery of Data Quality Rules", "Title": "A Case Study", "Abstract": "Poor quality data is a growing and costly problem that af- fects many enterprises across all aspects of their business ranging from operational efficiency to revenue protection. In this paper, we present an application – Data Quality Rules Accelerator (DQRA) – that accelerates Data Quality (DQ) efforts (e.g. data profiling and cleansing) by automatically discovering DQ rules for detecting inconsistencies in data. We then present two evaluations. The first evaluation compares DQRA to existing solutions; and shows that DQRA either outperformed or achieved performance comparable with these solutions on metrics such as precision, recall, and runtime. The second evaluation is a case study where DQRA was piloted at a large utilities company to improve data quality as part of a legacy migration effort. DQRA was able to discover rules that detected data inconsistencies directly impacting revenue and operational efficiency. Moreover, DQRA was able to significantly reduce the amount of effort required to develop these rules compared to the state of the practice. Finally, we describe ongoing efforts to deploy DQRA."}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "Teaching Reinforcement Learning with Mario", "Title": "An Argument and Case Study", "Abstract": "Integrating games into the computer science curriculum has been gaining acceptance in recent years, particularly when used to improve student engagement in introductory courses. This paper argues that games can also be useful in upper level courses, such as general artificial intelligence and machine learning. We provide a case study of using a Mario game in a machine learning class to provide one successful data point where both content-specific and general learning outcomes were successfully achieved"}
{"Type": "conference", "Year": "2011", "Area": "AI", "Where": "AAAI", "Abbreviation": "Playing to Program", "Title": "Towards an Intelligent Programming Tutor for RUR-PLE", "Abstract": "Intelligent tutoring systems (ITSs) provide students with a one-on-one tutor, allowing them to work at their own pace, and helping them to focus on their weaker areas. The RUR1–Python Learning Environment (RUR-PLE), a game-like virtual environment to help students learn to program, provides an interface for students to write their own Python code and visualize the code execution (Roberge 2005). RUR-PLE provides a fixed sequence of learning lessons for students to explore. We are extending RUR-PLE to develop the Playing to Program (PtP) ITS, which consists of three components: (1) a Bayesian student model that tracks student competence, (2) a diagnosis module that provides tailored feedback to students, and (3) a problem selection module that guides the student’s learning process. In this paper, we summarize RUR-PLE and the PtP design, and describe an ongoing user study to evaluate the predictive accuracy of our student modeling approach."}
