{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "CIS", "Title": "A Massively Concurrent Rule-Based System", "Abstract": "Recently researchers have suggested several computational models in which, one programs by specifying large networks of simple devices. Such models are interesting because they go to the roots of concurrency - the circuit level. A problem with the models is that it is unclear how to program large systems and expensive to implement many features that are taken for granted m symbolic programming languages. This paper describes the Concurrent Inference System (CIS), and its implementation on a massively concurrent network model of computation. It shows how much of the functionality of current rule-based systems can be implemented in a straightforward manner within such models. Unlike conventional implementations of rule-based systems in which the inference engine and rule sets are clearly divided at run time, CIS compiles the rules into a large static concurrent network of very simple devices. In this network the rules and inference engine are no longer distinct. The Thinking Machines Corporation, Connection Machine - a 65,536 processor SIMD computer - is then used to run the network. On the current implementation, real time user system interaction is possible with up to 100,000 rules."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "Merging Objects and Logic Programming", "Title": "Relational Semantics", "Abstract": "This paper proposes new semantics for merging object programming into logic programming. It differs from previous attempts in that it takes a relational view of method evaluation and inheritance mechanisms originating from object programming. A tight integration is presented, an extended rationale for adopting a success/failure semantics of backtrackable methods calls and for authorizing variable object calls is given. New method types dealing with non monotonicity and determinism necessary for this tight integration are discussed. The need for higher functions is justified from a user point of view. as well as from an implementation one. The system POL is only a piece of a more ambitious goal which is to merge logic programming, object programming and semantic data models which can be seen as an attempt to bridge the gap between AI and databases. The paper is restricted to a programming perspective."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "Comments on Kornfeld’s Equality for Prolog", "Title": "E-Unification as a Mechanism for Augmenting the Prolog Search Strategy", "Abstract": "The search strategy of standard Prolog can lead to a situation in which a predicate has to be evaluated in circumstances where it has an infeasibly large number of instantiations. The work by Kornfeld [8] addressed this important problem by means of an extension of unification which allows Prolog to be augmented by what is essentially a (non-standard) equality theory. This paper uses the notion of the general procedure introduced by van Emden and Lloyd [12] to formalize Kornfeld’s work. In particular, the formalization is used to make a careful analysis and evaluation of Kornfeld’s solution to the problem of delayed evaluation."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "Artificial Intelligence and Design", "Title": "A Mechanical Engineering View", "Abstract": "Most AI research into design has been based on or directed to the electrical circuit domain. This paper presents a mechanical engineer’s view. Design of mechanical parts and products differs from design of electrical circuits in several fundamental ways: materials selection, sensitivity to manufacturing issues, non-modularity, high coupling of form and function, and especially the role of 3-D geometry. These differences, and the role of analysis in mechanical design, are discussed. A model for design is also presented based on the basically iterative nature of the design process. A brief summary of the related research at the University of Massachusetts into application of AI to mechanical design is included."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "Integration of Multiple Knowledge Sources in ALADIN", "Title": "An Alloy Design System", "Abstract": "ALADIN' is a knowledge-based system that aids metallurgists in the design of new aluminum alloys. Alloy design is characterized by creativity, intuition and conceptual reasoning. The application of artificial intelligence to this domain poses a number of challenges, including: how to focus the search, how to deal with subproblem interactions, how to integrate multiple, incomplete design models and how to represent complex, metallurgical structure knowledge. In this paper, our approach to dealing with these problems is described."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "Saturn", "Title": "An Automatic Test Generation System for Digital Circuits", "Abstract": "This paper describes a novel test generation system, called Saturn, for testing digital circuits. The system differs from existing test generation systems in that it allows a designer to specify the structure and behavior of a design at a collection of abstraction levels that mirror the design refinement process. The system exploits the abstract design formulations to increase the efficiency of test generation by ignoring irrelevant detail whenever possible. These capabilities are made possible by using general representation and reasoning methods based on logic, which provide a declarative representation of a design, and permit using a single inference procedure for reasoning both forwards and backwards through the design for test generation."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "Knowledge-Based Simulation of a Glass Annealing Process", "Title": "An AI Application in the Glass Industry", "Abstract": "This paper describes a knowledge-based simulation system for a glass annealing process. The long ovens, known as lehrs, in which annealing takes place are not well understood by their operators. In fact, only a few experts can predict the effects of a change in the lehr controls. Attempts to simulate the behavior of the lehr using conventional methods have not been successful due to the size and complexity of the lehr. Our knowledge-based approach is capable of both simulating the glass temperature curve in an annealing lehr and planning the necessary lehr control settings to achieve a desired curve. It consists of two cooperating expert systems, one rule-based and the other frame-based. The system also includes a high-bandwidth graphics display which allows operators to interactively test control-setting changes and ask for the control settings which meet desired specifications. A description of the domain, a history of the development, and details of the design are all presented, along with lessons learned from the experience."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "SCAT", "Title": "An Automatic-Programming Tool for Telecommunications Software", "Abstract": "The size, complexity and long life-time of telecommunications software, e.g. the programs for store program control (SPC) telephone exchanges, call for an increased software productivity and maintainability other than an improved quality. The availability of programming support environments based on standardized specification and programming languages greatly improves the software development process. Artificial Intelligence techniques are very promising aiming at further improvements and can provide a short-term payoff especially within an evolutionary approach leading up to an hybrid programming environment, i.e. a software environment made of both conventional and intelligent tools. The paper describes an intelligent tool, dubbed SCAT, based on ideas exploited by various automatic programming systems, like CHI, Programmer’s Apprentice and DEDALUS. SCAT is strictly related to the telecommunications domain, thus it differs from other systems in the domain specifity. SCAT partly automatizes the most crucial phase in the software development process, i.e. the transition from project’s detailed specification to the actual software implementation. SCAT has been tested in a few experimental software developments and in an actual application,i.e. the message handling system (MHS) to be made available in the Italian public packet switching network (ITAPAC)."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "PIES", "Title": "An Engineers Do-lt-Yourself Knowledge System for Interpretation of Parametric Test Data", "Abstract": "PIES is a knowledge system for interpreting the parametric test data collected at the end of complex semiconductor fabrication processes. The system transforms hundreds of measurements into a concise statement of the overall health of the process, and the nature and probable cause of any anomalies. A key feature of PIES is the structure of the knowledge-base, which reflects the way fabrication engineers reason causally about semiconductor failures. This structure permits fabrication engineers to do their own knowledge engineering, building the knowledge base, and then maintaining it to reflect process modifications and operating experience. The approach appears applicable to other process control and diagnosis tasks."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "StarPlan II", "Title": "Evolution of an Expert System", "Abstract": "An expert system for satellite anomaly resolution must perform monitoring, situation assessment, diagnosis, goal determination and planning functions in real time. StarPlan is such a system being developed at the Ford Aerospace Sunnyvale Operation. This paper details the evolution of the StarPlan architecture from a rule-based system in which multiple \"experts\" classified and resolved anomalies to a more generic architecture that utilizes an object model of the domain to perform fault diagnosis using causal reasoning. The StarPlan I architecture is described; the lessons learned in StarPlan I implementation are discussed; and the architecture of StarPlan II is presented."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "PROTEAN", "Title": "Deriving Protein Structure from Constraints", "Abstract": "PROTEAN is an evolving knowledge-based system that is intended to identify the three-dimensional conformations of proteins in solution. Using a variety of empirically derived constraints, PROTEAN must identify legal positions for each of a protein’s constituent structures (e.g., atoms, amino acids, helices) in three-dimensional space. In fact, because protein-structure analysis is an underconstrained problem, PROTEAN must identify the entire family of conformations allowed by available constraints. In this paper, we discuss PROTEAN’s approach to the protein-structure analysis problem and its current implementation within the BBl blackboard architecture."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "Back to Backtracking", "Title": "Controlling the ATMS", "Abstract": "The ATMS (Assumption-Based Truth Maintenance System) provides a very general facility for all types of default reasoning. One of the principal advantages of the ATMS is that all of the possible (usually mutually inconsistent) solutions or partial solutions are directly available to the problem solver. By exploiting this capability of the ATMS, the problem solver can efficiently work on all solutions simultaneously and avoid the computational expense of backtracking. However, for some applications this ATMS capability is more of a hindrance than a help and some form of backtracking is necessary. This paper first outlines some of the reasons why backtracking is still necessary, and presents a powerful backtracking algorithm which we have implemented which backtracks more efficiently than other approaches."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "Towards Explicit Integration of Knowledge in Expert Systems", "Title": "An Analysis of MYClN’s Therapy Selection Algorithm", "Abstract": "The knowledge integration problem arises in rule-based expert systems when two or more recommendations made by right-hand sides of rules must be combined. Current expert systems address this problem either by engineering the rule set to avoid it, or by using a single integration technique built into the interpreter, e.g., certainty factor combination. We argue that multiple techniques are needed and that their use -- and underlying assumptions -- should be made explicit. We identify some of the techniques used in MYCIN’s therapy selection algorithm to integrate the diverse goals it attempts to satisfy, and suggest how knowledge of such techniques could be used to support construction, explanation, and maintenance of expert systems."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "The Shifting Terminological Space", "Title": "An Impediment to Evolvability", "Abstract": "In an expert system, rules or methods interact by creating situations to which other rules or methods respond. We call the language in which these situations are represented the terminological space. In most expert systems, terms in this language often lack an independent definition, in which case they are implicitly defined by the way the rules or methods react to them. We argue that this hampers evolution, and argue for a separate, independently defined terminological space that is automatically maintained."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "MOLE", "Title": "A Knowledge Acquisition Tool that Uses its Head", "Abstract": "MOLE can help domain experts build a heuristic classification problem-solver by working with them to generate an initial knowledge base and then detect and remedy deficiencies in it. By exploiting several heuristic assumptions about the world, MOLE is able to minimize the information it needs to elicit from the domain expert. In particular, by using static techniques of analysis, MOLE is able to infer support values and fill in gaps when a knowledge base is under-specified. And by using dynamic techniques of analysis, MOLE is able to interactively refine the knowledge base."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "Knowledge Level Engineering", "Title": "Ontological Analysis", "Abstract": "Knowledge engineering suffers from a lack of formal tools for understanding domains of interest. Current practice relies on an intuitive, informal approach for collecting expert knowledge and formulating it into a representation scheme adequate for symbolic processing. Implicit in this process, the knowledge engineer formulates a model of the domain, and creates formal data structures (knowledge base) and procedures (inference engine) to solve the task at hand. Newell (1982) has proposed that there should be a knowledge level analysis to aid the development of AI systems in general and knowledge-based expert systems in particular. This paper describes a methodology, called ontological analysis, which provides this level of analysis. The methodology consists of an analysis tool and its principles of use that result in a formal specification of the knowledge elements in a task domain."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "AGNESS", "Title": "A Generalized Network-based Expert System Shell", "Abstract": "AGNESS is an expert system shell developed at the University of Minnesota. AGNESS is more general than other shells. It uses a computation network to represent expert defined rules, and can handle any well-defined inference method. The system works with non-numeric as well as numeric data, and shares constructs whenever possible to achieve increased storage efficiency. AGNESS uses a menu-driven user interface, and has several features that make the system friendly and convenient to use. The system includes eight explanation queries designed to increase the amount of information available to the user, the expert, and the knowledge engineer while remaining simple enough to be included in most of today’s expert system shells. AGNESS has been tested on several domains ranging from simplified problems to real world medical analysis."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "SYNTEL(TM)", "Title": "Knowledge Programming Using Functional Representations", "Abstract": "SYNTEL is a novel knowledge representation language that provides traditional features of expert system shells within a pure functional programming paradigm. However, it differs sharply from existing functional languages in many ways, ranging from its ability to deal with uncertainty to its evaluation procedures. A very flexible user-interface definition facility is tightly integrated with the SYNTEL interpreter, giving the knowledge engineer full control over both form and content of the end-user system. SYNTEL is fully implemented and has been successfully used to develop large knowledge bases dealing with problems of risk assessment."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "GBB", "Title": "A Generic Blackboard Development System", "Abstract": "This paper describes a generic blackboard development system (GBB) that unifies many characteristics of the blackboard systems constructed to date. The goal of GBB is to provide flexibility, ease of implementation, and efficient execution of the resulting application system. Efficient insertion/retrieval of blackboard objects is achieved using a language for specifying the detailed structure of the blackboard as well as how that structure is to be implemented for a specific application. These specifications are used to generate a blackboard database kernel tailored to the application. GBB consists of two distinct subsystems: a blackboard database development subsystem and a control shell. This paper focuses on the database support and pattern matching capabilities of GBB, and presents the concepts and functionality used in providing an efficient blackboard database development subsystem."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "Refining the Knowledge Base of a Diagnostic Expert System", "Title": "An Application of Failure-Driven Learning", "Abstract": "This paper discusses an application of failure-driven learning to the construction of the knowledge base of a diagnostic expert system. Diagnosis heuristics (i.e., efficient rules which encode empirical associations between atypical device behavior and device failures) are learned from information implicit in device models. This approach is desireable since less effort is required to obtain information about device functionality and connectivity to define device models than to encode and debug diagnosis heuristics from a domain expert. We give results of applying this technique in an expert system for the diagnosis of failures in the attitude control system of the DSCS-III satellite. The system is fully implemented in a combination of LISP and PROLOG on a Symbolics 3600. The results indicate that realistic applications can be built using this approach. The performance of the diagnostic expert system after learning is equivalent to and, in some cases, better than the performace of the expert system with rules supplied by a domain expert."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "Adapting MUMBLE", "Title": "Experience with Natural Language Generation", "Abstract": "This paper describes the construction of a MUMBLE-based [5] tactical component for the TEXT text generation system [7]. This new component, which produces fluent English sentences from the sequence of structured message units output from TEXT’s strategic component, has produced a 60-fold speed-up in sentence production. Adapting MUMBLE required work on each of the three parts of the MUMBLE framework: the interpreter, the grammar, and the dictionary. It also provided some insight into the organization of the generation process and the consequences of MUMBLE’s commitment to a deterministic model."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "Object Recognition in Structured and Random Environments", "Title": "Locating Address Blocks on Mail Pieces", "Abstract": "A framework for determining special interest objects in images is presented in the context of determining destination address blocks on images of mail pieces such as letters, magazines, and parcels. The images range from those having a high degree of global spatial structure (e.g., carefully prepared letter mail envelopes which conform to specifications) to those with no structure (e.g., magazines with randomly pasted address labels). A method of planning the use of a large numbers of specialized tools is given. The control utilizes a dependency graph, knowledge rules, and a blackboard."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "Reasoning with Simplifying Assumptions", "Title": "A Methodology and Example", "Abstract": "Simplifying assumptions are a powerful technique for dealing with complexity, which is used in all branches of science and engineering. This work develops a formal account of this technique in the context of heuristic search and automated reasoning. We also present a methodology for choosing appropriate simplifying assumptions in specific domains, and demonstrate the use of this methodology with an example of reasoning about typed partial functions in an automated programming assistant."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "Tweety–Still Flying", "Title": "Some Remarks on Abnormal Birds, Applicable Rules and a Default Prover", "Abstract": "This paper describes FAULTY, a default prover for a decidable subset of predicate calculus. FAULTY is based on McDermott’s and Doyle’s Nonmonotonic Logic I und avoids the well-known weakness of this logic by a restriction to specific theories, which are sufficient for default reasoning purposes, however. The dafaults are represented in a way that allows explicit control of their applicability. By blocking the applicability of a default the problem of interacting defaults can be avoided."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "Beyond Incremental Processing", "Title": "Tracking Concept Drift", "Abstract": "Learning in complex, changing environments requires methods that are able to tolerate noise (less than perfect feedback) and drift (concepts that change over time). These two aspects of complex environments interact with each other: when some particular learned predictor fails to correctly predict the expected outcome (or when the outcome occurs without having been preceded by the learned predictor), a learner must be able to determine whether the situation is an instance of noise or an indication that the concept is beginning to drift. We present a learning method that is able to learn complex Boolean characterizations while tolerating noise and drift. An analysis of the algorithm illustrates why it has these desirable behaviors, and empirical results from an implementation (called STAGGER) are presented to show its ability to track changing concepts over time."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "STAHLp", "Title": "Belief Revision in Scientific Discovery", "Abstract": "In this paper we describe the STAHLp system for inferring components of chemical substances - i.e., constructing componential models. STAHLp is a descendant of the STAHLp system (Zytkow and Simon, 1986); both use chemical reactions and any known models in order to construct new models. However, STAHLp employs a more unified and effective strategy for preventing, detecting, and recovering from erroneous inferences. This strategy is based partly upon the assumption-based method (de Kleer, 1984) of recording the source beliefs, or premises, which lead to each inferred belief (i.e., reaction or model). STAHL’s multiple methods for detecting and recovering from erroneous inferences have been reduced to one method in STAHLp, which can hypothesize faulty premises, revise them, and proceed to construct new models. The hypotheses made during belief revision can be viewed as interpretations from competing theories; how they are chosen thus determines how theories evolve after repeated revisions. We analyze this issue with an example involving the shift from phlogiston to oxygen theory."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "Not the Path to Perdition", "Title": "The Utility of Similarity-Based Learning", "Abstract": "A large portion of the research in machine learning has involved a paradigm of comparing many examples and analyzing them in terms of similarities and differences, assuming that the resulting generalizations will have applicability to new examples. While such research has been very successful, it is by no means obvious why similarity-based generalizations should be useful, since they may simply reflect coincidences. Proponents of explanation-based learning, a new, knowledge-intensive method of examining single examples to derive generalizations based on underlying causal models, could contend that their methods are more fundamentally grounded, and that there is no need to look for similarities across examples. In this paper, we present the issues, and then show why similarity-based methods are important. We present four reasons why robust machine learning must involve the integration of similarity-based and explanation-based methods. We argue that: 1) it may not always be practical or even possible to determine a causal explanation; 2) similarity usually implies causality; 3) similarity-based generalizations can be refined over time; 4) similarity-based and explanation-based methods complement each other in important ways."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "The FERMI System", "Title": "Inducing Iterative Macro-Operators from Experience", "Abstract": "Automated methods of exploiting past experience to reduce search vary from analogical transfer to chunking control knowledge. In the latter category, various forms of composing problem-solving operators into larger units have been explored. However, the automated formulation of effective macro-operators requires more than the storage and parametrization of individual linear operator sequences. This paper addresses the issue of acquiring conditional and iterative operators, presenting a concrete example implemented in the FERMI problem-solving system. In essence, the process combines empirical recognition of cyclic patterns in the problem-solving trace with analytic validation and subsequent formulation of general iterative rules. Such rules can prove extremely effective in reducing search beyond linear macro-operators produced by past techniques."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "Plausibility of Diagnostic Hypotheses", "Title": "The Nature of Simplicity", "Abstract": "In general diagnostic problems multiple disorders can occur simultaneously. AI systems have traditionally handled the potential combinatorial explosion of possible hypotheses in such problems by focusing attention on a few \"most plausible\" ones. This raises the issue of establishing what makes one hypothesis more plausible than others. Typically a hypothesis (a set of disorders) must not only account for the given manifestations, but it must also satisfy some notion of simplicity (or coherency, or parsimony, etc) to be considered. While various criteria for simplicity have been proposed in the past, these have been based on intuitive and subjective grounds. In this paper, we address the issue of if and when several previously-proposed criteria of parsimony are reasonable in the sense that they are guaranteed to at least identify the most probable hypothesis. Hypothesis likelihood is calculated using a recent extension of Bayesian classification theory for multimembership classification in causal diagnostic domains. The significance of this result is that it is now possible to decide objectively a priori the appropriateness of different criteria for simplicity in developing an inference method for certain classes of general diagnostic problems."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "Doing Time", "Title": "Putting Qualitative Reasoning on Firmer Ground", "Abstract": "Recent work in qualitative reasoning has focused on predicting the dynamic behavior of continuous physical systems. Significant headway has been made in identifying the principles necessary to predict this class of behavior. However, the predictive inference engines based on these principles are limited in their ability to reason about time. This paper presents a general approach to behavioral prediction which overcomes many of these limitations. Generality results from a clean separation between principles relating to time, continuity, and qualitative representations. The resulting inference mechanism, based on propagation of constraints, is applicable to a wide class of physical systems exhibiting discrete or continuous behavior, and can be used with a variety of representations (e.g., digital, quantitative, qualitative or symbolic abstractions). In addition, it provides a framework in which to explore a broad range of tasks including prediction, explanation, diagnosis, and design."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "Joint and LPA*", "Title": "Combination of Approximation and Search", "Abstract": "This paper describes two new algorithms, Joint and LPA*, which can be used to solve difficult combinatorial problems heuristically. The algorithms find reasonably short solution paths and are very fast. The algorithms work in polynomial time in the length of the solution. The algorithms have been benchmarked on the 15-puzzle, whose generalization has recently been shown to be NP hard, and outperform other known methods within this context."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "CHEF", "Title": "A Model of Case-Based Planning", "Abstract": "Case-based planning is based on the idea that a machine planner should make use of its own past experience in developing new plans, relying on its memories instead of a base of rules. Memories of past successes are accessed and modified to create new plans. Memories of past failures are used to warn the planner of impending problems and memories of past repairs are called upon to tell the planner how to deal with them. Successful plans are stored in memory, indexed by the goals they satisfy and the problems they avoid. Failures are also stored, indexed by the features in the world that predict them. By storing failures as well as successes, the planner is able to anticipate and avoid future plan failures. These ideas of memory, learning and planning are implemented in the case-based planner CHEF, which creates new plans in the domain of Szechwan cooking."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "Imposing Structure on Linear Programming Problems", "Title": "An Empirical Analysis of Expert and Novice Models", "Abstract": "Research on expert-novice differences falls into two complementary classes. The first assumes that novice skills are a subset of those of the expert, represented by the same vocabulary of concepts. The second approach emphasizes novices’ misconceptions and the different meanings they tend to attribute to concepts. Our evidence, based on observations of problem solving behavior of experts and novices in the area of mathematical programming, reveals both type of differences: while novices are to some extent underdeveloped experts, they also attribute different meanings to concepts. The research suggests that experts’ concepts can be characterized as being more differentiated than those of novices, where the differentiation enables experts to categorize problem descriptions accurately into standard archetypes and facilitates attribution of correct meanings to problem features. Our results are based on twenty-five protocols obtained from experts and novices attempting to structure problem descriptions into mathematical programming models. We have developed a model of knowledge in the LP domain that accommodates a continuum of expertise ranging from that of the expert who has a highly specialized vocabulary of LP concepts to that of a novice whose vocabulary might be limited to high school algebra. We discuss the normative implications of this model for pedagogical strategies employed by instructors, textbooks and intelligent tutoring systems."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "A Representation for Temporal Sequence and Duration in Massively Parallel Networks", "Title": "Exploiting Link Interactions", "Abstract": "One of the major representational problems in massively parallel or connectionist models is the difficulty of representing temporal constraints. Temporal constraints are important and crucial sources of information for event perception in general. This paper describes a novel scheme which provides massively parallel models with the ability to represent and recognize temporal constraints such as sequence and duration by exploiting link to link interactions. This relatively unexplored yet powerful mechanism is used to represent rule-like constraints and behaviors. The temporal sequence of a set of nodes is defined as the constraints or the temporal context, in which these nodes should be activated. This representation is quite robust in the sense that it captures subtleties in both the strength and scope (order) of temporal constraints. Duration is also represented using a similar mechanism. The duration of a concept is represented as a memory trace of the activation of this concept. The state of this trace can be used to generate a fuzzy set like classification of the duration."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "Chronological Ignorance", "Title": "Time, Nonmonotonicity, Necessity and Causal Theories", "Abstract": "Concerned with the problem of reasoning efficiently about change within a formal system, we identify the initiation problem. The solution to it which we offer, called the logic of chronological ignorance, combines temporal logic, nonmonotonic logic, and the modal logic of necessity. We identify a class of theories, called causal theories, which have elegant model-theoretic and complexity properties in the new logic."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "Pointwise Circumscription", "Title": "Preliminary Report", "Abstract": "Circumscription is the minimization of predicates subject to restrictions expressed by predicate formulas. We propose a modified notion of circumscription so that, instead of being a single minimality condition, it becomes an \"infinite conjunction\" of \"local\" minimality conditions; each of these conditions expresses the impossibility of changing the value of a predicate from true to false at one point. We argue that this \"pointwise\" circumscription is conceptually simpler than the traditional \"global\" approach and, at the same time, leads to generalizations with the additional flexibility needed in applications to the theory of commonsense reasoning."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "Time Representation", "Title": "A Taxonomy of Internal Relations", "Abstract": "James Allen in [All2] formulated a calculus of convex time intervals, which is being applied to commonsense reasoning by Allen, Pat Hayes, Henry Kautz and others [AllKau, AllHay]. For many purposes in AI, we need more general time intervals. We present a taxonomy of important binary relations between intervals which are unions of convex intervals, and we provide examples of these relations applied to the description of tasks and events. These relations appear to be necessary for such description. Finally, we provide logical definitions of a taxonomy of general binary relations between non-convex intervals."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "Dual Frames", "Title": "A New Tool for Semantic Parsing", "Abstract": "The dual frames method is a new tool for specifying and establishing semantic dependencies, which has been implemented in a parser of French called SABA. This method offers solutions to some typical problems of semantic parsing strategies - such as the difficulty of coping with different types of sentence structures and the amount of work needed to specify the vocabulary of a new domain - by providing a general and flexible tool which can handle all the kinds of meaningful terms which can appear in a sentence."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "Beyond Exploratory Programming", "Title": "A Methodology and Environment for Conceptual Natural Language Processing", "Abstract": "This paper presents an attempt to synthesize a methodology and environment which has features both of traditional software development methodologies and exploratory programming environments. The environment aids the development of conceptual natural language analyzers, a problem area where neither of these approaches alone adequately supports the construction of modifiable and maintainable systems. The paper describes problems with traditional approaches, the new \"parallel\" development methodology, and its supporting environment, called the PLUMber’s Apprentice."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "Tactile Recognition by Probing", "Title": "Identifying a Polygon on a Plane", "Abstract": "An outstanding problem in model-based recognition of objects by robot systems is how the system should proceed when the acquired data are insufficient to identify uniquely the model instance and model pose that best interpret the object. In this paper, we consider the situation in which some tactile data about the object are already available, but can be ambiguously interpreted. The problem is thus to acquire and process new tactile data in a sequential and eflicient manner, so that the object can be recognised and its location and orientation determined. An object model, in this initial analysis of the problem, is a polygon located on a plane; the case of planar objects presents some interesting problems, and is also an important prelude to recognition of three-dimensional (polyhedral) objects."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "Shape from Darkness", "Title": "Deriving Surface Information from Dynamic Shadows", "Abstract": "We present a new method, shape from darkness, for extracting surface shape information based on object self-shadowing under moving light sources. It is motivated by the problem of human perception of fractal textures under perspective. One-dimensional dynamic shadows are analyzed in the continuous case, and their behavior is categorized into three exhaustive shadow classes. The continuous problem is shown to be solved by the integration of ordinary differential equations, using information captured in a new image representation called the suntrace. The discretization of the one-dimensional problem introduces uncertainty in the discrete suntrace; however it is successfully recast as the satisfaction of 8n constraint equations in 2n unknowns. A form of relaxation appears to quickly converge these constraints to accurate surface reconstructions; we give several examples on simulated images. The shape from darkness method has two advantages: it does not require a reflectance map, and it works on non-smooth surfaces. We conclude with a discussion on the method’s accuracy and practicality, its relation to human perception, and its future extensions."}
{"Type": "conference", "Year": "1986", "Area": "AI", "Where": "AAAI", "Abbreviation": "Parts", "Title": "Structured Descriptions of Shape", "Abstract": "A shape representation is presented that has been shown competent to accurately describe an extensive variety of natural forms (e g., people, mountains, clouds, trees), as well as man-made forms, in a succinct and natural manner. The approach taken in this representational system is to describe scene structure at a scale that is similar to our naive perceptual notion of \"a part,' by use of descriptions that reflect a possible formative history of the object, e.g., how the object might have been constructed from lumps of clay. For this representation to be useful it must be possible to recover such descriptions from image data; we show that the primitive elements of this representation may be recovered in an overconstrained and therefore potentially reliable manner."}
