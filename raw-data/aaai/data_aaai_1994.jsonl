{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "ModGen", "Title": "Theorem Proving by Model Generation", "Abstract": "ModGen (Model Generation) is a complete theorem prover for first order logic with finite Herbrand domains. ModGen takes first order formulas as input, and generates models of the input formulas. ModGen consists of two major modules: a module for transforming the input formulas into propositional clauses, and a module to find models of the propositional clauses. The first module can be used by other researchers so that the SAT problems can be easily represented, stored and communicated. An important issue in the design of ModGen is to ensure that transformed propositional clauses are satisfiable iff the original formulas are. The second module can be easily replaced by any advanced SAT problem solver. ModGen is easy to use and very efficient. Many problems which are hard for general resolution theorem provers are found easy for ModGen."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Small is Beautiful", "Title": "A Brute-Force Approach to Learning First-Order Formulas", "Abstract": "We describe a method for learning formulas in first-order logic using a brute-force, smallest-first search. The method is exceedingly simple. It generates all irreducible well-formed formulas up to a fixed size and tests them against a set of examples. Although the method has some obvious limitations due to its computational complexity, it performs surprisingly well on some tasks. This paper describes experiments with two applications of the method in the MULTI-TAC sys- tem, a program synthesizer for constraint satisfaction problems. In the first application, axioms are learned, and in the second application, search control rules are learned. We describe these experiments, and consider why searching the space of small formulas makes sense in our applications."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Refining the Structure of Terminological Systems", "Title": "Terminology = Schema + Views", "Abstract": "Traditionally, the core of a Terminological Knowledge Representation System (TKRS) consists of a so-called TBox, where concepts are introduced, and an ABox, where facts about individuals are stated in terms of these concepts. This design has a drawback because in most applications the TBox has to meet two functions at a time: on the one hand, similar to a database schema, framelike structures with typing information are introduced through primitive concepts and primitive roles; on the other hand, views on the objects in the knowledge base are provided through defined concepts. We propose to account for this conceptual separation by partitioning the TBox into two components for primitive and defined concepts, which we call the schema and the view part. We envision the two parts to differ with respect to the language for concepts, the statements allowed, and the semantics. We argue that by this separation we achieve more conceptual clarity about the role of primitive and defined concepts and the semantics of terminological cycles. Moreover, three case studies show the computational benefits to be gained from the refined architecture."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Talking About AI", "Title": "Socially Defined Linguistic Subcontexts in AI", "Abstract": "This paper describes experiments documenting significant variations in word usage patterns within social subgroups of AI researchers. As some phrases have very different collocational patterns than their constituent words, we look beyond occurrences of individual words, to consider word phrases. The mutual information statistic is used to measure the information content of phrases beyond that of their constituent words. Previous research has shown that some phrases are much more informative as word pairs outside topicadly defined subsets of a document corpus than within it. In this paper we show that individual universities provide an analogous, socicslly defined context in which locally-used phrases are \"exported\" into general AI vocabulary."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Learning to Learn", "Title": "Automatic Adaptation of Learning Bias", "Abstract": "Traditionally, large areas of research in machine learning have concentrated on pattern recognition and its application to many diversified problems both within the realm of AI as well as outside of it. Over several decades of intensified research, an array of learning methodologies have been proposed, accompanied by attempts to evaluate these methods, with respect to one another on small sets of real world problems. Unfortunately, little emphasis was placed on the problem of learning bias - common to all learning algorithms - and a major culprit in preventing the construction of a zsniuerscsl pattern recognizer. State of the art learning algorithms exploit some inherent bias when performing pattern recognition on yet unseen patterns. Automatically adapting this learning bias - dependent on the type of pattern classification problems seen over time - is largely lacking. In this paper, weaknesses of the traditional one-shot learning environments are pointed out and the move towards a learning method displaying the ability to learn about lecarning is undertaken. Trans-dimensional learning is introduced as a means to automatically adjust learning bias and empirical evidence is provided showing that in some instances learning the whole can be simpler than learning a part of it."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Epsilon-Transformation", "Title": "Exploiting Phase Transitions to Solve Combinatorial Optimization Problems Initial Results", "Abstract": "It has been shown that there exists a transition in the average-case complexity of searching a random tree, from exponential to polynomial in the search depth. We develop a state-space transformation method, called e-transformation, that makes use of this complexity transition to find a suboptimal solution. The expected number of random tree nodes expanded by branch-and-bound (BnB) using e-transformation is cubic in the search depth, and the relative error of the solution cost compared to the optimal solution cost is bounded by a small constant. We also present an iterative version of e-transformation that can be used to find both optimal and suboptimal solutions. Depth-first BnB (DFBnB) using iterative e-transformation significantly improves upon truncated DFBnB on random trees with large branching factors and deep goal nodes, finding better solutions sooner on average. On the asymmetric traveling salesman problem, DFBnB using e-transformation outperforms a well-known local search method, and DFBnB using iterative e-transformation is superior to truncated DFBnB."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Unclear Distinctions Lead to Unnecessary Shortcomings", "Title": "Examining the Rule Versus Fact, Role versus Filler, and Type Versus Predicate Distinctions from a Connectionist Representation and Reasoning Perspective", "Abstract": "This paper deals with three distinctions pertaining to knowledge representation, namely, the rules vs facts distinction, roles vs fillers distinction, and predicates vs types distinction. Though these distinctions may indeed have some intuitive appeal, the exact natures of these distinctions are not entirely clear. This paper discusses some of the problems that arise when one accords these distinctions a prominent status in a connectionist system by choosing the representational structures so as to reflect these distinctions. The example we will look at in this paper is the connectionist reasoning system developed by Ajjanagadde and Shastri. Their system performs an interesting class of inferences using activation synchrony to represent dynamic bindings. The rule/fact, role/filler, type/predicate distinctions figure predominantly in the way knowledge is encoded in their system. We will discuss some significant shortcomings this leads to. Then, we will propose a much more uniform scheme for representing knowledge. The resulting system enjoys some significant advantages over Ajjanagadde and Shastri’s system, while retaining the idea of using synchrony to represent bindings."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Sensible Decisions", "Title": "Toward a Theory of Decision-Theoretic Information Invariants", "Abstract": "We propose a decision-theoretic notion of invariance in bounded rational decision making. We show how optimal decision making in sensory robotics can be approximately preserved under transformations of the decision rule. In particular, we present a decision theoretic analysis of the use of visual routines in action arbitration in real-time robot soccer. In this domain, stochastic dominance, and therefore decisions, can be sensed approximately from the environment, and we exploit this in our decision making."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Basic Meanings of Spatial Relations", "Title": "Computation and Evaluation in 3D Space", "Abstract": "Spatial relations play an important role in the research area of connecting visual and verbal space. In the last decade several approaches to semantics and computation of spatial relations in 2D space have been developed. Presented here is a new approach to the computation and evaluation of basic spatial relations’ meanings in 3D space. We propose the use of various kinds of approximations when defining the basic semantics. The vagueness of the applicability of a spatial relation is accounted for by a flexible evaluation component which enables a cognitively plausible continuous gradation. For validating the evolved methods we have integrated them into a workbench. This workbench allows us to investigate the structure of a spatial relation’s applicability region through various visualization methods."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Learning by Observation and Practice", "Title": "A Framework for Automatic Acquisition of Planning Operators", "Abstract": "The knowledge engineering bottleneck is a central problem in the field of Artificial Intelligence. This work addresses this problem in the context of planning systems. It automatically learns planning operators by observing expert agents and by subsequent knowledge refinement in a learning-by-doing paradigm. Our learning method is implemented on top of the PRODIGY architecture(Carbonell et al. 1992)."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Computer Simulation of Statistics and Educational Measurement StatSim", "Title": "An Intelligent Tutoring System for Statistics", "Abstract": "The purpose of this research is to develop an adaptive tutoring system which uses AI techniques to explore how to diagnose student’s misconceptions in problem solving and generate relevant instructions from the context. The system is called StatSim."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Model-Based Sensor Diagnosis", "Title": "When Monitoring Should be Monitored", "Abstract": "A complex industrial plant, such as a nuclear power plant, is monitored thanks to a number of sensors. The instrumentation may be itself a complex system liable to failures. We propose a model-based sensor diagnosis system which relies on the topological description of the plant and on a set of component models. This model implicitly conceals relations involving only sensor data. Such relations must always be verified if components behave normally; thus, the detection task consists of verifying these relations. So, this work is a first step in extending the scope of model-based diagnosis, since we question here the information stemming from the plant and normally considered as safe. As further studies, we wish to monitor this detection system itself; i.e., whenever the instrumentation is supposed to behave correctly, non-verified constraints point out to errors in the plant model."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Integrating Induction & Instruction", "Title": "Connectionist Advice Taking", "Abstract": "Humans improve their performance by means of a variety of learning strategies, including both gradual statistical induction from experience and rapid incorporation of advice. In many learning environments, these strategies may interact in complementary ways. The focus of this work is on cognitively plausible models of multistrategy learning involving the integration of inductive generalization and learning \"by being told.\" Such models might be developed by starting with an architecture for which advice taking is relatively easy, such as one based upon a sentential knowledge representation, and subsequently adding some form of inductive learning mechanism. Alternatively, such models might be grounded in a statistical learning framework appropriately extended to operationalize instruction. This latter approach is taken here. Specifically, connectionist back-propagation networks are made to instantaneously modify their behavior in response to quasi-linguistic advice."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Making the Most of What You’ve Got", "Title": "Using Models and Data to Improve Learning Rate and Prediction Accuracy", "Abstract": "Prediction and classification in areas such as engineering, medicine, and applied expert systems often relies on two sources of knowledge: actual data and a model of the domain. Recent efforts in machine learning have developed techniques that take advantage of both sources, but the methods are often tied to particular types of models and induction techniques. We propose two general techniques that allow induction methods, C4.5 in our case, to take advantage of an available model."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Dempster-Shafer and Bayesian Networks for CAD-Based Feature Extraction", "Title": "A Comparative Investigation and Analysis", "Abstract": "Information pertaining to real world problems often contains noises and uncertainties. This has been a major challenge faced by the contemporary AI researchers. Of various paradigms developed for handing uncertainties, the Dempster-Shafer theory (DS) and the Bayesian Belief Networks (BBN) have received considerable attention in the AI community recently. They have been successfully applied to problems in medical diagnosis, decision-making, image understanding, machine vision, etc.. Despite their obvious success, blindly using them without understanding their limitations may result in computational difficulty and unsatisfying inference results. The aim of this paper is to analyze and compare the performance of the two paradigms in extracting manufacturing features from the solid model descriptions of objects. Such a comparison will serve to identify their strengths, weakness, and appropriate application domains."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "When the Best Move Isn’t Optimal", "Title": "Q-learning with Exploration", "Abstract": "The most popular delayed reinforcement learning technique, Q-learning (Watkins 1989)) estimates the future reward expected from executing each action in every state. If these estimates are correct, then an agent can use them to select the action with maximal expected future reward in each state, and thus perform optimally. Watkins has proved that Q-learning produces an optimal policy (the function mapping states to actions) and that these estimates converge to the correct values given the optimal policy."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "HIPAIR", "Title": "Interactive Mechanism Analysis and Design Using Configuration Spaces", "Abstract": "We present an interactive problem solving environment for reasoning about shape and motion in mechanism design. Reasoning about shape and motion plays a central role in mechanism design because mechanisms perform functions by transforming motions via part interactions. The input motion, the part shapes, and the part contacts determine the output motion. Designers must reason about the interplay between shape and motion at every step of the design cycle."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Exploiting the Environment", "Title": "Urban Navigation as a Case Study", "Abstract": "The Situated Action approach to AI emphasizes the role of the environment in the generation and control of behavior; see (Norman 1993) for an introduction. Work to date has focused mainly on activity within spatially and temporally localized environments such as kitchens and video games (Agre and Chapman 1987; Agre and Horswill 1992). How useful is this perspective when larger-scale activities are considered? I attempt to answer this question by considering some issues related to navigation in urban environments. identify several constraints on the structure of street grids that make navigation much easier than arbitrary graph search. The ultimate goal is a theory of the relationship between features of an urban environment and the computational complexity of navigation. This work extends the sort of analysis advocated by (Agre and Horswill 1992; Horswill 1993)."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "GKR", "Title": "A Generic Model of Knowledge Representation", "Abstract": "We show in this paper a proposal for a new generic \nmodel of KR called GKR (Generic Knowledge Representation). This model has been developed as a result of the \nanalysis of the models described in the paper. The study of the successes and shortcomings of these \nmodels helped us to define GKR with several properties \nthat improve its representation ability."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Exploiting the Ordering of Observed Problem-Solving Steps for Knowledge Base Refinement", "Title": "An Apprenticeship Approach", "Abstract": "Apprenticeship is a powerful method of learning among humans in which a student refines his knowledge by observing and analyzing the problem-solving steps of an expert. In this paper we focus on knowledge base (KB) refinement for classification problems and examine how the ordering of the intermediate steps of an observed expert can be used to yield leverage in KB refinement. In the classical classification problem, the problem-solver is given an example consisting of a set of attributes and their corresponding values, and it must put the example in one of a pre-enumerated set of classes."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "The KM / KnEd System", "Title": "An Integrated Approach to Building Large-Scale Multifunctional Knowledge Bases", "Abstract": "In 1987, Dr. Bruce Porter began work at the University of Texas at Austin on the Botany Knowledge Base Project. The goal of the project is to develop a largescale multi-functional knowledge base in the area of Botany. This Botany Knowledge Base (BKB) is used to support research projects in question answering, automated modeling, and intelligent tutoring. Due to the size and complexity of the BKB, a decision was made in 1990 to begin construction of a new knowledge representation language and interface to support the knowledge base. The knowledge representation language was named KM, for Knowledge Manager, and the interface was named KnEd, for Knowledge Editor. The KM/KnEd system is similar to Doug Lenat’s CYC project and Doug Skuce’s CODE4 system."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "DANIEL", "Title": "Integrating Case-Based and Rule-Based Reasoning in Law", "Abstract": "This paper introduces DANlEL, an architecture for the integration of case-based reasoning and rule-based reasoning for legal interpretation. Rather than interleaving the reasoners and assuming their complementarity, like in previous approaches, they are applied concurrently. Conflicting interpretations are handled explicitly, based on domain knowledge and on the notion of redundancy. The principal problems of legal interpretation are the lack of deep models for legal reasoning, the existence of inherently ill-defined predicates and the frequent use of open-textured concepts, as pointed out in (Rissland and Skalak 1991). A hybrid approach to representing the legal sources and the use of meta-knowledge seems to be appropriate to solve these problems. The scope of DANIEL is not limited to this particular domain, since the noted difficulties do not occur exclusively, but prototypically in the law."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "SodaBot", "Title": "A Software Agent Environment and Construction System", "Abstract": "Much of the work done in the area of software agents can be placed into one of two categories: (1) highly theoretical treatment of agents’ intentions and capabilities; and (2) applied construction of specific agents. However, determining for what (and if) software agents are actually useful requires building many of them, and the agent construction process poses difficult technical challenges."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Guardian", "Title": "A Prototype Intelligent Agent for Intensive-Care Monitoring", "Abstract": "A surgical intensive care unit (ICU) is a challenging monitoring environment. The multitude of monitored variables, the high frequency of alarms, and the severity of likely complications and emergencies can overload the cognitive skills of even experienced clinicians. ICU monitoring is also complicated by changes in clinical context. Over the course of a few days, a patient may evolve from a high-vigilance immediate post-operative state to a convalescent state that involves entirely different sets of monitoring principles, problems, and treatments."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "HIPAIR", "Title": "Interactive Mechanism Analysis and Design Using Configuration Spaces", "Abstract": "We present an interactive problem solving environment for reasoning about shape and motion in mechanism design. Reasoning about shape and motion plays a central role in mechanism design because mechanisms perform functions by transforming motions via part interactions. The input motion, the part shapes, and the part contacts determine the output motion. Designers must reason about the interplay between shape and motion at every step of the design cycle."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "ALIVE", "Title": "Artificial Life Interactive Video Environment", "Abstract": "In this video we demonstrate a novel system which allows wireless full-body interaction between a 'human participant and a graphical world inhabited by autonomous agents. The system is called \"ALIVE\", an acronym for Artificial Life Interactive Video Environment. The goal of ALIVE is to present a virtual environment in which a user can interact, in natural and believable ways, with autonomous semi-intelligent agents whose behavior is equally natural and believable."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "A Reading Coach that Listens", "Title": "(Edited Video Transcript)", "Abstract": "At Carnegie Mellon University, Project LISTEN' is t'aking a novel approach to the problem of illiteracy. We have developed a prototype automated reading coach that listens to a child read aloud, and helps when needed. The coach provides a combination of reading and listening, in which the child reads wherever possible, and the coach helps wherever necessary -- a bit like training wheels on a bicycle."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Causal Default Reasoning", "Title": "Principles and Algorithms", "Abstract": "The minimal model semantics is a natural interpretation of defaults yet it often yields a behavior that is too weak. This weakness has been traced to the inabiity of minimal models to reflect certain implicit preferences among defaults, in particular, preferences for defaults grounded on more ’specific' information and preferences arising in causal domains. Recently, ’specificity' preferences have been explained in terms of conditionals. Here we aim to explain causal preferences. We draw mainly from ideas known in Bayesian Networks to formulate and formalize two principles that explain the basic preferences that arise in causal default reasoning. We then define a semantics based on those principles and show how variations of the algorithms used for inheritance reasoning and temporal projection can be used to compute in the resulting formalism."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Focusing on the Most Important Explanations", "Title": "Decision-Theoretic Horn Abduction", "Abstract": "This paper describes a new method, called Decision-Theoretic Horn Abduction (DTHA), for generating and focusing on the most important explanations. A procedure is given that can be used iteratively to generate a sequence of explanations from the most to the least important. The new method considers both the likelihood and utility of partial explanations and is applica ble to a wide range of tasks. This paper shows how it applies to an important engineering design task, namely Failure Modes and Effects Analysis (FMEA). A concrete example illustrates the advantages of the general approach in the context of FMEA."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "GENET", "Title": "A Connectionist Architecture for Solving Constraint Satisfaction Problems by Iterative Improvement", "Abstract": "New approaches to solving constraint satisfaction problems using iterative improvement techniques have been found to be successful on certain, very large problems such as the million queens. However, on highly constrained problems it is possible for these methods to get caught in local minima. In this paper we present GENET, a connectionist architecture for solving binary and general constraint satisfaction problems by iterative improvement. GENET incorporates a learning strategy to escape from local minima. Although GENET has been designed to be implemented on VLSI hardware, we present empirical evidence to show that even when simulated on a single processor GENET can outperfomr existing iterative improvement techniques on hard instances of certain constraint satisfaction problems."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Reasoning about Temporal Relations", "Title": "A Maximal Tractable Subclass of Allen’s Interval Algebra", "Abstract": "We introduce a new subclass of Allen’s interval algebra we call \"ORD-Horn subclass,\" which is a strict superset of the \"pointisable subclass.\" We prove that reasoning in the ORD-Horn subclass is a polynomial-time problem and show that the path-consistency method is sufficient for deciding satisfiability. Further, using an extensive machine-generated case analysis, we show that the ORD-Horn subclass is a maximal tractable subclass of the full algebra. In fact, it is the unique greatest tractable subclass amongst the subclasses that contain all basic relations."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "The Impact of Locality and Authority on Emergent Conventions", "Title": "Initial Observations", "Abstract": "In the design of systems of multiple agents, we must deal with the potential for conflict that is inherent in the interactions among agents; to ensure efficient operation, these interactions must be coordinated. We extend, in two related ways, an existing framework that allows behavioral conventions to emerge in agent societies. We first consider localizing agents, thus limiting their interactions. We then consider giving some agents authority over others by implementing asymmetric interactions. Our primary interest is to explore how locality and authority affect the emergence of conventions. Through computer simulations of agent societies of various configurations, we begin to develop an intuition about what features of a society promote or inhibit the spontaneous generation of coordinating conventions."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Coalition, Cryptography, and Stability", "Title": "Mechanisms for Coalition Formation in Task OrientedDomains", "Abstract": "Negotiation among multiple agents remains an important topic of research in Distributed Artificial Intelligence (DAI). Most previous work on this subject, however, has focused on bilateral negotiation, deals that are reached between two agents. There has also been research on n-agent agreement which has considered \"consensus mechanisms\" (such as voting), that allow the full group to coordinate itself. These group decision-making techniques, however, assume that the entire group will (or has to) coordinate its actions. Sub-groups cannot make sub-agreements that exclude other members of the group. In some domains, however, it may be possible for beneficial agreements to be reached among sub-groups of agents, who might be individually motivated to work together to the exclusion of others outside the group. This paper considers this more general case of n-agent coalition formation. We present a simple coalition formation mechanism that uses cryptographic techniques for subadditive Task Oriented Domains. The mechanism is efficient, symmetric, and individual rational. When the domain is also concave, the mechanism also satisfies coalition rationality."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Flexible Strategy Learning", "Title": "Analogical Replay of Problem Solving Episodes", "Abstract": "This paper describes the integration of analogical reasoning into general problem solving as a method of learning at the strategy level to solve problems more effectively. Learning occurs by the generation and replay of annotated derivational traces of problem solving episodes. The problem solver is extended with the ability to examine its decision cycle and accumulate knowledge from the chains of successes and failures encountered during its search experience. Instead of investing substantial effort deriving general rules of behavior to apply to individual decisions, the analogical reasoner compiles complete problem solving cases that are used to guide future similar situations. Learned knowledge is flexibly applied to new problem solving situations even if only a partial match exists among problems. We relate this work with other alternative strategy learning methods, and also with plan reuse. We demonstrate the effectiveness of the analogical replay strategy by providing empirical results on the performance of a fully implemented system, PRODIGY/ANALOGY, accumulating and reusing a large case library in a complex problem solving domain."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Exploiting the Ordering of Observed Problem-Solving Steps for Knowledge Base Refinement", "Title": "An Apprenticeship Approach", "Abstract": "Apprenticeship is a powerful method of learning among humans whereby a student refines his knowledge simply by observing and analyzing the problem-solving steps taken by an expert. This paper focuses on knowledge base (KB) refinement for classification problems and examines how the ordering of the problem-solving steps taken by an observed expert can be used to yield leverage in KB refinement. Questions examined include: What added information can be extracted from attribute ordering? How can this added information be utilized to identify and repair KB shortcomings? What assumptions must be made about the observed expert, and how important of a role do these assumptions play? The principles explored have been implemented in the SKIPPER apprentice, and empirical results are given for the audiology domain."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Bottom-Up Induction of Oblivious Read-Once Decision Graphs", "Title": "Strengths and Limitations", "Abstract": "We report improvements to HOODG, a supervised learning algorithm that induces concepts from labelled instances using oblivious, read-once decision graphs as the underlying hypothesis representation structure. While it is shown that the greedy approach to variable ordering is locally optimal, we also show an inherent limitation of all bottom-up induction algorithms, including HOODG, that construct such decision graphs bottom-up by minimizing the width of levels in the resulting graph. We report our empirical experiments that demonstrate the algorithm’s generalization power."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Decision Tree Pruning", "Title": "Biased or Optimal?", "Abstract": "We evaluate the performance of weakest-link pruning of decision trees using cross-validation. This technique maps tree pruning into a problem of tree selection: Find the best (i.e. the right-sized) tree, from a set of trees ranging in size from the unpruned tree to a null tree. For samples with at least 200 cases, extensive empirical evidence supports the following conclusions relative to tree selection: (a) 10-fold cross-validation is nearly unbiased; (b) not pruning a covering tree is highly biased; (c) 10-fold cross-validation is consistent with optimal tree selection for large sample sizes and (d) the accuracy of tree selection by 10-fold cross-validation is largely dependent on sample size, irrespective of the population distribution."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Bootstrapping Training-Data Representations for Inductive Learning", "Title": "A Case Study in Molecular Biology", "Abstract": "This paper describes a \"bootstrapping\" approach to the engineering of appropriate training-data representations for inductive learning. The central idea is to begin with an initial set of human-created features and then generate additional features that have syntactic forms that are similar to the human-engineered features. More specifically, we describe a two-stage process for the engineering of good representations for learning: first, generating by hand (usually in consultation with domain experts) an initial set of features that seem to help learning, and second, \"bootstrapping\" off of these features by developing and applying operators that generate new features that look syntactically like the expert-based features. Our experiments in the domain of DNA sequence identification show that an initial successful human-engineered representation for data can be expanded in this fashion to yield dramatically improved results for learning."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Catching a Baseball", "Title": "A Reinforcement Learning Perspective Using a Neural Network", "Abstract": "Moments after a baseball batter has hit a fly ball, an outfielder has to decide whether to run forward or backward to catch the ball. Judging a fly ball is a difficult task, especially when the fielder is in the plane of the ball' s trajectory. There exists several alternative hypotheses in the literature which identify different perceptual features available to the fielder that may provide useful cues as to the location of the ball’s landing point. A recent study in experimental psychology suggests that to intercept the ball, the fielder has to run such that the double derivative of tanf with respect to time is close to zero, where f is the elevation angle of the ball from the fielder’s perspective (McLeod and Dlenes 1993). We investigate whether d2 (tanf)/dt2 information is a useful cue to learn this task in the Adaptive Heuristic Critic (AHC) reinforcement learning framework. Our results provide supporting evidence that d2(tanf)/dt2 information furnishes strong initial cue in determining the landing point of the ball and plays a key role in the learning process. However our simulations show that during later stages of the ball’s flight, yet another perceptual feature, the perpendicular velocity of the ball (vp) with respect to the fielder, provides stronger cues as to the location of the landing point. The trained network generalized to novel circumstances and also exhibited some of the behaviors recorded by experimental psychologists on human data. We believe that much can be gained by using reinforcement learning approaches to learn common physical tasks, and similarly motivated work could stimulate useful interdisciplinary research on the subject."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Inducing Deterministic Prolog Parsers from Treebanks", "Title": "A Machine Learning Approach", "Abstract": "This paper presents a method for constructing deterministic Prolog parsers from corpora of parsed sentences. Our approach uses recent machine learning methods for inducing Prolog rules from examples (inductive logic programming). We discuss several advantages of this method compared to recent statistical methods and present results on learning complete parsers from portions of the ATIS corpus."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Visual Semantics", "Title": "Extracting Visual Information from Text Accompanying Pictures", "Abstract": "This research explores the interaction of textual and photographic information in document understanding. The problem of performing general-purpose vision without a priori knowledge is difficult at best. The use of collateral information in scene understanding has been explored in computer vision systems that use scene context in the task of object identification. The work described here extends this notion by defining visual semantics, a theory of systematically extracting picture-specific information from text accompanying a photograph. Specifically, this paper discusses the multi-stage processing of textual captions with the following objectives: (i) predicting which objects (implicitly or explicitly mentioned in the caption) are present in the picture and (ii) generating constraints useful in locating/identifying these objects. The implementation and use of a lexicon specifically designed for the integration of linguistic and visual information is discussed. Finally, the research described here has been successfully incorporated into PICTION, a caption-based face identification system."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Emergent Linguistic Rules from Inducing Decision Trees", "Title": "Disambiguating Discourse Clue Words", "Abstract": "We apply decision tree induction to the problem of discourse clue word sense disambiguation. The automatic partitioning of the training set which is intrinsic to decision tree induction gives rise to linguistically viable rules."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "L* Parsing", "Title": "A General Framework for Syntactic Analysis of Natural Language", "Abstract": "We describe a new algorithm for table-driven parsing with context-free grammars designed to support efficient syntactic analysis of natural language. The algorithm provides a general framework in which a variety of parser control strategies can be freely specified: bottom-up strategies, top-down strategies, and strategies that strike a balance between the two. The framework permits better sharing of parse forest substructure than other table-driven approaches, and facilitates the early termination of semantically ill-formed partial parses. The algorithm should thus find ready application to large-scale natural language processing."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "A Preference-Based Approach toDefault Reasoning", "Title": "Preliminary Report", "Abstract": "An approach to nonmonotonic inference, based on preference orderings between possible worlds or states of affairs, is presented. We begin with an extant weak theory of default conditionals; using this theory, orderings on worlds are derived. The idea is that if a conditional such as \"birds fly\" is true then, all other things being equal, worlds in which birds fly are preferred over those where they don’t. In this case, a red bird would fly by virtue of red-bird-worlds being among the least exceptional worlds in which birds fly. In this approach, irrelevant properties are correctly handled, as is specificity, reasoning within exceptional circumstances, and inheritance reasoning. A sound proof-theoretic characterisation is also given. Lastly, the approach is shown to subsume that of conditional entailment."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Least-Cost Flaw Repair", "Title": "A Plan Refinement Strategy for Partial-Order Planning", "Abstract": "We describe the least-cost flaw repair (LCFR) strategy for performing flaw selection during partial-order causal link (POCL) planning. LCFR can be seen as a generalization of Peot and Smith’s \"Delay Unforced Threats\" (DUnf) strategy (Peot and Smith 1993); where DUnf treats threats differently from open conditions, LCFR has a uniform mechanism for handling all flaws. We provide experimental results that demonstrate that the power of DUnf does not come from delaying threat repairs per se, but rather from the fact that this delay has the effect of imposing a partial preference for least-cost flaw selection. Our experiments also show that extending this to a complete preference for least-cost selection reduces search-space size even further. We consider the computational overhead of employing LCFR, and discuss techniques for reducing this overhead. In particular, we describe QLCFR, a strategy that reduces computational overhead by approximating repair costs."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Omnipotence without Omniscience", "Title": "Efficient Sensor Management for Planning", "Abstract": "Classical planners have traditionally made the closed world assumption - facts absent from the planner' s world model are false. Incomplete-information planners make the open world assumption - the truth value of a fact absent from the planner’s model is unknown, and must be sensed. The open world assumption leads to two difficulties: (1) H ow can the planner determine the scope of a universally quantified goal? (2) When is a sensory action redundant, yielding information already known to the planner? This paper describes the fully-implemented XII planner, which solves both problems by representing and reasoning about local closed world information (LCW). We report on experiments utilizing our UNIX softbot (software robot) which demonstrate that LCW can substantially improve the softbot’s performance by eliminating redundant information gathering."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "A Constraint-Based Approach to High-School Timetabling Problems", "Title": "A Case Study", "Abstract": "This paper describes a case study on a general-purpose Constraint Relaxation Problem solver, COASTOOL. Using COASTOOL,~ problem can be solved merely by declaring \"what is the problem,\" without programming \"how to solve it.\" The problem is solved by a novel method that generates a high-quality initial assignment using arc-consistency, and refines it using hill-climbing. This approach has been evaluated successfully by experiments with practical high-school timetabling problems in Japan. Consequently, COASTOOL is shown to be efficient at applications in high-school timetabling problems."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "HTN Planning", "Title": "Complexity and Expressivity", "Abstract": "Most practical work on AI planning systems during the last fifteen years has been based on hierarchical task network (HTN) d ecomposition, but until now, there has been very little analytical work on the properties of HTN planners. This paper describes how the complexity of HTN planning varies with various conditions on the task networks."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "How Things Appear to Work", "Title": "Predicting Behaviors from Device Diagrams", "Abstract": "This paper introduces a problem solving task involving common sense reasoning that humans are adept at, but one which has not received much attention within the area of cognitive modeling until recently. This is the task of predicting the operation of simple mechanical devices, in terms of behaviors of their components, from labeled schematic diagrams showing the spatial configuration of components and a given initial condition. We describe this task, present a cognitive process model developed from task and protocol analyses, and illustrate it using the example of a pressure gauge. Then the architecture of a corresponding computer model and a control algorithm embodying the cognitive strategy are proposed."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Automated Modeling for Answering Prediction Questions", "Title": "Selecting the Time Scale and System Boundary", "Abstract": "The ability to answer prediction questions is crucial to reasoning about physical systems. A prediction question poses a hypothetical scenario and asks for the resulting behavior of variables of interest. Prediction questions can be answered by simulating a model of the scenario. An appropriate system boundary, which separates aspects of the scenario that must be modeled from those that can be ignored, is critical to achieving a simple yet adequate model. This paper presents an efficient algorithm for system boundary selection, it shows the important role played by the model’s time scale, and it provides a separate algorithm for selecting this time scale. Both algorithms have been implemented in a compositional modeling program called TRIPEL and evaluated in the plant physiology domain."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Activity Analysis", "Title": "The Qualitative Analysis of Stationary Points for Optimal Reasoning", "Abstract": "We present a. theory of a modeler’s problem decomposition skills in the context of optical reasoning - the use of qualitative modeling to strategically guide numerical explorations of objective space. Our technique, called activity analysis, applies to the pervasive family of linear and non-linear, constrained optimization problems, and easily integrates with any existing numerical approach. Activity analysis draws from the power of two seemingly divergent perspectives - the global conflict-based approaches of combinatorial satisficing search, and the local gradient-based approaches of continuous optimization - combined with the underlying insights of engineering monotonicity analysis. The result is an approach that strategically cuts away subspaces that it can quickly rule out as suboptimal, and then guides the numerical methods to the remaining subspaces."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Robot Behavior Conflicts", "Title": "Can Intelligence Be Modularized?", "Abstract": "In this paper, we examine the modularity assumption of behaviour-based models: that complex functionalities can be achieved by decomposition into simpler behaviours. In particular we look at the issue of conflicts among robot behaviour modules. The chief contribution of this work is a formal characterization of temporal cycles in behaviour systems and the development of an algorithm for detecting and avoiding such conflicts. We develop the mechanisms of stimulus specialization and response generalization for eliminating conflicts. The probable conflicts can be detected and eliminated before implementation. However the process of cycle elimination weakens the behaviour structure. We show how (a) removing conflicts results in less flexible and less useful behaviour modules and (b) the probability of conflict is greater for more powerful behaviour systems. We conclude that purely reactive systems are limited by cyclic behaviours in the complexity of tasks they can perform."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Teleassistance", "Title": "Contextual Guidance for Autonomous Manipulation", "Abstract": "We present teleassistance, a two-tiered control structure for robotic manipulation that combines the advantages of autonomy and teleoperation. At the top level, a teleoperator provides global, deictic references via a natural sign language. Each sign indicates the next action to perform and a relative and hand-centered coordinate frame in which to perform it. For example, the teleoperator may point to an object for reaching, or preshape the hand for grasping. At the lower level autonomous servo routines run within the reference frames provided. Teleassistance offers two benefits. First, the servo routines can po- sition the robot in relative coordinates and interprefeedback within a constrained context. This significantly simplifies the computational load of the autonomous routines and requires only a sparse model of the task. Second, the operator’s actions are symbolic, conveying intent without requiring the person to literally control the robot. This helps to alleviate many of the problems inherent to teleoperation, including poor mappings between operator and robot physiology, reliance on a broad communication bandwidth, and the potential for robot damage when solely under remote control. To demonstrate the concept, a Utah/MIT hand mounted on a Puma 760 arm opens a door."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Reactive Deliberation", "Title": "An Architecture for Real-Time Intelligent Control in Dynamic Environments", "Abstract": "Reactive deliberation is a novel robot architecture that has been designed to overcome some of the problems posed by dynamic robot environments. It is argued that the problem of action selection in nontrivial domains cannot be intelligently resolved without attention to detailed planning. Experimental evidence is provided that the goals and actions of a robot must be evaluated at a rate commensurate with changes in the environment. The goal-oriented behaviours of reactive deliberation are a useful abstraction that allow sharing of scarce computational resources and effective goal-arbitration through inter-behaviour bidding. The effectiveness of reactive deliberation has been demonstrated through a tournament of one-on-one soccer games between real-world robots. Soccer is a dynamic environment; the locations of the ball and the robots are constantly changing. The results suggest that the architectural elements in reactive deliberation are sufficient for real-time intelligent control in dynamic environments."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "The Trailblazer Search", "Title": "A New Method for Searching and Capturing Moving Targets", "Abstract": "This paper proposes a new search algorithm for targets that move. Ishida and Korf presented an algorithm, called the moving target search, that captures a target while deciding each search step in constant time (Ishida and Korf 1991). However, this algorithm requires many search steps to solve problems, if it uses a heuristic function that initially returns inaccurate values. The trailblazer search stores path information of the region it has searched and exploits this information when making decisions. The algorithm maintains a map of the searched region, and chases the target once it falls on a path found on the map. We empirically show that the algorithm’s map function can significantly reduce the number of search steps, compared with the moving target search. We also discuss the efficiency of the trailblazer search, taking the maintenance cost of the map into consideration."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "ITS", "Title": "An Efficient Limited-Memory Heuristic Tree Search Algorithm", "Abstract": "This paper describes a new admissible tree search algorithm called Iterative Threshold Search (ITS). ITS can be viewed as a much-simplified version of MA* [1], and a generalized version of MREC [12]. We also present the following results: 1. Every node generated by ITS is also generated by IDA*, even if ITS is given no more memory than IDA*. In addition, there are trees on which ITS generates O(N) nodes in comparison to O(N log N) nodes generated by IDA*, where N is the number of nodes eligible for generation by A*. 2. Experimental tests show that if the node-generation time is high (as in most practical problems), ITS can provide significant savings in both number of node generations and running time. Our experimental results also suggest that in the average case both IDA* and ITS are asymptotically optimal on the traveling salesman problem."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Best-First Minimax Search", "Title": "Othello Results", "Abstract": "We present a very simple selective search algorithm for two-player games. It always expands next the frontier node that determines the minimax value of the root. The algorithm requires no information other than a static evaluation function, and its time overhead per node is similar to that of alpha-beta minimax. We also present an implementation of the algorithm that reduces its space complexity from exponential to linear in the search depth, at the cost of increased time complexity. In the game of Othello, using the evaluation function from BiIl (Lee and Mahajan 1990), best-first minimax outplays alpha-beta at moderate depths. A hybrid best-first extension algorithm, which combines alpha-beta and best-first minimax, performs significantly better than either pure algorithm even at greater depths. Similar results were also obtained for a class of random game trees."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "ChatterBots, TinyMuds, and the Turing Test", "Title": "Entering the Loebner Prize Competition", "Abstract": "The Turing Test was proposed by Alan Turing in 1950; he called it the Imitation Game. In 1991 Hugh Loebner started the Loebner prize competition, offering a 100,000 prize to the author of the first computer program to pass an unrestricted Turing test. Annual competitions are held each year with smaller prizes for the best program on a restricted Turing test. This paper describes the development of one such Turing System, including the technical design of the program and its performance on the first three Loebner Prize competitions. We also discuss the program’s four year development effort, which has depended heavily on constant interaction with people on the Internet via Tinymuds (multiuser network communication servers). Finally, we discuss the design of the Loebner competition itself, and address its usefulness in furthering the development of Artificial Intelligence."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Social Interaction", "Title": "Multimodal Conversation with Social Agents", "Abstract": "We present a new approach to human-computer interaction, called social interaction. Its main characteristics are summarized by the following three points. First, interactions are realized as multimodal (verbal and nonverbal) conversation using spoken language, facial expressions, and so on. Second, the conversants are a group of humans and social agents that are autonomous and social. Autonomy is an important property that allows agents to decide how to act in an ever-changing environment. Socialness is also an important property that allows agents to behave both cooperatively and collaboratively. Generally, conversation is a joint work and ill-structured. Its participants are required to be social as well as autonomous. Third, conversants often encounter communication mismatches (misunderstanding others’ intentions and beliefs) and fail to achieve their joint goals. The social agents, therefore, are always concerned with detecting communication mismatches. We realize a social agent that hears human-to-human conversation and informs what is causing the misunderstanding. It can also interact with humans by voice with facial displays and head (and eye) movement."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "Experimentally Evaluating Communicative Strategies", "Title": "The Effect of the Task", "Abstract": "Effective problem solving among multiple agents requires a better understanding of the role of communication in collaboration. In this paper we show that there are communicative strategies that greatly improve the performance of resource-bounded agents, but that these strategies are highly sensitive to the task requirements, situation parameters and agents’ resource limitations. We base our argument on two sources of evidence: (1) an analysis of a corpus of 55 problem solving dialogues, and (2) experimental simulations of collaborative problem solving dialogues in an experimental world, Design-World, where we parameterize task requirements, agents’ resources and communicative strategies."}
{"Type": "conference", "Year": "1994", "Area": "AI", "Where": "AAAI", "Abbreviation": "The Synergy of Music Theory and AI", "Title": "Learning Multi-Level Expressive Interpretation", "Abstract": "The paper presents interdisciplinary research in the intersection of AI (machine learning) and Art (music). We describe an implemented system that learns expressive interpretation of music pieces from performances by human musicians. The problem, shown to be very difficult in the introduction, is solved by combining insights from music theory with a new machine learning algorithm. Theoretically founded knowledge about music perception is used to transform the original learning problem to a more abstract level where relevant regularities become apparent. Experiments with performances of Chopin waltzes are presented; the results indicate musical understanding and the ability to learn a complex task from very little training data. As the system’s domain knowledge is based on two established theories of tonal music, the results also have interesting implications for music theory."}
