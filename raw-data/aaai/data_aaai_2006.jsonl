{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Slashpack", "Title": "An Integrated Tool for Gathering and Managing Hypertext Data", "Abstract": "Many interesting Web-based AI problems require the ability to collect,store and process large text datasets. To address this problem, we have developed Slashpack, an integrated toolkit for collecting and managing hypertext data. Currently, we are using Slashpack to study the effectiveness of tagging as a mechanism for organizing and searching blogs, and also to study community structure in the blogosphere."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "RL-CD", "Title": "Dealing with Non-Stationarity in Reinforcement Learning", "Abstract": "This student abstract describes ongoing investigations regarding an approach for dealing with non-stationarity in reinforcement learning (RL) problems. We briefly propose and describe a method for managing multiple partial models of the environment and comment previous results which show that the proposed mechanism has better convergence times comparing to standard RL algorithms. Current efforts include the development of a more robust approach, capable of dealing with noisy environments, and also investigations regarding the possibility of using partial models in order to aliviate learning problems in systems with an explosive number of states."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "SemNews", "Title": "A Semantic News Framework", "Abstract": "SemNews is a semantic news service that monitors different RSS news feeds and provides structured representations of the meaning of news. SemNews uses OntoSem Natural Language Processing system to understand the text, and exports the computed facts back to the Web in OWL."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "KDMAS", "Title": "A Multi-Agent System for Knowledge Discovery via Planning", "Abstract": "In the real world, there are some domain knowledge discovery problems that can be formulated into knowledge-based planning problems, such as chemical reaction process and biological pathway discovery problems. A view of these domain problems can be re-cast as a planning problem, such that initial and final states are known and processes can be captured as abstract operators that modify the environment. We believe that AI planning technology can provide a modeling formalism for this task such that hypotheses can be generated, tested, queried and qualitatively simulated to improve the domain knowledge and rules. Our approach is to build a general multi-agent system for knowledge discovery (KDMAS) via planning for any domain whose problems can be modeled as AI planning problems. The plans produced are hypotheses capturing relevant qualitative information regarding domain knowledge. We will use the biological pathway domain as a model to present our approach."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Memeta", "Title": "A Framework for Multi-Relational Analytics on the Blogosphere", "Abstract": "The memeta project is developing a framework for studying the structure and content of the blogosphere. We are particularly interested in how metadata about blogs can be discovered, extracted and computed, and how this metadata can be modeled, represented and analyzed to provide new blog related services."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Organizing and Searching the World Wide Web of Facts—Step One", "Title": "The One-Million Fact Extraction Challenge", "Abstract": "Due to the inherent difficulty of processing noisy text, the potential of the Web as a decentralized repository of human knowledge remains largely untapped during Web search. The access to billions of binary relations among named entities would enable new search paradigms and alternative methods for presenting the search results. A first concrete step towards building large searchable repositories of factual knowledge is to derive such knowledge automatically at large scale from textual documents. Generalized contextual extraction patterns allow for fast iterative progression towards extracting one million facts of a given type (e.g., Person-BornIn-Year) from 100 million Web documents of arbitrary quality. The extraction starts from as few as 10 seed facts, requires no additional input knowledge or annotated text, and emphasizes scale and coverage by avoiding the use of syntactic parsers, named entity recognizers, gazetteers, and similar text processing tools and resources."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Overcoming the Brittleness Bottleneck using Wikipedia", "Title": "Enhancing Text Categorization with Encyclopedic Knowledge", "Abstract": "When humans approach the task of text categorization, they interpret the specific wording of the document in the much larger context of their background knowledge and experience. On the other hand, state-of-the-art information retrieval systems are quite brittle - they traditionally represent documents as bags of words, and are restricted to learning from individual word occurrences in the (necessarily limited) training set. For instance, given the sentence \"Wal-Mart supply chain goes real time\", how can a text categorization system know that Wal-Mart manages its stock with RFID technology? And having read that \"Ciprofloxacin belongs to the quinolones group\", how on earth can a machine know that the drug mentioned is an antibiotic produced by Bayer? In this paper we present algorithms that can do just that. We propose to enrich document representation through automatic use of a vast compendium of human knowledge - an encyclopedia. We apply machine learning techniques to Wikipedia, the largest encyclopedia to date, which surpasses in scope many conventional encyclopedias and provides a cornucopia of world knowledge. Each Wikipedia article represents a concept, and documents to be categorized are represented in the rich feature space of words and relevant Wikipedia concepts. Empirical results confirm that this knowledge-intensive representation brings text categorization to a qualitatively new level of performance across a diverse collection of datasets."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "OntoSearch", "Title": "A Full-Text Search Engine for the Semantic Web", "Abstract": "OntoSearch, a full-text search engine that exploits ontological knowledge for document retrieval, is presented in this paper. Different from other ontology based search engines, OntoSearch does not require a user to specify the associated concepts of his/her queries. Domain ontology in OntoSearch is in the form of a semantic network. Given a keyword based query, OntoSearch infers the related concepts through a spreading activation process in the domain ontology. To provide personalized information access, we further develop algorithms to learn and exploit user ontology model based on a customized view of the domain ontology. The proposed system has been applied to the domain of searching scientific publications in the ACM Digital Library. The experimental results support the efficacy of the OntoSearch system by using domain ontology and user ontology for enhanced search performance."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Detecting Spam Blogs", "Title": "A Machine Learning Approach", "Abstract": "Weblogs or blogs are an important new way to publish information, engage in discussions, and form communities on the Internet. The has unfortunately been infected by several varieties of spam-like content. Blog search engines, for example, are inundated by posts from splogs -- false blogs with machine generated or hijacked content whose sole purpose is to host ads or raise the PageRank of target sites. We discuss how SVM models based on local and link-based features can be used to detect splogs. We present an evaluation of learned models and their utility to blog search engines; systems that employ techniques differing from those of conventional web search engines."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Perspective Taking", "Title": "An Organizing Principle for Learning in Human-Robot Interaction", "Abstract": "The ability to interpret demonstrations from the perspective of the teacher plays a critical role in human learning. Robotic systems that aim to learn effectively from human teachers must similarly be able to engage in perspective taking. We present an integrated architecture wherein the robot's cognitive functionality is organized around the ability to understand the environment from the perspective of a social partner as well as its own. The performance of this architecture on a set of learning tasks is evaluated against human data derived from a novel study examining the importance of perspective taking in human learning. Perspective taking, both in humans and in our architecture, focuses the agent's attention on the subset of the problem space that is important to the teacher. This constrained attention allows the agent to overcome ambiguity and incompleteness that can often be present in human demonstrations and thus learn what the teacher intends to teach."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Know Thine Enemy", "Title": "A Champion RoboCup Coach Agent", "Abstract": "In a team-based multiagent system, the ability to construct a model of an opponent team's joint behavior can be useful for determining an agent's expected distribution over future world states, and thus can inform its planning of future actions. This paper presents an approach to team opponent modeling in the context of the RoboCup simulation coach competition. Specifically, it introduces an autonomous coach agent capable of analyzing past games of the current opponent, advising its own team how to play against this opponent, and identifying patterns or weaknesses on the part of the opponent. Our approach is fully implemented and tested within the RoboCup soccer server, and was the champion of the RoboCup 2005 simulation coach competition."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Walk the Talk", "Title": "Connecting Language, Knowledge, and Action in Route Instructions", "Abstract": "Following verbal route instructions requires knowledge of language, space, action and perception. We present Marco, an agent that follows free-form, natural language route instructions by representing and executing a sequence of compound action specifications that model which actions to take under which conditions. Marco infers implicit actions from knowledge of both linguistic conditional phrases and from spatial action and local configurations. Thus, Marco performs explicit actions, implicit actions necessary to achieve the stated conditions, and exploratory actions to learn about the world. We gathered a corpus of 786 route instructions from six people in three large-scale virtual indoor environments. Thirty-six other people followed these instructions and rated them for quality. These human participants finished at the intended destination on 69% of the trials. Marco followed the same instructions in the same environments, with a success rate of 61%. We measured the efficacy of action inference with Marco variants lacking action inference: executing only explicit actions, Marco succeeded on just 28% of the trials. For this task, inferring implicit actions is essential to follow poor instructions, but is also crucial for many highly-rated route instructions."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Intuitive linguistic Joint Object Reference in Human-Robot Interaction", "Title": "Human Spatial Reference Systems and Function-Based Categorization for Symbol Grounding", "Abstract": "The visionary goal of an easy to use service robot implies intuitive styles of interaction between humans and robots. Such natural interaction can only be achieved if means are found to bridge the gap between the forms of object perception and spatial knowledge maintained by such robots, and the forms of language, used by humans, to communicate such knowledge. Part of bridging this gap consists of allowing user and robot to establish joint reference on objects in the environment - without forcing the user to use unnatural means for object reference. We present an approach to establishing joint object reference which makes use of natural object classification and a computational model of basic intrinsic and relative reference systems. Our object recognition approach assigns natural categories (e.g. 'desk', 'chair', 'table') to new objects based on their functional design. With basic objects within the environment classified, we can then make use of a computational reference model, to process natural projective relations (e.g. 'the briefcase to the left of the chair'), allowing users to refer to objects which cannot be classified reliably by the recognition system alone."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "TacTex-05", "Title": "A Champion Supply Chain Management Agent", "Abstract": "Supply chains are ubiquitous in the manufacturing of many complex products. Traditionally, supply chains have been created through the interactions of human representatives of the companies involved, but advances in autonomous agent technologies have sparked an interest in automating the process. The Trading Agent Competition Supply Chain Management (TAC SCM) scenario provides a unique testbed for studying supply chain management agents. This paper introduces TacTex-05 (the champion agent from the 2005 competition), describes its constituent intelligent components, and examines the success of the complete agent through analysis of competition results and controlled experiments."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Constraints", "Title": "The Ties that Bind", "Abstract": "Constraints can serve as a unifying force in artificial intelligence."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "From the Programmer’s Apprentice to Human-Robot Interaction", "Title": "Thirty Years of Research on Human-Computer Collaboration", "Abstract": "We summarize the continuous thread of research we have conducted over the past thirty years on human-computer collaboration. This research reflects many of the themes and issues in operation in the greater field of AI over this period, such as knowledge representation and reasoning, planning and intent recognition, learning, and the interplay of human theory and computer engineering."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Multimodal Cognitive Architecture", "Title": "Making Perception More Central to Intelligent Behavior", "Abstract": "I propose that the notion of cognitive state be broadened from the current predicate-symbolic, Language-of-Thought framework to a multi-modal one, where perception and kinesthetic modalities participate in thinking. In contrast to the roles assigned to perception and motor activities as modules external to central cognition in the currently dominant theories in AI and Cognitive Science, in the proposed approach, central cognition incorporates parts of the perceptual machinery. I motivate and describe the proposal schematically, and describe the implementation of a bi-modal version in which a diagrammatic representation component is added to the cognitive state. The proposal explains our rich multimodal internal experience, and can be a key step in the realization of embodied agents. The proposed multimodal cognitive state can significantly enhance the agent’s problem solving."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Integrated AI in Space", "Title": "The Autonomous Sciencecraft on Earth Observing One", "Abstract": "The Earth Observing One spacecraft has been under the control of AI software for several years ñ experimentally since 2003 and since November 2004 as the primary operations system. This software includes: model-based planning and scheduling, procedural execution, and event detection software learned by support vector machine (SVM) techniques. This software has enabled a 100x increase in the mission science return per data downlinked and a >$1M/year reduction in operations costs. In this paper we discuss the AI software used, the impact of the software, and lessons learned with implications for future AI research."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Responsive Information Architect", "Title": "Enabling Context-Sensitive Information Seeking", "Abstract": "Information seeking is an important but often difficult task especially when involving large and complex data sets. We hypothesize that a context-sensitive interaction paradigm can greatly assist users in their information seeking. Such a paradigm allows a system to both understand user data requests and present the requested information in context. Driven by this hypothesis, we have developed a suite of intelligent user interaction technologies and integrated them in a full-fledged, context-sensitive information system. In this paper, we review two sets of key technologies: context-sensitive multimodal input interpretation and automated multimedia output generation. We also share our evaluation results, which indicate that our approaches are capable of supporting context-sensitive information seeking for practical applications."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Overview of AutoFeed", "Title": "An Unsupervised Learning System for Generating Webfeeds", "Abstract": "The AutoFeed system automatically extracts data from semi-structured web sites. Previously, researchers have developed two types of supervised learning approaches for extracting web data: methods that create precise, site-specific extraction rules and methods that learn less-precise site-independent extraction rules. In either case, significant training is required. AutoFeed follows a third, more ambitious approach, in which unsupervised learning is used to analyze sites and discover their structure. Our method relies on a set of heterogeneous \"experts\", each of which is capable of identifying certain types of generic structure. Each expert represents its discoveries as \"hints\". Based on these hints, our system clusters the pages and identifies semi-structured data that can be extracted. To identify a good clustering, we use a probabilistic model of the hint-generation process. This paper summarizes our formulation of the fully-automatic web-extraction problem, our clustering approach, and our results on a set of experiments."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "TempoExpress", "Title": "An Expressivity-Preserving Musical Tempo Transformation System", "Abstract": "The research described in this paper focuses on global tempo transformations of monophonic audio recordings of saxophone jazz performances. More concretely, we have investigated the problem of how a performance played at a particular tempo can be automatically rendered at another tempo while preserving its expressivity. To do so we have developed a case-based reasoning system called TempoExpress. The results we have obtained have been extensively compared against a standard technique called uniform time stretching (UTS), and show that our approach is superior to UTS."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Large Scale Knowledge Base Systems", "Title": "An Empirical Evaluation Perspective", "Abstract": "In this paper, we discuss how our work on evaluating Semantic Web knowledge base systems (KBSs) contributes to address some broader AI problems. First, we show how our approach provides a benchmarking solution to the Semantic Web, a new application area of AI. Second, we discuss how the approach is also beneficial in a more traditional AI context. We focus on issues such as scalability, performance tradeoffs, and the comparison of different classes of systems."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Activity-Centric Email", "Title": "A Machine Learning Approach", "Abstract": "Our use of ordinary desktop applications (such as email, Web, calendars) is often a manifestation of the activities with which we are engaged. Planning a conference trip involves sending travel expense forms, and visits to airline and hotel sites. Renovating a kitchen involves sketches, product specifications, emails with the architect and spreadsheets for tracking expenses. Every enterprise has (often implicit) processes for managing customer queries, requesting maintenance, hiring a new employee, purchasing equipment, and so on. Unfortunately, ordinary desktop applications do not know anything about these activities. Within an enterprise, many activities have been formalized into business workflows such as hiring or ordering equipment. However, the way people interact with these workflows is often through email and desktop applications. If these applications are not aware of the activity context, people bear the burden of organizing their information into activities, typically using crude techniques such as manual search, file directories, and email folders/threads. Email has emerged as the primary tool for people to communicate about their work and manage activities. Motivated by the importance of email in conducting activities, we have recently developed several machine learning algorithms for automatically discovering and tracking activities in email. We observe that activities come in many forms, from structured workflows to informal person-to-person communication. In this paper, we summarize our efforts to provide automated assistance with two types of activities: rigid structured activities, and unstructured conversational activities."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Beyond Bags of Words", "Title": "Modeling Implicit User Preferences in Information Retrieval", "Abstract": "This paper reports on recent work in the field of information retrieval that attempts to go beyond the overly simplified approach of representing documents and queries as bags of words. Simple models make it difficult to accurately model a user's information need. The model presented in the paper is based on Markov random fields and allows almost arbitrary features to be encoded. This provides a powerful mechanism for modeling many of the implicit constraints a user has in mind when formulating a query. Simple instantiations of the model that consider dependencies between the terms in a query have shown to significantly outperform bag of words models. Further extensions of the model are possible to incorporate even more complex constraints based other domain knowledge. Finally, we describe what place our model has within the broader realm of artificial intelligence and propose several open questions that may be of general interest to the field."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "The Synthy Approach for End to End Web Services Composition", "Title": "Planning with Decoupled Causal and Resource Reasoning", "Abstract": "Web services offer a unique opportunity to simplify application integration by defining common, web-based, platform-neutral, standards for publishing service descriptions to a registry, finding and invoking them -- not necessarily by the same parties. Viewing software components as web services, the current solutions to web services composition based on business web services (using WSDL, BPEL, SOAP etc.) or semantic web services (using ontologies, goal-directed reasoning etc.) are both piecemeal and insufficient for building practical applications. Inspired by the work in AI planning on decoupling causal (planning) and resource reasoning (scheduling), we introduced the first integrated work in composing web services end to end from specification to deployment by synergistically combining the strengths of the current approaches. The solution is based on a novel two--staged composition approach that addresses the information modeling aspects of web services, provides support for contextual information while composing services, employs efficient decoupling of functional and non-functional requirements, and leads to improved scalability and failure handling. A prototype of the solution has been implemented in the Synthy service composition system and applied to a number of composition scenarios from the telecom domain. The application of planning to web services has also brought new plan and planner usability-driven research issues to the fore for AI."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Progress in Textual Case-Based Reasoning", "Title": "Predicting the Outcome of Legal Cases from Text", "Abstract": "This paper reports on a project that explored reasoning with textual cases in the context of legal reasoning. The work is anchored in both Case-Based Reasoning (CBR) and AI and Law. It introduces the SMILE+IBP framework that generates a case-based analysis and prediction of the outcome of a legal case given a brief textual summary of the case facts. The focal research question in this work was to find a good text representation for text classification. An evaluation showed that replacing case-specific names by roles and adding NLP lead to higher performance for assigning CBR indices. The NLP-based representation produced the best results for reasoning with the automatically indexed cases."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "TPBOSCourier", "Title": "A Transportation Procurement System (for the Procurement of Courier Services)", "Abstract": "TPBOSCourier is the Transportation Procurement and Bid Optimization System (TPBOS) for Philips Electronics to automate and optimize its procurement of courier services. It was jointly developed by Red Jasper Limited and the Hong Kong University of Science and Technology, and has been successfully deployed in 2005. Philips typically procures courier services for more than 2500 shipping lanes annually and the use of the software has resulted in significant cost and time savings in analyzing and optimizing procurement decisions. This paper explains the development and design of the TPBOSCourier."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Machine Translation for Manufacturing", "Title": "A Case Study at Ford Motor Company", "Abstract": "Machine Translation was one of the first applications of Artificial Intelligence technology that was deployed to solve real-world problems. Since the early 1960s, researchers have been building and utilizing computer systems that can translate from one language to another without extensive human intervention. In the late 1990s, Ford Vehicle Operations began working with Systran Software Inc to adapt and customize their Machine Translation (MT) technology in order to translate Ford's vehicle assembly build instructions from English to German, Spanish, Dutch and Portuguese. The use of Machine Translation (MT) was made necessary by the vast amount of dynamic information that needed to be translated in a timely fashion. Our MT system has already translated over 5 million instructions into these target languages and is an integral part of our manufacturing process planning to support Ford's assembly plants in Europe, Mexico and South America. In this paper, we focus on how AI techniques, such as knowledge representation and natural language processing, can improve the accuracy of Machine Translation in a dynamic environment such as auto manufacturing."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "CM-Extractor", "Title": "An Application for Automating Medical Quality Measures Abstraction in a Hospital Setting", "Abstract": "In the US, health care providers are required to report evidence-based quality measures to various governmental and independent regulatory agencies. Abstracting appropriate facts from a patient’s medical record provides the data for these measures. Finding and maintaining qualified staff for this vital function is a challenge to many healthcare providers. Emerging systems and technologies in large-scale clinical repositories and AI techniques for information extraction have the potential to make the process of collecting measures more consistent, accurate and efficient. This paper presents CM-Extractor, a computerized system that automates the process of quality measures abstraction using natural language processing and a rule-based approach. An evaluation of a deployed system used for hospital inpatient cases is discussed. The results showed that the NLP performed with high accuracy across multiple types of medical documents, and users were able to significantly improve productivity. Challenges remain in the areas of availability of electronic patient data and a model for deploying and supporting solutions on a large scale."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "AWDRAT", "Title": "A Cognitive Middleware System for Information Survivability", "Abstract": "The Infrastructure of modern society is controlled by software systems that are vulnerable to attacks. Many such attacks, launched by \"recreational hackers\" have already led to severe disruptions and significant cost. It, therefore, is critical that we find ways to protect such systems and to enable them to continue functioning even after a successful attack. This paper describes AWDRAT, a middleware system for providing survivability to both new and legacy applications. AWDRAT stands for Architectural-differencing, Wrappers, Diagnosis, Recovery, Adaptive software, and Trust-modeling. AWDRAT uses these techniques to gain visibility into the execution of an application system and to compare the application's actual behavior to that which is expected. In the case of a deviation, AWDRAT conducts a diagnosis that figures out which computational resources are likely to have been compromised and then adds these assessments to its trust-model. The trust model in turn guides the recovery process, particularly by guiding the system in its choice among functionally equivalent methods and resources. AWDRAT has been used on an example application system, a graphical editor for constructing mission plans. We present data showing the effectiveness of AWDRAT in detecting a variety of compromises to the application system."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "CPM", "Title": "Context-Aware Power Management in WLANs", "Abstract": "In this paper, we present a novel approach for tuning power modes of wireless 802.11 interfaces. We use K-means and simple correlation techniques to analyze user's interaction with applications based on mouse clicks. This provides valuable contextual hints that are used to anticipate future network access patterns and intent of users. Based on those hints, we adapt the power mode of the wireless network interface to optimize both energy usage and bandwidth usage. Evaluation results (based on real data gathered from interaction with a desktop) show significant improvements over earlier power management schemes."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "MedEthEx", "Title": "A Prototype Medical Ethics Advisor", "Abstract": "As part of a larger Machine Ethics Project, we are developing an ethical advisor that provides guidance to health care workers faced with ethical dilemmas. MedEthEx is an implementation of Beauchamp’s and Childress' Principles of Biomedical Ethics that harnesses machine learning techniques to abstract decision principles from cases in a particular type of dilemma with conflicting prima facie duties and uses these principles to determine the correct course of action in similar and new cases. We believe that accomplishing this will be a useful first step towards creating machines that can interact with those in need of health care in a way that is sensitive to ethical issues that may arise."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Trip Router with Individualized Preferences (TRIP)", "Title": "Incorporating Personalization into Route Planning", "Abstract": "Popular route planning systems (Windows Live Local, Yahoo! Maps, Google Maps, etc.) generate driving directions using a static library of roads and road attributes. They ignore both the time at which a route is to be traveled and, more generally, the preferences of the drivers they serve. We present a set of methods for including driver preferences and time-variant traffic condition estimates in route planning. These methods have been incorporated into a working prototype named TRIP. Using a large database of GPS traces logged by drivers, TRIP learns time-variant traffic speeds for every road in a widespread metropolitan area. It also leverages a driver’s past GPS logs when responding to future route queries to produce routes that are more suited to the driver’s individual driving preferences. Using experiments with real driving data, we demonstrate that the routes produced by TRIP are measurably closer to those actually chosen by drivers than are the routes produced by routers that use static heuristics."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Local Negotiation in Cellular Networks", "Title": "From Theory to Practice", "Abstract": "This paper describes a novel negotiation protocol for cellular networks, which intelligently improves the performance of the network. Our proposed reactive mechanism enables the dynamic adaptation of the base stations to continuous changes in service demands, thereby improving the overall network performance. This mechanism is important when a frequent global optimization is infeasible or substantially costly. The proposed local negotiation mechanism is incorporated into a simulated network based on cutting-edge industry technologies. Experimental results suggest a rapid adjustment to changes in bandwidth demand and overall improvement in the number of served users over time. Although we tested our algorithm based on the service level, which is measured as the number of covered handsets, our algorithm supports negotiation for any set of parameters, aiming to optimize network's performance according to any measure of performance specified by the service provider."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "The Tactical Language and Culture Training System", "Title": "A Demonstration", "Abstract": "In this demonstration we will present the Tactical Iraqi, one of the implementations of the Tactical Language and Culture Training System (TLTS). The system helps learners acquire basic communicative skills in foreign languages and cultures. Learners practice their communication skills in a simulated village, where they must develop rapport with the local people, who in turn will help them accomplish missions such as post-war reconstruction. Each learner is ac-companied by a virtual aide who can provide assistance and guidance if needed. The aide can also act as a virtual tutor as part of an intelligent tutoring system, giving the learners feedback on their performance. Learners communicate via a multimodal interface, which permits them to speak and choose gestures on behalf of their character in the game. The system employs video game technologies and design techniques, in order to motivate and engage learners."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "SEMAPLAN", "Title": "Combining Planning with Semantic Matching to Achieve Web Service Composition", "Abstract": "In this paper, we present a novel algorithm to compose Web services in the presence of semantic ambiguity by combining semantic matching and AI planning algorithms. We use cues from domain-independent and domain-specific ontologies to compute an overall semantic similarity score between ambiguous terms. This semantic similarity score is used by AI planning algorithms to guide the searching process when composing services. Experimental results indicate that planning with semantic matching produces better results than planning or semantic matching alone. The solution is suitable for semi-automated composition tools or directory browsers."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "LOCATE Intelligent Systems Demonstration", "Title": "Adapting Help to the Cognitive Styles of Users", "Abstract": "LOCATE is workspace layout design software that also serves as a testbed for developing and refining principles of adaptive aiding. This demonstration illustrates LOCATE's ability to determine user cognitive styles and provide help matched to those styles. To match LOCATE's help to users' cognitive styles, users are assessed along a Wholist-Analytic dimension and a Verbal-Imagery-Kinesthetic \"trimension\" and scoring places the user's style at a point in the resultant three-dimensional space. That information is stored in a User Model maintained by LOCATE and, whenever help is requested, material is provided in a form consistent with the system's inference about the user's cognitive style. Help options provided to users for selecting alternative forms of help permit the system to track those selections and allow for system adaptation to the user's preferred style of help."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "SemNews", "Title": "A Semantic News Framework", "Abstract": "SemNews is a semantic news service that monitors different RSS news feeds and provides structured representations of the meaning of news. As new content appears, SemNews extracts the summary from the RSS description and processes it using OntoSem, which is a sophisticated text understanding system. The extracted meaning from the RSS descriptions of the news articles are then converted into Semantic Web representation such as RDF."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Phoebus", "Title": "A System for Extracting and Integrating Data from Unstructured and Ungrammatical Sources", "Abstract": "With the proliferation of online classifieds and auctions comes a new need to meaningfully search and organize the items for sale. However, since the seller's item descriptions are not structured and do not conform to a standard set of values (think \"Chevy\" versus \"Chevrolet\"), searching and organizing this data is difficult. This paper describes a working demonstration of the Phoebus system which uses both record linkage and information extraction to parse out the meaningful attributes of an item description and assign them standard values. This allows the data to be sorted, searched and linked to other data sources where standard values for the attributes are required to link the sources together."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Erdos", "Title": "Cost-Effective Peripheral Robotics for AI Education", "Abstract": "This work combines hardware, software, and curricula in order to create robots capable enough to advance the field of AI yet inexpensive enough to be widely accessible. Costs are kept low by pairing iRobot’s roombas with existing laptop or palmtop computers and their accessories. The result is a sub-$200 untethered physical platform capable of running and testing state-of-the-art AI algorithms."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "DIARC", "Title": "A Testbed for Natural Human-Robot Interaction", "Abstract": "Autonomous human-like robots that interact in natural language with people in real-time pose many design challenges, from the functional organization of the robotic architecture, to the computational infrastructure possibly employing middle-ware for distributed computing, to the hardware operating many specialized devices for sensory and effector processing in addition to embedded controllers and standard computational boards. The task is to achieve a functional integration of very diverse modules that operate at different temporal scales using different representations on parallel hardware in a reliable and fault-tolerant manner that allows for"}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Model Counting", "Title": "A New Strategy for Obtaining Good Bounds", "Abstract": "Model counting is the classical problem of computing the number of solutions of a given propositional formula. It vastly generalizes the NP-complete problem of propositional satisfiability, and hence is both highly useful and extremely expensive to solve in practice. We present a new approach to model counting that is based on adding a carefully chosen number of so-called streamlining constraints to the input formula in order to cut down the size of its solution space in a controlled manner. Each of the additional constraints is a randomly chosen XOR or parity constraint on the problem variables, represented either directly or in the standard CNF form. Inspired by a related yet quite different theoretical study of the properties of XOR constraints, we provide a formal proof that with high probability, the number of XOR constraints added in order to bring the formula to the boundary of being unsatisfiable determines with high precision its model count. Experimentally, we demonstrate that this approach can be used to obtain good bounds on the model counts for formulas that are far beyond the reach of exact counting methods. In fact, we obtain the first non-trivial solution counts for very hard, highly structured combinatorial problem instances. Note that unlike other counting techniques, such as Markov Chain Monte Carlo methods, we are able to provide high-confidence guarantees on the quality of the counts obtained."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "From Pigeons to Humans", "Title": "Grounding Relational Learning in Concrete Examples", "Abstract": "We present a cognitive model that bridges work in analogy and category learning. The model, Building Relations through Instance Driven Gradient Error Shifting (BRIDGES), extends ALCOVE, an exemplar-based connectionist model of human category learning (Kruschke, 1992). Unlike ALCOVE which is limited to featural or spatial representations, BRIDGES can appreciate analogical relationships between stimuli and stored predicate representations of exemplars. Like ALCOVE, BRIDGES learns to shift attention over the course of learning to reduce error and, in the process, alters its notion of similarity. A shift toward relational sources of similarity allows BRIDGES to display what appears to be an understanding of abstract domains, when in fact performance is driven by similarity-based structural alignment (i.e., analogy) to stored exemplars. Supportive simulations of animal, infant, and adult learning are provided. We end by considering possible extensions of BRIDGES suitable for computationally demanding applications."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Evaluating Preference-based Search Tools", "Title": "A Tale of Two Approaches", "Abstract": "People frequently use the world-wide web to find their most preferred item among a large range of options. We call this task preference-based search. The most common tool for preference-based search on the WWW today obtains users’ preferences by asking them to fill in a form. It then returns a list of items that most closely match these preferences. Recently, several researchers have proposed tools for preference-based search that elicit preferences from the critiques a user actively makes on examples shown to them. We carried out a user study in order to compare the performance of traditional preference-based search tools using form-filling with two different versions of an example-critiquing tool. The results show that example critiquing achieves almost three times the decision accuracy, while requiring only slightly higher interaction effort."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "kFOIL", "Title": "Learning Simple Relational Kernels", "Abstract": "A novel and simple combination of inductive logic programming with kernel methods is presented. The kFOIL algorithm integrates the well-known inductive logic programming system FOIL with kernel methods. The feature space is constructed by leveraging FOIL search for a set of relevant clauses. The search is driven by the performance obtained by a support vector machine based on the resulting kernel. In this way, kFOIL implements a dynamic propositionalization approach. Both classification and regression tasks can be naturally handled. Experiments in applying kFOIL to well-known benchmarks in chemoinformatics show the promise of the approach."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Minimum Description Length Principle", "Title": "Generators Are Preferable to Closed Patterns", "Abstract": "The generators and the unique closed pattern of an equivalence class of itemsets share a common set of transactions. The generators are the minimal ones among the equivalent itemsets, while the closed pattern is the maximum one. As a generator is usually smaller than the closed pattern in cardinality, by the Minimum Description Length Principle, the generator is preferable to the closed pattern in inductive inference and classification. To efficiently discover frequent generators from a large dataset, we develop a depth-first algorithm called Gr-growth. The idea is novel in contrast to traditional breadth-first bottom-up generator-mining algorithms. Our extensive performance study shows that Gr-growth is significantly faster (an order or even two orders of magnitudes when the support thresholds are low) than the existing generator mining algorithms. It can be also faster than the state-of-the-art frequent closed itemset mining algorithms such as FPclose and CLOSET+."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Multi-Conditional Learning", "Title": "Generative/Discriminative Training for Clustering and Classification", "Abstract": "This paper presents multi-conditional learning (MCL), a training criterion based on a product of multiple conditional likelihoods. When combining the traditional conditional probability of \"label given input\" with a generative probability of \"input given label\" the later acts as a surprisingly effective regularizer. When applied to models with latent variables, MCL combines the structure-discovery capabilities of generative topic models, such as latent Dirichlet allocation and the exponential family harmonium, with the accuracy and robustness of discriminative classifiers, such as logistic regression and conditional random fields. We present results on several standard text data sets showing significant reductions in classification error due to MCL regularization, and substantial gains in precision and recall due to the latent structure discovered under MCL."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Nonnegative Matrix Factorization and Probabilistic Latent Semantic Indexing", "Title": "Equivalence Chi-Square Statistic, and a Hybrid Method", "Abstract": "Non-negative Matrix Factorization (NMF) and Probabilistic Latent Semantic Indexing (PLSI) have been successfully applied to document clustering recently. In this paper, we show that PLSI and NMF optimize the same objective function, although PLSI and NMF are different algorithms as verified by experiments. This provides a theoretical basis for a new hybrid method that runs PLSI and NMF alternatively, each jumping out of local minima of the other method successively, thus achieving better final solution. Extensive experiments on 5 real-life datasets show relations between NMF and PLSI, and indicate the hybrid method lead to significant improvements over NMF-only or PLSI-only methods. We also show that at first order approximation, NMF is identical to Chi-square Statistic."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Anytime Induction of Decision Trees", "Title": "An Iterative Improvement Approach", "Abstract": "Most existing decision tree inducers are very fast due to their greedy approach. In many real-life applications, however, we are willing to allocate more time to get better decision trees. Our recently introduced LSID3 contract anytime algorithm allows computation speed to be traded for better tree quality. As a contract algorithm, LSID3 must be allocated its resources a priori, which is not always possible. In this work, we present IIDT, a general framework for interruptible induction of decision trees that need not be allocated resources a priori. The core of our proposed framework is an iterative improvement algorithm that repeatedly selects a subtree whose reconstruction is expected to yield the highest marginal utility. The algorithm then rebuilds the subtree with a higher allocation of resources. IIDT can also be configured to receive training examples as they become available, and is thus appropriate for incremental learning tasks. Empirical evaluation with several hard concepts shows that IIDT exhibits good anytime behavior and significantly outperforms greedy inducers when more time is available. A comparison of IIDT to several modern decision tree learners showed it to be superior."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Multiparty Proactive Communication", "Title": "A Perspective for Evolving Shared Mental Models", "Abstract": "Helping behavior in effective teams is enabled by some overlapping \"shared mental models\" that are developed and maintained by members of the team. In this paper, we take the perspective that multiparty \"proactive\" communication is critical for establishing and maintaining such a shared mental model among teammates, which is the basis for agents to offer proactive help and to achieve coherent teamwork. We first provide formal semantics for multiparty proactive performatives within a team setting. We then examine how such performatives result in updates to mental model of teammates, and how such updates can trigger helpful behaviors from other teammates."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "ODPOP", "Title": "An Algorithm for Open/Distributed Constraint Optimization", "Abstract": "We propose ODPOP, a new distributed algorithm for"}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Behaviosites", "Title": "Manipulation of Multiagent System Behavior through Parasitic Infection", "Abstract": "In this paper we present the Behaviosite Paradigm, a new approach to coordination and control of distributed agents in a multiagent system, inspired by biological parasites with behavior manipulation properties. Behaviosites are code modules that \"infect\" a system, attaching themselves to agents and altering the sensory activity and actions of those agents. These behavioral changes can be used to achieve altered, potentially improved, performance of the overall system; thus, Behaviosites provide a mechanism for distributed control over a distributed system. Behaviosites need to be designed so that they are intimately familiar with the internal workings of the environment and of the agents operating within it. To demonstrate our approach, we use behaviosites to control the behavior of a swarm of simple agents. With a relatively low infection rate, a few behaviosites can engender desired behavior over the swarm as a whole: keeping it in one place, leading it through checkpoints, or moving the swarm from one stable equilibrium to another. We contrast behaviosites as a distributed swarm control mechanism with alternatives, such as the use of group leaders, herders, or social norms."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Contract Enactment in Virtual Organizations", "Title": "A Commitment-Based Approach", "Abstract": "A virtual organization (VO) is a dynamic collection of entities (individuals, enterprises, and information resources) collaborating on some computational activity. VOs are an emerging means to model, enact, and manage large-scale computations. VOs consist of autonomous, heterogeneous members, often dynamic exhibiting complex behaviors. Thus, VOs are best modeled via multiagent systems. An agent can be an individual such as a person, business partner, or a resource. An agent may also be a VO. A VO is an agent that comprises other agents. Contracts provide a natural arms-length abstraction for modeling interaction among autonomous and heterogeneous agents. The interplay of contracts and VOs is the subject of this paper. The core of this paper is an approach to formalize VOs and contracts based on commitments. Our main contributions are (1) a formalization of VOs, (2) a discussion of certain key properties of VOs, and (3) an identification of a variety of VO structures and an analysis of how they support contract enactment. We evaluate our approach with an analysis of several scenarios involving the handling of exceptions and conflicts in contracts."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Mechanisms for Partial Information Elicitation", "Title": "The Truth, but Not the Whole Truth", "Abstract": "We examine a setting in which a buyer wishes to purchase probabilistic information from some agent. The seller must invest effort in order to gain access to the information, and must therefore be compensated appropriately. However, the information being sold is hard to verify and the seller may be tempted to lie in order to collect a higher payment. While it is generally easy to design information elicitation mechanisms that motivate the seller to be truthful, we show that if the seller has additional relevant information it does not want to reveal, the buyer must resort to elicitation mechanisms that work only some of the time. The optimal design of such mechanisms is shown to be computationally hard. We show two different algorithms to solve the mechanism design problem, each appropriate (from a complexity point of view) in different scenarios."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Keeping in Touch", "Title": "Maintaining Biconnected Structure by Homogeneous Robots", "Abstract": "For many distributed autonomous robotic systems, it is important to maintain communication connectivity among the robots. That is, each robot must be able to communicate with each other robot, perhaps through a series of other robots. Ideally, this property should be robust to the removal of any single robot from the system. In (Ahmadi, Stone 2006) we define a property of a team's communication graph that ensures this property, called biconnectivity. In that paper, a distributed algorithm to check if a team of robots is biconnected and its correctness proof are also presented. In this paper we provide distributed algorithms to add and remove robots to/from a multi-robot team while maintaining the biconnected property. These two algorithms are implemented and tested in the Player/Stage simulator."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Compiling Uncertainty Away", "Title": "Solving Conformant Planning Problems using a Classical Planner (Sometimes)", "Abstract": "Even under polynomial restrictions on plan length, conformant planning remains a very hard computational problem as plan verification itself can take exponential time. This heavy price cannot be avoided in general although in many cases conformant plans are verifiable efficiently by means of simple forms of disjunctive inference. This raises the question of whether it is possible to identify and use such forms of inference for developing an efficient but incomplete planner capable of solving non-trivial problems quickly. In this work, we show that this is possible by mapping conformant into classical problems that are then solved by an off-the-shelf classical planner. The formulation is sound as the classical plans obtained are all conformant, but it is incomplete as the inverse relation does not always hold. The translation accommodates `reasoning by cases' by means of an `split-protect-and-merge' strategy; namely, atoms L/Xi that represent conditional beliefs `if Xi then L' are introduced in the classical encoding, that are combined by suitable actions to yield the literal L when the disjunction X1 or ... or Xn holds and certain invariants in the plan are verified. Empirical results over a wide variety of problems illustrate the power of the approach."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Learning Partially Observable Action Models", "Title": "Efficient Algorithms", "Abstract": "We present tractable, exact algorithms for learning actions' effects and preconditions in partially observable domains. Our algorithms maintain a propositional logical representation of the set of possible action models after each observation and action execution. The algorithms perform exact learning of preconditions and effects in any deterministic action domain. This includes STRIPS actions and actions with conditional effects. In contrast, previous algorithms rely on approximations to achieve tractability, and do not supply approximation guarantees. Our algorithms take time and space that are polynomial in the number of domain features, and can maintain a representation that stays compact indefinitely. Our experimental results show that we can learn efficiently and practically in domains that contain over 1000's of features (more than 2^1000 states)."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Factored Planning", "Title": "How, When, and When Not", "Abstract": "Automated domain factoring, and planning methods that utilize them, have long been of interest to planning researchers. Recent work in this area yielded new theoretical insight and algorithms, but left many questions open: How to decompose a domain into factors? How to work with these factors? And whether and when decomposition-based methods are useful? This paper provides theoretical analysis that answers many of these questions: it proposes a novel approach to factored planning; proves its theoretical superiority over previous methods; provides insight into how to factor domains; and uses its novel complexity results to analyze when factored planning is likely to perform well, and when not. It also establishes the key role played by the domain's causal graph in the complexity analysis of planning algorithms."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "PPCP", "Title": "Efficient Probabilistic Planning with Clear Preferences in Partially-Known Environments", "Abstract": "For most real-world problems the agent operates in only partially-known environments. Probabilistic planners can reason over the missing information and produce plans that take into account the uncertainty about the environment. Unfortunately though, they can rarely scale up to the problems that are of interest in real-world. In this paper, however, we show that for a certain subset of problems we can develop a very efficient probabilistic planner. The proposed planner, called PPCP, is applicable to the problems for which it is clear what values of the missing information would result in the best plan. In other words, there exists a clear preference for the actual values of the missing information. For example, in the problem of robot navigation in partially-known environments it is always preferred to find out that an initially unknown location is traversable rather than not. The planner we propose exploits this property by using a series of deterministic A*-like searches to construct and refine a policy in anytime fashion. On the theoretical side, we show that once converged, the policy is guaranteed to be optimal under certain conditions. On the experimental side, we show the power of PPCP on the problem of robot navigation in partially-known terrains. The planner can scale up to very large environments with thousands of initially unknown locations. We believe that this is several orders of magnitude more unknowns than what the current probabilistic planners developed for the same problem can handle. Also, despite the fact that the problem we experimented on in general does not satisfy the conditions for the solution optimality, PPCP still produces the solutions that are nearly always optimal."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Motion-Based Autonomous Grounding", "Title": "Inferring External World Properties from Encoded Internal Sensory States Alone", "Abstract": "How can we build artificial agents that can autonomously explore and understand their environments? An immediate requirement for such an agent is to learn how its own sensory state corresponds to the external world properties: It needs to learn the semantics of its internal state (i.e., grounding). In principle, we as programmers can provide the agents with the required semantics, but this will compromise the autonomy of the agent. To overcome this problem, we may fall back on natural agents and see how they acquire meaning of their own sensory states, their neural firing patterns. We can learn a lot about what certain neural spikes mean by carefully controlling the input stimulus while observing how the neurons fire. However, neurons embedded in the brain do not have direct access to the outside stimuli, so such a stimulus-to-spike association may not be learnable at all. How then can the brain solve this problem? (We know it does.) We propose that motor interaction with the environment is necessary to overcome this conundrum. Further, we provide a simple yet powerful criterion, sensory invariance, for learning the meaning of sensory states. The basic idea is that a particular form of action sequence that maintains invariance of a sensory state will express the key property of the environmental stimulus that gave rise to the sensory state. Our experiments with a sensorimotor agent trained on natural images show that sensory invariance can indeed serve as a powerful objective for semantic grounding."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Running the Table", "Title": "An AI for Computer Billiards", "Abstract": "Billiards is a game of both strategy and physical skill. To succeed, a player must be able to select strong shots, and then execute them accurately and consistently. Several robotic billiards players have recently been developed. These systems address the task of executing shots on a physical table, but so far have incorporated little strategic reasoning. They require AI to select the \"best\" shot taking into account the accuracy of the robotics, the noise inherent in the domain, the continuous nature of the search space, the difficulty of the shot, and the goal of maximizing the chances of winning. This paper develops and compares several approaches to establishing a strong AI for billiards. The resulting program, PickPocket, won the first international computer billiards competition."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Reinforcement Learning with Human Teachers", "Title": "Evidence of Feedback and Guidance with Implications for Learning Performance", "Abstract": "As robots become a mass consumer product, they will need to learn new skills by interacting with typical human users. Past approaches have adapted reinforcement learning (RL) to accept a human reward signal; however, we question the implicit assumption that people shall only want to give the learner feedback on its past actions. We present findings from a human user study showing that people use the reward signal not only to provide feedback about past actions, but also to provide future directed rewards to guide subsequent actions. Given this, we made specific modifications to the simulated RL robot to incorporate guidance. We then analyze and evaluate its learning performance in a second user study, and we report significant improvements on several measures. This work demonstrates the importance of understanding the human-teacher/robot-learner system as a whole in order to design algorithms that support how people want to teach while simultaneously improving the robot's learning performance."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "DD* Lite", "Title": "Efficient Incremental Search with State Dominance", "Abstract": "This paper presents DD* Lite, an efficient incremental search algorithm for problems that can capitalize on state dominance. Dominance relationships between nodes are used to prune graphs in search algorithms. Thus, exploiting state dominance relationships can considerably speed up search problems in large state spaces, such as mobile robot path planning considering uncertainty, time, or energy constraints. Incremental search techniques are useful when changes can occur in the search graph, such as when re-planning paths for mobile robots in partially known environments. While algorithms such as D* and D* Lite are very efficient incremental search algorithms, they cannot be applied as formulated to search problems in which state dominance is used to prune the graph. DD* Lite extends D* Lite to seamlessly support reasoning about state dominance. It maintains the algorithmic simplicity and incremental search capability of D* Lite, while resulting in orders of magnitude increase in search efficiency in large state spaces with dominance. We illustrate the efficiency of DD* Lite with simulation results from applying the algorithm to a path planning problem with time and energy constraints. We also prove that DD* Lite is sound, complete, optimal, and efficient."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Disco—Novo—GoGo", "Title": "Integrating Local Search and Complete Search with Restarts", "Abstract": "A hybrid algorithm is devised to boost the performance of complete search on under-constrained problems. We suggest to use random variable selection in combination with restarts, augmented by a coarse-grained local search algorithm that learns favorable value heuristics over the course of several restarts. Numerical results show that this method can speed-up complete search by orders of magnitude."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Prob-Maxn", "Title": "Playing N-Player Games with Opponent Models", "Abstract": "Much of the work on opponent modeling for game tree search has been unsuccessful. In two-player, zero-sum games, the gains from opponent modeling are often outweighed by the cost of modeling. Opponent modeling solutions simply cannot search as deep as the highly optimized minimax search with alpha-beta pruning. Recent work has begun to look at the need for opponent modeling in n-player or general-sum games. We introduce a probabilistic approach to opponent modeling in n-player games called probmaxn, which can robustly adapt to unknown opponents. We implement probmaxn in the game of Spades, showing that probmaxn is highly effective in practice, beating out the maxn and softmaxn algorithms when faced with unknown opponents."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Identifiability in Causal Bayesian Networks", "Title": "A Sound and Complete Algorithm", "Abstract": "This paper addresses the problem of identifying causal effects from nonexperimental data in a causal Bayesian network, i.e., a directed acyclic graph that represents causal relationships. The identifiability question asks whether it is possible to compute the probability of some set of (effect) variables given intervention on another set of (intervention) variables, in the presence of non-observable (i.e., hidden or latent) variables. It is well known that the answer to the question depends on the structure of the causal Bayesian network, the set of observable variables, the set of effect variables, and the set of intervention variables. Our work is based on the work of Tian, Pearl, Huang, and Valtorta (Tian and Pearl 2002a; 2002b; 2003; Huang and Valtorta 2006a) and extends it. We show that the identify algorithm that Tian and Pearl define and prove sound for semi-Markovian models can be transfered to general causal graphs and is not only sound, but also complete. This result effectively solves the identifiability question for causal Bayesian networks that Pearl posed in 1995 (Pearl 1995), by providing a sound and complete algorithm for identifiability."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "Focused Real-Time Dynamic Programming for MDPs", "Title": "Squeezing More Out of a Heuristic", "Abstract": "Real-time dynamic programming (RTDP) is a heuristic search algorithm for solving MDPs. We present a modified algorithm called Focused RTDP with several improvements. While RTDP maintains only an upper bound on the long-term reward function, FRTDP maintains two-sided bounds and bases the output policy on the lower bound. FRTDP guides search with a new rule for outcome selection, focusing on parts of the search graph that contribute most to uncertainty about the values of good policies. FRTDP has modified trial termination criteria that should allow it to solve some problems (within ε) that RTDP cannot. Experiments show that for all the problems we studied, FRTDP significantly outperforms RTDP and LRTDP, and converges with up to six times fewer backups than the state-of-the-art HDP algorithm."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "When Gossip is Good", "Title": "Distributed Probabilistic Inference for Detection of Slow Network Intrusions", "Abstract": "Intrusion attempts due to self-propagating code are becoming an increasingly urgent problem, in part due to the homogeneous makeup of the internet. Recent advances in anomalybased intrusion detection systems (IDSs) have made use of the quickly spreading nature of these attacks to identify them with high sensitivity and at low false positive (FP) rates. However, slowly propagating attacks are much more difficult to detect because they are cloaked under the veil of normal network traffic, yet can be just as dangerous due to their exponential spread pattern. We extend the idea of using collaborative IDSs to corroborate the likelihood of attack by imbuing end hosts with probabilistic graphical models and using random messaging to gossip state among peer detectors. We show that such a system is able to boost a weak anomaly detector D to detect an order-of-magnitude slower worm, at false positive rates less than a few per week, than would be possible using D alone at the end-host or on a network aggregation point. We show that this general architecture is scalable in the sense that a fixed absolute false positive rate can be achieved as the network size grows, spreads communication bandwidth uniformly throughout the network, and makes use of the increased computation power of a distributed system. We argue that using probabilistic models provides more robust detections than previous collaborative counting schemes and allows the system to account for heterogeneous detectors in a principled fashion."}
{"Type": "conference", "Year": "2006", "Area": "AI", "Where": "AAAI", "Abbreviation": "CUI Networks", "Title": "A Graphical Representation for Conditional Utility Independence", "Abstract": "We introduce CUI networks, a compact graphical representation of utility functions over multiple attributes. CUI networks model multiattribute utility functions using the well studied and widely applicable utility independence concept. We show how conditional utility independence leads to an effective functional decomposition that can be exhibited graphically, and how local, compact data at the graph nodes can be used to calculate joint utility. We discuss aspects of elicitation and network construction, and contrast our new representation with previous graphical preference modeling."}
