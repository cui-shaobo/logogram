{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "AAAI", "Abbreviation": "Model Induction", "Title": "A New Source of CSP Model Redundancy", "Abstract": "Based on the notions of viewpoints, models, and channeling constraints, the paper introduces model induction, a systematic transformation of constraints in an existing model to constraints in another viewpoint. Meant to be a general CSP model operator, model induction is useful in generating redundant models, which can be further induced or combined with the original model or other mutually redundant models. We propose three ways of combining redundant models using model induction, model channeling, and model intersection. Experimental results on the Langford’s problem confirm that our proposed combined models exhibit improvements in efficiency and robustness over the original single models."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "AAAI", "Abbreviation": "ASSAT", "Title": "Computing Answer Sets of a Logic Program by SAT Solvers", "Abstract": "We propose a new translation from normal logic programs with constraints under the answer set semantics to propositional logic. Given a logic program, we show that by adding, for each loop in the program, a corresponding loop formula to the program’s completion, we obtain a one-to-one correspondence between the answer sets of the program and the models of the resulting propositional theory. Compared with the translation by Ben-Eliyahu and Dechter, ours has the advantage that it does not use any extra variables, and is considerably simpler, thus easier to understand. However, in the worst case, it requires computing exponential number of loop formulas. To address this problem, we propose an approach that adds loop formulas a few at a time, selectively. Based on these results, we implemented a system called ASSAT(X), depending on the SAT solver X used, and tested it on a variety of benchmarks including the graph coloring, the blocks world planning, and Hamiltonian Circuit domains. The results are compared with those by smodels and dlv, and it shows a clear edge of ASSAT(X) over them in these domains."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "AAAI", "Abbreviation": "Structural Extension to Logistic Regression", "Title": "Discriminative Parameter Learning of Belief Net Classifiers", "Abstract": "Bayesian belief nets (BNs) are often used for classification tasks --- typically to return the most likely \"class label\" for each specified instance. Many BN-learners, however, attempt to find the BN that maximizes a different objective function (viz., likelihood, rather than classification accuracy), typically by first learning an appropriate graphical structure, then finding the maximal likelihood parameters for that structure. As these parameters may not maximize the classification accuracy, \"discriminative learners\" follow the alternative approach of seeking the parameters that maximize conditionallikelihood (CL), over the distribution of instances the BN will have to classify. This paper first formally specifies this task, and shows how it relates to logistic regression, which corresponds to finding the optimal CL parameters for a naive-bayes structure. After analyzing its inherent (sample and computational) complexity, we then present a general algorithm for this task, ELR, which applies to arbitrary BN structures and which works effectively even when given the incomplete training data. This paper presents empirical evidence that ELR works better than the standard \"generative\" approach in a variety of situations, especially in common situation where the BN-structure is incorrect."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "AAAI", "Abbreviation": "Vote Elicitation", "Title": "Complexity and Strategy-Proofness", "Abstract": "Preference elicitation is a central problem in AI, and has received significant attention in single-agent settings. It is also a key problem in multiagent systems, but has received little attention here so far. In this setting, the agents may have different preferences that often must be aggregated using voting. This leads to interesting issues because what, if any, information should be elicited from an agent depends on what other agents have revealed about their preferences so far. In this paper we study effective elicitation, and its impediments, for the most common voting protocols. It turns out that in the Single Transferable Vote protocol, even knowing when to terminate elicitation is NP-complete, while this is easy for all the other protocols under study. Even for these protocols, determining how to elicit effectively is NP-complete, even with perfect suspicions about how the agents will vote. The exception is the Plurality protocol where such effective elicitation is easy. We also show that elicitation introduces additional opportunities for strategic manipulation by the voters. We demonstrate how to curtail the space of elicitation schemes so that no such additional strategic issues arise."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "AAAI", "Abbreviation": "Dispersion Games", "Title": "General Definitions and Some Specific Learning Results", "Abstract": "Dispersion games are the generalization of the anti-coordination game to arbitrary numbers of agents and actions. In these games agents prefer outcomes in which the agents are maximally dispersed over the set of possible actions. This class of games models a large number of natural problems, including load balancing in computer science, niche selection in economics, and division of roles within a team in robotics. Our work consists of two main contributions. First, we formally define and characterize some interesting classes of dispersion games. Second, we present several learning strategies that agents can use in these games, including traditional learning rules from game theory and artificial intelligence, as well as some special purpose strategies. We then evaluate analytically and empirically the performance of each of these strategies."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "AAAI", "Abbreviation": "CobotDS", "Title": "A Spoken Dialogue System for Chat", "Abstract": "We describe CobotDS, a spoken dialogue system providing access to a well-known internet chat server called LambdaMOO. CobotDS provides real-time, two-way, natural language communication between a phone user and the multiple users in the text environment. We describe a number of the challenging design issues we faced, and our use of summarization, social filtering and personalized grammars in tackling them. We report a number of empirical findings from a small user study."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "AAAI", "Abbreviation": "CD*", "Title": "A Real-Time Resolution Optimal Re-Planner for Globally Constrained Problems", "Abstract": "Many problems in robotics and AI, such as the find-path problem, call for optimal solutions that satisfy global constraints. The problem is complicated when the cost information is unknown, uncertain, or changing during execution of the solution. Such problems call for efficient re-planning during execution to account for the new information acquired. This paper presents a novel real-time algorithm, Constrained D* (CD*), that re-plans resolution optimal solutions subject to a global constraint. CD* performs a binary search on a weight parameter that sets the balance between the optimality and feasibility cost metrics. In each stage of the search, CD* uses Dynamic A* (D*) to update the weight selection for that stage. On average, CD* updates a feasible and resolution optimal plan in less than a second, enabling it to be used in a real-time robot controller. Results are presented for simulated problems. To the author’s knowledge, CD* is the fastest algorithm to solve this class of problems."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "AAAI", "Abbreviation": "FastSLAM", "Title": "A Factored Solution to the Simultaneous Localization and Mapping Problem", "Abstract": "The ability to simultaneously localize a robot and accurately map its surroundings is considered by many to be a key prerequisite of truly autonomous robots. However, few approaches to this problem scale up to handle the very large number of landmarks present in real environments. Kalman filter-based algorithms, for example, require time quadratic in the number of landmarks to incorporate each sensor observation. This paper presents FastSLAM, an algorithm that recursively estimates the full posterior distribution over robot pose and landmark locations, yet scales logarithmically with the number of landmarks in the map. This algorithm is based on an exact factorization of the posterior into a product of conditional landmark distributions and a distribution over robot paths. The algorithm has been run successfully on as many as 50,000 landmarks, environments far beyond the reach of previous approaches. Experimental results demonstrate the advantages and limitations of the FastSLAM algorithm on both simulated and real-world data."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "AAAI", "Abbreviation": "Watch Their Moves", "Title": "Applying Probabilistic Multiple Object Tracking to Autonomous Robot Soccer", "Abstract": "In many autonomous robot applications robots must be capable of estimating the positions and motions of moving objects in their environments. In this paper, we apply probabilistic multiple object tracking to estimating the positions of opponent players in autonomous robot soccer. We extend an existing tracking algorithm to handle multiple mobile sensors with uncertain positions, discuss the specification of probabilistic models needed by the algorithm, and describe the required vision-interpretation algorithms. The multiple object tracking has been successfully applied throughout the RoboCup 2001 world championship."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "AAAI", "Abbreviation": "SetA*", "Title": "An Efficient BDD-Based Heuristic Search Algorithm", "Abstract": "In this paper we combine the goal directed search of A* with the ability of BDDs to traverse an exponential number of states in polynomial time. We introduce a new algorithm, SetA*, that generalizes A* to expand sets of states in each iteration. SetA* has substantial advantages over BDDA*, the only previous BDD-based A* implementation we are aware of. Our experimental evaluation proves SetA* to be a powerful search paradigm. For some of the studied problems it outperforms BDDA*, A*, and BDD-based breadth-first search by several orders of magnitude. We believe exploring sets of states to be essential when the heuristic function is weak. For problems with strong heuristics, SetA* efficiently specializes to single-state search and consequently challenges single-state heuristic search in general."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "AAAI", "Abbreviation": "The Interface between P and NP", "Title": "COL, XOR, NAE, 1-in-k, and Horn SAT", "Abstract": "We study in detail the interface between P and NP by means of five new problem classes. Like the well known 2+p-SAT problem, these new problems smoothly interpolate between P and NP by mixing together a polynomial and a NP-complete problem. In many cases, the polynomial subproblem can dominate the problem’s satisfiability and the search complexity. However, this is not always the case, and understanding why remains a very interesting open question. We identify phase transition behavior in each of these problem classes. Surprisingly we observe transitions with both smooth and sharp regions. Finally we show how these problem classes can help to understand algorithm behavior by considering search trajectories through the phase space."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "AAAI", "Abbreviation": "PROMPTDIFF", "Title": "A Fixed-Point Algorithm for Comparing Ontology Versions", "Abstract": "As ontology development becomes a more ubiquitous and collaborative process, the developers face the problem of maintaining versions of ontologies akin to maintaining versions of software code in large software projects. Versioning systems for software code provide mechanisms for tracking versions, checking out versions for editing, comparing different versions, and so on.We can directly reuse many of these mechanisms for ontology versioning. However, version comparison for code is based on comparing text files---an approach that does not work for comparing ontologies. Two ontologies can be identical but have different text representation. We have developed the PromptDiff algorithm, which integrates different heuristic matchers for comparing ontology versions. We combine these matchers in a fixed-point manner, using the results of one matcher as an input for others until the matchers produce no more changes. The current implementation includes ten matchers but the approach is easily extendable to an arbitrary number of matchers. Our evaluation showed that PromptDiff correctly identified 96% of the matches in ontology versions from large projects."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "AAAI", "Abbreviation": "Searching for Backbones and Fat", "Title": "A Limit-Crossing Approach with Applications", "Abstract": "Backbone variables are the elements that are common to all optimal solutions of a problem instance. We call variables that are absent from every optimal solution fat variables. Identification of backbone and fat variables is a valuable asset when attempting to solve complex problems. In this paper, we demonstrate a method for identifying backbones and fat. Our method is based on an intuitive concept, which we refer to as limit-crossing. Limit-crossing occurs when we force the lower bound of a graph problem to exceed the upper bound by applying the lower-bound function to a constrained version of the graph. A desirable feature of this procedure is that it uses approximation functions to derive exact information about optimal solutions. In this paper, we prove the validity of the limit-crossing concept as well as other related properties. Then we exploit limit-crossing and devise a pre-processing tool for discovering backbone and fat arcs for various instances of the Asymmetric Traveling Salesman Problem (ATSP). Our experimental results demonstrate the power of the limit-crossing method. We compare our pre-processor with the Carpaneto, Dell'Amico, and Toth pre-processor for several different classes of ATSP instances and reveal dramatic performance improvements."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "AAAI", "Abbreviation": "Optimal Schedules for Parallelizing Anytime Algorithms", "Title": "The Case of Independent Processes", "Abstract": "The performance of anytime algorithms having a non-deterministic nature can be improved by solving simultaneously several instances of the algorithm-problem pairs. These pairs may include different instances of a problem (like starting from a different initial state), different algorithms (if several alternatives exist), or several instances of the same algorithm (for non-deterministic algorithms). In this paper we present a general framework for optimal parallelization of independent processes. We show a mathematical model for this framework, present algorithms for optimal scheduling, and demonstrate its usefulness on a real problem."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "AAAI", "Abbreviation": "The OD Theory of TOD", "Title": "The Use and Limits of Temporal Information for Object Discovery", "Abstract": "We present the theory behind TOD (the Temporal Object Discoverer), a novel unsupervised system that uses only temporal information to discover objects across image sequences acquired by any number of uncalibrated cameras. The process is divided into three phases: (1) Extraction of each pixel’s temporal signature, a partition of the pixel’s observations into sets that stem from different objects; (2) Construction of a global schedule that explains the signatures in terms of the lifetimes of a set of quasi-static objects; (3) Mapping of each pixel’s observations to objects in the schedule according to the pixel’s temporal signature. Our Global Scheduling (GSched) algorithm provably constructs a valid and complete global schedule when certain observability criteria are met. Our Quasi-Static Labeling (QSL) algorithm uses the schedule created by GSched to produce the maximally-informative mapping of each pixel’s observations onto the objects they stem from. Using GSched and QSL, TOD ignores distracting motion, correctly deals with complicated occlusions, and naturally groups observations across cameras. The sets of 2D masks recovered are suitable for unsupervised training and initialization of object recognition and tracking systems."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "AAAI", "Abbreviation": "Reviewing the Design of DAML+OIL", "Title": "An Ontology Language for the Semantic Web", "Abstract": "In the current \"Syntactic Web,\" uninterpreted syntactic constructs are given meaning only by private off-line agreements that are inaccessible to computers. In the Semantic Web vision, this is replaced by a web where both data and its semantic definition are accessible and manipulable by computer software. DAML+OIL is an ontology language specifically designed for this use in the Web; it exploits existing Web standards (XML and RDF), adding the familiar ontological primitives of object oriented and frame based systems, and the formal rigor of a very expressive description logic. The definition of DAML+OIL is now over a year old, and the language has been in fairly widespread use. In this paper, we review DAML+OIL’s relation with its key ingredients (XML, RDF, OIL, DAML-ONT, Description Logics), we discuss the design decisions and trade-offs that were the basis for the language definition, and identify a number of implementation challenges posed by the current language. These issues are important for designers of other representation languages for the Semantic Web, be they competitors or successors of DAML+OIL, such as the language currently under definition by W3C."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "AAAI", "Abbreviation": "MiTAP, Text and Audio Processing for Bio-Security", "Title": "A Case Study", "Abstract": "MiTAP (MITRE Text and Audio Processing) is a prototype system available for monitoring infectious disease outbreaks and other global events. MiTAP focuses on providing timely, multi-lingual, global information access to medical experts and individuals involved in humanitarian assistance and relief work. Multiple information sources in multiple languages are automatically captured, filtered, translated, summarized, and categorized by disease, region, information source, person, and organization. Critical information is automatically extracted and tagged to facilitate browsing, searching, and sorting. The system supports shared situational awareness through collaboration, allowing users to submit other articles for processing, annotate existing documents, post directly to the system, and flag messages for others to see. MiTAP currently stores eight hundred thousand articles and processes an additional 2000 to 10,000 daily, delivering up-to-date information to dozens of regular users."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "AAAI", "Abbreviation": "RightNow eService Center", "Title": "Internet Customer Service Using a Self-Learning Knowledge Base", "Abstract": "Delivering effective customer service via the Internet requires attention to many aspects of knowledge management if it is to be convenient and satisfying for customers, while at the same time efficient and economical for the company or other organization. In RightNow eService Center, such management is enabled by automatically gathering meta-knowledge about the Answer documents held in the core knowledge base. A variety of AI techniques are used to facilitate the construction, maintenance, and navigation of the knowledge base. These include collaborative filtering, swarm intelligence, fuzzy logic, natural language processing, text clustering, and classification rule learning. Customers using eService Center report dramatic decreases in support costs and increases in customer satisfaction due to the ease of use provided by the \"self-learning\" features of the knowledge base."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "AAAI", "Abbreviation": "UTTSExam", "Title": "A Campus-Wide University Exam-Timetabling System", "Abstract": "UTTSExam is the exam-scheduling portion of the University Timetable Scheduler (UTTS) software, an automated university timetabling program developed in the National University of Singapore. It was successfully used to schedule the examination timetable for the first semester of the 2001/2002 academic year in NUS, a task involving 27,235 students taking 1,350 exams. The use of the software resulted in significant time savings in the scheduling of the timetable and a shortening of the examination period. This paper explains the development and design of UTTSExam."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "AAAI", "Abbreviation": "A Structure Based Configuration Tool", "Title": "Drive Solution Designer – DSD", "Abstract": "In this paper, we describe the configuration tool Drive Solution Designer (DSD). The DSD is used by sales engineers of the company Lenze AG (www.lenze.com) for the configuration of complex drive systems in order to make on-site offers together with the customer. The aim of this process is to generate a consistent solution which fulfills the functional requirements of the user along with optimization criteria such as price and delivery time. The preparation of a technical offer requires fundamental knowledge of complex physical and in particular technical correlations of drive components, in depth knowledge of the product catalog as well as high empirical knowledge about the order of the parameterization of the components. In order to meet these requirements knowledge-based AI-techniques are required. In the DSD we use a domain independent incremental structure-based configuration approach with different knowledge representation mechanisms and a sophisticated declarative control. Currently DSD is used with great success by approx. 150 sales engineers of the company Lenze for the design layout task. The introduction of the DSD lead to a drastic time reduction for drive solution development and reduces incorrect solutions to nearly 0 percent."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "AAAI", "Abbreviation": "AI on the Battlefield", "Title": "An Experimental Exploration", "Abstract": "The US Army Battle Command Battle Lab conducted an experiment with the ICCES system -- an integrated decision aid for performing several critical steps of a US Army Brigade Military Decision Making Process: from capturing a high-level Course of Action to producing a detailed analysis and plan of tasks. The system integrated several available technologies based largely on AI techniques, ranging from qualitative spatial interpretation of course-of-action diagrams to interleaved adversarial planning and scheduling. The experiment dispelled concerns about potential negative impacts of such tools on the creative aspects of the art of war, showed a potential for dramatic time savings in the MDMP process, and confirmed the maturity and suitability of the technologies for near-future deployment."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "AAAI", "Abbreviation": "Getting from Here to There", "Title": "Interactive Planning and Agent Execution for Optimizing Travel", "Abstract": "Planning and monitoring a trip is a common but complicated human activity. Creating an itinerary is nontrivial because it requires coordination with existing schedules and making a variety of interdependent choices. Once planned, there are many possible events that can affect the plan, such as schedule changes or flight cancellations, and checking for these possible events requires time and effort. In this paper, we describe how Heracles and Theseus, two information gathering and monitoring tools that we built, can be used to simplify this process. Heracles is a hierarchical constraint planner that aids in interactive itinerary development by showing how a particular choice (e.g., destination airport) affects other choices (e.g., possible modes of transportation, available airlines, etc.). Heracles builds on an information agent platform, called Theseus, that provides the technology for efficiently executing agents for information gathering and monitoring tasks. In this paper we present the technologies underlying these systems and describe how they are applied to build a state-of-the-art travel system."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "AAAI", "Abbreviation": "WhyNot", "Title": "Debugging Failed Queries in Large Knowledge Bases", "Abstract": "When a query to a knowledge-based system fails and returns \"unknown\", users are confronted with a problem: Is relevant knowledge missing or incorrect? Is there a problem with the inference engine? Was the query ill-conceived? Finding the culprit in a large and complex knowledge base can be a hard and laborious task for knowledge engineers and might be impossible for non-expert users. To support such situations we developed a new tool called \"WhyNot\" as part of the PowerLoom knowledge representation and reasoning system. To debug a failed query, WhyNot tries to generate a small set of plausible partial proofs that can guide the user to what knowledge might have been missing, or where the system might have failed to make a relevant inference. A first version of the system has been deployed to help debug queries to a version of the Cyc knowledge base containing over 1,000,000 facts and over 35,000 rules."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "AAAI", "Abbreviation": "Student Modeling for a Web-Based Learning Environment", "Title": "A Data Mining Approach", "Abstract": "In this paper, we report on an on-going study of how data mining techniques, if incorporated into web learning environments, can enhance the overall qualities of learning. The focus is on building student models, using a clustering technique based on large generalized sequences recording both learners’ web browsing behavior and the contents of the pages being browsed."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "AAAI", "Abbreviation": "MAKEBELIEVE", "Title": "Using Commonsense Knowledge to Generate Stories", "Abstract": "This paper introduces MAKEBELIEVE, an interactive story generation agent that uses commonsense knowledge to generate short fictional texts from an initial seed story step supplied by the user. A subset of commonsense de-scribing causality, such as the sentence \"a consequence of drinking alcohol is intoxication,\" is selected from the on-tology of the Open Mind Commonsense Knowledge Base. Binary causal relations are extracted from these sentences and stored as crude trans-frames. By performing fuzzy, creativity-driven inference over these frames, creative \"causal chains\" are produced for use in story generation. The current system has mostly local pair-wise constraints between steps in the story, though global constraints such as narrative structure are being added."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "AAAI", "Abbreviation": "Localizing while Mapping", "Title": "A Segment Approach", "Abstract": "Localization in mobile robotics is a well studied problem in many environments. Map building with occupancy grids (probabilistic finite element maps of the environment) is also fairly well understood. However, trying to accomplish both localization and mapping at once has proven to be a difficult task. Updating a map with new sensor information before determining the correct adjustment for the starting point and heading can be disastrous. We hope to solve this problem by performing localization on higher-level objects computed from the raw sensor readings. Any object which can be detected from a single sensor reading can be used to help the localization process. In addition, once the data from the robot’s sensors have been turned into a map with human-identifiable features, the map can easily be augmented with additional information about the objects (unique names, attributes, etc)."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "AAAI", "Abbreviation": "BN-Tools", "Title": "A Software Toolkit for Experimentation in BBNs", "Abstract": "In this paper, I describe BN-Tools, an open-source, Java-based library for experimental research in Bayesian belief networks that implements several popular algorithms for estimation (approximate inference) and learning along with a graphical user interface for interactive display and editing of graphical models. Included in the discussion are our implementations of the Lauritzen-Spiegelhalter algorithm, an adaptive importance sampling algorithm, the K2 learning algorithm, and two genetic algorithm wrappers."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "AAAI", "Abbreviation": "Generalized Features", "Title": "Their Application to Classification", "Abstract": "Classification learning algorithms in general, and text classification methods in particular, tend to focus on features of individual training examples, rather than on the relationships between the examples. However, in many situations a set of items contains more information than just feature values of individual items. We propose to recognize and put in use generalized features (or set features) that describe a training example, but that depend on the dataset as a whole, with the goal of achieving better classification accuracy. In particular, we work on the integration of temporal relations into conventional word-based email classification."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "AAAI", "Abbreviation": "SpeechWeb", "Title": "A Web of Natural-Language Speech Applications", "Abstract": "SpeechWeb consists of a collection of hyperlinked natural-language interfaces to applications which can be accessed through the Internet from speech browsers running on PCs. The applications contain hyperlinks which the browser uses to navigate SpeechWeb. The natural-language interfaces have been constructed as executable specifications of attribute grammars using a domain-specific programming language built for this purpose. The approach to natural-language processing is based on a new efficient compositional semantics that accommodates arbitrarily-nested quantification and negation. The user-independent speech browser is grammar based, and novel techniques have been developed to improve recognition accuracy."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "AAAI", "Abbreviation": "FlexBot, Groo, Patton and Hamlet", "Title": "Research Using Computer Games as a Platform", "Abstract": "This paper describes four systems we intend to demonstrate at the AAAI-02 Conference. The first system is FlexBot, a software agent research platform built using the Half-Life game engine. The remaining three systems are research applications that were developed on top of the FlexBot architecture: (1) Groo -- an efficient bot constructed using behavior-based techniques. (2) Patton -- a system for monitoring and controlling bots through remote, possibly mobile, devices. (3) Hamlet -- the first part of a system for monitoring players and dynamically adjusting gameplay to promote dramatic/narrative immersion. This demonstration is designed to show FlexBot in action and to exhibit the flexibility, efficiency and overall ease with which the FlexBot architecture supports a variety of AI research tasks."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "AAAI", "Abbreviation": "UTTSExam", "Title": "A University Examination Timetable Scheduler", "Abstract": "UTTSExam is a university examination timetable-scheduling program that was successfully employed to create the examination timetable for semester 1 of the 2001/2002 academic year in the National University of Singapore. This demonstration provides insight on the various components of the system, including the hybrid centralized cum decentralized scheduling strategy, the Combined Method scheduling algorithm and the overall process required to create the final timetable."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "AAAI", "Abbreviation": "Disciple-RKF/COG", "Title": "Agent Teaching by Subject Matter Experts", "Abstract": "Disciple-RKF/COG is a learning agent shell that can perform many knowledge engineering tasks, and can be used to develop knowledge based systems by subject matter experts, with limited assistance from knowledge engineers. The expert and the agent engage into a mixed-initiative reasoning process during which the expert is teaching the agent his problem solving expertise, and the agent learns from the expert, building, verifying, and improving its knowledge base. Disciple-RKF/COG is used in several courses at the US Army War College. In the \"Military Applications of Artificial Intelligence\" course the students teach personal Disciple agents their own reasoning in Center of Gravity analysis. In the \"Case Studies in Center of Gravity Analysis\" course, a Disciple agent that was taught the expertise of the course’s instructor helps the students to learn about center of gravity analysis, and to develop a case study analysis report."}
{"Type": "conference", "Year": "2002", "Area": "AI", "Where": "AAAI", "Abbreviation": "JYAG and IDEY", "Title": "A Template-Based Generator and Its Authoring Tool", "Abstract": "JYAG is the Java implementation of a real-time, general-purpose, template-based generation system (YAG, Yet Another Generator). JYAG enables interactive applications to adapt natural language output to the interactive context without requiring developers to write all possible output strings ahead of time or to embed extensive knowledge of the grammar of the target language in the application. Currently, designers of interactive systems who might wish to include dynamically generated text face a number of barriers; for example designers must decide (1) How hard will it be to link the application to the generator? (2) Will the generator be fast enough? (3) How much linguistic information will the application need to provide in order to get reasonable quality output? (4) How much effort will be required to write a generation grammar that covers all the potential outputs of the application? The design and implementation of our template-based generation system, JYAG, is intended to address each of these concerns."}
