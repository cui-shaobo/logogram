{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "AAAI", "Abbreviation": "TREAT", "Title": "A Better Match Algorithm for AI Production Systems", "Abstract": "This paper presents the TREAT match algorithm for AI production systems. The TREAT algorithm introduces a new method of state saving in production system interpreters called conflict-set support. Also presented are the results of an empirical study comparing the performance of the TREAT match with the commonly assumed best algorithm for this problem, the RETE match. On five different OPS5 production system programs TREAT outperformed RETE, often by more than fifty percent. This supports an unsubstantiated conjecture made by McDermott, Newell and Moore, that the state saving mechanism employed in the RETE match, condition-element support, may not be worthwhile."}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "AAAI", "Abbreviation": "Joshua", "Title": "Uniform Access to Heterogeneous Knowledge Structures, or, Why Joshing Is Better than Conniving or Planning", "Abstract": "This paper presents Joshua, a system which provides syntactically uniform access to heterogeneously implemented knowledge bases. Its power comes from the observation that there is a Protocol of Inference consisting of a small set of abstract actions, each of which can be implemented in many ways. We use the object-oriented programming facilities of Flavors to control the choice of implementation. A statement is an instance of a class identified with its predicate. The steps of the protocol are implemented by methods inherited from the classes. Inheritance of protocol methods is a compile-time operation, leading to very fine-grained control with little run-time cost. Joshua has two major advantages: First, a Joshua programmer can easily change his program to use more efficient data structures without changing the rule set or other knowledge-level structures. We show how we thus sped up one application by a factor of 3. Second, it is straightforward to build an interface which incorporates an existing tool into Joshua, without modifying the tool. We show how a different TMS, implemented for another system, was thus interfaced to Joshua."}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "AAAI", "Abbreviation": "Foundations of Assumption-Based Truth Maintenance Systems", "Title": "Preliminary Report", "Abstract": "In this paper we (1) define the concept of a Clause Management System (CMS) - a generalization of de Kleer’s ATMS, (2) motivate such systems in terms of efficiency of search and abductive reasoning,"}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "AAAI", "Abbreviation": "Proof Analogy in Interactive Theorem Proving", "Title": "A Method to Express and Use It Via Second Order Pattern Matching", "Abstract": "A method is presented to express and use syntactic analogies between proofs in interactive theorem proving and proof checking. Up to now, very few papers have addressed instances of this problem. The paradigm of \"proposition as types\" is adopted and proofs are represented as terms. The proposed method is to transform a known proof of a theorem into what might become a proof of an \"analogous\" -according to the user-proposition, namely the one to be proved. This transformation is expressed by means of second order pattern matching (this may be seen as a generalisation of rewriting rules), thus allowing the use of variable function symbols. For the moment, it is up to the user to discover the transformation rule, and the paper deals only with the problem of managing it. We explain the proposed analogy treatment with a fully developed running example."}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "AAAI", "Abbreviation": "Real-Time Heuristic Search", "Title": "First Results", "Abstract": "Existing heuristic search algorithms are not applicable to real-time applications because they cannot commit to a move before an entire solution is found. We present a special case of minimax lookahead search to handle this problem, and an analog of alpha-beta pruning that significantly improves the efficiency of the algorithm. In addition, we present a new algorithm, called Real-Time-A*, for searching when actions must actually be executed, as opposed to merely simulated. Finally, we examine the nature of the tradeoff between computation and execution cost."}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "AAAI", "Abbreviation": "Path Dissolution", "Title": "A Strongly Complete Rule of Inference", "Abstract": "We introduce path dissolution, a rule of inference that operates on formulas in negation normal form. Path dissolution is strongly complete; i.e., it has the property that, given an unsatisfiable ground formula, any sequence of dissolution steps will produce the empty graph. This is accomplished by strictly reducing (at each step) the number of c-paths in the formula. Dissolution, unlike most resolution-based inference rules, does not directly lift into first-order logic; techniques for employing dissolution at the first order level are discussed."}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "AAAI", "Abbreviation": "Material Handling", "Title": "A Conservative Domain for Neural Connectivity and Propagation", "Abstract": "Two important components of connectionist models are the connectivity between units and the propagation rule for mapping outputs of units to inputs of units. The biological domains where these models are usually applied are nonconservative, in that a single output signal produced by one unit can become the input to zero, one, or many subsequent units. The connectivity matrices and propagation rules common in these domains reflect this nonconservativism in both learning and performance. CASCADE is a connectionist system for performing material handling in a discrete parts manufacturing environment. We have described elsewhere the architecture and implementation of CASCADE [PARU86a] and its formal correspondence [PARU86c], ] [PARU87a] with the PDP model [RUME86]. The signals that CASCADE passes between units correpond to discrete physical objects, and thus must obey certain conservation laws not observed by conventional neural architectures. This paper briefly reviews the problem domain and the connectionist structure of CASCADE, describes CASCADE’s scheme for maintaining connectivity information and propagating signals, and reports some experiments with the system."}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "AAAI", "Abbreviation": "AQUA", "Title": "Asking Questions and Understanding Answers", "Abstract": "Story understanding programs are often designed to answer questions to demonstrate that they have adequately understood a story (e.g., [Leh78]). In contrast, we claim that asking questions is central to understanding. Reading a story involves the generation of questions, which in turn focus the understander on the relevant aspects of the story as it reads further. We are interested in the kinds of questions that people ask as they read. In this paper, we talk about the origin of these questions in the basic cycle of understanding, and their effect on processing. We present an understanding algorithm based on our theory of questions, which we have implemented in a computer program called AQUA (Asking Questions and Understanding Answers)."}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "AAAI", "Abbreviation": "Analogical Processing", "Title": "A Simulation and Empirical Corroboration", "Abstract": "This paper compares the performance of the Structure-Mapping Engine (SME), a cognitive simulation of analogy, with two aspects of human performance. Gentner’s Structure-Mapping theory predicts that soundness is highest for relational matches, while accessibility is highest for surface matches. These predictions have been borne out in psychological studies, and here we demonstrate that SME replicates these results. In particular, we ran SME on the same stories used in the psychological studies with two different kinds of match rules. In analogy mode, SME closely captures the human soundness ordering. In mere-appearance mode, SME captures the accessibility ordering. We briefly review the psychological studies, describe our computational experiments, and discuss the utility of SME as a cognitive modeling tool."}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "AAAI", "Abbreviation": "Pengi", "Title": "An Implementation of a Theory of Activity", "Abstract": "AI has generally interpreted the organized nature of everyday activity in terms of plan-following. Nobody could doubt that people often make and follow plans. But the complexity, uncertainty, and immediacy of the real world require a central role for moment-to-moment improvisation. But before and beneath any planning ahead, one continually decides what to do now. Investigation of the dynamics of everyday routine activity reveals important regularities in the interaction of very simple machinery with its environment. We have used our dynamic theories to design a program, called Pengi, that engages in complex, apparently planful activity without requiring explicit models of the world."}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "AAAI", "Abbreviation": "Compare and Contrast", "Title": "A Test of Expertise", "Abstract": "In this paper we present three key elements of case-based reasoning (\"CBR\") and describe how these are realized in our HYPO program which performs legal reasoning in the domain of trade secret law by comparing and contrasting cases. More specifically, the key elements involve how prior cases are used for: (1) Credit assignment of factual features; (2) Justification; and (3) Argument in domains that do not necessarily have strong causal theories or well-understood empirical regularities. We show how HYPO uses \"dimensions\", \"case-analysis-record\" and \"claim lattice\" mechanisms to perform indexing and relevancy assessment of past cases dynamically and how it compares and contrasts cases to come up with the best cases pro and con a decision."}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "AAAI", "Abbreviation": "Reducing Indeterminism in Consultation", "Title": "A Cognitive Model of User/Librarian Interactions", "Abstract": "In information facilities such as libraries, finding documents that are relevant to a user query is difficult because of the indeterminism involved in the process by which documents are indexed, and the latitude users have in choosing terms to express a query on a particular topic. Reference librarians play an important support role in coping with this indeterminism, focusing user queries through an interactive dialog. Based on thirty detailed observations of user/librarian interactions obtained through a field experiment, we have developed a computational model designed to simulate the reference librarian. The consultation includes two phases. The first is handle search, where the user’s rough problem statement and a user stereotyping imposed by the librarian are used in determining the appropriate tools (handles). The second phase is document search, involving the search for documents within a chosen handle. We are collaborating with the university library for putting our model to use as an intelligent assistant for an online retrieval system."}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "AAAI", "Abbreviation": "Incremental Inference", "Title": "Getting Multiple Agents to Agree on What to Do Next", "Abstract": "This paper presents a symbolic reasoning algorithm for use in the construction of mixed-initiative interfaces; that is, interfaces allowing several human or machine agents to share collectively the control of an ongoing, real-time activity. The algorithm, called Incremental Inference, is based on propositional logic and is related in structure to the Truth Maintenance System; however, the notion of justifications in the Truth Maintenance System is replaced with a simpler notion of recency. Basic properties of the Incremental Inference mechanism are described and compared with those of the Truth Maintenance System, and an example is provided drawn from the domain of SPECTRUM, a knowledge-based system for the geological interpretation of imaging spectrometer data."}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "AAAI", "Abbreviation": "More on Inheritance Hierarchies with Exceptions", "Title": "Default Theories and Inferential Distance", "Abstract": "In Artificial Intelligence, well-understood reasoning systems and tractable reasoning systems have often seemed mutually exclusive. This has been exemplified by nonmonotonic reasoning formalisms and inheritance-with-exceptions reasoners. These have epitomized the two extremes: the former not even semidecidable, the latter completely ad hoc. We previously presented a formal mechanism for specifying inheritance systems, and minimal criteria for acceptable inheritance reasoning. This left open the problem of realizing an acceptable reasoner. Since then, Touretzky has developed a reasoner that appears to meet our criteria. We show that his reasoner is formally adequate, and explore some of the implications of this result vis-a-vis the study of nonmonotonic reasoning."}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "AAAI", "Abbreviation": "Circumscriptive Theories", "Title": "A Logic-Based Framework for Knowledge Representation, Preliminary Report", "Abstract": "The use of circumscription for formalizing commonsense knowledge and reasoning requires that a circumscription policy be selected for each particular application: we should specify which predicates are circumscribed, which predicates and functions are allowed to vary, what priorities between the circumscribed predicates are established, etc. The circumscription policy is usually described either informally or using suitable metamathematical notation. In this paper we propose a simple and general formalism which permits describing circumscription policies by axioms, included in the knowledge base along with the axioms describing the objects of reasoning. This method allows us to formalize some important forms of metalevel reasoning in the circumscriptive theory itself."}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "AAAI", "Abbreviation": "TAXI", "Title": "A Taxonomic Assistant", "Abstract": "The task of constructing knowledge bases is a difficult one due to their size and complexity. A useful aid for this task would be a system which has both knowledge about a particular knowledge representation scheme and tools with which to manipulate the representation’s components. Such a system would be a knowledge maniupulation system (KMS). This paper describes a KMS called TAXI which is used to manipulate knowledge in the form of a taxonomic knowledge representation scheme. The particular taxonomic representation used is discussed, along with support for the usefulness of a KMS for this particular representation scheme. Tools provided in the TAXI system are described, as are possible applications for the system."}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "AAAI", "Abbreviation": "All I Know", "Title": "An Abridged Report", "Abstract": "Current approaches to formalizing non-monotonic reasoning using logics of belief require new metalogical properties over sets of sentences to be defined. This research attempts to show how some of these patterns of reasoning can be captured using only the classical notions of logic (satisfiability, validity, implication). This is done by extending a logic of belief so that it is possible to say that only a certain proposition (or finite set of them) is believed. This research also extends previous approaches to handle quantifiers and equality, provides a semantic account of certain types of non-monotonicity, and through a simple proof theory, allows formal derivations to be generated."}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "AAAI", "Abbreviation": "Assimilation", "Title": "A Strategy for Implementing Self-Reorganizing Knowledge Bases", "Abstract": "Assimilation is a process by which a knowledge base restructures itself to improve the organization of and access to information in the base. This paper presents a strategy for implementing assimilation in propositional knowledge bases which distinguish between the axioms of the system’s knowledge (called the context) and the derived consequences of those axioms (called the belief space). The strategy in question takes advantage of housekeeping phases in which the system discards accumulated clutter to discover useful patterns of access on the basis of which the context can be reorganized. Unused axioms are replaced by their more useful consequences; derivable generalizations that shorten common inference paths are added to the belief space."}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "AAAI", "Abbreviation": "PROLEARN", "Title": "Towards a Prolog Interpreter that Learns", "Abstract": "An adaptive interpreter for a programming language adapts to particular applications by learning from execution experience. This paper describes PROLEARN, a prototype adaptive interpreter for a subset of Prolog. It uses two methods to speed up a given program: explanation-based generalization and partial evaluation. The generalization of computed results differentiates PROLEARN from programs that cache and reuse specific values. We illustrate PROLEARN on several simple programs and evaluate its capabilities and limitations. The effects of adding a learning component to Prolog can be summarized as follows: the more search and subroutine calling in the original query, the more speedup after learning; a learned subroutine may slow down queries that match its head but fail its body."}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "AAAI", "Abbreviation": "BAGGER", "Title": "An EBL System that Extends and Generalizes Explanations", "Abstract": "This paper addresses the important issue in explanation-based learning of generalizing number. Most research in explanation-based learning involves relaxing constraints on the variables in an explanation, rather than generalizing the number of inference rules used. However, many concepts require generalizing the structure of the explanation. An explanation-based approach to the problem of generalizing to N is presented. The fully-implemented BAGGER system analyzes explanation structures and detects extendible repeated, inter-dependent applications of rules. When any are found, the explanation is extended is extended so that an arbitrary number of repeated applications of the original rule are supported. The final structure is then generalized and a new rule produced. An important property of the extended rules is that their preconditions are expressed in terms of the initial state - they do not depend on the results of intermediate applications of the original rule. To illustrate the approach, portions of several situation calculus examples from the blocks world are analyzed. The approach presented leads to the acquisition of efficient plans that can be used to clear an object directly supporting an arbitrary number of other objects, build towers of arbitrary height, and unstack towers containing any number of blocks."}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "AAAI", "Abbreviation": "UNITRAN", "Title": "An Interlingual Approach to Machine Translation", "Abstract": "Machine translation has been a particularly difficult problem in the area of Natural Language Processing for over two decades. Early approaches to translation failed in part because interaction effects of complex phenomena made translation appear to be unmanageable. Later approaches to the problem have succeeded but are based on many language-specific rules. To capture all natural language phenomena, rule-based systems require an overwhelming number of rules; thus, such translation systems either have limited coverage, or poor performance due to formidable grammar size. This paper presents an implementation of an \"interlingual\" approach to natural language translation. The UNITRAN system relies on principle-based descriptions of grammar rather than rule-oriented descriptions. The model is based on linguistically motivated principles and their associated parameters of variation. Because a few principles cover all languages, the unmanageable grammar size of alternative approaches is no longer a problem."}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "AAAI", "Abbreviation": "Porting and Extensible Natural Language Interface", "Title": "A Case History", "Abstract": "The KING KONG linguistic interface was developed at MITRE to be a portable natural Ianguage interface for expert systems. It is possible to port KING KONG from one expert system to another without writing more than a modest amount of code, regardless of backend architecture. We describe porting it from its original expert system backend to another expert system which was radically different in domain and representation."}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "AAAI", "Abbreviation": "PROMPT", "Title": "An Innovative Design Tool", "Abstract": "We describe a system, Prompt, used to design physical systems. Prompt employs a multi-level approach to design. When simple constraint propagation over prototypes [Adda85] fails, Prompt can significantly modify prototypes by reasoning about their structure and physics. Prompt derives the behavior of a prototype from its structure using knowledge of physics stored in a Graph of Models. It then uses heuristics called Modification operators to control the process of modifying the prototypes. In this paper we describe how our system works in the domain of structural design. We describe the kinds of analysis Prompt performs on beams and how it makes innovative changes to prototypes."}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "AAAI", "Abbreviation": "Troubleshooting", "Title": "When Modeling Is the Trouble", "Abstract": "This paper shows how order of magnitude reasoning has been successfully used for troubleshooting complex analog circuits. The originality of this approach was to be able to remove the gap between the information required to apply a general theory of diagnosis and the limited information actually available. The expert’s ability to detect a defect by reasoning about the significant changes in behavior it induces is extensively exploited here: as a kind of reasoning that justifies the qualitative modeling, as a heuristic that defines a strategy and as a working hypothesis that makes clear the scope of this approach."}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "AAAI", "Abbreviation": "Energy Constraints on Deformable Models", "Title": "Recovering Shape and Non-Rigid Motion", "Abstract": "We propose a paradigm for shape and motion reconstruction based on dynamic energy constraints. Objects are modeled as deformable elastic bodies and constraints derived from image data are modeled as external forces applied to these bodies. The external constraint forces are designed to mold a deformable body into a configuration that satisfies the constraints, making the model consistent with the images. We present a particular shape model whose internal forces induce a preference for surface continuity and axial symmetry. We develop a constraint force for dynamic stereo images and present results for the recovery of shape and non-rigid motion from natural imagery."}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "AAAI", "Abbreviation": "Perceptual Significance Hierarchy", "Title": "A Computer Vision Theory for Color Separation", "Abstract": "A Perceptual Significance Hierarchy (PSH) for line art images is developed which represents the relative perceptual significance of each image component. This is possible through the use of a set of image-features which are used by the human visual system. The PSH and related rho-space computer vision algorithms can be used to automate the fake color separation process used by the printing industry. This is accomplished by adding rudimentary visual processing capabiliities to a computer graphics system."}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "AAAI", "Abbreviation": "Data Validation during Diagnosis", "Title": "A Step beyond Traditional Sensor Validation", "Abstract": "A well known problem in diagnosis is the difficulty of providing correct diagnostic conclusions in light incorrect or missing data. Traditional approaches to solving this problem, as typified in the domains of various complex mechanical systems, validate data by using various kinds of redundancy in sensor hardware. While such techniques are useful, we propose that another level of redundancy exists beyond the hardware level, the redundancy provided by expectations derived during diagnosis. That is, in the process of exploring the space of possible malfunctions, initial data and intermediate conclusions set up expectations of the characteristics of the final answer. These expectations then provide a basis for judging the validity of the derived answer. We will show how such expectation- based data validation is a natural part of diagnosis as performed by hierarchical classification expert systems."}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "AAAI", "Abbreviation": "MU", "Title": "A Development Environment for Prospective Reasoning Systems", "Abstract": "We describe a style of problem solving, prospective reasoning, and a development environment, MU, for building prospective reasoning systems. Prospective reasoning is a form of planning in which knowledge of the state of the world and the effects of actions is incomplete. We illustrate one implementation of prospective reasoning in MU with examples from medical diagnosis."}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "AAAI", "Abbreviation": "TEST", "Title": "A Model-Driven Application Shell", "Abstract": "TEST (Troubleshooting Expert System Tool) is an application shell that provides a domain-independent diagnostic problem solver together with a library of schematic prototypes. TEST fills a design niche halfway between rule-based and causal-model approaches. This approach has resulted in a design that meets several functional requirements for an effective troubleshooting shell. Most critically, TEST can represent both the impact of failure-modes on a machine or system of interest, as well as the heurist!c problem-solving behavior which can lead to rapid conclusions. This paper provides an overview of TEST’s approach to diagnosis. As a special purpose application shell, TEST provides considerably more leverage to developers than can be gained through the use of general purpose heuristic classification systems."}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "AAAI", "Abbreviation": "Assessing the Maintainability of XCON-in-RlME", "Title": "Coping with the Problems of a VERY Large Rule-Base", "Abstract": "XCON is a rule-based expert system that configures computer systems. Over 7 years, XCON has grown to 6,200 rules, of which approximately 50% change every year. While the performance of XCON is satisfactory, it is increasingly becoming more difficult to change. With the goal of facilitating maintenance, DEC has developed a new rule-based language, RIME, in which the successor to XCON, XCON-in-RIME, is being written. This paper evaluates the potential for enhanced maintainability of XCON-in-RIME over XCON."}
{"Type": "conference", "Year": "1987", "Area": "AI", "Where": "AAAI", "Abbreviation": "Design as Refinement Plus Constraint Propagation", "Title": "The VEXED Experience", "Abstract": "Underlying any system that does design is a model of the design process and a division of labor between the system and the user. We are just beginning to understand what the main alternative models are, what their strengths and weaknesses are, and for which domains and tasks each is appropriate. The research reported here is an attempt to further that understanding by studying a particular model, the model of design as top down refinement plus constraint propagation, with the user making control decisions and the system carrying them out. We have studied this model by embodying it in VEXED, a design aid for NMOS digital circuits, and by experimenting with this system. Our primary conclusion is that this model needs further elaboration, but seems like a good basic model on which to build such systems."}
