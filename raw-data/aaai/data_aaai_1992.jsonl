{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "AAAI", "Abbreviation": "Constrained Intelligent Action", "Title": "Planning Under the Influence of a Master Agent", "Abstract": "In this paper we analyze a particular model of control among intelligent agents, that of non-absolute control. Non-absolute control involves a \"supervisor\" agent that issues orders to a \"subordinate\" agent. An example might be a human agent on Earth directing the activities of a Mars-based semi-autonomous vehicle. Both agents operate with essentially the same goals. The subordinate agent, however, is assumed to have access to some information that the supervisor does not have. The agent is thus expected to exercise its judgment in following orders (i.e., following the true intent of the supervisor, to the best of its ability). After presenting our model, we discuss the planning problem: how would a subordinate agent choose among alternative plans? Our solutions focus on evaluating the distance between candidate plans."}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "AAAI", "Abbreviation": "Reactive Navigation through Rough Terrain", "Title": "Experimental Results", "Abstract": "This paper describes a series of experiments that were performed on the Rocky III robot. Rocky III is a small autonomous rover capable of navigating through rough outdoor terrain to a predesignated area, searching that area for soft soil, acquiring a soil sample, and depositing the sample in a container at its home base. The robot is programmed according to a reactive behavior-control paradigm using the ALFA programming language. This style of programming produces robust autonomous performance while requiring significantly less computational resources than more traditional mobile robot control systems. The code for Rocky III runs on an 8-bit processor and uses about 10k of memory."}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "AAAI", "Abbreviation": "Learning 10,000 Chunks", "Title": "What’s It Like Out There?", "Abstract": "This paper describes an initial exploration into large learning systems, i.e., systems that learn a large number of rules. Given the well-known utility problem in learning systems, efficiency questions are a major concern. But the questions are much broader than just efficiency, e.g., will the effectiveness of the learned rules change with scale? This investigation uses a single problem-solving and learning system, Dispatcher-Soar, to begin to get answers to these questions. Dispatcher-Soar has currently learned 10,112 new productions, on top of an initial system of 1,819 productions, so its total size is 11,931 productions. This represents one of the largest production systems in existence, and by far the largest number of rules ever learned by an AI system. This paper presents a variety of data from our experiments with Dispatcher-Soar and raises important questions for large learning systems."}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "AAAI", "Abbreviation": "Mega-Classification", "Title": "Discovering Motifs in Massive Datastreams", "Abstract": "We report on the development and application of an efficient unsupervised learning procedure for the classification of an unsegmented datastream, given a set of probabilistic binary similarity judgments between regions in the stream. Our method is effective on very large databases, and tolerates the presence of noise in the similarity judgements and in the extents of similar regions. We applied this method to the problem of finding the sequence-level building blocks of proteins. After verifying the effectiveuess of the clusterer by testing it on synthetic protein data with known evolutionary history, we applied the method to a large protein sequence database (a datastream of more than IO^7 elements) and found about 10,000 protein sequence classes. The motifs defined by these classes are of biological interest, and have the potential to supplement or replace the existing manual annotation of protein sequence databases."}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "AAAI", "Abbreviation": "Building Large-Scale and Corporate-Wide Case-Based Systems", "Title": "Integration of the Organizational and Machine Executable Algorithms", "Abstract": "This paper reports a case study on a large-scale and corporate-wide case-based system. Unlike most papers for the AAAI conference, which exclusively focus on algorithms and models executed on computer systems, this paper heavily involves organizational activities and structures as a part of algorithms in the system. It is our claim that successful corporate-wide deployment of the case-base system must involve organizational efforts as a part of an algorithmic loop in the system in a broad sense. We have established a corporate-wide case acquisition algorithm, which is performed by persons, and developed the SQUAD Software Quality Control Advisor system, which facilitates sharing and spreading of experiences corporate-wide. The major findings were that the key for the success is not necessary in complex and sophisticated AI theories, in fact, we use very simple algorithms, but the integration of mechanisms and algorithms executed by machines and persons involved."}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "AAAI", "Abbreviation": "Reasoning as Remembering", "Title": "The Theory and Practice of CBR", "Abstract": "In this paper, we describe a design of wafer-scale integration for massively parallel memory-based reasoning (WSI-MBR). WSI-MBR attains about 2 million parallelism on a single 8 inch wafer using the state-of-the-art fabrication technologies. While WSI-MBR is specialized to memory-based reasoning, which is one of the mainstream approachs in massively parallel artificial intelligence research, the level of parallelism attained far surpasses any existing massively parallel hardware. Combination of memory array and analog weight computing circuits enable us to attain super high-density implementation with nanoseconds order inference time. Simulation results indicates that inherent robustness of the memory-based reasoning paradigm overcomes the possible precision degradation and fabrication defects in the wafer-scale integration. Also, the WSI-MBR provides a compact (desk-top size) massively parallel computing environment."}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "AAAI", "Abbreviation": "Symmetry as Bias", "Title": "Rediscovering Special Relativity", "Abstract": "This paper describes a rational reconstruction of Einstein’s discovery of special relativity, validated through an implementation: the Erlanger program. Einstein’s discovery of special relativity revolutionized both the content of physics and the research strategy used by theoretical physicists. This research strategy entails a mutual bootstrapping process between a hypothesis space for biases, defined through different postulated symmetries of the universe, and a hypothesis space for physical theories. The invariance principle mutually constrains these two spaces. The invariance principle enables detecting when an evolving physical theory becomes inconsistent with its bias, and also when the biases for theories describing different phenomena are inconsistent. Structural properties of the invariance principle facilitate generating a new bias when an inconsistency is detected. After a new bias is generated, this principle facilitates reformulating the old, inconsistent theory by treating the latter as a limiting approximation."}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "AAAI", "Abbreviation": "Discovery of Equations", "Title": "Experimental Evaluation of Convergence", "Abstract": "Systems that discover empirical equations from data require large scale testing to become a reliable research tool. In the central part of this paper we discuss two convergence tests for large scale evaluation of equation finders and we demonstrate that our system, which we introduce earlier, has the desired convergence properties. Our system can detect a broad range of equations useful in different sciences, and can be easily expanded by addition of new variable transformations. Previous systems, such as BACON or ABACUS, disregarded or oversimplified the problems of error analysis and error propagation, leading to paradoxical results and impeding the true world applications. Our system treats experimental error in a systematic and statistically sound manner. It propagates error to the transformed variables and assigns error to parameters in equations. It uses errors in weighted least squares fitting, in the evaluation of equations, including their acceptance, rejection and ranking, and uses parameter error to eliminate spurious parameters. The system detects equivalent terms (variables) and equations, and it removes the repetitions. This is important for convergence tests and system efficiency. Thanks to the modular structure, our system can be easily expanded, modified, and used to simulate other equation finders."}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "AAAI", "Abbreviation": "Operational Definition Refinement", "Title": "A Discovery Process", "Abstract": "Operational definitions link scientific attributes to experimental situations, prescribing for the experimenter the actions and measurements needed to measure or control attribute values. While very important in real science, operational procedures have been neglected in machine discovery. We argue that in the preparatory stage of the empirical discovery process each operational definition must be adjusted to the experimental task at hand. This is done in the interest of error reduction and repeatability of measurements. Both small error and high repeatability are instrumental in theory formation. We demonstrate that operational procedure refinement is a discovery process that resembles the discovery of scientific laws. We demonstrate how the discovery task can be reduced to an application of the FAHRENHEIT discovery system. A new type of independent variables, the experiment refinement variables, have been introduced to make the application of FAHRENHEIT theoretically valid. This new extension to FAHRENHEIT uses simple operational procedures, as well as the system’s experimentation and theory formation capabilities to collect real data in a science laboratory and to build theories of error and repeatability that are used to refine the operational procedures. We present the application of FAHRENHEIT in the context of dispensing liquids in a chemistry laboratory."}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "AAAI", "Abbreviation": "ChiMerge", "Title": "Discretization of Numeric Attributes", "Abstract": "Many classification algorithms require that the training data contain only discrete attributes. To use such an algorithm when there are numeric attributes, all numeric values must first be converted into discrete values-a process called discretization. This paper describes ChiMerge, a general, robust algorithm that uses the x2 statistic to discretize (quantize) numeric attributes."}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "AAAI", "Abbreviation": "The Feature Selection Problem", "Title": "Traditional Methods and a New Algorithm", "Abstract": "For real-world concept learning problems, feature selection is important to speed up learning and to improve concept quality. We review and analyze past approaches to feature selection and note their strengths and weaknesses. We then introduce and theoretically examine a new algorithm Relief which selects relevant features using a statistical method. Relief does not depend on heuristics, is accurate even if features interact, and is noise-tolerant. It requires only linear time in the number of given features and the number of training instances, regardless of the target concept complexity. The algorithm also has certain limitations such as non-optimal feature set size. Ways to overcome the limitations are suggested. We also report the test results of comparison between Relief and other feature selection algorithms. The empirical results support the theoretical analysis, suggesting a practical approach to feature selection for real-world problems."}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "AAAI", "Abbreviation": "COGIN", "Title": "Symbolic Induction with Genetic Algorithms", "Abstract": "COGIN is a system designed for induction of symbolic decision models from pre-classed examples based on the use of genetic algorithms (GAS). Much research in symbolic induction has focused on techniques for reducing classification inaccuracies that arise from inherent limits of underlying incremental search techniques. Genetic Algorithms offer an intriguing alternative to step-wise model construction, relying instead on model evolution through global competition. The difficulty is in providing an effective framework for the GA to be practically applied to complex induction problems. COGIN merges traditional induction concepts with genetic search to provide such a framework, and recent experimental results have demonstrated its advantage relative to basic stepwise inductive approaches. In this paper, we describe the essential elements of the COGIN approach and present a favorable comparison of COGIN results with those produced by a more sophisticated stepwise approach (with support post processing) on standardized multiplexor problems."}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "AAAI", "Abbreviation": "Using Knowledge-Based Neural Networks to Improve Algorithms", "Title": "Refining the Chou-Fasman Algorithm for Protein Folding", "Abstract": "We describe a method for using machine learning to refine algorithms represented as generalized finite-state automata. The knowledge in an automaton is translated into an artificial neural network, and then refined with backpropagation on a set of examples. Our technique for translating an automaton into a network extends KBANN, a system that translates a set of propositional rules into a corresponding neural network. The extended system, FSKBANN, allows one to refine the large class of algorithms that can be represented as state-based processes. As a test, we use FSKBANN to refine the Chou-Fasman algorithm, a method for predicting how globular proteins fold. Empirical evidence shows the refined algorithm FSKBANN produces is statistically significantly more accurate than both the original Chou-Fasman algorithm and a neural network trained using the standard approach."}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "AAAI", "Abbreviation": "Adapting Bias by Gradient Descent", "Title": "An Incremental Version of Delta-Bar-Delta", "Abstract": "Appropriate bias is widely viewed as the key to efficient learning and generalization. I present a new algorithm, the Incremental Delta-Bar-Delta (IDBD) algorithm, for the learning of appropriate biases based on previous learning experience. The IDBD algorithm is developed for the case of a simple, linear learning system-the LMS or delta rule with a separate learning-rate parameter for each input. The IDBD algorithm adjusts the learning-rate parameters, which are an important form of bias for this system. Because bias in this approach is adapted based on previous learning experience, the appropriate testbeds are drifting or non-stationary learning tasks. For particular tasks of this type, I show that the IDBD algorithm performs better than ordinary LMS and in fact finds the optimal learning rates. The IDBD algorithm extends and improves over prior work by Jacobs and by me in that it is fully incremental and has only a single free parameter. This paper also extends previous work by presenting a derivation of the IDBD algorithm as gradient descent in the space of learning-rate parameters. Finally, I offer a novel interpretation of the IDBD algorithm as an incremental form of hold-one-out cross validation."}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "AAAI", "Abbreviation": "Reinforcement Learning with Perceptual Aliasing", "Title": "The Perceptual Distinctions Approach", "Abstract": "It is known that Perceptual Aliasing may significantly diminish the effectiveness of reinforcement learning algorithms [Whitehead and Ballard, 1991]. Perceptual aliasing occurs when multiple situations that are indistinguishable from immediate perceptual input require different responses from the system. For example, if a robot can only see forward, yet the presence of a battery charger behind it determines whether or not it should backup, immediate perception alone is insufficient for determining the most appropriate action. It is problematic since reinforcement algorithms typically learn a control policy from immediate perceptual input to the optimal choice of action. This paper introduces the predictive distinctions approach to compensate for perceptual aliasing caused from incomplete perception of the world. An additional component, a predictive model, is utilized to track aspects of the world that may not be visible at all times. In addition to the control policy, the model must also be learned, and to allow for stochastic actions and noisy perception, a probabilistic model is learned from experience. In the process, the system must discover, on its own, the important distinctions in the world. Experimental results are given for a simple simulated domain, and additional issues are discussed."}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "AAAI", "Abbreviation": "Acquisition of Automatic Activity through Practice", "Title": "Changes in Sensory Input", "Abstract": "This paper will present computer models of three robotic motion planning and learning systems which use a multi-sensory learning strategy for learning and control. In these systems machine vision input is used to plan and execute movements utilizing an algorithmic controller while at the same time neural networks learn the control of those motions using feedback provided by position and velocity sensors in the actuators. A specific advantage of this approach is that, in addition to the system learning a more automatic behavior, it employs a computationally less costly sensory system more tightly coupled from perception to action."}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "AAAI", "Abbreviation": "COMPOSER", "Title": "A Probabilistic Solution to the Utility Problem in Speed-Up Learning", "Abstract": "In machine learning there is considerable interest in techniques which improve planning ability. Initial investigations have identified a wide variety of techniques to address this issue. Progress has been hampered by the utility problem, a basic tradeoff between the benefit of learned knowledge and the cost to locate and apply relevant knowledge. In this paper we describe the COMPOSER system which embodies a probabilistic solution to the utility problem. We outline the statistical foundations of our approach and compare it against four other approaches which appear in the literature."}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "AAAI", "Abbreviation": "Parsing Run Amok", "Title": "Relation-Driven Control for Text Analysis", "Abstract": "Traditional syntactic models of parsing have been inadequate for task-driven processing of extended text, because they spend most of their time on misdirected linguistic analysis, leading to problems with both efficiency and coverage. Statistical and domain-driven processing offer compelling possibilities, but only as a complement to syntactic processing. For semantically-oriented tasks such as data extraction from text, the problem is how to combine the coverage of these \"weaker\" methods with the detail and accuracy of traditional lingusitic analysis. A good approach is to focus linguistic analysis on relations that directly impact the semantic results, detaching these relations from the complete constituents to which they belong. This approach results in a faster, more robust, and potentially more accura.te parser for real text."}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "AAAI", "Abbreviation": "Shipping Departments vs. Shipping Pacemakers", "Title": "Using Thematic Analysis to Improve Tagging Accuracy", "Abstract": "Thematic analysis is best manifested by contrasting collocations such as \"shipping pacemakers\" vs. \"shipping departments\". While in the first pair, the pacemakers are being shipped, in the second one, the departments are probably engaged in some shipping activity, but are not being shipped. Text pre-processors, intended to inject corpus-based intuition into the parsing process, must adequately distinguish between such cases. Although statistical tagging [Church et al., 1989; Meteer et al., 1991; Brill, 1992; Cutting et al., 1992] has attained impressive results overall, the analysis of multiple-content-word strings (i.e., collocations) has presented a weakness, and caused accuracy degradation. To provide acceptable coverage (i.e., 90% of collocations), a tagger must have accessible a large database ( i.e., 250,000 pairs) of individually analyzed collocations. Consequently, training must be based on a corpus ranging well over 50 million words. Since such large corpus does not exist in a tagged form, training must be from raw corpus. In this paper we present an algorithm for text tagging based on thematic analysis. The algorithm yields high-accuracy results. We provide empirical results: The program NLcp (NL corpus processing) acquired a 250,000 thematic-relation database through the 85-million word Wall-Street Journal Corpus. It was tested over the Tipster 66,000-word Joint-Venture corpus."}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "AAAI", "Abbreviation": "Learning from Goal Interactions in Planning", "Title": "Goal Stack Analysis and Generalization", "Abstract": "This paper presents a methodology which enables the derivation of goal ordering rules from the analysis of problem failures. We examine all the planning actions that lead to failures. If there are restrictions imposed by a problem state on taking possible actions, the restrictions manifest themselves in the form of a restricted set of possible bindings. Our method makes use of this observation to derive general control rules which are guaranteed to be correct. The overhead involved in learning is low because our method examines only the goal stacks retrieved from the leaf nodes of a failure search tree rather than the whole tree. Empirical tests show that the rules derived by our system PAL, after sufficient training, performs as well as or better than those derived by systems such as PRODIGY/EBL and STATIC."}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "AAAI", "Abbreviation": "Improved Decision-Making in Game Trees", "Title": "Recovering from Pathology", "Abstract": "In this paper we address the problem of making correct decisions in the context of game-playing. Specifically, we address the problem of reducing or eliminating pathology in game trees. However, the framework used in the paper applies to decision making that depends on evaluating complex Boolean expressions. The main contribution of this paper is in casting general evaluation of game trees as belief propagation in causal trees. This allows us to draw several theoretically and practically interesting corollaries. In the Bayesian framework we typically do not want to ignore any evidence, even if it may be inaccurate. Therefore, we evaluate the game tree on several levels rather than just the deepest one. Choosing the correct move in a game can be implemented in a straightforward fashion by an efficient linear-time algorithm adapted from the procedure for belief propagation in causal trees. We propose a probabilistically sound heuristic that allows us to reduce the effects of pathology significantly."}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "AAAI", "Abbreviation": "Modeling Accounting Systems to Support Multiple Tasks", "Title": "A Progress Report", "Abstract": "A domain model in SAVILE represents the steps involved in producing and processing financial data in a company, using an ontology appropriate for several reasoning tasks in accounting and auditing. SAVILE is an implemented program that demonstrates the adequacy and appropriateness of this ontology of financial data processing for evaluating internal controls, designing tests, and other audit planning related tasks. This paper discusses the rationale, syntax, semantics, and implementation of the ontology as it stands today."}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "AAAI", "Abbreviation": "Linear-Space Best-First Search", "Title": "Summary of Results", "Abstract": "Best-first search is a general search algorithm that, always expands next a frontier node of lowest cost. Its applicability, however, is limited by its exponential memory requirement. Iterative deepening, a previous approach to this problem, does not expand nodes in best-first order if the cost function can decrease along a path. We present a linear-space best-first search algorithm (RBFS) that always explores new nodes in best-first order, regardless of the cost function, and expands fewer nodes than iterative deepening with a nondecreasing cost function. On the sliding-tile puzzles, RBFS with a weighted evaluation function dramatically reduces computation time with only a small penalty in solution cost. In general, RBFS reduces the space complexity of best-first search from exponential to linear, at the cost of only constant factor in time complexity in our experiments."}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "AAAI", "Abbreviation": "An Average-Case Analysis of Branch-and-Bound with Applications", "Title": "Summary of Results", "Abstract": "Motivated by an anomaly in branch-and-bound (BnB) search, we analyze its average-case complexity. We first delineate exponential vs polynomial average-case complexities of BnB. When best-first BnB is of linear complexity, we show that depth-first BnB has polynomial complexity. For problems on which best-first BnB haa exponential complexity, we obtain an expression for the heuristic branching factor. Next, we apply our analysis to explain an anomaly in lookahead search on sliding-tile puzzles, and to predict the existence of an average-case complexity transition of BnB on the Asymmetric Traveling Salesman Problem. Finally, by formulating IDA* as costbounded BnB, we show its aaverage-case optimality, which also implies that RBFS is optimal on average."}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "AAAI", "Abbreviation": "Formalizing Reasoning about Change", "Title": "A Qualitative Reasoning Approach (Preliminary Report)", "Abstract": "The development of a formal logic for reasoning about change has proven to be surprisingly difficult. Furthermore, the logics that have been developed have found surprisingly little application in those fields, such as Qualitative Reasoning, that are concerned with building programs that emulate human common-sense reasoning about change. In this paper, we argue that a basic tenet of qualitative reasoning practice-the separation of modeling and simulation-obviates many of the difficulties faced by previous attempts to formalize reasoning about change. Our analysis helps explain why the QR community has been nonplussed by some of the problems studied in the nonmonotonic reasoning community. Further, the formalism we present provides both the beginnings of a formal foundation for qualitative reasoning, and a framework in which to study a number of open problems in qualitative reasoning."}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "AAAI", "Abbreviation": "A Logic of Knowledge and Belief for Recursive Modeling", "Title": "A Preliminary Report", "Abstract": "To make informed decisions in a multiagent environment, an agent needs to model itself, the world, and the other agents, including the models that those other agents might be employing. We present a framework for recursive modeling that uses possible worlds semantics, and is based on extending the Kripke structure so that an agent can model the information it thinks that another agent has in each of the possible worlds, which in turn can be modeled with Kripke structures. Using recursive nesting, we can define the propositional attitudes of agents to distinguish between the concepts of knowledge and belief. Through the Three Wise Men example, we show how our framework is useful for deductive reasoning, and we suggest that it might provide a meeting ground between decision theoretic and deductive methods for multiagent reasoning."}
{"Type": "conference", "Year": "1992", "Area": "AI", "Where": "AAAI", "Abbreviation": "Self-Explanatory Simulations", "Title": "Scaling Up to Large Models", "Abstract": "Qualitative reasoners have been hamstrung by the inability to analyze large models. This includes self-explanatory simulators, which tightly integrate qualitative and numerical models to provide both precision and explanatory power. While they have important potential applications in training, instruction, and conceptual design, a critical step towards realizing this potential is the ability to build simulators for medium-sized systems (i.e., on the order of ten to twenty independent parameters). This paper describes a new method for developing self-explanatory simulators which scales up. While our method involves qualitative analysis, it does not rely on envisioning or any other form of qualitative simulation. We describe the results of an implemented system which uses this method, and analyze its limitations and potential."}
