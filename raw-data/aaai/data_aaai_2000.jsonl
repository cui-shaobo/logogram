{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "Maintainability", "Title": "A Weaker Stabilizability Like Notion for High Level Control", "Abstract": "The goal of most agents is not just to reach a goal state, but rather also (or alternatively) to put restrictions on its trajectory, in terms of states it must avoid and goals that it must maintain. This is analogous to the notions of `safety' and `stability' in the discrete event systems and temporal logic community. In this paper we argue that the notion of `stability' is too strong for formulating `maintenance' goals of an agent -- in particular, reactive and software agents, and give examples of such agents. We present a weaker notion of `maintainability' and show that our agents which do not satisfy the stability criteria, do satisfy the weaker criteria. We give algorithms to test maintainability, and also to generate control for maintainability. We then develop the notion of `supportability' that generalizes both `maintainability' and `stabilizability, develop an automata theory that distinguishes between exogenous and control actions, and develop a temporal logic based on it."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "Agent Capabilities", "Title": "Extending BDI Theory", "Abstract": "This paper presents a formalisation of capabilities within the framework of beliefs, goals and intentions (BDI) and indicates how capabilities can affect agent reasoning about its intentions. We define a style of agent commitment which we refer to as a self-aware agent which allows an agent to modify its goals and intentions as its capabilities change. We also indicate which aspects of the specification of a BDI interpreter are affected by the introduction of capabilities and give some indications of additional reasoning which could be incorporated into an agent system on the basis of both the theoretical analysis and the existing implementation. The introduction of capabilities in the BDI framework has a number of advantages such as a better mapping of theory to intuition, the elimination of a mismatch between theory and actual systems, and an indication of issues and areas for further development of implemented reasoning in agent systems."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "Iterative Combinatorial Auctions", "Title": "Theory and Practice", "Abstract": "Combinatorial auctions, which allow agents to bid directly for bundles of resources, are necessary for optimal auction-based solutions to resource allocation problems with agents that have non-additive values for resources, such as distributed scheduling and task assignment problems. We introduce iBundle, the first iterative combinatorial auction that is optimal for a reasonable agent bidding strategy, in this case myopic best-response bidding. Its optimality is proved with a novel connection to primal-dual optimization theory. We demonstrate orders of magnitude performance improvements over the only other known optimal combinatorial auction, the Generalized Vickrey Auction."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "Preventing Strategic Manipulation in Iterative Auctions", "Title": "Proxy Agents and Price-Adjustment", "Abstract": "Iterative auctions have many computational advantages over sealed-bid auctions, but can present new possibilities for strategic manipulation. We propose a two-stage technique to make iterative auctions that compute optimal allocations with myopic best-response bidding strategies more robust to manipulation. First, introduce proxy bidding agents to constrain bidding strategies to (possibly untruthful) myopic best-response. Second, after the auction terminates adjust the prices towards those given in the Vickrey auction, a sealed-bid auction in which truth-revelation is optimal. We present an application of this methodology to iBundle, an iterative combinatorial auction which gives optimal allocations for myopic best-response agents."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "Cobot in LambdaMOO", "Title": "A Social Statistics Agent", "Abstract": "We describe our development of Cobot, a software agent who lives in LambdaMOO, a popular virtual world frequented by hundreds of users. We present a detailed discussion of the functionality that has made him one of the objects most frequently interacted with in LambdaMOO, human or artificial."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "Deliberation in Equilibrium", "Title": "Bargaining in Computationally Complex Problems", "Abstract": "We develop a normative theory of interaction - negotiation in particular - among self-interested computationally limited agents where computational actions are game-theoretically treated as part of an agent’s strategy. We focus on a 2-agent setting where each agent has an intractable individual problem, and there is a potential gain from pooling the problems, giving rise to an intractable joint problem. At any time, an agent can compute to improve its solution to its problem, its opponent’s problem, or the joint problem. At a deadline the agents then decide whether to implement the joint solution, and if so, how to divide its value (or cost). We present a fully normative model for controlling anytime algorithms where each agent has statistical performance profiles which are optimally conditioned on the problem instance as well as on the path of results of the algorithm run so far. Using this model, we analyze the perfect Bayesian equilibria of the games which differ based on whether the performance profiles are deterministic or stochastic, whether the deadline is known or not, and whether the proposer is known in advance. Finally, we present algorithms for finding the equilibria."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "Self-Organization of Innate Face Preferences", "Title": "Could Genetics Be Expressed through Learning?", "Abstract": "Self-organizing models develop realistic cortical structures when given approximations of the visual environment as input, and are an effective way to model the development of face recognition abilities. However, environment-driven self-organization alone cannot account for the fact that newborn human infants will preferentially attend to face-like stimuli even immediately after birth. Recently it has been proposed that internally generated input patterns, such as those found in the developing retina and in PGO waves during REM sleep, may have the same effect on self-organization as does the external environment. Internal pattern generators constitute an efficient way to specify, develop, and maintain functionally appropriate perceptual organization. They may help express complex structures from minimal genetic information, and retain this genetic structure within a highly plastic system. Simulations with the RF-LISSOM model show that such preorganization can account for newborn face preferences, providing a computational framework for examining how genetic influences interact with experience to construct a complex system."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "Anchoring Symbols to Sensor Data", "Title": "Preliminary Report", "Abstract": "Anchoring is the process of creating and maintaining the correspondence between symbols and percepts that refer to the same physical objects. Although this process must necessarily be present in any symbolic reasoning system embedded in a physical environment (e.g., an autonomous robot), no systematic study of anchoring as a clearly separated problem has been reported in the intelligent system community. In this paper, we propose a domain-independent definition of the anchoring problem, and identify its three basic functionalities: find, reacquire, and track. We illustrate our definition on two working systems in two different domains: an unmanned airborne vehicle for aerial surveillance; and a mobile robot for office navigation"}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "Reading a Robot’s Mind", "Title": "A Model of Utterance Understanding Based on the Theory of Mind Mechanism", "Abstract": "The purpose of this paper is to construct a methodology for smooth communications between humans and robots. Here, focus is on a mindreading mechanism, which is indispensable in human-human communications. We propose a model of utterance understanding based on this mechanism. Concretely speaking, we apply the model of a mindreading system [Baron-Cohen 96] to a model of human-robot communications. Moreover, we implement a robot interface system that applies our proposed model. Psychological experiments were carried out to explore the validity of the following hypothesis: by reading a robot’s mind, a human can estimate the robot’s intention with ease, and, moreover, the person can even understand the robot’s unclear utterances made by synthesized speech sounds. The results of the experiments statistically supported our hypothesis."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "The Game of Hex", "Title": "An Automatic Theorem Proving Approach to Game Programming", "Abstract": "The game of Hex is a two-player game with simple rules, a deep underlying mathematical beauty, and a strategic complexity comparable to that of Chess and Go. The massive game-tree search techniques developed mostly for Chess, and successfully used for Checkers, Othello, and a number of other games, become less useful for games with large branching factors like Go and Hex. We offer a new approach, which results in superior playing strength. This approach emphasizes deep analysis of relatively few game positions. In order to reach this goal, we develop an automatic theorem proving technique for topological analysis of Hex positions. We also discuss in detail an idea of modeling Hex positions with electrical resistor circuits. We explain how this approach is implemented in Hexy - the strongest known Hex-playing computer program, able to compete with best human players."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "Acquiring Problem-Solving Knowledge from End Users", "Title": "Putting Interdependency Models to the Test", "Abstract": "Developing tools that allow non-programmers to enter knowledge has been an ongoing challenge for AI. In recent years researchers have investigated a variety of promising approaches to knowledge acquisition (KA), but they have often been driven by the needs of knowledge engineers rather than by end users. This paper reports on a series of experiments that we conducted in order to understand how far a particular KA tool that we are developing is from meeting the needs of end users, and to collect valuable feedback to motivate our future research. This KA tool, called EMeD, exploits Interdependency Models that relate individual components of the knowledge base in order to guide users in specifying problem-solving knowledge. We describe how our experiments helped us address several questions and hypotheses regarding the acquisition of problem-solving knowledge from end users and the benefits of Interdependency Models, and discuss what we learned in terms of improving not only our KA tools but also about KA research and experimental methodology."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "Predicting UNIX Command Lines", "Title": "Adjusting to User Patterns", "Abstract": "As every user has his own idiosyncrasies and preferences, an interface that is honed for one user may be problematic for another. To accommodate a diverse range of users, many computer applications therefore include an interface that can be customized --- e.g., by adjusting parameters, or defining macros. This allows each user to have his ``own'' version of the interface, honed to his specific preferences. However, most such interfaces require the user to perform this customization by hand -- a tedious process that requires the user to be aware of his personal preferences. We are therefore exploring adaptive interfaces, that can autonomously determine the user’s preference, and adjust the interface appropriately. This paper describes such an adaptive system --- here a UNIX-shell that can predict the user’s next command, and then use this prediction to simplify the user’s future interactions. These predictions are determined by combining the distributions learned by a set of relatively simple experts, each using its own type of information. In a series of experiments, on real-world data, we demonstrate that this system can correctly predict the user’s next command almost 50% of the time, and can do so robustly -- across a range of different users."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "ADVISOR", "Title": "A Machine Learning Architecture for Intelligent Tutor Construction", "Abstract": "We have constructed a two-agent machine learning architecture for intelligent tutoring systems (ITS). The purpose of this architecture is to centralize the reasoning of an ITS into a single component to allow customization of teaching goals and simplify performance improvements. The first agent is responsible for learning a model of how students perform using the tutor in a variety of contexts. The second agent is provided this model of student behavior and a goal specifying the desired educational objective. Reinforcement learning is used by this agent to derive a teaching policy that meets the specified educational goal. Component evaluation studies show each agent performs adequately in isolation. We have also conducted an evaluation with actual students of the complete architecture. Results show our architecture was successful in learning a teaching policy that met the educational objective provided. Although this set of machine learning agents has been integrated with a specific intelligent tutor, the general technique could be applied to a broad class of ITS."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "Social Choice Theory and Recommender Systems", "Title": "Analysis of the Axiomatic Foundations of Collaborative Filtering", "Abstract": "The growth of Internet commerce has stimulated the use of collaborative filtering (CF) algorithms as recommender systems. Such systems leverage knowledge about the behavior of multiple users to recommend items of interest to individual users. CF methods have been harnessed to make recommendations about such items as web pages, movies, books, and toys. Researchers have proposed several variations of the technology. We take the perspective of CF as a methodology for combining preferences. The preferences predicted for the end user is some function of all of the known preferences for everyone in a database. Social Choice theorists, concerned with the properties of voting methods, have been investigating preference aggregation for decades. At the heart of this body of work is Arrow’s result demonstrating the impossibility of combining preferences in a way that satisfies several desirable and innocuous-looking properties. We show that researchers working on CF algorithms often make similar assumptions. We elucidate these assumptions and extend results from Social Choice theory to CF methods. We show that only very restrictive CF functions are consistent with desirable aggregation properties. Finally, we discuss practical implications of these results."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "The Rules Behind Roles", "Title": "Identifying Speaker Role in Radio Broadcasts", "Abstract": "Previous work has shown that providing information about story structure is critical for browsing audio broadcasts. We investigate the hypothesis that Speaker Role is an important cue to story structure. We implement an algorithm that classifies story segments into three Speaker Roles based on several content and duration features. The algorithm correctly classifies about 80% of segments (compared with a baseline frequency of 36%) when applied to ASR derived transcriptions of broadcast data."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "Statistics-Based Summarization — Step One", "Title": "Sentence Compression", "Abstract": "We discuss the problem of generating text that preserves certain ambiguities, a capability that is useful in applications such as machine translation. We show that it is relatively simple to extend a hybrid symbolic/statistical generator to do ambiguity preservation. The paper gives algorithms and examples, and it discusses practical linguistic difficulties that arise in ambiguity preservation."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "Iterative Flattening", "Title": "A Scalable Method for Solving Multi-Capacity Scheduling Problems", "Abstract": "One challenge for research in constraint-based scheduling has been to produce scalable solution procedures under fairly general representational assumptions. Quite often, the computational burden of techniques for reasoning about more complex types of temporal and resource capacity constraints places fairly restrictive limits on the size of problems that can be effectively addressed. In this paper, we focus on developing a scalable heuristic procedure to an extended, multi-capacity resource version of the job shop scheduling problem (MCJSSP). Our starting point is a previously developed procedure for generating feasible solutions to more complex, multi-capacity scheduling problems with maximum time lags. Adapting the procedure to exploit the simpler temporal structure of MCJSSP, we are able to produce a quite efficient solution generator. However, the procedure only indirectly attends to MCJSSP’s objective criteria and produces sub-optimal solutions. To provide a scalable, optimizing procedure, we propose a simple, local-search procedure called {em iterative flattening}, which utilizes the core solution generator to perform an extended iterative improvement search. Despite its simplicity, experimental analysis shows the iterative improvement search to be quite effective. On a set of reference problems ranging in size from 100 to 900 activities, the iterative flattening procedure efficiently and consistently produces solutions within 10% of computed upper bounds. Overall, the concept of iterative flattening is quite general and provides an interesting new basis for designing more sophisticated local search procedures."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "Discovering State Constraints in DISCOPLAN", "Title": "Some New Results", "Abstract": "DISCOPLAN is an implemented set of efficient preplanning algorithms intended to enable faster domain-independent planning. It includes algorithms for discovering state constraints (invariants) that have been shown to be very useful, for example, for speeding up SAT-based planning. DISCOPLAN originally discovered only certain types of implicative constraints involving up to two fluent literals and any number of static literals, where one of the fluent literals contains all of the variables occurring in the other literals; only planning domains with strips-like operators were handled. We have now extended discoplan in several directions. We describe new techniques that handle operators with conditional effects, and enable discovery of several new types of constraints. Moreover, discovered constraints can be fed back into the discovery process to obtain additional constraints. Finally, we outline unimplemented (but provably correct) methods for discovering additional types of constraints, including XOR constraints, and constraints involving arbitrarily many fluent literals."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "TCBB Scheme", "Title": "Applications to Single Machine Job Sequencing Problems", "Abstract": "Transpose-and-Cache Branch-and-Bound (TCBB) has shown promise in solving large single machine quadratic penalty problems. There exist other classes of single machine job sequencing problems which are of more practical importance and which are also of considerable interest in the area of AI search. In the weighted earliness tardiness problem (WET), the best known heuristic estimate is not consistent; this is contrary to the general belief about relaxation-based heuristic. In the quadratic penalty problem involving setup times (SQP) of jobs, the evaluation function is non-order-preserving In this paper, we present the TCBB scheme to solve these problems as well. Experiments indicate that (i) for the WET problem, the TCBB scheme is highly effective in solving large problem instances and (ii) for the SQP problem, it can solve larger instances than algorithm GREC in a given available memory."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "RealPlan", "Title": "Decoupling Causal and Resource Reasoning in Planning", "Abstract": "Recent work has demonstrated that treating resource reasoning separately from causal reasoning can lead to improved planning performance and rational resource management where increase in resources does not degrade planning performance. However, the resources were scheduled procedurally and limited to cases that could be solved backtrack-free. Terming the decoupled framework as RealPlan, in this work, I extend it with a general approach to convert the resource allocation problem as a declaratively specified dynamic constraint satisfaction problem (DCSP), compile it into CSP and solve it with a CSP solver. By doing so, the resource scheduling problem can be handled in its full complexity and can provide a computational characterization of the different scheduling classes. The CSP formulation also facilitates planner-scheduler interaction by helping the scheduler interpret the resource allocation policies proposed by the planner in terms of constraints on values of scheduling variables. Moreover, if the extraction of causal plan is also formulated as a CSP problem, the two CSPs can enable dependency directed backtracking between them. I have implemented declarative scheduling on top of Graphplan and GP-CSP planners (which poses the backward search of Graphplan as a CSP problem), and the resulting planners reiterate the benefits of decoupling planning and scheduling while providing elegant CSP models (RealPlan-MS, RealPlan-PP) for investigating planner-scheduler communication."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "Multi-Fidelity Robotic Behaviors", "Title": "Acting with Variable State Information", "Abstract": "Our work is driven by one of the core purposes of artificial intelligence: to develop real robotic agents that achieve complex high-level goals in real-time environments. Robotic behaviors select actions as a function of the state of the robot and of the world. Designing robust and appropriate robotic behaviors is a well-recognized and difficult problem due to the noise, uncertainty and cost of acquiring the necessary state information. We addressed this challenge within the concrete domain of robotic soccer with the fully autonomous Sony legged robots. In this paper, we present one of the outcomes of this research: the introduction of multi-fidelity behaviors to explicitly and efficiently adapt to different levels of state information accuracy. The paper motivates and introduces our general approach and then reports on our concrete work with the Sony robots. The multi-fidelity behaviors we developed allow the robots to successfully achieve their goals in a dynamic and adversarial environment. A robot acts according to a set of behaviors that aggressively balance the cost of acquiring state information with the value of that information to the robot’s ability to achieve its high-level goals. The paper includes empirical experiments which support our method of balancing the cost and benefit of the incrementally-accurate state information."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "Property Mapping", "Title": "A Simple Technique for Mobile Robot Programming", "Abstract": "In this paper we turn to the mobile robot programming problem, which is a software engineering challenge that is not easily conquered using contemporary software engineering best practices. We propose robot observability as a measure of the diagnostic transparency of a situated robot program, then describe property mapping as a simple language-independent approach to implementing reliable robot programs by maximizing robot observability. Examples from working real-world robots are given in Lisp and Java."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "Dynamic Representations and Escaping Local Optima", "Title": "Improving Genetic Algorithms and Local Search", "Abstract": "Local search algorithms often get trapped in local optima.Algorithms such as tabu search and simulated annealing 'escape' local optima by accepting non-improving moves. Another possibility is to dynamically change between representations; a local optimum under one representation may not be a local optimum under another. emph{Shifting} is a mechanism which dynamically switches between Gray code representations in order to escape local optima. Gray codes are widely used in conjunction with genetic algorithms and bit-climbing algorithms for parameter optimization problems. We present new theoretical results that substantially improve our understanding of the shifting mechanism, on the number of Gray codes accessible via shifting, and on how neighborhood structure changes during shifting. We show that shifting can significantly improve the performance of a simple hill-climber; it can also help to improve one of the best genetic algorithms currently available."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "Depth-First Branch-and-Bound versus Local Search", "Title": "A Case Study", "Abstract": "Depth-first branch-and-bound (DFBnB) is a complete algorithm that is typically used to find optimal solutions of difficult combinatorial optimization problems. It can also be adapted to an approximation algorithm and run as an anytime algorithm. In this paper, we study DFBnB as an approximation and anytime algorithm. We compare DFBnB against the Kanellakis-Papadimitriou local search algorithm, the best known approximation algorithm, on the asymmetric Traveling Salesman Problem (ATSP), an important NP-hard problem. Our experimental results show that DFBnB significantly outperforms the local search on large ATSP and various ATSP structures, finding better solutions faster than the local search; and the quality of approximate solutions from a prematurely terminated DFBnB, called truncated DFBnB, is several times better than that from the local search."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "Behavior Acquisition and Classification", "Title": "A Case Study in Robotic Soccer", "Abstract": "Increasingly in domains with multiple intelligent agents, each agent must be able to identify what the other agents are doing. This is especially important when there are adversarial agents inferring with the accomplishment of goals. Once identified, the agents can then respond to recent strategies and adapt to improve performance. We present an approach to doing adaptation which relies on classification of the current adversary into predefined adversary classes. For feature extraction, we present a windowing technique to abstract useful but not overly complicated features. The feature extraction and classification steps are fully implemented in the domain of simulated robotic soccer, and experimental results are presented."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "MURDOCH", "Title": "Publish/Subscribe Task Allocation for Heterogeneous Agents", "Abstract": "In this paper, we describe a novel approach to the problem of dynamic task allocation among groups of heterogeneous agents. Specifically, we advocate the use of publish/subscribe messaging, a well-researched and commercially proven message brokering paradigm that is readily applicable to distributed control. We present Murdoch, an implemented publish-subscribe system, and explain how it can facilitate multi-robot coordination. The system allows the user a high-level interface for posing tasks to a group of autonomous homogeneous or heterogenous robots. Rather than assigning a task to an individual or a group, the user simply poses the task to the system as a whole (with no knowledge of individual agents)."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "Knowledge Representation on the Internet", "Title": "Achieving Interoperability in a Dynamic, Distributed Environment", "Abstract": "The Internet’s explosive growth is making it harder and harder to harness its potential. However, the field of knowledge representation, particularly the subfield of ontologies, can provide techniques for improving the ability of agents to work with Internet information. SHOE (Simple HTML Ontology Extensions) is a semantic markup language designed specifically for the Internet. It includes features that allow knowledge representation in distributed enviroments, and since the Internet is dynamic, allows ontologies to evolve in a controlled way."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "Identifying Words to Explain to a Reader", "Title": "A Preliminary Study", "Abstract": "The core idea of this paper is familiar to teachers: While a child is reading, explain unfamiliar words. Project LISTEN’s Reading Tutor listens to children read aloud and helps them learn to read. We want the Reading Tutor to explain unfamiliar words. To elicit explanations from an expert, the computer should suggest -- or let the expert select -- words to annotate. To capture explanations, the expert will type in and then narrate an explanation. Text and narration will be saved for later use. To utilize explanations during assisted reading, we will display the explanations as extra sentences to be read aloud with the computer’s help. Explanations will be provided on student request or computer tutor initiative. We focus here on how to select words for annotation."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "Adaptive Learning Systems", "Title": "A Model for Business Entrepreneurs to Implement IT", "Abstract": "Adaptive learning systems offer promise in revolutionizing the way we interact with computers. Realizing its full potential will involve development of these systems in everyday tools. This paper describes a progress toward this end: the development of an agent as a means of an active decision support tool for business entrepreneurs during the IT implementation process. The system features ALSTA, a personal assistant who augments the users perceptions as they make decision regarding IT implementation"}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "Selective Sampling with Co-Testing", "Title": "Preliminary Results", "Abstract": "We present a novel approach to selective sampling, co-testing, which can be applied to problems with redundant views (i.e., problems with multiple disjoint sets of attributes that can be used for learning). The main idea behind co-testing consists of selecting the queries among the unlabeled examples on which the existing views disagree."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "Sensible Agents", "Title": "Demonstration of Dynamic Adaptive Autonomy", "Abstract": "The analysis and design of large, complex systems mandates a formal methodology and supporting tools to assist system development teams throughout the system lifecycle. The multitude of personnel, the diversity of viewpoints, and the transient nature of personnel and technology in relation to the system lifecycle constrains the process by which 1) application domain requirements are acquired, analyzed and modeled, 2) a system architecture is derived from those requirements, 3) technology decisions are made and implementation progresses, and 4) the system is tested and maintained. A formal methodology for the entire lifecycle keeps team members coordinated and offers a mechanism to gauge progress. Large projects with many personnel responsible for making decisions require a formal process and automated support to assist team members in documenting their decisions. Traceability of decisions and documentation rationale is key to understanding the impact of decisions related to modeling, design, implementation, test, and maintenance. The SEPA effort proposes both a methodology and supporting tool suite (leveraging various knowledge representation and reasoning schemes) to facilitate development of object-oriented designs from evolving requirements. SEPA creates traceable, comprehensible, and extensible system design specifications based on requirements from system clients and domain experts. The funnel abstraction is chosen to represent the narrowing, refining, and structuring of user requirements into a system design. User inputs are refined by: (1) merging inputs from multiple sources, (2) distinguishing between inputs relating to system requirements and those relating to general domain knowledge, (3) constructing an object-oriented architecture, (4) mapping requirements to technology solutions, and (5) providing a framework for evaluating system design."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "O-Plan", "Title": "A Web-Based AI Planning Agent", "Abstract": "In these demonstrations we show O-Plan, an AI planning agent working over the WWW. There are a number of demonstrations ranging from a simple \"single shot\" generation of Unix systems administration scripts through to comprehensive use of AI technologies across the whole planning lifecycle in military and civilian crisis situations The applications are derived from actual user requirements and domain knowledge. The AI planning technologies demonstrated include: * Domain knowledge elicitation * Rich plan representation and use * Hierarchical Task Network Planning * Detailed constraint management * Goal structure-based plan monitoring * Dynamic issue handling * Plan repair in low and high tempo situations * Interfaces for users with different roles * Management of planning and execution workflow The featured demonstrations, and others, are available at"}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "Untangle", "Title": "A New Ontology for Card Catalog Systems", "Abstract": "The ontology used by most card catalog and bibliographic systems is based on a now outdated assumption that users of the systems would be looking for books on shelves, and therefore only books were first-class objects, with people. organizations, etc. as simple attributes. This limited the ability of a user to browse. A new ontology for card catalog systems is proposed that suggests that persons, organizations, conferences, etc., should be first-class objects with attributes and relations of their own, creating a rich space of background information that helps users find what they are looking for. This new ontology has been implemented in a knowledge-based system called Untangle, which demonstrates two key advantages of this rich information space: it enables automatic augmentation of the data through reasoning, and it enables a new paradigm for search that combines querying and browsing."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "MarketSAT", "Title": "An Extremely Decentralized (but Really Slow) Algorithm for Propositional Satisfiability", "Abstract": "We describe MarketSAT, a highly decentralized, market-based algorithm for propositional satisfiability. The approach is based on a formulation of satisfiability as production on a supply chain, where producers of particular variable assignments must acquire licenses to fail to satisfy particular clauses. MarketSAT employs a market protocol for general supply chain problems, which we show to be expressively equivalent to 3SAT. Experiments suggest that MarketSAT reliably converges to market allocations corresponding to satisfiable truth assignments. We experimentally compare the computational performance with GSAT, a centralized local search algorithm."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "A Consistency-Based Model for Belief Change", "Title": "Preliminary Report", "Abstract": "We develop a general, consistency-based framework for belief change. Informally, in revising K by A, we begin with A and incorporate as much of K as consistently possible. Formally, a knowledge base K and sentence A are expressed, via renaming propositions in K, in separate languages. Using a maximization process, we assume the languages are the same insofar as consistently possible. Lastly, we express the resultant knowledge base in a single language. There may be more than one way in which A can be so extended by K: in choice revision, one such extension represents the revised state; alternately revision consists of the intersection of all such extensions. The most general formulation of our approach is flexible enough to express various other approaches to revision and update, the merging of knowledge bases, and the incorporation of static and dynamic integrity constraints. Our framework differs from work based on ordinal conditional functions, notably with respect to iterated revision. We argue that the approach is well-suited for implementation: choice revision gives better complexity results than general revision; the approach can be expressed in terms of a finite knowledge base; and the scope of a revision can be restricted to just those propositions mentioned in the sentence for revision A."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "Computing Circumscriptive Databases by Integer Programming", "Title": "Revisited", "Abstract": "In this paper, we consider a method of omputing minimal models in circumscription using integer programming in propositional logic and first-order logic with domain closure axioms and unique name axioms. This kind of treatment is very important since this enable to apply various technique developed in operations research to nonmonotonic reasoning.\nNerode et al. are the first to propose a method of computing circumscription using integer programming. They claimed their method was correct for circumscription with fixed predicate, but we show that their method does not correctly reflect their claim. We show a correct method of computing all the minimal models not only with fixed predicates but also with varied predicates and we extend our method to compute prioritized circumscription as well."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "Using Prior Knowledge", "Title": "Problems and Solutions", "Abstract": "Encoding knowledge is time consuming and expensive. A possible solution to reduce the cost of developing a new knowledge base (KB) is to reuse existing knowledge. Previous work addressing this problem has focused on standards for representing, exchanging, and accessing knowledge, and on creating large repositories of knowledge. Results on the level of reuse achievable have been reported. In this paper, we focus on the process of reuse and report a case study on constructing a KB by reusing existing knowledge. The reuse process involved the following steps: translation, comprehension, slicing, reformulation, and merging. We discuss technical problems encountered at each of these steps and explain how we solved them."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "PROMPT", "Title": "Algorithm and Tool for Automated Ontology Merging and Alignment", "Abstract": "Researchers in the ontology-design field have developed the content for ontologies in many domain areas. Recently, ontologies have become increasingly common on the World-Wide Web where they provide semantics for annotations in Web pages. This distributed nature of ontology development has led to a large number of ontologies covering overlapping domains. In order for these ontologies to be reused, they first need to be merged or aligned to one another. The processes of ontology alignment and merging are usually handled manually and often constitute a large and tedious portion of the sharing process. We have developed and implemented PROMPT, an algorithm that provides a semi-automatic approach to ontology merging and alignment. PROMPT performs some tasks automatically and guides the user in performing other tasks for which his intervention is required. PROMPT also determines possible inconsistencies in the state of the ontology, which result from the user’s actions, and suggests ways to remedy these inconsistencies. PROMPT is based on an extremely general knowledge model and therefore can be applied across various platforms. Our formative evaluation showed that a human expert followed 90% of the suggestions that PROMPT generated and that 74% of the total knowledge-base operations invoked by the user were suggested by PROMPT."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "cc-Golog", "Title": "Towards More Realistic Logic-Based Robot Controllers", "Abstract": "High-level robot controllers in realistic domains typically deal with processes which operate concurrently, change the world continuously, and where the execution of actions is event-driven as in ``charge the batteries as soon as the voltage level is low''. While non-logic-based robot control languages are well suited to express such scenarios, they fare poorly when it comes to projecting, in a conspicuous way, how the world evolves when actions are executed. On the other hand, a logic-based control language like congolog, based on the situation calculus, is well-suited for the latter. However, it has problems expressing event-driven behavior. In this paper, we show how these problems can be overcome by first extending the situation calculus to support continuous change and event-driven behavior and then presenting ccgolog, a variant of congolog which is based on the extended situation calculus. One benefit of ccgolog is that it narrows the gap in expressiveness compared to non-logic-based control languages while preserving a semantically well-founded projection mechanism."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "What Sensing Tells Us", "Title": "Towards a Formal Theory of Testing for Dynamical Systems", "Abstract": "Just as actions can have indirect effects on the state of the world, so too can sensing actions have indirect effects on an agent’s state of knowledge. In this paper, we investigate what sensing actions tell us, i.e., what an agent comes to know indirectly from the outcome of a sensing action, given knowledge of its actions and state constraints that hold in the world. Building on this foundation, we define the notion of a test, a complex action designed to achieve a knowledge goal. We show how such tests can be computed, or alternately, how they can be specified as complex actions. \nTo this end, we propose a formalization of the notion of testing within a dialect of the situation calculus that includes knowledge and knowledge-producing actions. Realizing this formalization requires addressing the ramification problem for knowledge-producing actions. We formalize simple tests as sensing actions. Complex tests are described in the logic programming language Golog. We examine what it means to perform a test, and how the outcome of a test affects an agent’s state of knowledge. Finally we discuss the issue of selecting tests to confirm, refute, or discriminate a space of hypotheses. The work presented in this paper is relevant to a number of application domains including diagnostic problem solving, natural language understanding, plan recognition, and active vision."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "GeoRep", "Title": "A Flexible Tool for Spatial Representation of Line Drawings", "Abstract": "A key problem in diagrammatic reasoning is understanding how people reason about qualitative relationships in diagrams. We claim that progress in diagrammatic reasoning is slowed by two problems: (1) researchers tend to start from scratch, creating new spatial reasoners for each new problem area, and (2) constraints from human visual processing are rarely considered. To address these problems, we created GeoRep, a spatial reasoning engine that generates qualitative spatial descriptions from line drawings. GeoRep has been successfully used in several research projects, including cognitive simulation studies of human vision. In this paper, we outline GeoRep’s architecture, explain the domain-independent and domain-specific aspects of its processing, and motivate the representations it produces. We then survey how GeoRep has been used in three different projects--a model of symmetry, a model of understanding juxtaposition diagrams of physical situations, and a system for reasoning about military courses of action."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "STA", "Title": "Spatio-Temporal Aggregation with Applications to Analysis of Diffusion-Reaction Phenomena", "Abstract": "Spatio-temporal data sets arise when time-varying physical fields are discretized for simulation or analysis. Examples of time-varying fields are isothermal regions in the sea, or pattern formations in natural systems, such as convection rolls or diffusion-reaction systems. The analysis of these data sets is essential to generate qualitative interpretations for human understanding. This paper presents Spatio-Temporal Aggregation (STA), a system for recognizing and tracking qualitative structures in spatio-temporal data sets. STA algorithms record and maintain temporal events and compile event se-quences into concise history descriptions. This is carried out at several levels of description, from the bottom up: first, low level events are identified and tracked, and then a subset of those events, relevant at the next description level, is identified. The process is iterated until a high level narrative of the system’s temporal evolution is obtained. STA has been demonstrated on a class of diffusion-reaction systems in two dimensions and has successfully generated high-level symbolic descriptions of systems similar to those produced by scientists through carefully hand-tuned computational experiments."}
{"Type": "conference", "Year": "2000", "Area": "AI", "Where": "AAAI", "Abbreviation": "Towards Feasible Approach to Plan Checking under Probabilistic Uncertainty", "Title": "Interval Methods", "Abstract": "The main problem of {it planning} is to find a sequence of actions that an agent must perform to achieve a given objective. An important part of planning is checking whether a given plan achieves the desired objective. Historically, in AI, the planning and plan checking problems were mainly formulated and solved in a {it deterministic} environment, when the initial state is known precisely and when the results of each action in each state is known (and uniquely determined). In this deterministic case, planning is difficult, but plan checking is straightforward. In many real-life situations, we only know the probabilities of different fluents; in such situations, even plan checking becomes computationally difficult. {em In this paper, we describe how methods of interval computations can be used to get a feasible approximation to plan checking under probabilistic uncertainty.} The resulting method is a natural generalization of 0-approximation proposed earlier to describe planning in the case of partial knowledge. It turns out that some of the resulting probabilistic techniques coincides with heuristically proposed ``fuzzy`` methods. Thus, we justify these fuzzy heuristics as a reasonable feasible approximation to the (NP-hard) probabilistic problem."}
