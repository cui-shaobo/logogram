{"Type":"conference","Year":2023,"Area":"AI","Where":"eacl-2023","Abbreviation":"DyLoRA","Title":"Parameter-Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation","Abstract":"With the ever-growing size of pretrained models (PMs), fine-tuning them has become more expensive and resource-hungry. As a remedy, low-rank adapters (LoRA) keep the main pretrained weights of the model frozen and just introduce some learnable truncated SVD modules (so-called LoRA blocks) to the model. While LoRA blocks are parameter-efficient, they suffer from two major problems: first, the size of these blocks is fixed and cannot be modified after training (for example, if we need to change the rank of LoRA blocks, then we need to re-train them from scratch); second, optimizing their rank requires an exhaustive search and effort. In this work, we introduce a dynamic low-rank adaptation (<MASKED_ACRONYM>) technique to address these two problems together. Our <MASKED_ACRONYM> method trains LoRA blocks for a range of ranks instead of a single rank by sorting the representation learned by the adapter module at different ranks during training. We evaluate our solution on different natural language understanding (GLUE benchmark) and language generation tasks (E2E, DART and WebNLG) using different pretrained models such as RoBERTa and GPT with different sizes. Our results show that we can train dynamic search-free models with <MASKED_ACRONYM> at least 4 to 7 times (depending to the task) faster than LoRA without significantly compromising performance. Moreover, our models can perform consistently well on a much larger range of ranks compared to LoRA.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.7272727273}