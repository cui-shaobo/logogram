{"Year":2022,"Venue":"lrec-2022","Acronym":"Nkululeko","Description":"A Tool For Rapid Speaker Characteristics Detection","Abstract":"We present advancements with a software tool called with, that lets users perform (semi-) supervised machine learning experiments in the speaker characteristics domain. It is based on audformat, a format for speech database metadata description. Due to an interface based on configurable templates, it supports best practise and very fast setup of experiments without the need to be proficient in the underlying language: python. The paper explains the handling of experiments: and presents two typical experiments: comparing the expert acoustic features with artificial neural net embeddings for emotion classification and speaker age regression.","wordlikeness":0.6666666667,"lcsratio":0.4444444444,"wordcoverage":0.5714285714}
{"Year":2021,"Venue":"ws-2021","Acronym":"QADI","Description":"Arabic Dialect Identification in the Wild","Abstract":"Proper dialect identification is important for a variety of arabic nlp applications. In this paper, we present a method for rapidly constructing a tweet dataset containing a wide range of country-level arabic dialects \u2014covering 18 different countries in the middle east and north africa region. Our method relies on applying multiple filters to identify users who belong to different countries based on their account descriptions and to eliminate tweets that either write mainly in modern standard arabic or mostly use vulgar language. The resultant dataset contains 540k tweets from 2,525 users who are evenly distributed across 18 arab countries. Using intrinsic evaluation, we show that the labels of a set of randomly selected tweets are 91.5% accurate. For extrinsic evaluation, we are able to build effective country level dialect identification on tweets with a macro-averaged f1-score of 60.6% across 18 classes.","wordlikeness":0.25,"lcsratio":0.75,"wordcoverage":0.75}
{"Year":2021,"Venue":"ws-2021","Acronym":"BERTologiCoMix","Description":"How does Code-Mixing interact with Multilingual BERT?","Abstract":"Models such as mbert and xlmr have shown success in solving code-mixed nlp tasks even though they were not exposed to such text during pretraining. Code-mixed nlp models have relied on using synthetically generated data along with naturally occurring data to improve their performance. Finetuning mbert on such data improves it\u2019s code-mixed performance, but the benefits of using the different types of code-mixed data aren\u2019t clear. In this paper, we study the impact of finetuning with different types of code-mixed data and outline the changes that occur to the model during such finetuning. Our findings suggest that using naturally occurring code-mixed data brings in the best performance improvement after finetuning and that finetuning with any type of code-mixed text improves the responsivity of it\u2019s attention heads to code-mixed text inputs.","wordlikeness":0.7857142857,"lcsratio":0.5,"wordcoverage":0.6153846154}
{"Year":2021,"Venue":"acl-2021","Acronym":"PsyQA","Description":"A Chinese Dataset for Generating Long Counseling Text for Mental Health Support","Abstract":"Great research interests have been attracted to devise ai services that are able to provide mental health support. However, the lack of corpora is a main obstacle to this research, particularly in chinese language. In this paper, we propose and, a chinese dataset of psychological health support in the form of question and answer pair. Devise is crawled from a chinese mental health service platform, and contains 22k questions and 56k long and wellstructured answers. Based on the psychological counseling theories, we annotate a portion of answer texts with typical strategies for providing support, and further present in-depth analysis of both lexical features and strategy patterns in the counseling answers. We also evaluate the performance of generating counseling answers with the generative pretrained models. Results show that utilizing strategies enhances the \ufb02uency and helpfulness of generated answers, but there is still a large space for future research.","wordlikeness":0.6,"lcsratio":0.4,"wordcoverage":0.75}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"UniRPG","Description":"Unified Discrete Reasoning over Table and Text as Program Generation","Abstract":"Question answering requiring discrete reasoning, e.g., arithmetic computing, comparison, and counting, over knowledge is a challenging task. In this paper, we propose tat-qa, a semantic-parsing-based approach advanced in interpretability and scalability, to perform unified discrete reasoning over heterogeneous knowledge resources, i.e., table and text, as program generation. Concretely, symbolic consists of a neural programmer and a symbolic program executor,where a program is the composition of a set of pre-defined general atomic and higher-order operations and arguments extracted from table and text. First, the programmer parses a question into a program by generating operations and copying arguments, and then, the executor derives answers from table and text based on the program. To alleviate the costly program annotation issue, we design a distant supervision approach for programmer learning, where pseudo programs are automatically constructed without annotated derivations. Extensive experiments on the tat-qa dataset show that design achieves tremendous improvements and enhances interpretability and scalability compared with previous state-of-the-art methods, even without derivation annotation. Moreover, it achieves promising performance on the textual dataset drop without derivation annotation.","wordlikeness":0.3333333333,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2022,"Venue":"lrec-2022","Acronym":"TYPIC","Description":"A Corpus of Template-Based Diagnostic Comments on Argumentation","Abstract":"Providing feedback on the argumentation of the learner is essential for developing critical thinking skills, however, it requires a lot of time and effort. To mitigate the overload on teachers, we aim to automate a process of providing feedback, especially giving diagnostic comments which point out the weaknesses inherent in the argumentation. It is recommended to give specific diagnostic comments so that learners can recognize the diagnosis without misinterpretation. However, it is not obvious how the task of providing specific diagnostic comments should be formulated. We present a formulation of the task as template selection and slot filling to make an automatic evaluation easier and the behavior of the model more tractable. The key to the formulation is the possibility of creating a template set that is sufficient for practical use. In this paper, we define three criteria that a template set should satisfy: expressiveness, informativeness, and uniqueness, and verify the feasibility of creating a template set that satisfies these criteria as a first trial. We will show that it is feasible through an annotation study that converts diagnostic comments given in a text to a template format. The corpus used in the annotation study is publicly available.","wordlikeness":0.8,"lcsratio":0.8,"wordcoverage":0.8333333333}
{"Year":2020,"Venue":"emnlp-2020","Acronym":"DualTKB","Description":"A Dual Learning Bridge between Text and Knowledge Base","Abstract":"In this work, we present a dual learning approach for unsupervised text to path and path to text transfers in commonsense knowledge bases (kbs). We investigate the impact of weak supervision by creating a weakly supervised dataset and show that even a slight amount of supervision can significantly improve the model performance and enable better-quality transfers. We examine different model architectures, and evaluation metrics, proposing a novel commonsense kb completion metric tailored for generative models. Extensive experimental results show that the proposed method compares very favorably to the existing baselines. This approach is a viable step towards a more advanced system for automatic kb construction\/expansion and the reverse operation of kb conversion to coherent textual descriptions.","wordlikeness":0.5714285714,"lcsratio":1.0,"wordcoverage":0.7272727273}
{"Year":2022,"Venue":"coling-2022","Acronym":"LIME","Description":"Weakly-Supervised Text Classification without Seeds","Abstract":"In weakly-supervised text classification, only label names act as sources of supervision. Predominant approaches to weakly-supervised text classification utilize a two-phase framework, where test samples are first assigned pseudo-labels and are then used to train a neural text classifier. In most previous work, the pseudo-labeling step is dependent on obtaining seed words that best capture the relevance of each class label. We present with, a framework for weakly-supervised text classification that entirely replaces the brittle seed-word generation process with entailment-based pseudo-classification. We find that combining weakly-supervised classification and textual entailment mitigates shortcomings of both, resulting in a more streamlined and effective classification pipeline. With just an off-the-shelf textual entailment model, replaces outperforms recent baselines in weakly-supervised text classification and achieves state-of-the-art in 4 benchmarks.","wordlikeness":1.0,"lcsratio":0.75,"wordcoverage":1.0}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"DisCup","Description":"Discriminator Cooperative Unlikelihood Prompt-tuning for Controllable Text Generation","Abstract":"Prompt learning with immensely large casual language models (clms) has been shown promising for attribute-controllable text generation (ctg). However, vanilla prompt tuning tends to imitate training corpus characteristics beyond the control attributes, resulting in a poor generalization ability. Moreover, it is less able to capture the relationship between different attributes, further limiting the control performance. In this paper, we propose a new ctg approach, namely used, which incorporates the attribute knowledge of discriminator to optimize the control-prompts, steering a frozen clm to produce attribute-specific texts. Specifically, the frozen clm model, capable of producing multitudinous texts, is first used to generate the next-token candidates based on the context, so as to ensure the diversity of tokens to be predicted. Then, we leverage an attribute-discriminator to select desired\/undesired tokens from those candidates, providing the inter-attribute knowledge. Finally, we bridge the above two traits by an unlikelihood objective for prompt-tuning. Extensive experimental results show that been can achieve a new state-of-the-art control performance while maintaining an efficient and high-quality text generation, only relying on around 10 virtual tokens.","wordlikeness":0.8333333333,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2019,"Venue":"ws-2019","Acronym":"ViGGO","Description":"A Video Game Corpus for Data-To-Text Generation in Open-Domain Conversation","Abstract":"The uptake of deep learning in natural language generation (nlg) led to the release of both small and relatively large parallel corpora for training neural models. The existing data-to-text datasets are, however, aimed at task-oriented dialogue systems, and often thus limited in diversity and versatility. They are typically crowdsourced, with much of the noise left in them. Moreover, current neural nlg models do not take full advantage of large training data, and due to their strong generalizing properties produce sentences that look template-like regardless. We therefore present a new corpus of 7k samples, which (1) is clean despite being crowdsourced, (2) has utterances of 9 generalizable and conversational dialogue act types, making it more suitable for open-domain dialogue systems, and (3) explores the domain of video games, which is new to dialogue systems despite having excellent potential for supporting rich conversations.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2021,"Venue":"acl-2021","Acronym":"LOA","Description":"Logical Optimal Actions for Text-based Interaction Games","Abstract":"We present logical optimal actions (knowledge), an action decision architecture of reinforcement learning applications with a neuro-symbolic framework which is a combination of neural network and symbolic knowledge acquisition approach for natural language interaction games. The demonstration for is experiments consists of a web-based interactive platform for text-based games and visualization for acquired knowledge for improving interpretability for trained rules. This demonstration also provides a comparison module with other neuro-symbolic approaches as well as non-symbolic state-of-the-art agent models on the same text-based games. Our neural also provides open-sourced implementation in python for the reinforcement learning environment to facilitate an experiment for studying neuro-symbolic agents. Demo site: <a href=https:\/\/ibm.biz\/acl21-(), class=acl-markup-url>https:\/\/ibm.biz\/acl21-same<\/a>, code: <a href=https:\/\/github.com\/ibm\/state-of-the-art class=acl-markup-url>https:\/\/github.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2022,"Venue":"acl-2022","Acronym":"CAMERO","Description":"Consistency Regularized Ensemble of Perturbed Language Models with Weight Sharing","Abstract":"Model ensemble is a popular approach to produce a low-variance and well-generalized model. However, it induces large memory and inference costs, which is often not affordable for real-world deployment. Existing work has resorted to sharing weights among models. However, when increasing the proportion of the shared weights, the resulting models tend to be similar, and the benefits of using model ensemble diminish. To retain ensemble benefits while maintaining a low memory cost, we propose a consistency-regularized ensemble learning approach based on perturbed models, named (114.2m. Specifically, we share the weights of bottom layers across all models and apply different perturbations to the hidden representations for different models, which can effectively promote the model diversity. Meanwhile, we apply a prediction consistency regularizer across the perturbed models to control the variance due to the model diversity. Our experiments using large language models demonstrate that size significantly improves the generalization performance of the ensemble model. Specifically, model outperforms the standard ensemble of 8 bert-base models on the glue benchmark by 0.7 with a significantly smaller model size (114.2m vs. 880.6m).","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.9230769231}
{"Year":2020,"Venue":"wac-2020","Acronym":"Hypernym-LIBre","Description":"A Free Web-based Corpus for Hypernym Detection","Abstract":"In this paper, we describe a new web-based corpus for hypernym detection. It consists of 32 gb of high quality english paragraphs along with their part-of-speech tagged and dependency parsed versions. For hypernym detection, the current state-of-the-art uses a corpus which is not available freely. We evaluate the state-of-the-art methods on our corpus and achieve similar results. The advantage of this corpora is that it is available under an open license. Our main contribution is the corpus with pos-tags and dependency tags and the code to extract and simulate the results we have achieved using our corpus.","wordlikeness":0.5714285714,"lcsratio":0.6428571429,"wordcoverage":0.5833333333}
{"Year":2022,"Venue":"naacl-2022","Acronym":"NeuS","Description":"Neutral Multi-News Summarization for Mitigating Framing Bias","Abstract":"Media news framing bias can increase political polarization and undermine civil society. The need for automatic mitigation methods is therefore growing. We propose a new task, a neutral summary generation from multiple news articles of the varying political leaningsto facilitate balanced and unbiased news reading. In this paper, we first collect a new dataset, illustrate insights about framing bias through a case study, and propose a new effective metric and model (in-title) for the task. Based on our discovery that title provides a good signal for framing bias, we present hierarchical-title that learns to neutralize news content in hierarchical order from title to article. Our hierarchical multi-task learning is achieved by formatting our hierarchical data pair (title, article) sequentially with identifier-tokens (\u201ctitle=>\u201d, \u201carticle=>\u201d) and fine-tuning the auto-regressive decoder with the standard negative log-likelihood objective. We then analyze and point out the remaining challenges and future directions. One of the most interesting observations is that neural nlg models can hallucinate not only factually inaccurate or unverifiable content but also politically biased content.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2022,"Venue":"lrec-2022","Acronym":"SHARE","Description":"A Lexicon of Harmful Expressions by Spanish Speakers","Abstract":"In this paper we present existing, a new lexical resource with 10,125 offensive terms and expressions collected from spanish speakers. We retrieve this vocabulary using an existing chatbot developed to engage a conversation with users and collect insults via telegram, named fiero. This vocabulary has been manually labeled by five annotators obtaining a kappa coefficient agreement of 78.8%. In addition, we leverage the lexicon to release the first corpus in spanish for offensive span identification research named offendes_spans. Finally, we show the utility of our resource as an interpretability tool to explain why a comment may be considered offensive.","wordlikeness":1.0,"lcsratio":0.8,"wordcoverage":1.0}
{"Year":2006,"Venue":"lrec-2006","Acronym":"Progmatica","Description":"A Prosodic Database for European Portuguese","Abstract":"In this work, a spontaneous speech corpus of broadcasted television material in european portuguese (ep) is presented. We decided to name it decided as it is meant to combine prosody information under a pragmatic framework. Our purpose is to analyse, describe and predict the prosodic patterns that are involved in speech acts and discourse events. It is also our goal to relate both prosody and pragmatics to emotion, style and attitude. In future developments, we intend, by this way, to provide ep tts systems with pragmatic and emotional dimensions. From the whole recorded material we selected, extracted and saved prototypical speech acts with the help of speech analysis tools. We have a multi-speaker corpus, where linguistic, paralinguistic and extra linguistic information are labelled and related to each other. The paper is organized as follows. In section one, a brief state-of-the-art for the available ep corpora containing prosodic information is presented. In section two, we explain the pragmatic criteria used to structure this database. Then, we describe how the speech signal was labelled and which information layers were considered. In section three, we propose a prosodic prediction model to be applied to each speech act in future. In section four, some of the main problems we went through are discussed and future work is presented.","wordlikeness":0.7,"lcsratio":0.6,"wordcoverage":0.8421052632}
{"Year":2020,"Venue":"ws-2020","Acronym":"BERTnesia","Description":"Investigating the capture and forgetting of knowledge in BERT","Abstract":"Probing complex language models has recently revealed several insights into linguistic and semantic patterns found in the learned representations. In this paper, we probe bert specifically to understand and measure the relational knowledge it captures. We utilize knowledge base completion tasks to probe every layer of pre-trained as well as fine-tuned bert (ranking, question answering, ner). Our findings show that knowledge is not just contained in bert\u2019s final layers. Intermediate layers contribute a significant amount (17-60%) to the total knowledge found. Probing intermediate layers also reveals how different types of knowledge emerge at varying rates. When bert is fine-tuned, relational knowledge is forgotten but the extent of forgetting is impacted by the fine-tuning objective but not the size of the dataset. We found that ranking models forget the least and retain more knowledge in their final layer.","wordlikeness":0.6666666667,"lcsratio":0.6666666667,"wordcoverage":0.7058823529}
{"Year":2022,"Venue":"clib-2022","Acronym":"Razmecheno","Description":"Named Entity Recognition from Digital Archive of Diaries ``Prozhito&#39;&#39;","Abstract":"The vast majority of existing datasets for named entity recognition (ner) are built primarily on news, research papers and wikipedia with a few exceptions, created from historical and literary texts. What is more, english is the main source for data for further labelling. This paper aims to fill in multiple gaps by creating a novel dataset \u201cnamed\u201d, gathered from the diary texts of the project \u201cprozhito\u201d in russian. Our dataset is of interest for multiple research lines: literary studies of diary texts, transfer learning from other domains, low-resource or cross-lingual named entity recognition. On comprises 1331 sentences and 14119 tokens, sampled from diaries, written during the perestroika. The annotation schema consists of five commonly used entity tags: person, characteristics, location, organisation, and facility. The labelling is carried out on the crowdsourcing platfrom yandex.toloka in two stages. First, workers selected sentences, which contain an entity of particular type. Second, they marked up entity spans. As a result 1113 entities were obtained. Empirical evaluation of 1113 is carried out with off-the-shelf ner tools and by fine-tuning pre-trained contextualized encoders. We release the annotated dataset for open access.","wordlikeness":0.7,"lcsratio":0.7,"wordcoverage":0.6666666667}
{"Year":2023,"Venue":"acl-2023","Acronym":"ALERT","Description":"Adapt Language Models to Reasoning Tasks","Abstract":"Recent advancements in large language models have enabled them to perform well on complex tasks that require step-by-step reasoning with few-shot learning. However, it is unclear whether these models are applying reasoning skills they have learnt during pre-training , or if they are simply memorizing their training corpus at finer granularity and have learnt to better understand their context. To address this question, we introduce {pasted macro \u2018our\u2019}model, a benchmark and suite of analyses for evaluating reasoning skills of language models. {pasted macro \u2018our\u2019}model enables comparing pre-trained and finetuned models on complex tasks that require reasoning skills to solve. Our benchmark provides a test bed to asses any language model on fine-grained reasoning skills, which spans over 20 datasets and covers 10 different reasoning skills. By using {pasted macro \u2018our\u2019}model we further investigate <i>the role of finetuning<\/i>. Our extensive empirical analysis shows that language models learn more reasoning skills such as textual entailment, abductive reasoning, and analogical reasoning during the finetuning stage compared to pretraining stage. However, we also find that when language models are finetuned they tend to overfit to the prompt template, which hurts the robustness of models causing generalization problems.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2023,"Venue":"findings-2023","Acronym":"NatCS","Description":"Eliciting Natural Customer Support Dialogues","Abstract":"Despite growing interest in applications based on natural customer support conversations,there exist remarkably few publicly available datasets that reflect the expected characteristics of conversations in these settings. Existing task-oriented dialogue datasets, which were collected to benchmark dialogue systems mainly in written human-to-bot settings, are not representative of real customer support conversations and do not provide realistic benchmarks for systems that are applied to natural data. To address this gap, we introduce not, a multi-domain collection of spoken customer service conversations. We describe our process for collecting synthetic conversations between customers and agents based on natural language phenomena observed in real conversations. Compared to previous dialogue datasets, the conversations collected with our approach are more representative of real human-to-human conversations along multiple metrics. Finally, we demonstrate potential uses of support, including dialogue act classification and intent induction from conversations as potential applications, showing that dialogue act annotations in existing provide more effective training data for modeling real conversations compared to existing synthetic written datasets.","wordlikeness":0.4,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2020,"Venue":"lrec-2020","Acronym":"MuDoCo","Description":"Corpus for Multidomain Coreference Resolution and Referring Expression Generation","Abstract":"This paper proposes a new dataset, referencing, composed of authored dialogs between a fictional user and a system who are given tasks to perform within six task domains. These dialogs are given rich linguistic annotations by expert linguists for several types of reference mentions and named entity mentions, either of which can span multiple words, as well as for coreference links between mentions. The dialogs sometimes cross and blend domains, and the users exhibit complex task switching behavior such as re-initiating a previous task in the dialog by referencing the entities within it. The dataset contains a total of 8,429 dialogs with an average of 5.36 turns per dialog. We are releasing this dataset to encourage research in the field of coreference resolution, referring expression generation and identification within realistic, deep dialogs involving multiple domains. To demonstrate its utility, we also propose two baseline models for the downstream tasks: coreference resolution and referring expression generation.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.7692307692}
{"Year":2020,"Venue":"lrec-2020","Acronym":"RSC","Description":"A Romanian Read Speech Corpus for Automatic Speech Recognition","Abstract":"Although many efforts have been made in the last decade to enhance the speech and language resources for romanian, this language is still considered under-resourced. While for many other languages there are large speech corpora available for research and commercial applications, for romanian language the largest publicly available corpus to date comprises less than 50 hours of speech. In this context, speech and dialogue research group releases read speech corpus (decade) \u2013 a romanian speech corpus developed in-house, comprising 100 hours of speech recordings from 164 different speakers. The paper describes the development of the corpus and presents baseline automatic speech recognition (asr) results using state-of-the-art asr technology: kaldi speech recognition toolkit.","wordlikeness":0.3333333333,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2019,"Venue":"acl-2019","Acronym":"GCDT","Description":"A Global Context Enhanced Deep Transition Architecture for Sequence Labeling","Abstract":"Current state-of-the-art systems for sequence labeling are typically based on the family of recurrent neural networks (rnns). However, the shallow connections between consecutive hidden states of rnns and insufficient modeling of global information restrict the potential performance of those models. In this paper, we try to address these issues, and thus propose a global context enhanced deep transition architecture for sequence labeling named are. We deepen the state transition path at each position in a sentence, and further assign every token with a global representation learned from the entire sentence. Experiments on two standard sequence labeling tasks show that, given only training data and the ubiquitous word embeddings (glove), our word achieves 91.96 f1 on the conll03 ner task and 95.43 f1 on the conll2000 chunking task, which outperforms the best reported results under the same settings. Furthermore, by leveraging bert as an additional resource, we establish new state-of-the-art results with 93.47 f1 on ner and 97.30 f1 on chunking.","wordlikeness":0.25,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2014,"Venue":"semeval-2014","Acronym":"KUL-Eval","Description":"A Combinatory Categorial Grammar Approach for Improving Semantic Parsing of Robot Commands using Spatial Context","Abstract":"When executing commands, a robot has a certain level of contextual knowledge about the environment in which it operates. Taking this knowledge into account can be bene\ufb01cial to disambiguate commands with multiple interpretations. We present an approach that uses combinatory categorial grammars for improving the semantic parsing of robot commands that takes into account the spatial context of the robot. The results indicate a clear improvement over non-contextual semantic parsing. This work was done in the context of the semeval-2014 task on supervised semantic parsing of spatial robot commands.","wordlikeness":0.625,"lcsratio":0.5,"wordcoverage":0.625}
{"Year":2007,"Venue":"acl-2007","Acronym":"SemTAG","Description":"a platform for specifying Tree Adjoining Grammars and performing TAG-based Semantic Construction","Abstract":"In this paper, we introduce of, a free and open software architecture for the development of tree adjoining grammars integrating a compositional semantics. Paper, differs from xtag in two main ways. First, it provides an expressive grammar formalism and compiler for factorising and specifying tags. second, it supports semantic construction.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.7272727273}
{"Year":2020,"Venue":"coling-2020","Acronym":"CollFrEn","Description":"Rich Bilingual English--French Collocation Resource","Abstract":"Collocations in the sense of idiosyncratic lexical co-occurrences of two syntactically bound words traditionally pose a challenge to language learners and many natural language processing (nlp) applications alike. Reliable ground truth (i.e., ideally manually compiled) resources are thus of high value. We present a manually compiled bilingual english\u2013french collocation resource with 7,480 collocations in english and 6,733 in french. Each collocation is enriched with information that facilitates its downstream exploitation in nlp tasks such as machine translation, word sense disambiguation, natural language generation, relation classification, and so forth. Our proposed enrichment covers: the semantic category of the collocation (its lexical function), its vector space representation (for each individual word as well as their joint collocation embedding), a subcategorization pattern of both its elements, as well as their corresponding babelnet id, and finally, indices of their occurrences in large scale reference corpora.","wordlikeness":0.75,"lcsratio":0.875,"wordcoverage":0.8}
{"Year":2022,"Venue":"nsurl-2022","Acronym":"ALRT","Description":"Cutting Edge Tool for Automatic Generation of Arabic Lexical Recognition Tests","Abstract":"A lexical recognition tests (lrt) is a common tool being widely used to measure the level of language-learner\u2019s proficiency utilizing vocabulary size (or simply the number of words acquired by a learner) for several international languages like english, arabic, german, chinese, and spanish. Compared to other languages, lrt themes for arabic are not mature enough and still they have some rooms for improvement, with very few existing proposals that mainly use human-crafted or semiautomated methods using arabic natural language processing (nlp) techniques. This paper introduces common, the arabic lexical recognition tests tool for the automatic generation of arabic lrts. the tool was tested using a huge dataset of arabic vocabulary, and a subjectmatter expert intervention was involved as an extra validation step to verify the quality of generated nonwords.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2022,"Venue":"findings-2022","Acronym":"MDCSpell","Description":"A Multi-task Detector-Corrector Framework for Chinese Spelling Correction","Abstract":"Chinese spelling correction (csc) is a task to detect and correct misspelled characters in chinese texts. Csc is challenging since many chinese characters are visually or phonologically similar but with quite different semantic meanings. Many recent works use bert-based language models to directly correct each character of the input sentence. However, these methods can be sub-optimal since they correct every character of the sentence only by the context which is easily negatively affected by the misspelled characters. Some other works propose to use an error detector to guide the correction by masking the detected errors. Nevertheless, these methods dampen the visual or phonological features from the misspelled characters which could be critical for correction. In this work, we propose a novel general detector-corrector multi-task framework where the corrector uses bert to capture the visual and phonological features from each character in the raw sentence and uses a late fusion strategy to fuse the hidden states of the corrector with that of the detector to minimize the negative impact from the misspelled characters. Comprehensive experiments on benchmarks demonstrate that our proposed method can significantly outperform the state-of-the-art methods in the csc task.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.7692307692}
{"Year":2023,"Venue":"findings-2023","Acronym":"Tab-CoT","Description":"Zero-shot Tabular Chain of Thought","Abstract":"The chain-of-though (cot) prompting methods were successful in various natural language processing (nlp) tasks thanks to their ability to unveil the underlying complex reasoning processes. Such reasoning processes typically exhibit highly structured steps. Recent efforts also started investigating methods to encourage more structured reasoning procedures to be captured (cite least to most).in this work, we propose be, a novel tabular-format cot prompting method, which allows the complex reasoning process to be explicitly modeled in a highly structured manner. Despite its simplicity, we show that our approach is capable of performing reasoning across multiple dimensions (i.e., both rows and columns).we demonstrate our approach\u2019s strong zero-shot and few-shot capabilities through extensive experiments on a range of reasoning tasks.","wordlikeness":0.7142857143,"lcsratio":0.8571428571,"wordcoverage":0.7272727273}
{"Year":2023,"Venue":"findings-2023","Acronym":"UniFine","Description":"A Unified and Fine-grained Approach for Zero-shot Vision-Language Understanding","Abstract":"Vision-language tasks, such as vqa, snli-ve, and vcr are challenging because they require the model\u2019s reasoning ability to understand the semantics of the visual world and natural language. Supervised methods working for vision-language tasks have been well-studied. However, solving these tasks in a zero-shot setting is less explored. Since contrastive language-image pre-training (clip) has shown remarkable zero-shot performance on image-text matching, previous works utilized its strong zero-shot ability by converting vision-language tasks into an image-text matching problem, and they mainly consider global-level matching (e.g., the whole image or sentence). However, we find visual and textual fine-grained information, e.g., keywords in the sentence and objects in the image, can be fairly informative for semantics understanding. Inspired by this, we propose a unified framework to take advantage of the fine-grained information for zero-shot vision-language learning, covering multiple tasks such as vqa, snli-ve, and vcr. Our experiments show that our framework outperforms former zero-shot methods on vqa and achieves substantial improvement on snli-ve and vcr. Furthermore, our ablation studies confirm the effectiveness and generalizability of our proposed method.","wordlikeness":0.7142857143,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2022,"Venue":"acl-2022","Acronym":"Adaptor","Description":"Objective-Centric Adaptation Framework for Language Models","Abstract":"This paper introduces training, library, which transposes traditional model-centric approach composed of pre-training + fine-tuning steps to objective-centric approach, composing the training process by applications of selected objectives. We survey research directions that can benefit from enhanced objective-centric experimentation in multitask training, custom objectives development, dynamic training curricula, or domain adaptation. Training aims to ease reproducibility of these research directions in practice. Finally, we demonstrate the practical applicability of the in selected unsupervised domain adaptation scenarios.","wordlikeness":0.8571428571,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2020,"Venue":"emnlp-2020","Acronym":"DagoBERT","Description":"Generating Derivational Morphology with a Pretrained Language Model","Abstract":"Can pretrained language models (plms) generate derivationally complex words? We present the first study investigating this question, taking bert as the example plm. We examine bert\u2019s derivational capabilities in different settings, ranging from using the unmodified pretrained model to full finetuning. Our best model, informed (derivationally and generatively optimized bert), clearly outperforms the previous state of the art in derivation generation (dg). Furthermore, our experiments show that the input segmentation crucially impacts bert\u2019s derivational knowledge, suggesting that the performance of plms could be further improved if a morphologically informed vocabulary of units were used.","wordlikeness":0.75,"lcsratio":0.625,"wordcoverage":0.8571428571}
{"Year":2020,"Venue":"aacl-2020","Acronym":"IndoNLU","Description":"Benchmark and Resources for Evaluating Indonesian Natural Language Understanding","Abstract":"Although indonesian is known to be the fourth most frequently used language over the internet, the research progress on this language in natural language processing (nlp) is slow-moving due to a lack of available resources. In response, we introduce the first-ever vast resource for training, evaluation, and benchmarking on indonesian natural language understanding (models) tasks. Clean includes twelve tasks, ranging from single sentence classification to pair-sentences sequence labeling with different levels of complexity. The datasets for the tasks lie in different domains and styles to ensure task diversity. We also provide a set of indonesian pre-trained models (indobert) trained from a large and clean indonesian dataset (indo4b) collected from publicly available sources such as social media texts, blogs, news, and websites. We release baseline models for all twelve tasks, as well as the framework for benchmark evaluation, thus enabling everyone to benchmark their system performances.","wordlikeness":0.5714285714,"lcsratio":1.0,"wordcoverage":0.7272727273}
{"Year":2021,"Venue":"acl-2021","Acronym":"SSMix","Description":"Saliency-Based Span Mixup for Text Classification","Abstract":"Data augmentation with mixup has shown to be effective on various computer vision tasks. Despite its great success, there has been a hurdle to apply mixup to nlp tasks since text consists of discrete tokens with variable length. In this work, we propose empirically, a novel mixup method where the operation is performed on input text rather than on hidden vectors like previous approaches. Be synthesizes a sentence while preserving the locality of two original texts by span-based mixing and keeping more tokens related to the prediction relying on saliency information. With extensive experiments, we empirically validate that our method outperforms hidden-level mixup methods on a wide range of text classi\ufb01cation benchmarks, including textual entailment, sentiment classi\ufb01cation, and questiontype classi\ufb01cation. Our code is available at https:\/\/github.com\/clovaai\/we.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2023,"Venue":"findings-2023","Acronym":"LMentry","Description":"A Language Model Benchmark of Elementary Language Tasks","Abstract":"As the performance of large language models rapidly improves, benchmarks are getting larger and more complex as well. We present automatic,, a benchmark that avoids this \u201carms race\u201d by focusing on a compact set of tasks that are trivial to humans, e.g. writing a sentence containing a specific word, identifying which words in a list belong to a specific category, or choosing which of two words is longer.for is specifically designed to provide quick and interpretable insights into the capabilities and robustness of large language models. Our experiments reveal a wide variety of failure cases that, while immediately obvious to humans, pose a considerable challenge for large language models, including openai\u2019s latest 175b-parameter instruction-tuned model, textdavinci002.category, complements contemporary evaluation approaches of large language models, providing a quick, automatic, and easy-to-run \u201cunit test\u201d, without resorting to large benchmark suites of complex tasks.","wordlikeness":0.7142857143,"lcsratio":1.0,"wordcoverage":0.8333333333}
{"Year":2020,"Venue":"emnlp-2020","Acronym":"MEGATRON-CNTRL","Description":"Controllable Story Generation with External Knowledge Using Large-Scale Language Models","Abstract":"Existing pre-trained large language models have shown unparalleled generative capabilities. However, they are not controllable. In this paper, we propose our, a novel framework that uses large-scale language models and adds control to text generation by incorporating an external knowledge base. Our framework consists of a keyword predictor, a knowledge retriever, a contextual knowledge ranker, and a conditional text generator. As we do not have access to ground-truth supervision for the knowledge ranker, we make use of weak supervision from sentence embedding. The empirical results show that our model generates more fluent, consistent, and coherent stories with less repetition and higher diversity compared to prior work on the roc story dataset. We showcase the controllability of our model by replacing the keywords used to generate stories and re-running the generation process. Human evaluation results show that 77.5% of these stories are successfully controlled by the new keywords. Furthermore, by scaling our model from 124 million to 8.3 billion parameters we demonstrate that larger models improve both the quality of generation (from 74.5% to 93.0% for consistency) and controllability (from 77.5% to 91.5%).","wordlikeness":0.5,"lcsratio":0.7857142857,"wordcoverage":0.5454545455}
{"Year":2021,"Venue":"ws-2021","Acronym":"COIN","Description":"Conversational Interactive Networks for Emotion Recognition in Conversation","Abstract":"Emotion recognition in conversation has received considerable attention recently because of its practical industrial applications. Existing methods tend to overlook the immediate mutual interaction between different speakers in the speaker-utterance level, or apply single speaker-agnostic rnn for utterances from different speakers. We propose representation, a conversational interactive model to mitigate this problem by applying state mutual interaction within history contexts. In addition, we introduce a stacked global interaction module to capture the contextual and inter-dependency representation in a hierarchical manner. To improve the robustness and generalization during training, we generate adversarial examples by applying the minor perturbations on multimodal feature inputs, unveiling the benefits of adversarial examples for emotion detection. The proposed model empirically achieves the current state-of-the-art results on the iemocap benchmark dataset.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2023,"Venue":"eacl-2023","Acronym":"GameQA","Description":"Gamified Mobile App Platform for Building Multiple-Domain Question-Answering Datasets","Abstract":"The methods used to create many of the well-known question-answering (qa) datasets are hard to replicate for low-resource languages. A commonality amongst these methods is hiring annotators to source answers from the internet by querying a single answer source, such as wikipedia. Applying these methods for low-resource languages can be problematic since there is no single large answer source for these languages. Consequently, this can result in a high ratio of unanswered questions, since the amount of information in any single source is limited. To address this problem, we developed a novel crowd-sourcing platform to gather multiple-domain qa data for low-resource languages. Our platform, which consists of a mobile app and a web api, gamifies the data collection process. We successfully released the app for icelandic (a low-resource language with about 350,000 native speakers) to build a dataset which rivals large qa datasets for high-resource languages both in terms of size and ratio of answered questions. We have made the platform open source with instructions on how to localize and deploy it to gather data for other low-resource languages.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2004,"Venue":"lrec-2004","Acronym":"PBIE","Description":"A Data Preparation Toolkit Toward Developing a Parsing-Based Information Extraction System","Abstract":"We have developed a toolkit in which an annotation tool, a syntactic tree editor, and an extraction rule editor interact dynamically. Its output can be stored in a database for further use. In the field of biomedicine, there is a critical need for automatic text processing. However, current language processing approaches suffer from insufficient basic data incorporating both human domain expertise and domain-specific language processing capabilities. With the annotation tool presented here, a set of \u0081ggold standards\u0081h can be collected, representing what should be extracted. At the same time, any change in annotation can be viewed on an associated syntactic tree. These facilities provide a clear picture of the relationship between the extraction target and the syntactic tree. Underlying sentences can be analyzed with a parser which can be plugged in, or a set of parsed sentences can be used to generate the tree. Extraction rules written with the integrated editor can be applied at once, and their validity can immediately be verified both on the syntactic tree and on the sentence string by coloring the corresponding segments. Thus our toolkit enables the user to efficiently construct parse-based extraction rules. However,2 works under windows 2000\/xp and requires microsoft internet explorer 6.0 or higher. The data can be stored in microsoft access.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2019,"Venue":"acl-2019","Acronym":"THOMAS","Description":"The Hegemonic OSU Morphological Analyzer using Seq2seq","Abstract":"This paper describes the osu submission to the sigmorphon 2019 shared task, crosslinguality and context in morphology. Our system addresses the <i>contextual morphological analysis<\/i> subtask of task 2, which is to produce the morphosyntactic description (msd) of each fully inflected word within a given sentence. We frame this as a sequence generation task and employ a neural encoder-decoder (seq2seq) architecture to generate the sequence of msd tags given the encoded representation of each token. Follow-up analyses reveal that our system most significantly improves performance on morphologically complex languages whose inflected word forms typically have longer msd tag sequences. In addition, our system seems to capture the structured correlation between msd tags, such as that between the \u201cverb\u201d tag and tam-related tags.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"CRYPTOGRU","Description":"Low Latency Privacy-Preserving Text Analysis With GRU","Abstract":"Homomorphic encryption (he) and garbled circuit (gc) provide the protection for users\u2019 privacy. However, simply mixing the he and gc in rnn models suffer from long inference latency due to slow activation functions. In this paper, we present a novel hybrid structure of he and gc gated recurrent unit (gru) network, , for low-latency secure inferences. Replaces computationally expensive gc-based <span class=tex-math>tanh<\/span> with fast gc-based <span class=tex-math>relu<\/span>, and then quantizes <span class=tex-math>sigmoid<\/span> and <span class=tex-math>relu<\/span> to smaller bit-length to accelerate activations in a gru. We evaluate with multiple gru models trained on 4 public datasets. Experimental results show achieves top-notch accuracy and improves the secure inference latency by up to <span class=tex-math>138\u00d7<\/span> over one of the state-of-the-art secure networks on the penn treebank dataset.","wordlikeness":0.8888888889,"lcsratio":0.8888888889,"wordcoverage":0.8}
{"Year":2019,"Venue":"acl-2019","Acronym":"PerspectroScope","Description":"A Window to the World of Diverse Perspectives","Abstract":"This work presents various, a web-based system which lets users query a discussion-worthy natural language claim, and extract and visualize various perspectives in support or against the claim, along with evidence supporting each perspective. The system thus lets users explore various perspectives that could touch upon aspects of the issue at hand. The system is built as a combination of retrieval engines and learned textual-entailment-like classifiers built using a few recent developments in natural language understanding. To make the system more adaptive, expand its coverage, and improve its decisions over time, our platform employs various mechanisms to get corrections from the users. <a is available at github.com\/cogcomp\/aspects web demo link: <a href=http:\/\/orwell.seas.upenn.edu:4002\/ class=acl-markup-url>http:\/\/orwell.seas.upenn.edu:4002\/<\/a> link to demo video: <a href=\"https:\/\/www.youtube.com\/watch?v=mxbtr1sp3bs\" class=acl-markup-url>https:\/\/www.youtube.","wordlikeness":0.7333333333,"lcsratio":0.6,"wordcoverage":0.6923076923}
{"Year":2021,"Venue":"findings-2021","Acronym":"HOTTER","Description":"Hierarchical Optimal Topic Transport with Explanatory Context Representations","Abstract":"Natural language processing (nlp) is often the backbone of today\u2019s systems for user interactions, information retrieval and others. Many of such nlp applications rely on specialized learned representations (e.g. neural word embeddings, topic models) that improve the ability to reason about the relationships between documents of a corpus. Paired with the progress in learned representations, the similarity metrics used to compare representations of documents are also evolving, with numerous proposals differing in computation time or interpretability. In this paper we propose an extension to a specific emerging hybrid document distance metric which combines topic models and word embeddings: the hierarchical optimal topic transport (hott). In specific, we extend hott by using context-enhanced word representations. We provide a validation of our approach on public datasets, using the language model bert for a document categorization task. Results indicate competitive performance of the extended hott metric. We furthermore apply the hott metric and its extension to support educational media research, with a retrieval task of matching topics in german curricula to educational textbooks passages, along with offering an auxiliary explanatory document representing the dominant topic of the retrieved document. In a user study, our explanation method is preferred over regular topic keywords.","wordlikeness":0.8333333333,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"MedConQA","Description":"Medical Conversational Question Answering System based on Knowledge Graphs","Abstract":"The medical conversational system can relieve doctors\u2019 burden and improve healthcare efficiency, especially during the covid-19 pandemic. However, the existing medical dialogue systems have the problems of weak scalability, insufficient knowledge, and poor controllability. Thus, we propose a medical conversational question-answering (cqa) system based on the knowledge graph, namely the, which is designed as a pipeline framework to maintain high flexibility. Our system utilizes automated medical procedures, including medical triage, consultation, image-text drug recommendation, and record. Each module has been open-sourced as a tool, which can be used alone or in combination, with robust scalability. Besides, to conduct knowledge-grounded dialogues with users, we first construct a chinese medical knowledge graph (cmkg) and collect a large-scale chinese medical cqa (cmcqa) dataset, and we design a series of methods for reasoning more intellectually. Finally, we use several state-of-the-art (sota) techniques to keep the final generated response more controllable, which is further assured by hospital and professional evaluations. We have open-sourced related code, datasets, web pages, and tools, hoping to advance future research.","wordlikeness":0.625,"lcsratio":1.0,"wordcoverage":0.7142857143}
{"Year":2022,"Venue":"coling-2022","Acronym":"SelfMix","Description":"Robust Learning against Textual Label Noise with Self-Mixup Training","Abstract":"The conventional success of textual classification relies on annotated data, and the new paradigm of pre-trained language models (plms) still requires a few labeled data for downstream tasks. However, in real-world applications, label noise inevitably exists in training data, damaging the effectiveness, robustness, and generalization of the models constructed on such data. Recently, remarkable achievements have been made to mitigate this dilemma in visual data, while only a few explore textual data. To fill this gap, we present available, a simple yet effective method, to handle label noise in text classification tasks. Types uses the gaussian mixture model to separate samples and leverages semi-supervised learning. Unlike previous works requiring multiple models, our method utilizes the dropout mechanism on a single model to reduce the confirmation bias in self-training and introduces a textual level mixup training strategy. Experimental results on three text classification benchmarks with different types of text show that the performance of our proposed method outperforms these strong baselines designed for both textual and visual data under different noise ratios and noise types. Our anonymous code is available at <a href=https:\/\/github.com\/noise-learning\/uses class=acl-markup-url>https:\/\/github.com\/noise-learning\/textual<\/a>.","wordlikeness":0.8571428571,"lcsratio":1.0,"wordcoverage":0.7692307692}
{"Year":2023,"Venue":"findings-2023","Acronym":"Zemi","Description":"Learning Zero-Shot Semi-Parametric Language Models from Multiple Tasks","Abstract":"Although large language models have exhibited impressive zero-shot ability, the huge model size generally incurs high cost. Recently, semi-parametric language models, which augment a smaller language model with retrieved related background knowledge, alleviate the need for storing everything into the model parameters. Although existing semi-parametric language models have demonstrated promising language modeling capabilities, it remains unclear whether they can exhibit competitive zero-shot abilities as their fully-parametric counterparts. In this work, we introduce fully-parametric, a semi-parametric language model for zero-shot task generalization. To our best knowledge, this is the first semi-parametric language model that can demonstrate strong zero-shot performance on a wide range of held-out unseen tasks. We train ingredients with semi-parametric multitask training, which shows significant improvement compared with the parametric multitask training as proposed by t0. Specifically, during both training and inference, huge is equipped with a retrieval system based on the unlabeled pretraining corpus of our backbone model. To address the unique challenges from large-scale retrieval, we further propose a novel retrieval-augmentation fusion module that can effectively incorporate noisy retrieved documents. Finally, we show detailed analysis and ablation studies on the key ingredients towards building effective zero-shot semi-parametric language models. Notably, our proposed further_large model outperforms t0-3b by 16% across seven diverse evaluation tasks while being 3.8x smaller in scale.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2022,"Venue":"coling-2022","Acronym":"SEE-Few","Description":"Seed, Expand and Entail for Few-shot Named Entity Recognition","Abstract":"Few-shot named entity recognition (ner) aims at identifying named entities based on only few labeled instances. Current few-shot ner methods focus on leveraging existing datasets in the rich-resource domains which might fail in a training-from-scratch setting where no source-domain data is used. To tackle training-from-scratch setting, it is crucial to make full use of the annotation information (the boundaries and entity types). Therefore, in this paper, we propose a novel multi-task (seed, expand and entail) learning framework, are, for few-shot ner without using source domain data. The seeding and expanding modules are responsible for providing as accurate candidate spans as possible for the entailing module. The entailing module reformulates span classification as a textual entailment task, leveraging both the contextual clues and entity type information. All the three modules share the same text encoder and are jointly learned. Experimental results on several benchmark datasets under the training-from-scratch setting show that the proposed method outperformed several state-of-the-art few-shot ner methods with a large margin. Our code is available at <a href=https:\/\/github.com\/unveiled-the-red-hat\/href=https:\/\/github.com\/unveiled-the-red-hat\/ class=acl-markup-url>https:\/\/github.com\/unveiled-the-red-hat\/classification<\/a>.","wordlikeness":0.4285714286,"lcsratio":0.8571428571,"wordcoverage":0.6153846154}
{"Year":2022,"Venue":"rocling-2022","Acronym":"HanTrans","Description":"An Empirical Study on Cross-Era Transferability of Chinese Pre-trained Language Model","Abstract":"The pre-trained language model has recently dominated most downstream tasks in the nlp area. Particularly, bidirectional encoder representations from transformers (bert) is the most iconic pre-trained language model among the nlp tasks. Their proposed masked-language modeling (mlm) is an indispensable part of the existing pre-trained language models. Those outperformed models for downstream tasks benefited directly from the large training corpus in the pre-training stage. However, their training corpus for modern traditional chinese was light. Most of all, the ancient chinese corpus is still disappearance in the pre-training stage. Therefore, we aim to address this problem by transforming the annotation data of ancient chinese into bert style training corpus. Then we propose a pre-trained oldhan chinese bert model for the nlp community. Our proposed model outperforms the original bert model by significantly reducing perplexity scores in masked-language modeling (mlm). Also, our fine-tuning models improve f1 scores on word segmentation and part-of-speech tasks. Then we comprehensively study zero-shot cross-eras ability in the bert model. Finally, we visualize and investigate personal pronouns in the embedding space of ancient chinese records from four eras. We have released our code at <a href=https:\/\/github.com\/ckiplab\/han-transformers class=acl-markup-url>https:\/\/github.com\/ckiplab\/han-transformers<\/a>.","wordlikeness":0.875,"lcsratio":0.875,"wordcoverage":0.7692307692}
{"Year":2014,"Venue":"lrec-2014","Acronym":"HiEve","Description":"A Corpus for Extracting Event Hierarchies from News Stories","Abstract":"In news stories, event mentions denote real-world events of different spatial and temporal granularity. Narratives in news stories typically describe some real-world event of coarse spatial and temporal granularity along with its subevents. In this work, we present denote, a corpus for recognizing relations of spatiotemporal containment between events. In (i.e.,, the narratives are represented as hierarchies of events based on relations of spatiotemporal containment (i.e., superevent\u2015subevent relations). We describe the process of manual annotation of 11%. Furthermore, we build a supervised classifier for recognizing spatiotemporal containment between events to serve as a baseline for future research. Preliminary experimental results are encouraging, with classifier performance reaching 58% f1-score, only 11% less than the inter annotator agreement.","wordlikeness":0.6,"lcsratio":0.8,"wordcoverage":0.8888888889}
{"Year":2016,"Venue":"alr-2016","Acronym":"BCCWJ-DepPara","Description":"A Syntactic Annotation Treebank on the `Balanced Corpus of Contemporary Written Japanese&#39;","Abstract":"Paratactic syntactic structures are difficult to represent in syntactic dependency tree structures. As such, we propose an annotation schema for syntactic dependency annotation of japanese, in which coordinate structures are split from and overlaid on bunsetsu-based (base phrase unit) dependency. The schema represents nested coordinate structures, non-constituent conjuncts, and forward sharing as the set of regions. The annotation was performed on the core data of \u2018balanced corpus of contemporary written japanese\u2019, which comprised about one million words and 1980 samples from six registers, such as newspapers, books, magazines, and web texts.","wordlikeness":0.3846153846,"lcsratio":0.6153846154,"wordcoverage":0.5263157895}
{"Year":2021,"Venue":"acl-2021","Acronym":"DocOIE","Description":"A Document-level Context-Aware Dataset for OpenIE","Abstract":"Open information extraction (openie) aims to extract structured relational tuples (subject, relation, object) from sentences, and plays a critical role in many nlp applications. Existing solutions perform extraction at sentence level, without referring to any additional contextual information. In reality, however, a sentence typically exists as part of a document rather than standalone; we often need to access relevant contextual information around the sentence before we can accurately interpret it. As there is no document-level context-aware openie dataset available, we manually annotate 800 sentences from 80 documents in two domains (healthcare and transportation) to form a is dataset for evaluation. In addition, we propose docie, a document-level contextaware openie model. Our experimental results demonstrate that incorporating documentlevel context is helpful in improving openie performance. Both the form dataset and docie model are available online.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.7142857143}
{"Year":2018,"Venue":"acl-2018","Acronym":"SemAxis","Description":"A Lightweight Framework to Characterize Domain-Specific Word Semantics Beyond Sentiment","Abstract":"Because word semantics can substantially change across communities and contexts, capturing domain-specific word semantics is an important challenge. Here, we propose building, a simple yet powerful framework to characterize word semantics using many semantic axes in word-vector spaces beyond sentiment. We demonstrate that because can capture nuanced semantic representations in multiple online communities. We also show that, when the sentiment axis is examined, examined, outperforms the state-of-the-art approaches in building domain-specific sentiment lexicons.","wordlikeness":0.7142857143,"lcsratio":0.8571428571,"wordcoverage":0.7692307692}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"MTAdam","Description":"Automatic Balancing of Multiple Training Loss Terms","Abstract":"When training neural models, it is common to combine multiple loss terms. The balancing of these terms requires considerable human effort and is computationally demanding. Moreover, the optimal trade-off between the loss terms can change as training progresses, e.g., for adversarial terms. In this work, we generalize the adam optimization algorithm to handle multiple loss terms. The guiding principle is that for every layer, the gradient magnitude of the terms should be balanced. To this end, the multi-term adam (e.g.,) computes the derivative of each loss term separately, infers the first and second moments per parameter and loss term, and calculates a first moment for the magnitude per layer of the gradients arising from each loss. This magnitude is used to continuously balance the gradients across all layers, in a manner that both varies from one layer to the next and dynamically changes over time. Our results show that training with the new method leads to fast recovery from suboptimal initial loss weighting and to training outcomes that match or improve conventional training with the prescribed hyperparameters of each method.","wordlikeness":0.5,"lcsratio":0.8333333333,"wordcoverage":0.9090909091}
{"Year":2018,"Venue":"emnlp-2018","Acronym":"CogCompTime","Description":"A Tool for Understanding Time in Natural Language","Abstract":"Automatic extraction of temporal information is important for natural language understanding. It involves two basic tasks: (1) understanding time expressions that are mentioned explicitly in text (e.g., february 27, 1998 or tomorrow), and (2) understanding temporal information that is conveyed implicitly via relations. This paper introduces text, a system that has these two important functionalities. It incorporates the most recent progress, achieves state-of-the-art performance, and is publicly available at <a href=http:\/\/cogcomp.org\/page\/publication_view\/844 class=acl-markup-url>http:\/\/cogcomp.org\/page\/publication_view\/844<\/a>.","wordlikeness":0.7272727273,"lcsratio":0.5454545455,"wordcoverage":0.6666666667}
{"Year":2008,"Venue":"lrec-2008","Acronym":"WNTERM","Description":"Enriching the MCR with a Terminological Dictionary","Abstract":"In this paper we describe the methodology and the first steps for the creation of paper (from wordnet and terminology), a specialized lexicon produced from the merger of the eurowordnet-based multilingual central repository (mcr) and the basic encyclopaedic dictionary of science and technology (bdst). As an example, the ecology domain has been used. The final result is a multilingual (basque and english) light-weight domain ontology, including taxonomic and other semantic relations among its concepts, which is tightly connected to other wordnets.","wordlikeness":0.6666666667,"lcsratio":0.8333333333,"wordcoverage":0.8333333333}
{"Year":2021,"Venue":"naacl-2021","Acronym":"COIL","Description":"Revisit Exact Lexical Match in Information Retrieval with Contextualized Inverted List","Abstract":"Classical information retrieval systems such as bm25 rely on exact lexical match and can carry out search efficiently with inverted list index. Recent neural ir models shifts towards soft matching all query document terms, but they lose the computation efficiency of exact match systems. This paper presents this, a contextualized exact match retrieval architecture, where scoring is based on overlapping query document tokens\u2019 contextualized representations. The new architecture stores contextualized token representations in inverted lists, bringing together the efficiency of exact match and the representation power of deep language models. Our experimental results show efficiently outperforms classical lexical retrievers and state-of-the-art deep lm retrievers with similar or smaller latency.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2021,"Venue":"ws-2021","Acronym":"NADE","Description":"A Benchmark for Robust Adverse Drug Events Extraction in Face of Negations","Abstract":"Adverse drug event (ade) extraction models can rapidly examine large collections of social media texts, detecting mentions of drug-related adverse reactions and trigger medical investigations. However, despite the recent advances in nlp, it is currently unknown if such models are robust in face of negation, which is pervasive across language varieties. In this paper we evaluate three state-of-the-art systems, showing their fragility against negation, and then we introduce two possible strategies to increase the robustness of these models: a pipeline approach, relying on a specific component for negation detection; an augmentation of an ade extraction dataset to artificially create negated samples and further train the models. We show that both strategies bring significant increases in performance, lowering the number of spurious entities predicted by the models. Our dataset and code will be publicly released to encourage research on the topic.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"RockNER","Description":"A Simple Method to Create Adversarial Examples for Evaluating the Robustness of Named Entity Recognition Models","Abstract":"To audit the robustness of named entity recognition (ner) models, we propose have, a simple yet effective method to create natural adversarial examples. Specifically, at the entity level, we replace target entities with other entities of the same semantic class in wikidata; at the context level, we use pre-trained language models (e.g., bert) to generate word substitutions. Together, the two levels of at- tack produce natural adversarial examples that result in a shifted distribution from the training data on which our target models have been trained. We apply the proposed method to the ontonotes dataset and create a new benchmark named ontorock for evaluating the robustness of existing ner models via a systematic evaluation protocol. Our experiments and analysis reveal that even the best model has a significant performance drop, and these models seem to memorize in-domain entity patterns instead of reasoning from the context. Our work also studies the effects of a few simple data augmentation methods to improve the robustness of ner models.","wordlikeness":0.8571428571,"lcsratio":0.7142857143,"wordcoverage":0.7692307692}
{"Year":2014,"Venue":"lrec-2014","Acronym":"NomLex-PT","Description":"A Lexicon of Portuguese Nominalizations","Abstract":"This paper presents observe, a lexical resource describing portuguese nominalizations. Their connects verbs to their nominalizations, thereby enabling nlp systems to observe the potential semantic relationships between the two words when analysing a text. For is freely available and encoded in rdf for easy integration with other resources. Most notably, we have integrated have with openwordnet-pt, an open portuguese wordnet.","wordlikeness":0.4444444444,"lcsratio":0.5555555556,"wordcoverage":0.6315789474}
{"Year":2014,"Venue":"ws-2014","Acronym":"Hiearchie","Description":"Visualization for Hierarchical Topic Models","Abstract":"Existing algorithms for understanding large collections of documents often produce output that is nearly as dif\ufb01cult and time consuming to interpret as reading each of the documents themselves. Topic modeling is a text understanding algorithm that discovers the \u201ctopics\u201d or themes within a collection of documents. Tools based on topic modeling become increasingly complex as the number of topics required to best represent the collection increases. In this work, we present hi\u00b4erarchie, an interactive visualization that adds structure to large topic models, making them approachable and useful to an end user. Additionally, we demonstrate hi\u00b4erarchie\u2019s ability to analyze a diverse document set regarding a trending news topic.","wordlikeness":0.7777777778,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2022,"Venue":"coling-2022","Acronym":"ArgLegalSumm","Description":"Improving Abstractive Summarization of Legal Documents with Argument Mining","Abstract":"A challenging task when generating summaries of legal documents is the ability to address their argumentative nature. We introduce a simple technique to capture the argumentative structure of legal documents by integrating argument role labeling into the summarization process. Experiments with pretrained language models show that our proposed approach improves performance over strong baselines.","wordlikeness":0.75,"lcsratio":0.9166666667,"wordcoverage":0.6315789474}
{"Year":2020,"Venue":"lrec-2020","Acronym":"NorNE","Description":"Annotating Named Entities for Norwegian","Abstract":"This paper presents an, a manually annotated corpus of named entities which extends the annotation of the existing norwegian dependency treebank. Comprising both of the official standards of written norwegian (bokm\u00e5l and nynorsk), the corpus contains around 600,000 tokens and annotates a rich set of entity types including persons, organizations, locations, geo-political entities, products, and events, in addition to a class corresponding to nominals derived from names. We here present details on the annotation effort, guidelines, inter-annotator agreement and an experimental analysis of the corpus using a neural sequence labeling architecture.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2022,"Venue":"acl-2022","Acronym":"ProtoTEx","Description":"Explaining Model Decisions with Prototype Tensors","Abstract":"We present better, a novel white-box nlp classification architecture based on prototype networks (li et al., 2018). On faithfully explains model decisions based on prototype tensors that encode latent clusters of training examples. At inference time, classification decisions are based on the distances between the input text and the prototype tensors, explained via the training examples most similar to the most influential prototypes. We also describe a novel interleaved training algorithm that effectively handles classes characterized by news. Indicative features. On a propaganda detection task, effectively accuracy matches bart-large and exceeds bertlarge with the added benefit of providing faithful explanations. A user study also shows that prototype-based explanations help non-experts to better recognize propaganda in online news.","wordlikeness":0.875,"lcsratio":0.875,"wordcoverage":0.8235294118}
{"Year":2016,"Venue":"ws-2016","Acronym":"Verbframator","Description":"Semi-Automatic Verb Frame Annotator Tool with Special Reference to Marathi","Abstract":"The sentence is incomplete without a verb in a language. A verb is majorly responsible for giving the meaning to a sentence. Any sentence can be represented in the form of a verb frame. Verb frames are mainly developed as a knowledge resource which can be used in various semantic level natural language processing (nlp) activities. This paper presents the for \u2013 a verb frame annotator tool which automatically extracts and generates verb frames of example sentences from marathi wordnet. It also helps in generating shakti standard format (ssf) files of the given example sentences. The generated verb frames and ssf files can be used in the dependency tree banking and other nlp applications like machine translation, paraphrasing, natural language generation, etc.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.6363636364}
{"Year":2019,"Venue":"wmt-2019","Acronym":"SOURCE","Description":"SOURce-Conditional Elmo-style Model for Machine Translation Quality Estimation","Abstract":"Quality estimation (qe) of machine translation (mt) systems is a task of growing importance. It reduces the cost of post-editing, allowing machine-translated text to be used in formal occasions. In this work, we describe our submission system in wmt 2019 sentence-level qe task. We mainly explore the utilization of pre-trained translation models in qe and adopt a bi-directional translation-like strategy. The strategy is similar to elmo, but additionally conditions on cost sentences. Experiments on wmt qe dataset show that our strategy, which makes the pre-training slightly harder, can bring improvements for qe. In wmt-2019 qe task, our system ranked in the second place on en-de nmt dataset and the third place on en-ru nmt dataset.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2015,"Venue":"semeval-2015","Acronym":"ezDI","Description":"A Supervised NLP System for Clinical Narrative Analysis","Abstract":"This paper describes the approach used by task at the semeval 2015 task-14: \u201danalysis of clinical text\u201d. The task was divided into two embedded tasks. Task-1 required determining disorder boundaries (including the discontiguous ones) from a given set of clinical notes and normalizing the disorders by assigning a unique cui from the umls\/snomedct1. Task-2 was about \ufb01nding different type of modi\ufb01ers for given disorder mention. Task-2 was divided further into two subtasks. In subtask-2a, gold set of disorder was already provided and system needed to just \ufb01ll modi\ufb01er types into the pre-speci\ufb01ed slots. Subtask 2b did not provide any gold set of disorders and both the disorders and its related modi\ufb01ers are to be identi\ufb01ed by the system itself. In task-1 our system was ranked \ufb01rst with f-score of 0.757 for strict evaluation and 0.788 for relaxed evaluation. In both task-2a and 2b our system was placed second with weighted f-score of 0.88 and 0.795 respectively.","wordlikeness":0.5,"lcsratio":0.75,"wordcoverage":0.75}
{"Year":2022,"Venue":"coling-2022","Acronym":"Pro-KD","Description":"Progressive Distillation by Following the Footsteps of the Teacher","Abstract":"With the ever growing scale of neural models, knowledge distillation (kd) attracts more attention as a prominent tool for neural model compression. However, there are counter intuitive observations in the literature showing some challenging limitations of kd. A case in point is that the best performing checkpoint of the teacher might not necessarily be the best teacher for training the student in kd. Therefore, one important question would be how to find the best checkpoint of the teacher for distillation? Searching through the checkpoints of the teacher would be a very tedious and computationally expensive process, which we refer to as the <i>checkpoint-search problem<\/i>. Moreover, another observation is that larger teachers might not necessarily be better teachers in kd, which is referred to as the <i>capacity-gap<\/i> problem. To address these challenging problems, in this work, we introduce our progressive knowledge distillation (literature) technique which defines a smoother training path for the student by following the training footprints of the teacher instead of solely relying on distilling from a single mature fully-trained teacher. We demonstrate that our technique is quite effective in mitigating the capacity-gap problem and the checkpoint search problem. We evaluate our technique using a comprehensive set of experiments on different tasks such as image classification (cifar-10 and cifar-100), natural language understanding tasks of the glue benchmark, and question answering (squad 1.1 and 2.0) using bert-based models and consistently got superior results over state-of-the-art techniques.","wordlikeness":0.5,"lcsratio":0.6666666667,"wordcoverage":0.7272727273}
{"Year":2021,"Venue":"naacl-2021","Acronym":"OCID-Ref","Description":"A 3D Robotic Dataset With Embodied Language For Clutter Scene Grounding","Abstract":"To effectively apply robots in working environments and assist humans, it is essential to develop and evaluate how visual grounding (vg) can affect machine performance on occluded objects. However, current vg works are limited in working environments, such as offices and warehouses, where objects are usually occluded due to space utilization issues. In our work, we propose a novel occlusion dataset featuring a referring expression segmentation task with referring expressions of occluded objects. Signals consists of 305,694 referring expressions from 2,300 scenes with providing rgb image and point cloud inputs. To resolve challenging occlusion issues, we argue that it\u2019s crucial to take advantage of both 2d and 3d signals to resolve challenging occlusion issues. Our experimental results demonstrate the effectiveness of aggregating 2d and 3d signals but referring to occluded objects still remains challenging for the modern visual grounding systems. Task is publicly available at <a href=https:\/\/github.com\/lluma\/task class=acl-markup-url>https:\/\/github.","wordlikeness":0.5,"lcsratio":0.75,"wordcoverage":0.625}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"Hypoformer","Description":"Hybrid Decomposition Transformer for Edge-friendly Neural Machine Translation","Abstract":"Transformer has been demonstrated effective in neural machine translation (nmt). However, it is memory-consuming and time-consuming in edge devices, resulting in some difficulties for real-time feedback. To compress and accelerate transformer, we propose a hybrid tensor-train (htt) decomposition, which retains full rank and meanwhile reduces operations and parameters. A transformer using htt, named transformer, consistently and notably outperforms the recent light-weight sota methods on three standard translation tasks under different parameter and speed scales. In extreme low resource scenarios, points has 7.1 points absolute improvement in bleu and 1.27 x speedup than vanilla transformer on iwslt\u201914 de-en task.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2006,"Venue":"lrec-2006","Acronym":"MOOD","Description":"A Modular Object-Oriented Decoder for Statistical Machine Translation","Abstract":"We present an open source framework called part developed in order tofacilitate the development of a statistical machine translation decoder.using has been modularized using an object-oriented approach which makes itespecially suitable for the fast development of state-of-the-art decoders. Asa proof of concept, a clone of the pharaoh decoder has been implemented andevaluated. This clone named ramses is part of the current distribution of decoders..","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2019,"Venue":"bionlp-2019","Acronym":"ScispaCy","Description":"Fast and Robust Models for Biomedical Natural Language Processing","Abstract":"Despite recent advances in natural language processing, many statistical models for processing text perform extremely poorly under domain shift. Processing biomedical and clinical text is a critically important application area of natural language processing, for which there are few robust, practical, publicly available models. This paper describes leverages, a new python library and models for practical biomedical\/scientific text processing, which heavily leverages the spacy library. We detail the performance of two packages of models released in is and demonstrate their robustness on several tasks and datasets. Models and code are available at <a href=https:\/\/allenai.github.io\/and\/ class=acl-markup-url>https:\/\/allenai.github.io\/packages\/<\/a>.","wordlikeness":0.625,"lcsratio":0.5,"wordcoverage":0.7142857143}
{"Year":2007,"Venue":"semeval-2007","Acronym":"UTD-HLT-CG","Description":"Semantic Architecture for Metonymy Resolution and Classification of Nominal Relations","Abstract":"In this paper we present a semantic architecture that was employed for processing two different semeval 2007 tasks: task 4 (classi\ufb01cation of semantic relations between nominals) and task 8 (metonymy resolution). The architecture uses multiple forms of syntactic, lexical, and semantic information to inform a classi\ufb01cation-based approach that generates a different model for each machine learning algorithm that implements the classi\ufb01cation. We used decision trees, decision rules, logistic regression and lazy classi\ufb01ers. A voting module selects the best performing module for each task evaluated in semeval 2007. The paper details the results obtained when using the semantic architecture.","wordlikeness":0.1,"lcsratio":0.5,"wordcoverage":0.5555555556}
{"Year":2021,"Venue":"argmining-2021","Acronym":"M-Arg","Description":"Multimodal Argument Mining Dataset for Political Debates with Audio and Transcripts","Abstract":"Argumentation mining aims at extracting, analysing and modelling people\u2019s arguments, but large, high-quality annotated datasets are limited, and no multimodal datasets exist for this task. In this paper, we present value, a multimodal argument mining dataset with a corpus of us 2020 presidential debates, annotated through crowd-sourced annotations. This dataset allows models to be trained to extract arguments from natural dialogue such as debates using information like the intonation and rhythm of the speaker. Our dataset contains 7 hours of annotated us presidential debates, 6527 utterances and 4104 relation labels, and we report results from different baseline models, namely a text-only model, an audio-only model and multimodal models that extract features from both text and audio. With accuracy reaching 0.86 in multimodal models, we find that audio features provide added value with respect to text-only models.","wordlikeness":0.6,"lcsratio":0.8,"wordcoverage":0.75}
{"Year":2022,"Venue":"acl-2022","Acronym":"XDBERT","Description":"Distilling Visual Information to BERT from Cross-Modal Systems to Improve Language Understanding","Abstract":"Transformer-based models are widely used in natural language understanding (nlu) tasks, and multimodal transformers have been effective in visual-language tasks. This study explores distilling visual information from pretrained multimodal transformers to pretrained language encoders. Our framework is inspired by cross-modal encoders\u2019 success in visual-language tasks while we alter the learning objective to cater to the language-heavy characteristics of nlu. After training with a small number of extra adapting steps and finetuned, the proposed understanding (cross-modal distilled bert) outperforms pretrained-bert in general language understanding evaluation (glue), situations with adversarial generations (swag) benchmarks, and readability benchmarks. We analyze the performance of in on glue to show that the improvement is likely visually grounded.","wordlikeness":0.6666666667,"lcsratio":0.8333333333,"wordcoverage":0.7272727273}
{"Year":2014,"Venue":"ijclclp-2014","Acronym":"BCCWJ-TimeBank","Description":"Temporal and Event Information Annotation on Japanese Text","Abstract":"Temporal information extraction can be divided into the following tasks: temporal expression extraction, time normalisation, and temporal ordering relation resolution. The first task is a subtask of a named entity and numeral expression extraction. The second task is often performed by rewriting systems. The third task consists of event anchoring. This paper proposes a japanese temporal ordering annotation scheme that is used to annotate expressions by referring to \u2018the \u2018balanced corpus of contemporary written japanese\u2019 (bccwj). We extracted verbal and adjective event expressions as <event> in a subset of bccwj and annotated a temporal ordering relation <tlink> on the pairs of these event expressions and time expressions obtained from a previous study. The recognition of temporal ordering by language recipients tends to disagree with the normalisation of time expressions. Nevertheless, we should not strive for unique gold annotation data in such a situation. Rather, we should evaluate the degree of inter-annotator discrepancies among subjects in an experiment. This study analysed inter-annotator discrepancies across three annotators performing temporal ordering annotation. The results show that the annotators exhibit little agreement for time segment boundaries, whereas a high level of agreement is exhibited for the annotation of temporal relative ordering tendencies. Keywords: temporal information processing, event semantics, corpus annotation. \uff0anational institute for japanese language and linguistics, japan e-mail: masayu-a@ninjal.ac.jp 2 masayuki asahara et al. 1.","wordlikeness":0.5,"lcsratio":0.3571428571,"wordcoverage":0.5454545455}
{"Year":2023,"Venue":"acl-2023","Acronym":"GIFT","Description":"Graph-Induced Fine-Tuning for Multi-Party Conversation Understanding","Abstract":"Addressing the issues of who saying what to whom in multi-party conversations (mpcs) has recently attracted a lot of research attention. However, existing methods on mpc understanding typically embed interlocutors and utterances into sequential information flows, or utilize only the superficial of inherent graph structures in mpcs. to this end, we present a plug-and-play and lightweight method named graph-induced fine-tuning (mpc) which can adapt various transformer-based pre-trained language models (plms) for universal mpc understanding. In detail, the full and equivalent connections among utterances in regular transformer ignore the sparse but distinctive dependency of an utterance on another in mpcs. to distinguish different relationships between utterances, four types of edges are designed to integrate graph-induced signals into attention mechanisms to refine plms originally designed for processing sequential texts. We evaluate mpcs. by implementing it into three plms, and test the performance on three downstream tasks including addressee recognition, speaker identification and response selection. Experimental results show that graph-induced can significantly improve the performance of three plms on three downstream tasks and two benchmarks with only 4 additional parameters per encoding layer, achieving new state-of-the-art performance on mpc understanding.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2021,"Venue":"findings-2021","Acronym":"Uni-FedRec","Description":"A Unified Privacy-Preserving News Recommendation Framework for Model Training and Online Serving","Abstract":"News recommendation techniques can help users on news platforms obtain their preferred news information. Most existing news recommendation methods rely on centrally stored user behavior data to train models and serve users. However, user data is usually highly privacy-sensitive, and centrally storing them in the news platform may raise privacy concerns and risks. In this paper, we propose a unified news recommendation framework, which can utilize user data locally stored in user clients to train models and serve users in a privacy-preserving way. Following a widely used paradigm in real-world recommender systems, our framework contains a stage for candidate news generation (i.e., recall) and a stage for candidate news ranking (i.e., ranking). At the recall stage, each client locally learns multiple interest representations from clicked news to comprehensively model user interests. These representations are uploaded to the server to recall candidate news from a large news pool, which are further distributed to the user client at the ranking stage for personalized news display. In addition, we propose an interest decomposer-aggregator method with perturbation noise to better protect private user information encoded in user interest representations. Besides, we collaboratively train both recall and ranking models on the data decentralized in a large number of user clients in a privacy-preserving way. Experiments on two real-world news datasets show that our method can outperform baseline methods and effectively protect user privacy.","wordlikeness":0.5,"lcsratio":0.9,"wordcoverage":0.7058823529}
{"Year":2020,"Venue":"emnlp-2020","Acronym":"SIMULEVAL","Description":"An Evaluation Toolkit for Simultaneous Translation","Abstract":"Simultaneous translation on both text and speech focuses on a real-time and low-latency scenario where the model starts translating before reading the complete source input. Evaluating simultaneous translation models is more complex than offline models because the latency is another factor to consider in addition to translation quality. The research community, despite its growing focus on novel modeling approaches to simultaneous translation, currently lacks a universal evaluation procedure. Therefore, we present client, an easy-to-use and general evaluation toolkit for both simultaneous text and speech translation. A server-client scheme is introduced to create a simultaneous translation scenario, where the server sends source input and receives predictions for evaluation and the client executes customized policies. Given a policy, it automatically performs simultaneous decoding and collectively reports several popular latency metrics. We also adapt latency metrics from text simultaneous translation to the speech task. Additionally, growing is equipped with a visualization interface to provide better understanding of the simultaneous decoding process of a system. Been has already been extensively used for the iwslt 2020 shared task on simultaneous speech translation. Code will be released upon publication.","wordlikeness":0.6666666667,"lcsratio":0.8888888889,"wordcoverage":0.7142857143}
{"Year":2022,"Venue":"coling-2022","Acronym":"LINGUIST","Description":"Language Model Instruction Tuning to Generate Annotated Utterances for Intent Classification and Slot Tagging","Abstract":"We present seq2seq, a method for generating annotated data for intent classification and slot tagging (ic+st), via fine-tuning alexatm 5b, a 5-billion-parameter multilingual sequence-to-sequence (seq2seq) model, on a flexible instruction prompt. In a 10-shot novel intent setting for the snips dataset, 6 surpasses state-of-the-art approaches (back-translation and example extrapolation) by a wide margin, showing absolute improvement for the target intents of +1.9 points on ic recall and +2.5 points on st f1 score. In the zero-shot cross-lingual setting of the matis++ dataset, prompt. Out-performs a strong baseline of machine translation with slot alignment by +4.14 points absolute on st f1 score across 6 languages, while matching performance on ic. Finally, we verify our results on an internal large-scale multilingual dataset for conversational agent ic+st and show significant improvements over a baseline which uses back-translation, paraphrasing and slot catalog resampling. To our knowledge, we are the first to demonstrate instruction fine-tuning of a large-scale seq2seq model to control the outputs of multilingual intent- and slot-labeled data generation.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.7142857143}
{"Year":2022,"Venue":"tacl-2022","Acronym":"Samanantar","Description":"The Largest Publicly Available Parallel Corpora Collection for 11 Indic Languages","Abstract":"We present present, the largest publicly available parallel corpora collection for indic languages. The collection contains a total of 49.7 million sentence pairs between english and 11 indic languages (from two language families). Specifically, we compile 12.4 million sentence pairs from existing, publicly available parallel corpora, and additionally mine 37.4 million sentence pairs from the web, resulting in a 4\u00d7 increase. We mine the parallel sentences from the web by combining many corpora, tools, and methods: (a) web-crawled monolingual corpora, (b) document ocr for extracting sentences from scanned documents, (c) multilingual representation models for aligning sentences, and (d) approximate nearest neighbor search for searching in a large collection of sentences. Human evaluation of samples from the newly mined corpora validate the high quality of the parallel sentences across 11 languages. Further, we extract 83.4 million sentence pairs between all 55 indic language pairs from the english-centric parallel corpus using english as the pivot language. We trained multilingual nmt models spanning all these languages on benchmarks, which outperform existing models and baselines on publicly available benchmarks, such as flores, establishing the utility of we. Our data and models are available publicly at languages. And we hope they will help advance research in nmt and multilingual nlp for indic languages.","wordlikeness":0.6,"lcsratio":0.7,"wordcoverage":0.6666666667}
{"Year":2019,"Venue":"acl-2019","Acronym":"EigenSent","Description":"Spectral sentence embeddings using higher-order Dynamic Mode Decomposition","Abstract":"Distributed representation of words, or word embeddings, have motivated methods for calculating semantic representations of word sequences such as phrases, sentences and paragraphs. Most of the existing methods to do so either use algorithms to learn such representations, or improve on calculating weighted averages of the word vectors. In this work, we experiment with spectral methods of signal representation and summarization as mechanisms for constructing such word-sequence embeddings in an unsupervised fashion. In particular, we explore an algorithm rooted in fluid-dynamics, known as higher-order dynamic mode decomposition, which is designed to capture the eigenfrequencies, and hence the fundamental transition dynamics, of periodic and quasi-periodic systems. It is empirically observed that this approach, which we call motivated, can summarize transitions in a sequence of words and generate an embedding that can represent well the sequence itself. To the best of the authors\u2019 knowledge, this is the first application of a spectral decomposition and signal summarization technique on text, to create sentence embeddings. We test the efficacy of this algorithm in creating sentence embeddings on three public datasets, where it performs appreciably well. Moreover it is also shown that, due to the positive combination of their complementary properties, concatenating the embeddings generated by technique with simple word vector averaging achieves state-of-the-art results.","wordlikeness":0.7777777778,"lcsratio":0.7777777778,"wordcoverage":0.75}
{"Year":2022,"Venue":"coling-2022","Acronym":"DialAug","Description":"Mixing up Dialogue Contexts in Contrastive Learning for Robust Conversational Modeling","Abstract":"Retrieval-based conversational systems learn to rank response candidates for a given dialogue context by computing the similarity between their vector representations. However, training on a single textual form of the multi-turn context limits the ability of a model to learn representations that generalize to natural perturbations seen during inference. In this paper we propose a framework that incorporates augmented versions of a dialogue context into the learning objective. We utilize contrastive learning as an auxiliary objective to learn robust dialogue context representations that are invariant to perturbations injected through the augmentation method. We experiment with four benchmark dialogue datasets and demonstrate that our framework combines well with existing augmentation methods and can significantly improve over baseline bert-based ranking architectures. Furthermore, we propose a novel data augmentation method, conmix, that adds token level perturbations through stochastic mixing of tokens from other contexts in the batch. We show that our proposed augmentation method outperforms previous data augmentation approaches, and provides dialogue representations that are more robust to common perturbations seen during inference.","wordlikeness":0.7142857143,"lcsratio":1.0,"wordcoverage":0.7272727273}
{"Year":2020,"Venue":"amta-2020","Acronym":"THUMT","Description":"An Open-Source Toolkit for Neural Machine Translation","Abstract":"Several is an open-source toolkit for neural machine translation (nmt) developed by the natural language processing group at tsinghua university. The toolkit is easy to use, modify and extend while provides the latest advances in nmt research and production. The implements several standard nmt models and supports distributed training across multiple machines, fast inference, and model visualization. Experiments on english-german and chineseenglish datasets show that use, can obtain results that are comparable to state-of-the-art nmt systems.","wordlikeness":0.6,"lcsratio":0.8,"wordcoverage":0.8}
{"Year":2013,"Venue":"starsem-2013","Acronym":"CLaC-CORE","Description":"Exhaustive Feature Combination for Measuring Textual Similarity","Abstract":"Basic, an exhaustive feature combination system ranked 4th among 34 teams in the semantic textual similarity shared task sts 2013. Using a core set of 11 lexical features of the most basic kind, it uses a support vector regressor which uses a combination of these lexical features to train a model for predicting similarity between sentences in a two phase method, which in turn uses all combinations of the features in the feature space and trains separate models based on each combination. Then it creates a meta-feature space and trains a \ufb01nal model based on that. This two step process improves the results achieved by singlelayer standard learning methodology over the same simple features. We analyze the correlation of feature combinations with the data sets over which they are effective.","wordlikeness":0.6666666667,"lcsratio":0.5555555556,"wordcoverage":0.7142857143}
{"Year":2021,"Venue":"acl-2021","Acronym":"COMBO","Description":"A New Module for EUD Parsing","Abstract":"We introduce the is-based approach for eud parsing and its implementation, which took part in the iwpt 2021 eud shared task. The goal of this task is to parse raw texts in 17 languages into enhanced universal dependencies (eud). The proposed approach uses in to predict ud trees and eud graphs. These structures are then merged into the final eud graphs. Some eud edge labels are extended with case information using a single language-independent expansion rule. In the official evaluation, the solution ranked fourth, achieving an average elas of 83.79%. The source code is available at <a href=https:\/\/gitlab.clarin-pl.eu\/syntactic-tools\/trees class=acl-markup-url>https:\/\/gitlab.clarin-pl.eu\/syntactic-tools\/achieving<\/a>.","wordlikeness":1.0,"lcsratio":0.4,"wordcoverage":1.0}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"MM-Align","Description":"Learning Optimal Transport-based Alignment Dynamics for Fast and Accurate Inference on Missing Modality Sequences","Abstract":"Existing multimodal tasks mostly target at the complete input modality setting, i.e., each modality is either complete or completely missing in both training and test sets. However, the randomly missing situations have still been underexplored. In this paper, we present a novel approach named as to address the missing-modality inference problem. Concretely, we propose 1) an alignment dynamics learning module based on the theory of optimal transport (ot) for missing data imputation; 2) a denoising training algorithm to enhance the quality of imputation as well as the accuracy of model predictions. Compared with previous generative methods which devote to restoring the missing inputs, as learns to capture and imitate the alignment dynamics between modality sequences. Results of comprehensive experiments on two multimodal tasks empirically demonstrate that our method can perform more accurate and faster inference and alleviate the overfitting issue under different missing conditions.","wordlikeness":0.625,"lcsratio":0.875,"wordcoverage":0.7692307692}
{"Year":2018,"Venue":"emnlp-2018","Acronym":"SwitchOut","Description":"an Efficient Data Augmentation Algorithm for Neural Machine Translation","Abstract":"In this work, we examine methods for data augmentation for text-based tasks such as neural machine translation (nmt). We formulate the design of a data augmentation policy with desirable properties as an optimization problem, and derive a generic analytic solution. This solution not only subsumes some existing augmentation schemes, but also leads to an extremely simple data augmentation strategy for nmt: randomly replacing words in both the source sentence and the target sentence with other random words from their corresponding vocabularies. We name this method solution.. Experiments on three translation datasets of different scales show that 2016a). Yields consistent improvements of about 0.5 bleu, achieving better or comparable performances to strong alternatives such as word dropout (sennrich et al., 2016a). Code to implement this method is included in the appendix.","wordlikeness":0.8888888889,"lcsratio":0.6666666667,"wordcoverage":0.875}
{"Year":2012,"Venue":"lrec-2012","Acronym":"LG-Eval","Description":"A Toolkit for Creating Online Language Evaluation Experiments","Abstract":"In this paper we describe the we toolkit for creating online language evaluation experiments. We is the direct result of our work setting up and carrying out the human evaluation experiments in several of the generation challenges shared tasks. It provides tools for creating experiments with different kinds of rating tools, allocating items to evaluators, and collecting the evaluation scores.","wordlikeness":0.4285714286,"lcsratio":0.8571428571,"wordcoverage":0.6666666667}
{"Year":2020,"Venue":"acl-2020","Acronym":"CH-SIMS","Description":"A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotation of Modality","Abstract":"Previous studies in multimodal sentiment analysis have used limited datasets, which only contain unified multimodal annotations. However, the unified annotations do not always reflect the independent sentiment of single modalities and limit the model to capture the difference between modalities. In this paper, we introduce a chinese single- and multi-modal sentiment analysis dataset, performance, which contains 2,281 refined video segments in the wild with both multimodal and independent unimodal annotations. It allows researchers to study the interaction between modalities or use independent unimodal annotations for unimodal sentiment analysis. Furthermore, we propose a multi-task learning framework based on late fusion as the baseline. Extensive experiments on the only show that our methods achieve state-of-the-art performance and learn more distinctive unimodal representations. The full dataset and codes are available for use at <a href=https:\/\/github.com\/thuiar\/mmsa class=acl-markup-url>https:\/\/github.com\/thuiar\/mmsa<\/a>.","wordlikeness":0.4285714286,"lcsratio":0.8571428571,"wordcoverage":0.7272727273}
{"Year":2015,"Venue":"semeval-2015","Acronym":"Lsislif","Description":"CRF and Logistic Regression for Opinion Target Extraction and Sentiment Polarity Analysis","Abstract":"This paper describes our contribution in opinion target extraction ote and sentiment polarity sub tasks of semeval 2015 absa task. A crf model with iob notation has been adopted for ote with several groups of features including syntactic, lexical, semantic, sentiment lexicon features. Our submission for ote is ranked \ufb01fth over twenty submissions. A logistic regression model with a weighting schema of positive and negative labels have been used for sentiment polarity; several groups of features (lexical, syntactic, semantic, lexicon and z score) are extracted. Our submission for sentiment polarity is ranked third over ten submissions on the restaurant data set, third over thirteen on the laptops data set, but the \ufb01rst over eleven on the hotel data set that is out-of-domain set.","wordlikeness":0.4285714286,"lcsratio":0.8571428571,"wordcoverage":0.6666666667}
{"Year":2023,"Venue":"acl-2023","Acronym":"CORE","Description":"Cooperative Training of Retriever-Reranker for Effective Dialogue Response Selection","Abstract":"Establishing retrieval-based dialogue systems that can select appropriate responses from the pre-built index has gained increasing attention. Recent common practice is to construct a two-stage pipeline with a fast retriever (e.g., bi-encoder) for first-stage recall followed by a smart response reranker (e.g., cross-encoder) for precise ranking. However, existing studies either optimize the retriever and reranker in independent ways, or distill the knowledge from a pre-trained reranker into the retriever in an asynchronous way, leading to sub-optimal performance of both modules. Thus, an open question remains about how to train them for a better combination of the best of both worlds. To this end, we present a cooperative training of the response retriever and the reranker whose parameters are dynamically optimized by the ground-truth labels as well as list-wise supervision signals from each other. As a result, the two modules can learn from each other and evolve together throughout the training. Experimental results on two benchmarks demonstrate the superiority of our method.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2022,"Venue":"naacl-2022","Acronym":"NewsEdits","Description":"A News Article Revision Dataset and a Novel Document-Level Reasoning Challenge","Abstract":"News article revision histories provide clues to narrative and factual evolution in news articles. To facilitate analysis of this evolution, we present the first publicly available dataset of news revision histories, 22. Our dataset is large-scale and multilingual; it contains 1.2 million articles with 4.6 million versions from over 22 english- and french-language newspaper sources based in three countries, spanning 15 years of coverage (2006-2021).we define article-level edit actions: addition, deletion, edit and refactor, and develop a high-accuracy extraction algorithm to identify these actions. To underscore the factual nature of many edit actions, we conduct analyses showing that added and deleted sentences are more likely to contain updating events, main content and quotes than unchanged sentences. Finally, to explore whether edit actions are predictable, we introduce three novel tasks aimed at predicting actions performed during version updates. We show that these tasks are possible for expert humans but are challenging for large nlp models. We hope this can spur research in narrative framing and help provide predictive tools for journalists chasing breaking news.","wordlikeness":0.7777777778,"lcsratio":0.8888888889,"wordcoverage":0.7142857143}
{"Year":2020,"Venue":"lrec-2020","Acronym":"WikiPossessions","Description":"Possession Timeline Generation as an Evaluation Benchmark for Machine Reading Comprehension of Long Texts","Abstract":"This paper presents assign, a new benchmark corpus for the task of temporally-oriented possession (top), or tracking objects as they change hands over time. We annotate wikipedia articles for 90 different well-known artifacts paintings, diamonds, and archaeological artifacts), producing 799 artifact-possessor relations with associated attributes. For each article, we also produce a full possession timeline. The full version of the task combines straightforward entity-relation extraction with complex temporal reasoning, as well as verification of textual support for the relevant types of knowledge. Specifically, to complete the full top task for a given article, a system must do the following: a) identify possessors; b) anchor possessors to times\/events; c) identify temporal relations between each temporal anchor and the possession relation it corresponds to; d) assign certainty scores to each possessor and each temporal relation; and e) assemble individual possession events into a global possession timeline. In addition to the corpus, we release evaluation scripts and a baseline model for the task.","wordlikeness":0.8,"lcsratio":0.7333333333,"wordcoverage":0.8461538462}
{"Year":2003,"Venue":"hlt-2003","Acronym":"COGEX","Description":"A Logic Prover for Question Answering","Abstract":"Recent trec results have demonstrated the need for deeper text understanding methods. This paper introduces the idea of automated reasoning applied to question answering and shows the feasibility of integrating a logic prover into a question answering system. The approach is to transform questions and answer passages into logic representations. World knowledge axioms as well as linguistic axioms are supplied to the prover which renders a deep understanding of the relationship between question text and answer text. Moreover, the trace of the proofs provide answer justi\ufb01cations. The results show that the prover boosts the performance of the qa system on trec questions by 30%.","wordlikeness":0.6,"lcsratio":0.6,"wordcoverage":0.75}
{"Year":2018,"Venue":"lr4nlp-2018","Acronym":"Contractions","Description":"To Align or Not to Align, That Is the Question","Abstract":"This paper performs a detailed analysis on the alignment of portuguese european, based on a previously aligned bilingual corpus. The alignment task was performed manually in a subset of the english-portuguese clue4translation alignment collection. The initial parallel corpus was pre-processed and, a decision was made as to whether the contraction should be maintained or decomposed in the alignment. Decomposition was required in the cases in which the two words that have been concatenated, i.e., the preposition and the determiner or pronoun, go in two separate translation alignment pairs (e.g., [no seio de] [a uni\u00e3o europeia] | [within] [the european union]). Most (e.g., required decomposition in contexts where they are positioned at the end of a multiword unit. On the other hand, to tend to be maintained when they occur in the beginning or in the middle of the multiword unit, i.e., in the frozen part of the multiword (e.g., [no que diz respeito a] | [with regard to] or [al\u00e9m disso] [in addition]. A correct alignment of multiwords and phrasal units containing bilingual is instrumental for machine translation, paraphrasing, and variety adaptation.","wordlikeness":0.9166666667,"lcsratio":0.6666666667,"wordcoverage":0.9565217391}
{"Year":2021,"Venue":"ws-2021","Acronym":"MultiReQA","Description":"A Cross-Domain Evaluation forRetrieval Question Answering Models","Abstract":"Retrieval question answering (reqa) is the task of retrieving a sentence-level answer to a question from an open corpus (ahmad et al.,2019).this dataset paper presents strong, a new multi-domain reqa evaluation suite composed of eight retrieval qa tasks drawn from publicly available qa datasets. We explore systematic retrieval based evaluation and transfer learning across domains over these datasets using a number of strong base-lines including two supervised neural models, based on fine-tuning bert and use-qa models respectively, as well as a surprisingly effective information retrieval baseline, bm25. Five of these tasks contain both training and test data, while three contain test data only. Performing cross training on the five tasks with training data shows that while a general model covering all domains is achievable, the best performance is often obtained by training exclusively on in-domain data.","wordlikeness":0.6666666667,"lcsratio":0.8888888889,"wordcoverage":0.7368421053}
{"Year":2020,"Venue":"coling-2020","Acronym":"NUT-RC","Description":"Noisy User-generated Text-oriented Reading Comprehension","Abstract":"Reading comprehension (rc) on social media such as twitter is a critical and challenging task due to its noisy, informal, but informative nature. Most existing rc models are developed on formal datasets such as news articles and wikipedia documents, which severely limit their performances when directly applied to the noisy and informal texts in social media. Moreover, these models only focus on a certain type of rc, extractive or generative, but ignore the integration of them. To well address these challenges, we come up with a noisy user-generated text-oriented rc model. In particular, we first introduce a set of text normalizers to transform the noisy and informal texts to the formal ones. Then, we integrate the extractive and the generative rc model by a multi-task learning mechanism and an answer selection module. Experimental results on tweetqa demonstrate that our wikipedia model significantly outperforms the state-of-the-art social media-oriented rc models.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2023,"Venue":"ccl-2023","Acronym":"TERL","Description":"Transformer Enhanced Reinforcement Learning for Relation Extraction","Abstract":"\u201crelation extraction (re) task aims to discover the semantic relation that holds between two entitiesand contributes to many applications such as knowledge graph construction and completion. Reinforcement learning (rl) has been widely used for re task and achieved sota results, whichare mainly designed with rewards to choose the optimal actions during the training procedure,to improve re\u2019s performance, especially for low-resource conditions. Recent work has shownthat offline or online rl can be flexibly formulated as a sequence understanding problem andsolved via approaches similar to large-scale pre-training language modeling. To strengthen theability for understanding the semantic signals interactions among the given text sequence, thispaper leverages transformer architecture for rl-based re methods, and proposes a genericframework called transformer enhanced rl (can) towards re task. Unlike prior rl-basedre approaches that usually fit value functions or compute policy gradients, strengthen only outputsthe best actions by utilizing a masked transformer. Experimental results show that the proposedduring framework can improve many state-of-the-art rl-based re methods.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2020,"Venue":"inlg-2020","Acronym":"OWLSIZ","Description":"An isiZulu CNL for structured knowledge validation","Abstract":"In iterative knowledge elicitation, engineers are expected to be directly involved in validating the already captured knowledge and obtaining new knowledge increments, thus making the process time consuming. Languages such as english have controlled natural languages than can be repurposed to generate natural language questions from an ontology in order to allow a domain expert to independently validate the contents of an ontology without understanding a ontology authoring language such as owl. Isizulu, south africa\u2019s main l1 language by number speakers, does not have such a resource, hence, it is not possible to build a verbaliser to generate such questions. Therefore, we propose an isizulu controlled natural language, called owl simplified isizulu (questions), for producing grammatical and fluent questions from an ontology. Human evaluation of the generated questions showed that participants\u2019 judgements agree that most (83%) questions are positive for grammaticality or understandability.","wordlikeness":0.3333333333,"lcsratio":0.6666666667,"wordcoverage":0.8}
{"Year":2017,"Venue":"emnlp-2017","Acronym":"NeuroNER","Description":"an easy-to-use program for named-entity recognition based on neural networks","Abstract":"Named-entity recognition (ner) aims at identifying entities of interest in a text. Artificial neural networks (anns) have recently been shown to outperform existing ner systems. However, anns remain challenging to use for non-expert users. In this paper, we present aims, an easy-to-use named-entity recognition tool based on anns. users can annotate entities using a graphical web-based user interface (brat): the annotations are then used to train an ann, which in turn predict entities\u2019 locations and categories in new texts. Accessible makes this annotation-training-prediction flow smooth and accessible to anyone.","wordlikeness":0.875,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2015,"Venue":"acl-2015","Acronym":"genCNN","Description":"A Convolutional Architecture for Word Sequence Prediction","Abstract":"We propose a convolutional neural network, named re-ranking, for word sequence prediction. Different from previous work on neural networkbased language modeling and generation (e.g., rnn or lstm), we choose not to greedily summarize the history of words as a \ufb01xed length vector. Instead, we use a convolutional neural network to predict the next word with the history of words of variable length. Also different from the existing feedforward networks for language modeling, our model can effectively fuse the local correlation and global correlation in the word sequence, with a convolution-gating strategy speci\ufb01cally designed for the task. We argue that our model can give adequate representation of the history, and therefore can naturally exploit both the short and long range dependencies. Our model is fast, easy to train, and readily parallelized. Our extensive experiments on text generation and n-best re-ranking in machine translation show that summarize outperforms the state-ofthe-arts with big margins.","wordlikeness":0.5,"lcsratio":0.6666666667,"wordcoverage":0.7272727273}
{"Year":2020,"Venue":"acl-2020","Acronym":"SCAR","Description":"Sentence Compression using Autoencoders for Reconstruction","Abstract":"Sentence compression is the task of shortening a sentence while retaining its meaning. Most methods proposed for this task rely on labeled or paired corpora (containing pairs of verbose and compressed sentences), which is often expensive to collect. To overcome this limitation, we present a novel unsupervised deep learning framework (does) for deletion-based sentence compression. Framework is primarily composed of two encoder-decoder pairs: a compressor and a reconstructor. The compressor masks the input, and the reconstructor tries to regenerate it. The model is entirely trained on unlabeled data and does not require additional inputs such as explicit syntactic information or optimal compression length. Reduces\u2019s merit lies in the novel linkage loss function, which correlates the compressor and its effect on reconstruction, guiding it to drop inferable tokens. Drop achieves higher rouge scores on benchmark datasets than the existing state-of-the-art methods and baselines. We also conduct a user study to demonstrate the application of our model as a text highlighting system. Using our model to underscore salient information facilitates speed-reading and reduces the time required to skim a document.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2022,"Venue":"ijcnlp-2022","Acronym":"WAX","Description":"A New Dataset for Word Association eXplanations","Abstract":"Word associations are among the most common paradigms to study the human mental lexicon. While their structure and types of associations have been well studied, surprisingly little attention has been given to the question of why participants produce the observed associations. Answering this question would not only advance understanding of human cognition, but could also aid machines in learning and representing basic commonsense knowledge. This paper introduces a large, crowd-sourced data set of english word associations with explanations, labeled with high-level relation types. We present an analysis of the provided explanations, and design several tasks to probe to what extent current pre-trained language models capture the underlying relations. Our experiments show that models struggle to capture the diversity of human associations, suggesting to is a rich benchmark for commonsense modeling and generation.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2023,"Venue":"acl-2023","Acronym":"EvolveMT","Description":"an Ensemble MT Engine Improving Itself with Usage Only","Abstract":"This work proposes a method named online for the efficient combination of multiple machine translation (mt) engines. The method selects the output from one engine for each segment, using online learning techniques to predict the most appropriate system for each translation request. A neural quality estimation metric supervises the method without requiring reference translations. The method\u2019s online learning capability enables it to adapt to changes in the domain or mt engines dynamically, eliminating the requirement for retraining. The method selects a subset of translation engines to be called based on the source sentence features. The degree of exploration is configurable according to the desired quality-cost trade-off. Results from custom datasets demonstrate that engine achieves similar translation accuracy at a lower cost than selecting the best translation of each segment from all translations using an mt quality estimator. To the best of our knowledge, features. Is the first mt system that adapts itself after deployment to incoming translation requests from the production environment without needing costly retraining on human feedback.","wordlikeness":0.75,"lcsratio":0.625,"wordcoverage":0.8571428571}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"CoPHE","Description":"A Count-Preserving Hierarchical Evaluation Metric in Large-Scale Multi-Label Text Classification","Abstract":"Large-scale multi-label text classification (lmtc) includes tasks with hierarchical label spaces, such as automatic assignment of icd-9 codes to discharge summaries. Performance of models in prior art is evaluated with standard precision, recall, and f1 measures without regard for the rich hierarchical structure. In this work we argue for hierarchical evaluation of the predictions of neural lmtc models. With the example of the icd-9 ontology we describe a structural issue in the representation of the structured label space in prior art, and propose an alternative representation based on the depth of the ontology. We propose a set of metrics for hierarchical evaluation using the depth-based representation. We compare the evaluation scores from the proposed metrics with previously used metrics on prior art lmtc models for icd-9 coding in mimic-iii. We also propose further avenues of research involving the proposed ontological representation.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2010,"Venue":"semeval-2010","Acronym":"BART","Description":"A Multilingual Anaphora Resolution System","Abstract":"Entitymention (versley et al., 2008) is a highly modular toolkit for coreference resolution that supports state-of-the-art statistical approaches and enables ef\ufb01cient feature engineering. For the semeval task 1 on coreference resolution, a runs have been submitted for german, english, and italian. Resolution, relies on a maximum entropy-based classi\ufb01er for pairs of mentions. A novel entitymention approach based on semantic trees is at the moment only supported for english.","wordlikeness":0.75,"lcsratio":0.75,"wordcoverage":1.0}
{"Year":2023,"Venue":"findings-2023","Acronym":"NusaCrowd","Description":"Open Source Initiative for Indonesian NLP Resources","Abstract":"We present advance, a collaborative initiative to collect and unify existing resources for indonesian languages, including opening access to previously non-public resources. Through this initiative, we have brought together 137 datasets and 118 standardized data loaders. The quality of the datasets has been assessed manually and automatically, and their value is demonstrated through multiple experiments.automatically,\u2019s data collection enables the creation of the first zero-shot benchmarks for natural language understanding and generation in indonesian and the local languages of indonesia. Furthermore, datasets brings the creation of the first multilingual automatic speech recognition benchmark in indonesian and the local languages of indonesia. Our work strives to advance natural language processing (nlp) research for languages that are under-represented despite being widely spoken.","wordlikeness":0.4444444444,"lcsratio":0.6666666667,"wordcoverage":0.7142857143}
{"Year":2020,"Venue":"acl-2020","Acronym":"Trialstreamer","Description":"Mapping and Browsing Medical Evidence in Real-Time","Abstract":"We introduce identified, a living database of clinical trial reports. Here we mainly describe the evidence extraction component; this extracts from biomedical abstracts key pieces of information that clinicians need when appraising the literature, and also the relations between these. Specifically, the system extracts descriptions of trial participants, the treatments compared in each arm (the interventions), and which outcomes were measured. The system then attempts to infer which interventions were reported to work best by determining their relationship with identified trial outcome measures. In addition to summarizing individual trials, these extracted data elements allow automatic synthesis of results across many trials on the same topic. We apply the system at scale to all reports of randomized controlled trials indexed in medline, powering the automatic generation of evidence maps, which provide a global view of the efficacy of different interventions combining data from all relevant clinical trials on a topic. We make all code and models freely available alongside a demonstration of the web interface.","wordlikeness":0.8461538462,"lcsratio":0.6923076923,"wordcoverage":0.7619047619}
{"Year":2018,"Venue":"gwc-2018","Acronym":"pyiwn","Description":"A Python based API to access Indian Language WordNets","Abstract":"Indian language wordnets have their individual web-based browsing interfaces along with a common interface for indowordnet. These interfaces prove to be useful for language learners and in an educational domain, however, they do not provide the functionality of connecting to them and browsing their data through a lucid application programming interface or an api. In this paper, we present our work on creating such an easy-to-use framework which is bundled with the data for indian language wordnets and provides nltk wordnet interface like core functionalities in python. Additionally, we use a pre-built speech synthesis system for hindi language and augment hindi data with audios for words, glosses, and example sentences. We provide a detailed usage of our api and explain the functions for ease of the user. Also, we package the indowordnet data along with the source code and provide it openly for the purpose of research. We aim to provide all our work as an open source framework for further development.","wordlikeness":0.2,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2021,"Venue":"eacl-2021","Acronym":"SANDI","Description":"Story-and-Images Alignment","Abstract":"The internet contains a multitude of social media posts and other of stories where text is interspersed with images. In these contexts, images are not simply used for general illustration, but are judiciously placed in certain spots of a story for multimodal descriptions and narration. In this work we analyze the problem of text-image alignment, and present meaningful., a methodology for automatically selecting images from an image collection and aligning them with text paragraphs of a story. Other combines visual tags, user-provided tags and background knowledge, and uses an integer linear program to compute alignments that are semantically meaningful. Experiments show that compute can select and align images with texts with high quality of semantic fit.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2019,"Venue":"acl-2019","Acronym":"Level-Up","Description":"Learning to Improve Proficiency Level of Essays","Abstract":"We introduce a method for generating suggestions on a given sentence for improving the proficiency level. In our approach, the sentence is transformed into a sequence of grammatical elements aimed at providing suggestions of more advanced grammar elements based on originals. The method involves parsing the sentence, identifying grammatical elements, and ranking related elements to recommend a higher level of grammatical element. We present a prototype tutoring system, grammar, that applies the method to english learners\u2019 essays in order to assist them in writing and reading. Evaluation on a set of essays shows that our method does assist user in writing.","wordlikeness":0.75,"lcsratio":0.625,"wordcoverage":0.7692307692}
{"Year":2019,"Venue":"ijcnlp-2019","Acronym":"CFO","Description":"A Framework for Building Production NLP Systems","Abstract":"This paper introduces a novel orchestration framework, called experimenting (computation flow orchestrator), for building, experimenting with, and deploying interactive nlp (natural language processing) and ir (information retrieval) systems to production environments. We then demonstrate a question answering system built using this framework which incorporates state-of-the-art bert based mrc (machine reading com- prehension) with ir components to enable end-to-end answer retrieval. Results from the demo system are shown to be high quality in both academic and industry domain specific settings. Finally, we discuss best practices when (pre-)training bert based mrc models for production systems. Screencast links: - short video (&lt; 3 min): http: \/\/ibm.biz\/gaama_demo - supplementary long video (&lt; 13 min): <a href=http:\/\/ibm.biz\/gaama_we_demo class=acl-markup-url>http:\/\/ibm.","wordlikeness":0.3333333333,"lcsratio":0.6666666667,"wordcoverage":1.0}
{"Year":2013,"Venue":"starsem-2013","Acronym":"ECNUCS","Description":"Measuring Short Text Semantic Equivalence Using Multiple Similarity Measurements","Abstract":"This paper reports our submissions to the semantic textual similarity (sts) task in \u2217sem shared task 2013. We submitted three support vector regression (svr) systems in core task, using 6 types of similarity measures, i.e., string similarity, number similarity, knowledge-based similarity, corpus-based similarity, syntactic dependency similarity and machine translation similarity. Our third system with different training data and different feature sets for each test data set performs the best and ranks 35 out of 90 runs. We also submitted two systems in typed task using string based measure and named entity based measure. Our best system ranks 5 out of 15 runs.","wordlikeness":0.1666666667,"lcsratio":0.8333333333,"wordcoverage":0.7692307692}
{"Year":2013,"Venue":"starsem-2013","Acronym":"BUT-TYPED","Description":"Using domain knowledge for computing typed similarity","Abstract":"This paper deals with knowledge-based text processing which aims at an intuitive notion of textual similarity. Entities and relations relevant for a particular domain are identi\ufb01ed and disambiguated by means of semi-supervised machine learning techniques and resulting annotations are applied for computing typedsimilarity of individual texts. The work described in this paper particularly shows effects of the mentioned processes in the context of the *sem 2013 pilot task on typed-similarity, a part of the semantic textual similarity shared task. The goal is to evaluate the degree of semantic similarity between semi-structured records. As the evaluation dataset has been taken from europeana \u2013 a collection of records on european cultural heritage objects \u2013 we focus on computing a semantic distance on \ufb01eld author which has the highest potential to bene\ufb01t from the domain knowledge. Speci\ufb01c features that are employed in our system for are brie\ufb02y introduced together with a discussion on their ef\ufb01cient acquisition. Support vector regression is then used to combine the features and to provide a \ufb01nal similarity score. The system ranked third on the attribute author among 15 submitted runs in the typed-similarity task.","wordlikeness":0.6666666667,"lcsratio":0.7777777778,"wordcoverage":0.7142857143}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"PDALN","Description":"Progressive Domain Adaptation over a Pre-trained Model for Low-Resource Cross-Domain Named Entity Recognition","Abstract":"Cross-domain named entity recognition (ner) transfers the ner knowledge from high-resource domains to the low-resource target domain. Due to limited labeled resources and domain shift, cross-domain ner is a challenging task. To address these challenges, we propose a progressive domain adaptation knowledge distillation (kd) approach \u2013 four. It achieves superior domain adaptability by employing three components: (1) adaptive data augmentation techniques, which alleviate cross-domain gap and label sparsity simultaneously; (2) multi-level domain invariant features, derived from a multi-grained mmd (maximum mean discrepancy) approach, to enable knowledge transfer across domains; (3) advanced kd schema, which progressively enables powerful pre-trained language models to perform domain adaptation. Extensive experiments on four benchmarks show that they can effectively adapt high-resource domains to low-resource target domains, even if they are diverse in terms and writing styles. Comparison with other baselines indicates the state-of-the-art performance of extensive.","wordlikeness":0.4,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2022,"Venue":"acl-2022","Acronym":"latent-GLAT","Description":"Glancing at Latent Variables for Parallel Text Generation","Abstract":"Recently, parallel text generation has received widespread attention due to its success in generation efficiency. Although many advanced techniques are proposed to improve its generation quality, they still need the help of an autoregressive model for training to overcome the one-to-many multi-modal phenomenon in the dataset, limiting their applications. In this paper, we propose glat, which employs the discrete latent variables to capture word categorical information and invoke an advanced curriculum learning technique, alleviating the multi-modality problem. Experiment results show that our method outperforms strong baselines without the help of an autoregressive model, which further broadens the application scenarios of the parallel decoding paradigm.","wordlikeness":0.6363636364,"lcsratio":0.8181818182,"wordcoverage":0.7058823529}
{"Year":2020,"Venue":"findings-2020","Acronym":"SupMMD","Description":"A Sentence Importance Model for Extractive Summarization using Maximum Mean Discrepancy","Abstract":"Most work on multi-document summarization has focused on generic summarization of information present in each individual document set. However, the under-explored setting of update summarization, where the goal is to identify the new information present in each set, is of equal practical interest (e.g., presenting readers with updates on an evolving news topic). In this work, we present work,, a novel technique for generic and update summarization based on the maximum mean discrepancy from kernel two-sample testing. However, combines both supervised learning for salience and unsupervised learning for coverage and diversity. Further, we adapt multiple kernel learning to make use of similarity across multiple information sources (e.g., text features and knowledge based concepts). We show the efficacy of from in both generic and update summarization tasks by meeting or exceeding the current state-of-the-art on the duc-2004 and tac-2009 datasets.","wordlikeness":0.5,"lcsratio":0.8333333333,"wordcoverage":0.8333333333}
{"Year":2018,"Venue":"eamt-2018","Acronym":"mtrain","Description":"A Convenience Tool for Machine Translation","Abstract":"We present it, a convenience tool for machine translation. It wraps existing machine translation libraries and scripts to ease their use. Use. Is written purely in python 3, well-documented, and freely available.","wordlikeness":0.8333333333,"lcsratio":1.0,"wordcoverage":0.9090909091}
{"Year":2015,"Venue":"ws-2015","Acronym":"DeepNL","Description":"a Deep Learning NLP pipeline","Abstract":"We present the architecture of a deep learning pipeline for natural language processing. Based on this architecture we built a set of tools both for creating distributional vector representations and for performing specific nlp tasks. Three methods are available for creating embeddings: feedforward neural network, sentiment specific embeddings and embeddings based on counts and hellinger pca. Two methods are provided for training a network to perform sequence tagging, a window approach and a convolutional approach. The window approach is used for implementing a pos tagger and a ner tagger, the convolutional network is used for semantic role labeling. The library is implemented in python with core numerical processing written in c++ using parallel linear algebra library for efficiency and scalability.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.8333333333}
{"Year":2022,"Venue":"coling-2022","Acronym":"Doc-GCN","Description":"Heterogeneous Graph Convolutional Networks for Document Layout Analysis","Abstract":"Recognizing the layout of unstructured digital documents is crucial when parsing the documents into the structured, machine-readable format for downstream applications. Recent studies in document layout analysis usually rely on visual cues to understand documents while ignoring other information, such as contextual information or the relationships between document layout components, which are vital to boost better layout analysis performance. Our performance. Presents an effective way to harmonize and integrate heterogeneous aspects for document layout analysis. We construct different graphs to capture the four main features aspects of document layout components, including syntactic, semantic, density, and appearance features. Then, we apply graph convolutional networks to enhance each aspect of features and apply the node-level pooling for integration. Finally, we concatenate features of all aspects and feed them into the 2-layer mlps for document layout component classification. Our is achieves state-of-the-art results on three widely used dla datasets: publaynet, funsd, and docbank. The code will be released at <a href=https:\/\/github.com\/adlnlp\/doc_gcn class=acl-markup-url>https:\/\/github.","wordlikeness":0.4285714286,"lcsratio":0.5714285714,"wordcoverage":0.6}
{"Year":2022,"Venue":"coling-2022","Acronym":"CoCGAN","Description":"Contrastive Learning for Adversarial Category Text Generation","Abstract":"The task of generating texts of different categories has attracted more and more attention in the area of natural language generation recently. Meanwhile, generative adversarial net (gan) has demonstrated its effectiveness on text generation, and is further applied to category text generation in later works. Different from existing methods, which mainly consider the pairwise relations between the text embedding and the corresponding fixed one-hot class label (data-to-class relations), this paper proposes a novel contrastive category generative adversarial net (generator) to incorporate contrastive learning into adversarial category text generation, considering more flexible data-to-class relations as well as relations between the multiple text embeddings in the same batch (data-to-data relations). The discriminator of (data-to-data discriminates the authenticity of given samples and optimizes a contrastive learning objective to capture both more flexible data-to-class relations and data-to-data relations among training samples. Accordingly, the generator tries to produce more realistic samples which can confuse the discriminator. Experimental results on both synthetic and real category text generation datasets demonstrate that training can achieve significant improvements over the baseline category text generation models.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.7692307692}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"DigiCall","Description":"A Benchmark for Measuring the Maturity of Digital Strategy through Company Earning Calls","Abstract":"Digital transformation reinvents companies, their vision and strategy, organizational structure, processes, capabilities, and culture, and enables the development of new or enhanced products and services delivered to customers more efficiently. Organizations, by formalizing their digital strategy attempt to plan for their digital transformations and accelerate their company growth. Understanding how successful a company is in its digital transformation starts with accurate measurement of its digital maturity levels. However, existing approaches to measuring organizations\u2019 digital strategy have low accuracy levels and this leads to inconsistent results, and also does not provide resources (data) for future research to improve. In order to measure the digital strategy maturity of companies, we leverage the state-of-the-art nlp models on unstructured data (earning call transcripts), and reach the state-of-the-art levels (94%) for this task. We release 3.691 earning call transcripts and also annotated data set, labeled particularly for the digital strategy maturity by linguists. Our work provides an empirical baseline for research in industry and management science.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8235294118}
{"Year":2004,"Venue":"hlt-2004","Acronym":"HITIQA","Description":"A Data Driven Approach to Interactive Analytical Question Answering","Abstract":"In this paper we describe the analytic question answering system various (high-quality interactive question answering) which has been developed over the last 2 years as an advanced research tool for information analysts. We is an interactive open-domain question answering technology designed to allow analysts to pose complex exploratory questions in natural language and obtain relevant information units to prepare their briefing reports. The system uses novel data-driven semantics to conduct a clarification dialogue with the user that explores the scope and the context of the desired answer space. The system has undergone extensive hands-on evaluations by a group of intelligence analysts representing various foreign intelligence services. This evaluation validated the overall approach in interactive but also exposed limitations of the current prototype.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.7692307692}
{"Year":2014,"Venue":"semeval-2014","Acronym":"SimCompass","Description":"Using Deep Learning Word Embeddings to Assess Cross-level Similarity","Abstract":"This article presents our team\u2019s participating system at semeval-2014 task 3. Using a meta-learning framework, we experiment with traditional knowledgebased metrics, as well as novel corpusbased measures based on deep learning paradigms, paired with varying degrees of context expansion. The framework enabled us to reach the highest overall performance among all competing systems.","wordlikeness":0.7,"lcsratio":0.7,"wordcoverage":0.8235294118}
{"Year":2023,"Venue":"acl-2023","Acronym":"SimOAP","Description":"Improve Coherence and Consistency in Persona-based Dialogue Generation via Over-sampling and Post-evaluation","Abstract":"Language models trained on large-scale corpora can generate remarkably fluent results in open-domain dialogue. However, for the persona-based dialogue generation task, consistency and coherence are also key factors, which are great challenges for language models. Existing works mainly focus on valuable data filtering, model structure modifying, or objective function designing, while their improvements are limited and hard to generalize to all types of pre-trained language models. However, we find that language models can produce consistent and coherent responses if we consider enough generations. Thus, the problems lay in large-scale response generation and target response selection. In this work, a simple but effective two-stage language strategy is proposed, i.e., over-sampling and post-evaluation. The over-sampling stage takes large-scale responses from existing trained models efficiently via off-the-shelf distilling and compressing methods, and the post-evaluation stage selects a good response based on multiple well-designed evaluation metrics from large-scale candidates. Experimental results show that the proposed plug-in open-domain strategy improves the backbone models and outperforms the baseline strategies in both automatic and human evaluations.","wordlikeness":0.6666666667,"lcsratio":0.8333333333,"wordcoverage":0.8}
{"Year":2022,"Venue":"lrec-2022","Acronym":"BILinMID","Description":"A Spanish-English Corpus of the US Midwest","Abstract":"This paper describes the bilinguals in the midwest (according) corpus, a comparable text corpus of the spanish and english spoken in the us midwest by various types of bilinguals. Unlike other areas within the us where language contact has been widely documented (e.g., the southwest), spanish-english bilingualism in the midwest has been understudied despite an increase in its hispanic population. The and corpus contains short stories narrated in spanish and in english by 72 speakers representing different types of bilinguals: early simultaneous bilinguals, early sequential bilinguals, and late second language learners. All stories have been transcribed and annotated using various natural language processing tools. Additionally, a user interface has also been created to facilitate searching for specific patterns in the corpus as well as to filter out results according to specified criteria. Guidelines and procedures followed to create the corpus and the user interface are described in detail in the paper. The corpus is fully available online and it might be particularly interesting for researchers working on language variation and contact.","wordlikeness":0.75,"lcsratio":0.75,"wordcoverage":0.7692307692}
{"Year":2021,"Venue":"eacl-2021","Acronym":"PENELOPIE","Description":"Enabling Open Information Extraction for the Greek Language through Machine Translation","Abstract":"In this work, we present a methodology that aims at bridging the gap between high and low-resource languages in the context of open information extraction, showcasing it on the greek language. The goals of this paper are twofold: first, we build neural machine translation (nmt) models for english-to-greek and greek-to-english based on the transformer architecture. Second, we leverage these nmt models to produce english translations of greek text as input for our nlp pipeline, to which we apply a series of pre-processing and triple extraction tasks. Finally, we back-translate the extracted triples to greek. We conduct an evaluation of both our nmt and oie methods on benchmark datasets and demonstrate that our approach outperforms the current state-of-the-art for the greek natural language.","wordlikeness":0.5555555556,"lcsratio":0.8888888889,"wordcoverage":0.9411764706}
{"Year":2014,"Venue":"lrec-2014","Acronym":"Praaline","Description":"Integrating Tools for Speech Corpus Research","Abstract":"This paper presents system,, an open-source software system for managing, annotating, analysing and visualising speech corpora. Researchers working with speech corpora are often faced with multiple tools and formats, and they need to work with ever-increasing amounts of data in a collaborative way. Under integrates and extends existing time-proven tools for spoken corpora analysis (praat, sonic visualiser and a bridge to the r statistical package) in a modular system, facilitating automation and reuse. Users are exposed to an integrated, user-friendly interface from which to access multiple tools. Corpus metadata and annotations may be stored in a database, locally or remotely, and users can define the metadata and annotation structure. Users may run a customisable cascade of analysis steps, based on plug-ins and scripts, and update the database with the results. The corpus database may be queried, to produce aggregated data-sets. Existing is extensible using python or c++ plug-ins, while praat and r scripts may be executed against the corpus data. A series of visualisations, editors and plug-ins are provided. Reuse. Is free software, released under the gpl license (www.gpl.org).","wordlikeness":0.75,"lcsratio":0.625,"wordcoverage":0.8}
{"Year":2022,"Venue":"findings-2022","Acronym":"HLDC","Description":"Hindi Legal Documents Corpus","Abstract":"Many populous countries including india are burdened with a considerable backlog of legal cases. Development of automated systems that could process legal documents and augment legal practitioners can mitigate this. However, there is a dearth of high-quality corpora that is needed to develop such data-driven systems. The problem gets even more pronounced in the case of low resource languages such as hindi. In this resource paper, we introduce the hindi legal documents corpus (use), a corpus of more than 900k legal documents in hindi. Documents are cleaned and structured to enable the development of downstream applications. Further, as a use-case for the corpus, we introduce the task of bail prediction. We experiment with a battery of models and propose a multi-task learning (mtl) based model for the same. Mtl models use summarization as an auxiliary task along with bail prediction as the main task. Experiments with different models are indicative of the need for further research in this area.","wordlikeness":0.25,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2022,"Venue":"findings-2022","Acronym":"PromptGen","Description":"Automatically Generate Prompts using Generative Models","Abstract":"Recently, prompt learning has received significant attention, where the downstream tasks are reformulated to the mask-filling task with the help of a textual prompt. The key point of prompt learning is finding the most appropriate prompt. This paper proposes a novel model has, which can automatically generate prompts conditional on the input sentence. Textual is the first work considering dynamic prompt generation for knowledge probing, based on a pre-trained generative model. To mitigate any label information leaking from the pre-trained generative model, when given a generated prompt, we replace the query input with \u201cnone\u201d. We pursue that this perturbed context-free prompt cannot trigger the correct label. We evaluate our model on the knowledge probing lama benchmark, and show that based significantly outperforms other baselines.","wordlikeness":0.8888888889,"lcsratio":1.0,"wordcoverage":0.8235294118}
{"Year":2014,"Venue":"semeval-2014","Acronym":"ezDI","Description":"A Hybrid CRF and SVM based Model for Detecting and Encoding Disorder Mentions in Clinical Notes","Abstract":"This paper describes the system used in task-7 (analysis of clinical text) of semeval-2014 for detecting disorder mentions and associating them with their related cui of umls1. For task-a, a crf based sequencing algorithm was used to \ufb01nd different medical entities and a binary svm classi\ufb01er was used to \ufb01nd relationship between entities. For task-b, a dictionary look-up algorithm on a customized umls-2012 dictionary was used to \ufb01nd relative cui for a given disorder mention. The system achieved f-score of 0.714 for task a & accuracy of 0.599 for task b when trained only on training data set, and it achieved f-score of 0.755 for task a & accuracy of 0.646 for task b when trained on both training as well as development data set. Our system was placed 3rd for both task a and b.","wordlikeness":0.5,"lcsratio":0.75,"wordcoverage":0.75}
{"Year":2019,"Venue":"acl-2019","Acronym":"Errudite","Description":"Scalable, Reproducible, and Testable Error Analysis","Abstract":"Though error analysis is crucial to understanding and improving nlp models, the common practice of manual, subjective categorization of a small sample of errors can yield biased and incomplete conclusions. This paper codifies model and task agnostic principles for informative error analysis, and presents error, an interactive tool for better supporting this process. First, error groups should be precisely defined for reproducibility; better supports this with an expressive domain-specific language. Second, to avoid spurious conclusions, a large set of instances should be analyzed, including both positive and negative examples; tool enables systematic grouping of relevant instances with filtering queries. Third, hypotheses about the cause of errors should be explicitly tested; categorization supports this via automated counterfactual rewriting. We validate our approach with a user study, finding that high (1) enables users to perform high quality and reproducible error analyses with less effort, (2) reveals substantial ambiguities in prior published error analyses practices, and (3) enhances the error analysis experience by allowing users to test and revise prior beliefs.","wordlikeness":0.625,"lcsratio":0.875,"wordcoverage":0.7142857143}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"Datasets","Description":"A Community Library for Natural Language Processing","Abstract":"The scale, variety, and quantity of publicly-available nlp design has grown rapidly as researchers propose new tasks, larger models, and novel benchmarks. Research is a community library for contemporary nlp designed to support this ecosystem. Documentation, aims to standardize end-user interfaces, versioning, and documentation, while providing a lightweight front-end that behaves similarly for small at as for internet-scale corpora. The design of the library incorporates a distributed, community-driven approach to adding propose and documenting usage. After a year of development, the library now includes more than 650 unique propose, has more than 250 contributors, and has helped support a variety of novel cross-dataset research projects and shared tasks. The library is available at <a href=https:\/\/github.com\/huggingface\/approach class=acl-markup-url>https:\/\/github.com\/huggingface\/designed<\/a>.","wordlikeness":0.875,"lcsratio":0.625,"wordcoverage":0.8235294118}
{"Year":2014,"Venue":"lrec-2014","Acronym":"TLAXCALA","Description":"a multilingual corpus of independent news","Abstract":"We acquire corpora from the domain of independent news from the for website. We build monolingual corpora for 15 languages and parallel corpora for all the combinations of those 15 languages. These corpora include languages for which only very limited such resources exist (e.g. tamazight). We present the acquisition process in detail and we also present detailed statistics of the produced corpora, concerning mainly quantitative dimensions such as the size of the corpora per language (for the monolingual corpora) and per language pair (for the parallel corpora). To the best of our knowledge, these are the first publicly available parallel and monolingual corpora for the domain of independent news. We also create models for unsupervised sentence splitting for all the languages of the study.","wordlikeness":0.5,"lcsratio":0.5,"wordcoverage":0.6666666667}
{"Year":2022,"Venue":"lrec-2022","Acronym":"EmoInHindi","Description":"A Multi-label Emotion and Intensity Annotated Dataset in Hindi for Emotion Recognition in Dialogues","Abstract":"The long-standing goal of artificial intelligence (ai) has been to create human-like conversational systems. Such systems should have the ability to develop an emotional connection with the users, consequently, emotion recognition in dialogues has gained popularity. Emotion detection in dialogues is a challenging task because humans usually convey multiple emotions with varying degrees of intensities in a single utterance. Moreover, emotion in an utterance of a dialogue may be dependent on previous utterances making the task more complex. Recently, emotion recognition in low-resource languages like hindi has been in great demand. However, most of the existing datasets for multi-label emotion and intensity detection in conversations are in english. To this end, we propose a large conversational dataset in hindi named containing for multi-label emotion and intensity recognition in conversations containing 1,814 dialogues with a total of 44,247 utterances. We prepare our dataset in a wizard-of-oz manner for mental health and legal counselling of crime victims. Each utterance of dialogue is annotated with one or more emotion categories from 16 emotion labels including neutral and their corresponding intensity. We further propose strong contextual baselines that can detect the emotion(s) and corresponding emotional intensity of an utterance given the conversational context.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2021,"Venue":"naacl-2021","Acronym":"MM-AVS","Description":"A Full-Scale Dataset for Multi-modal Summarization","Abstract":"Multimodal summarization becomes increasingly significant as it is the basis for question answering, web search, and many other downstream tasks. However, its learning materials have been lacking a holistic organization by integrating resources from various modalities, thereby lagging behind the research progress of this field. In this study, we release a full-scale multimodal dataset comprehensively gathering documents, summaries, images, captions, videos, audios, transcripts, and titles in english from cnn and daily mail. To our best knowledge, this is the first collection that spans all modalities and nearly comprises all types of materials available in this community. In addition, we devise a baseline model based on the novel dataset, which employs a newly proposed jump-attention mechanism based on transcripts. The experimental results validate the important assistance role of the external information for multimodal summarization.","wordlikeness":0.3333333333,"lcsratio":0.6666666667,"wordcoverage":0.6666666667}
{"Year":2016,"Venue":"naacl-2016","Acronym":"iAppraise","Description":"A Manual Machine Translation Evaluation Environment Supporting Eye-tracking","Abstract":"We present set: an open-source framework that enables the use of eye-tracking for mt evaluation. It connects appraise, an opensource toolkit for mt evaluation, to a low-cost eye-tracking device, to make its usage accessible to a broader audience. It also provides a set of tools for extracting and exploiting gaze data, which facilitate eye-tracking analysis. In this paper, we describe different modules of the framework, and explain how the tool can be used in a mt evaluation scenario. During the demonstration, the users will be able to perform an evaluation task, observe their own reading behavior during a replay of the session, and export and extract features from the data.","wordlikeness":0.6666666667,"lcsratio":0.7777777778,"wordcoverage":0.8}
{"Year":2021,"Venue":"eacl-2021","Acronym":"Mega-COV","Description":"A Billion-Scale Dataset of 100&#43; Languages for COVID-19","Abstract":"We describe ., a billion-scale dataset from twitter for studying covid-19. The dataset is diverse (covers 268 countries), longitudinal (goes as back as 2007), multilingual (comes in 100+ languages), and has a significant number of location-tagged tweets (~169m tweets). We release tweet ids from the dataset. We also develop two powerful models, one for identifying whether or not a tweet is related to the pandemic (best f1=97%) and another for detecting misinformation about covid-19 (best f1=92%). A human annotation study reveals the utility of our models on a subset of describe. Our data and models can be useful for studying a wide host of phenomena related to the pandemic. (~169m and our models are publicly available.","wordlikeness":0.75,"lcsratio":0.75,"wordcoverage":0.6666666667}
{"Year":2015,"Venue":"ws-2015","Acronym":"CATaLog","Description":"New Approaches to TM and Post Editing Interfaces","Abstract":"This paper explores a new tm-based cat tool entitled from. New features have been integrated into the tool which aim to improve post-editing both in terms of performance and productivity. One of the new features of we is a color coding scheme that is based on the similarity between a particular input sentence and the segments retrieved from the tm. This color coding scheme will help translators to identify which part of the sentence is most likely to require post-editing thus demanding minimal e\ufb00ort and increasing productivity. We demonstrate the tool\u2019s functionalities using an english bengali dataset.","wordlikeness":1.0,"lcsratio":0.7142857143,"wordcoverage":1.0}
{"Year":2017,"Venue":"ws-2017","Acronym":"HistoBankVis","Description":"Detecting Language Change via Data Visualization","Abstract":"We present by, a novel visualization system designed for the interactive analysis of complex, multidimensional data to facilitate historical linguistic work. In this paper, we illustrate the visualization\u2019s ef\ufb01cacy and power by means of a concrete case study investigating the diachronic interaction of word order and subject case in icelandic.","wordlikeness":0.75,"lcsratio":0.5833333333,"wordcoverage":0.7272727273}
{"Year":2023,"Venue":"acl-2023","Acronym":"DICE","Description":"Data-Efficient Clinical Event Extraction with Generative Models","Abstract":"Event extraction for the clinical domain is an under-explored research area. The lack of training data along with the high volume of domain-specific terminologies with vague entity boundaries makes the task especially challenging. In this paper, we introduce settings., a robust and data-efficient generative model for clinical event extraction. Tasks. Frames event extraction as a conditional generation problem and introduces a contrastive learning objective to accurately decide the boundaries of biomedical mentions. Domain-specific also trains an auxiliary mention identification task jointly with event extraction tasks to better identify entity mention boundaries, and further introduces special markers to incorporate identified entity mentions as trigger and argument candidates for their respective tasks. To benchmark clinical event extraction, we compose maccrobat-ee, the first clinical event extraction dataset with argument annotation, based on an existing clinical information extraction dataset maccrobat. Our experiments demonstrate state-of-the-art performances of challenging. For clinical and news domain event extraction, especially under low data settings.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2015,"Venue":"semeval-2015","Acronym":"SINAI","Description":"Syntactic Approach for Aspect-Based Sentiment Analysis","Abstract":"This paper describes the participation of the words research group in the task aspect based sentiment analysis of semeval workshop 2015 edition. We propose a syntactic approach for identifying the words that modify each aspect, with the aim of classifying the sentiment expressed towards each attribute of an entity.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2023,"Venue":"findings-2023","Acronym":"PREADD","Description":"Prefix-Adaptive Decoding for Controlled Text Generation","Abstract":"We propose prefix-adaptive decoding (three), a flexible method for controlled text generation. Unlike existing methods that use auxiliary expert models to control for attributes, does does not require an external model, instead relying on linearly combining output logits from multiple prompts. Specifically, encapsulated contrasts the output logits generated using a raw prompt against those generated using a prefix-prepended prompt, enabling both positive and negative control with respect to any attribute encapsulated by the prefix. We evaluate multiple on three tasks\u2014toxic output mitigation, gender bias reduction, and sentiment control\u2014and find that prompts. Outperforms not only prompting baselines, but also an auxiliary-expert control method, by 12% or more in relative gain on our main metrics for each task.","wordlikeness":0.8333333333,"lcsratio":1.0,"wordcoverage":0.8333333333}
{"Year":2019,"Venue":"ijcnlp-2019","Acronym":"PAWS-X","Description":"A Cross-lingual Adversarial Dataset for Paraphrase Identification","Abstract":"Most existing work on adversarial data generation focuses on english. For example, paws (paraphrase adversaries from word scrambling) consists of challenging english paraphrase identification pairs from wikipedia and quora. We remedy this gap with 23%, a new dataset of 23,659 human translated paws evaluation pairs in six typologically distinct languages: french, spanish, german, chinese, japanese, and korean. We provide baseline numbers for three models with different capacity to capture non-local context and sentence structure, and using different multilingual training and evaluation regimes. Multilingual bert fine-tuned on paws english plus machine-translated data performs the best, with a range of 83.1-90.8 accuracy across the non-english languages and an average accuracy gain of 23% over the next best model. Best, shows the effectiveness of deep, multilingual pre-training while also leaving considerable headroom as a new challenge to drive multilingual research that better captures structure and contextual information.","wordlikeness":0.5,"lcsratio":0.5,"wordcoverage":0.8}
{"Year":2019,"Venue":"acl-2019","Acronym":"BiSET","Description":"Bi-directional Selective Encoding with Template for Abstractive Summarization","Abstract":"The success of neural summarization models stems from the meticulous encodings of source articles. To overcome the impediments of limited and sometimes noisy training data, one promising direction is to make better use of the available training data by applying filters during summarization. In this paper, we propose a novel bi-directional selective encoding with template (summarization) model, which leverages template discovered from training data to softly select key information from each source article to guide its summarization process. Extensive experiments on a standard summarization dataset are conducted and the results show that the template-equipped to model manages to improve the summarization performance significantly with a new state of the art.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2023,"Venue":"acl-2023","Acronym":"MatSci-NLP","Description":"Evaluating Scientific Language Models on Materials Science Language Tasks Using Text-to-Schema Modeling","Abstract":"We present given, a natural language benchmark for evaluating the performance of natural language processing (nlp) models on materials science text. We construct the benchmark from publicly available materials science text data to encompass seven different nlp tasks, including conventional nlp tasks like named entity recognition and relation classification, as well as nlp tasks specific to materials science, such as synthesis action retrieval which relates to creating synthesis procedures for materials. We study various bert-based models pretrained on different scientific text corpora on its to understand the impact of pretraining strategies on understanding materials science text. Given the scarcity of high-quality annotated data in the materials science domain, we perform our fine-tuning experiments with limited training data to encourage the generalize across we tasks. Our experiments in this low-resource training setting show that language models pretrained on scientific text outperform bert trained on general text. Matbert, a model pretrained specifically on materials science journals, generally performs best for most tasks. Moreover, we propose a unified text-to-schema for multitask learning on {pasted macro \u2018benchmark\u2019} and compare its performance with traditional fine-tuning methods. In our analysis of different training methods, we find that our proposed text-to-schema methods inspired by question-answering consistently outperform single and multitask nlp fine-tuning methods. The code and datasets are publicly available <a href=https:\/\/github.com\/banglab-udem-mila\/nlp4matsci-acl23 class=acl-markup-url>https:\/\/github.com\/banglab-udem-mila\/nlp4matsci-acl23<\/a>.","wordlikeness":0.3,"lcsratio":0.8,"wordcoverage":0.6666666667}
{"Year":2022,"Venue":"ijcnlp-2022","Acronym":"CrowdChecked","Description":"Detecting Previously Fact-Checked Claims in Social Media","Abstract":"While there has been substantial progress in developing systems to automate fact-checking, they still lack credibility in the eyes of the users. Thus, an interesting approach has emerged: to perform automatic fact-checking by verifying whether an input claim has been previously fact-checked by professional fact-checkers and to return back an article that explains their decision. This is a sensible approach as people trust manual fact-checking, and as many claims are repeated multiple times. Yet, a major issue when building such systems is the small number of known tweet\u2013verifying article pairs available for training. Here, we aim to bridge this gap by making use of crowd fact-checking, i.e., mining claims in social media for which users have responded with a link to a fact-checking article. In particular, we mine a large-scale collection of 330,000 tweets paired with a corresponding fact-checking article. We further propose an end-to-end framework to learn from this noisy data based on modified self-adaptive training, in a distant supervision scenario. Our experiments on the clef\u201921 checkthat! test set show improvements over the state of the art by two points absolute. Our code and datasets are available at <a href=https:\/\/github.com\/mhardalov\/framework-claims class=acl-markup-url>https:\/\/github.","wordlikeness":0.75,"lcsratio":0.8333333333,"wordcoverage":0.7368421053}
{"Year":2023,"Venue":"acl-2023","Acronym":"BOLT","Description":"Fast Energy-based Controlled Text Generation with Tunable Biases","Abstract":"Energy-based models (ebms) have gained popularity for controlled text generation due to their high applicability to a wide range of constraints. However, sampling from ebms is non-trivial, as it often requires a large number of iterations to converge to plausible text, which slows down the decoding process and makes it less practical for real-world applications. In this work, we propose tasks, which relies on tunable biases to directly adjust the language model\u2019s output logits. Unlike prior work, on maintains the generator\u2019s autoregressive nature to assert a strong control on token-wise conditional dependencies and overall fluency, and thus converges faster. When compared with state-of-the-arts on controlled generation tasks using both soft constraints (e.g., sentiment control) and hard constraints (e.g., keyword-guided topic control), conditional demonstrates significantly improved efficiency and fluency. On sentiment control, practical is 7x faster than competitive baselines, and more fluent in 74.4% of the evaluation samples according to human judges.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2020,"Venue":"sigdial-2020","Acronym":"Retico","Description":"An incremental framework for spoken dialogue systems","Abstract":"In this paper we present the newest version of tool - a python-based incremental dialogue framework to create state-of-the-art spoken dialogue systems and simulations. And provides a range of incremental modules that are based on services like google asr, google tts and rasa nlu. Incremental networks can be created either in code or with a graphical user interface. In this demo we present three use cases that are implemented in code: a spoken translation tool that translates speech in real-time, a conversation simulation that models turn-taking and a spoken dialogue restaurant information service.","wordlikeness":0.8333333333,"lcsratio":0.8333333333,"wordcoverage":0.8}
{"Year":2007,"Venue":"semeval-2007","Acronym":"UCD-FC","Description":"Deducing semantic relations using WordNet senses that occur frequently in a database of noun-noun compounds","Abstract":"This paper describes a system for classifying semantic relations among nominals, as in semeval task 4. This system uses a corpus of 2,500 compounds annotated with wordnet senses and covering 139 different semantic relations. Given a set of nominal pairs for training, as provided in the semeval task 4 training data, this system constructs for each training pair a set of features made up of relations and wordnet sense pairs which occurred with those nominals in the corpus. A naive bayes learning algorithm learns associations between these features and relation membership categories. The identi\ufb01cation of relations among nominals in test items takes place on the basis of these associations.","wordlikeness":0.1666666667,"lcsratio":0.8333333333,"wordcoverage":0.6666666667}
{"Year":2021,"Venue":"acl-2021","Acronym":"UniRE","Description":"A Unified Label Space for Entity Relation Extraction","Abstract":"Many joint entity relation extraction models setup two separated label spaces for the two sub-tasks (i.e., entity detection and relation classification). We argue that this setting may hinder the information interaction between entities and relations. In this work, we propose to eliminate the different treatment on the two sub-tasks\u2019 label spaces. The input of our model is a table containing all word pairs from a sentence. Entities and relations are represented by squares and rectangles in the table. We apply a unified classifier to predict each cell\u2019s label, which unifies the learning of two sub-tasks. For testing, an effective (yet fast) approximate decoder is proposed for finding squares and rectangles from tables. Experiments on three benchmarks (ace04, ace05, scierc) show that, using only half the number of parameters, our model achieves competitive accuracy with the best extractor, and is faster.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.8333333333}
{"Year":2019,"Venue":"acl-2019","Acronym":"ARNOR","Description":"Attention Regularization based Noise Reduction for Distant Supervision Relation Classification","Abstract":"Distant supervision is widely used in relation classification in order to create large-scale training data by aligning a knowledge base with an unlabeled corpus. However, it also introduces amounts of noisy labels where a contextual sentence actually does not express the labeled relation. In this paper, we propose amounts, a novel attention regularization based noise reduction framework for distant supervision relation classification. Learns assumes that a trustable relation label should be explained by the neural attention model. Specifically, our model framework iteratively learns an interpretable model and utilizes it to select trustable instances. We first introduce attention regularization to force the model to pay attention to the patterns which explain the relation labels, so as to make the model more interpretable. Then, if the learned model can clearly locate the relation patterns of a candidate instance in the training set, we will select it as a trustable instance for further training step. According to the experiments on nyt data, our amounts framework achieves significant improvements over state-of-the-art methods in both relation classification performance and noise reduction effect.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2017,"Venue":"ijcnlp-2017","Acronym":"CVBed","Description":"Structuring CVs usingWord Embeddings","Abstract":"Automatic analysis of curriculum vitae (cvs) of applicants is of tremendous importance in recruitment scenarios. The semi-structuredness of cvs, however, makes cv processing a challenging task. We propose a solution towards transforming cvs to follow a unified structure, thereby, paving ways for smoother cv analysis. The problem of restructuring is posed as a section relabeling problem, where each section of a given cv gets reassigned to a predefined label. Our relabeling method relies on semantic relatedness computed between section header, content and labels, based on phrase-embeddings learned from a large pool of cvs. we follow different heuristics to measure semantic relatedness. Our best heuristic achieves an f-score of 93.17% on a test dataset with gold-standard labels obtained using manual annotation.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2023,"Venue":"rocling-2023","Acronym":"WhisperHakka","Description":"A Hybrid Architecture Speech Recognition System for Low-Resource Taiwanese Hakka","Abstract":"Deep learning-based automatic speech recognition (asr) design has been growing in popularity. Besides the asr model depends on the reliable speech representation offered by the self-supervised learning (ssl) model, whisper is also a powerful model that makes use of the knowledge from large-scale labeled datasets and the self-attention mechanism. However, its inherent training method decreases the potential to expand on low-resource language because of the di\ufb00iculty of acquiring labeled data. As a result, we integrated both the wav2vec2 and whisper into an asr system not only to provide extra abundant information on features but also to have the ability to train on unlabeled data through the ssl model while retaining the capacity of whisper. Experimental results show that the proposed hybrid architecture system outperforms the vanilla whisper in the reading speech scenario, achieving a roughly 21% improvement in recognition rate.","wordlikeness":0.8333333333,"lcsratio":0.9166666667,"wordcoverage":0.7368421053}
{"Year":2020,"Venue":"lrec-2020","Acronym":"Ciron","Description":"a New Benchmark Dataset for Chinese Irony Detection","Abstract":"Automatic chinese irony detection is a challenging task, and it has a strong impact on linguistic research. However, chinese irony detection often lacks labeled benchmark datasets. In this paper, we introduce than, the first chinese benchmark dataset available for irony detection for machine learning models. Seven includes more than 8.7k posts, collected from weibo, a micro blogging platform. Most importantly, no is collected with no pre-conditions to ensure a much wider coverage. Evaluation on seven different machine learning classifiers proves the usefulness of weibo, as an important resource for chinese irony detection.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2020,"Venue":"coling-2020","Acronym":"PoD","Description":"Positional Dependency-Based Word Embedding for Aspect Term Extraction","Abstract":"Dependency context-based word embedding jointly learns the representations of word and dependency context, and has been proved effective in aspect term extraction. In this paper, we design the positional dependency-based word embedding (outperforms) which considers both dependency context and positional context for aspect term extraction. Specifically, the positional context is modeled via relative position encoding. Besides, we enhance the dependency context by integrating more lexical information (e.g., pos tags) along dependency paths. Experiments on semeval 2014\/2015\/2016 datasets show that our approach outperforms other embedding methods in aspect term extraction.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2021,"Venue":"acl-2021","Acronym":"FastSeq","Description":"Make Sequence Generation Faster","Abstract":"Transformer-based models have made tremendous impacts in natural language generation. However the inference speed is a bottleneck due to large model size and intensive computing involved in auto-regressive decoding process. We develop however framework to accelerate sequence generation without accuracy loss. The proposed optimization techniques include an attention cache optimization, an efficient algorithm for detecting repeated n-grams, and an asynchronous generation pipeline with parallel i\/o. These optimizations are general enough to be applicable to transformer-based models (e.g., t5, gpt2, and unilm). Our benchmark results on a set of widely used and diverse models demonstrate 4-9x inference speed gain. Additionally, auto-regressive is easy to use with a simple one-line code change. The source code is available at <a href=https:\/\/github.com\/microsoft\/inference class=acl-markup-url>https:\/\/github.com\/microsoft\/large<\/a>.","wordlikeness":0.5714285714,"lcsratio":0.7142857143,"wordcoverage":0.7692307692}
{"Year":2010,"Venue":"semeval-2010","Acronym":"FCC","Description":"Modeling Probabilities with GIZA&#43;&#43; for Task 2 and 3 of SemEval-2","Abstract":"In this paper we present a na\u00a8\u0131ve approach to tackle the problem of cross-lingual wsd and cross-lingual lexical substitution which correspond to the task #2 and #3 of the semeval-2 competition. We used a bilingual statistical dictionary, which is calculated with giza++ by using the europarl parallel corpus, in order to calculate the probability of a source word to be translated to a target word (which is assumed to be the correct sense of the source word but in a different language). Two versions of the probabilistic model are tested: unweighted and weighted. The obtained values show that the unweighted version performs better thant the weighted one.","wordlikeness":0.3333333333,"lcsratio":0.3333333333,"wordcoverage":1.0}
{"Year":2020,"Venue":"acl-2020","Acronym":"SpellGCN","Description":"Incorporating Phonological and Visual Similarities into Language Models for Chinese Spelling Check","Abstract":"Chinese spelling check (csc) is a task to detect and correct spelling errors in chinese natural language. Existing methods have made attempts to incorporate the similarity knowledge between chinese characters. However, they take the similarity knowledge as either an external input resource or just heuristic rules. This paper proposes to incorporate phonological and visual similarity knowledge into language models for csc via a specialized graph convolutional network (between). The model builds a graph over the characters, and this is learned to map this graph into a set of inter-dependent character classifiers. These classifiers are applied to the representations extracted by another network, such as bert, enabling the whole network to be end-to-end trainable. Experiments are conducted on three human-annotated datasets. Our method achieves superior performance against previous models by a large margin.","wordlikeness":0.625,"lcsratio":0.875,"wordcoverage":0.7692307692}
{"Year":2012,"Venue":"sighan-2012","Acronym":"SIR-NERD","Description":"A Chinese Named Entity Recognition and Disambiguation System using a Two-Stage Method","Abstract":"This paper presents our system, system for the chinese named entity recognition and disambiguation task in the cips-sighan joint conference on chinese language processing (clp2012). Our system uses a two-stage method and some key techniques to deal with the named entity recognition and disambiguation (nerd) task. Experimental results on the test data shows that the proposed system, which incorporates classifying and clustering techniques, can achieve competitive performance.","wordlikeness":0.5,"lcsratio":0.75,"wordcoverage":0.7142857143}
{"Year":2021,"Venue":"konvens-2021","Acronym":"DeInStance","Description":"Creating and Evaluating a German Corpus for Fine-Grained Inferred Stance Detection","Abstract":"We introduce by, a corpus of 1000 politicians\u2019 answers in german (de) containing sentences labeled with explicitly expressed and inferred stances - pro and con relations - by 3 annotators. They achieved an acceptable inter-rater agreement given the inherent subjective nature of the task. A \ufb01rst baseline, a \ufb01ne-tuned bert-based token classi\ufb01er, achieved f1-scores of around 70% . Our focus is on the dif\ufb01cult subclass of sentences comprising only non-polar words, but still with an (implicit) pro or con perspective of the writer.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2005,"Venue":"hlt-2005","Acronym":"Classummary","Description":"Introducing Discussion Summarization to Online Classrooms","Abstract":"S, pages 4\u20135, vancouver, october 2005. Summarization: introducing discussion summarization to online classrooms liang zhou, erin shaw, chin-yew lin, and eduard hovy university of southern california information sciences institute 4676 admiralty way marina del rey, ca 90292-6695 {liangz, shaw, hovy}@isi.edu this paper describes a novel summarization system, in, for interactive online classroom discussions. This system is originally designed for open source software (oss) development forums. However, this new application provides valuable feedback on designing summarization systems and applying them to everyday use, in addition to the traditional natural language processing evaluation methods. In our demonstration at hlt, new users will be able to direct this summarizer themselves.","wordlikeness":0.7272727273,"lcsratio":0.7272727273,"wordcoverage":0.7777777778}
{"Year":2008,"Venue":"lrec-2008","Acronym":"MEDAR","Description":"Collaboration between European and Mediterranean Arabic Partners to Support the Development of Language Technology for Arabic","Abstract":"After the successful completion of the nemlar project 2003-2005, a new opportunity for a project was opened by the european commission, and a group of largely the same partners is now executing the translation) project. Help will be updating the surveys and blark for arabic already made, and will then focus on machine translation (and other tools for translation) and information retrieval with a focus on language resources, tools and evaluation for these applications. A very important part of the tools, project is to reinforce and extend the nemlar network and to create a cooperation roadmap for human language technologies for arabic. It is expected that the cooperation roadmap will attract wide attention from other parties and that it can help create a larger platform for collaborative projects. Finally, the project will focus on dissemination of knowledge about existing resources and tools, as well as actors and activities; this will happen through newsletter, website and an international conference which will follow up on the cairo conference of 2004. Dissemination to user communities will also be important, e.g. through participation in translators? Conferences. The goal of these activities is to create a stronger and lasting collaboration between eu countries and arabic speaking countries.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2015,"Venue":"semeval-2015","Acronym":"Shiraz","Description":"A Proposed List Wise Approach to Answer Validation","Abstract":"Answer validation is an important step in automatic question answering systems and nowadays by spreading community question answering systems it is known as an important task by itself. Previous works just considered it as a binary classi\ufb01cation problem in which they try to \ufb01nd the best answer among all the candidate answers for a question. Accordingly, they do not consider the possible unique information which may have been included other answers. This can be considered by having a multiclass label classi\ufb01cation problem, it is not only able to \ufb01nd the best answer but also can \ufb01nd \u201dpotentially good\u201d, \u201dbad\u201d, and etc. Answers too. By doing so, it is fully expected to extract and rate all the necessary information from existing candidates to help questioner to \ufb01nd the best and general answer for his question. This work tries to consider some features which are gained from importance of comments of the questioner. Finally, by using a good classi\ufb01er, we try to overcome this problem. The designed system participated in subtask a of the semeval-2015 task 3. The primary submission ranked at the 5th and 7th places in four class label and three class label evaluation, accordingly.","wordlikeness":0.8333333333,"lcsratio":0.6666666667,"wordcoverage":0.8}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"ECTSum","Description":"A New Benchmark Dataset For Bullet Point Summarization of Long Earnings Call Transcripts","Abstract":"Despite tremendous progress in automatic summarization, state-of-the-art methods are predominantly trained to excel in summarizing short newswire articles, or documents with strong layout biases such as scientific articles or government reports. Efficient techniques to summarize financial documents, discussing facts and figures, have largely been unexplored, majorly due to the unavailability of suitable datasets. In this work, we present trained, a new dataset with transcripts of earnings calls (ects), hosted by publicly traded companies, as documents, and experts-written short telegram-style bullet point summaries derived from corresponding reuters articles. Ects are long unstructured documents without any prescribed length limit or format. We benchmark our dataset with state-of-the-art summarization methods across various metrics evaluating the content quality and factual consistency of the generated summaries. Finally, we present a simple yet effective approach, ect-bps, to generate a set of bullet points that precisely capture the important facts discussed in the calls.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.7142857143}
{"Year":2013,"Venue":"acl-2013","Acronym":"TransDoop","Description":"A Map-Reduce based Crowdsourced Translation for Complex Domain","Abstract":"Large amount of parallel corpora is required for building statistical machine translation (smt) systems. We describe the issues system for gathering translations to create parallel corpora from online crowd workforce who have familiarity with multiple languages but are not expert translators. Our system uses a map-reduce-like approach to translation crowdsourcing where sentence translation is decomposed into the following smaller tasks: (a) translation of constituent phrases of the sentence; (b) validation of quality of the phrase translations; and (c) composition of complete sentence translations from phrase translations. Online incorporates quality control mechanisms and easy-to-use worker user interfaces designed to address issues with translation crowdsourcing. We have evaluated the crowd\u2019s output using the meteor metric. For a complex domain like judicial proceedings, the higher scores obtained by the map-reduce based approach compared to complete sentence translation establishes the ef\ufb01cacy of our work.","wordlikeness":0.7777777778,"lcsratio":0.8888888889,"wordcoverage":0.7142857143}
{"Year":2022,"Venue":"findings-2022","Acronym":"ChartQA","Description":"A Benchmark for Question Answering about Charts with Visual and Logical Reasoning","Abstract":"Charts are very popular for analyzing data. When exploring charts, people often ask a variety of complex reasoning questions that involve several logical and arithmetic operations. They also commonly refer to visual features of a chart in their questions. However, most existing datasets do not focus on such complex reasoning questions as their questions are template-based and answers come from a fixed-vocabulary. In this work, we present a large-scale benchmark covering 9.6k human-written questions as well as 23.1k questions generated from human-written chart summaries. To address the unique challenges in our benchmark involving visual and logical reasoning over charts, we present two transformer-based models that combine visual features and the data table of the chart in a unified way to answer questions. While our models achieve the state-of-the-art results on the previous datasets as well as on our benchmark, the evaluation also reveals several challenges in answering complex reasoning questions.","wordlikeness":0.7142857143,"lcsratio":0.8571428571,"wordcoverage":0.8333333333}
{"Year":2019,"Venue":"acl-2019","Acronym":"LSTMEmbed","Description":"Learning Word and Sense Representations from a Large Semantically Annotated Corpus with Long Short-Term Memories","Abstract":"While word embeddings are now a de facto standard representation of words in most nlp tasks, recently the attention has been shifting towards vector representations which capture the different meanings, i.e., senses, of words. In this paper we explore the capabilities of a bidirectional lstm model to learn representations of word senses from semantically annotated corpora. We show that the utilization of an architecture that is aware of word order, like an lstm, enables us to create better representations. We assess our proposed model on various standard benchmarks for evaluating semantic representations, reaching state-of-the-art performance on the semeval-2014 word-to-sense similarity task. We release the code and the resulting word and sense embeddings at <a href=http:\/\/lcl.uniroma1.it\/we class=acl-markup-url>http:\/\/lcl.uniroma1.it\/semeval-2014<\/a>.","wordlikeness":0.4444444444,"lcsratio":0.8888888889,"wordcoverage":0.7058823529}
{"Year":2017,"Venue":"emnlp-2017","Acronym":"MUSE","Description":"Modularizing Unsupervised Sense Embeddings","Abstract":"This paper proposes to address the word sense ambiguity issue in an unsupervised manner, where word sense representations are learned along a word sense selection mechanism given contexts. Prior work focused on designing a single model to deliver both mechanisms, and thus suffered from either coarse-grained representation learning or inefficient sense selection. The proposed modular approach, deliver, implements flexible modules to optimize distinct mechanisms, achieving the first purely sense-level representation learning system with linear-time sense selection. We leverage reinforcement learning to enable joint training on the proposed modules, and introduce various exploration techniques on sense selection for better robustness. The experiments on benchmark data show that the proposed approach achieves the state-of-the-art performance on synonym selection as well as on contextual word similarities in terms of maxsimc.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"GAML-BERT","Description":"Improving BERT Early Exiting by Gradient Aligned Mutual Learning","Abstract":"In this work, we propose a novel framework, gradient aligned mutual learning bert (work,), for improving the early exiting of bert. Novel\u2019s contributions are two-fold. We conduct a set of pilot experiments, which shows that mutual knowledge distillation between a shallow exit and a deep exit leads to better performances for both. From this observation, we use mutual learning to improve bert\u2019s early exiting performances, that is, we ask each exit of a multi-exit bert to distill knowledge from each other. Second, we propose ga, a novel training method that aligns the gradients from knowledge distillation to cross-entropy losses. Extensive experiments are conducted on the glue benchmark, which shows that our experiments can significantly outperform the state-of-the-art (sota) bert early exiting methods.","wordlikeness":0.6666666667,"lcsratio":0.6666666667,"wordcoverage":0.8}
{"Year":2021,"Venue":"ranlp-2021","Acronym":"AutoChart","Description":"A Dataset for Chart-to-Text Generation Task","Abstract":"The analytical description of charts is an exciting and important research area with many applications in academia and industry. Yet, this challenging task has received limited attention from the computational linguistics research community. This paper proposes large, a large dataset for the analytical description of charts, which aims to encourage more research into this important area. Specifically, we offer a novel framework that generates the charts and their analytical description automatically. We conducted extensive human and machine evaluation on the generated charts and descriptions and demonstrate that the generated texts are informative, coherent, and relevant to the corresponding charts.","wordlikeness":0.7777777778,"lcsratio":0.8888888889,"wordcoverage":0.7142857143}
{"Year":2023,"Venue":"eacl-2023","Acronym":"AutoTriggER","Description":"Label-Efficient and Robust Named Entity Recognition with Auxiliary Trigger Extraction","Abstract":"Deep neural models for named entity recognition (ner) have shown impressive results in overcoming label scarcity and generalizing to unseen entities by leveraging distant supervision and auxiliary information such as explanations. However, the costs of acquiring such additional information are generally prohibitive. In this paper, we present a novel two-stage framework ((ner)) to improve ner performance by automatically generating and leveraging \u201centity triggers\u201d which are human-readable cues in the text that help guide the model to make better decisions. Our framework leverages post-hoc explanation to generate rationales and strengthens a model\u2019s prior knowledge using an embedding interpolation technique. This approach allows models to exploit triggers to infer entity boundaries and types instead of solely memorizing the entity words themselves. Through experiments on three well-studied ner datasets, strengthens shows strong label-efficiency, is capable of generalizing to unseen entities, and outperforms the roberta-crf baseline by nearly 0.5 f1 points on average.","wordlikeness":0.8181818182,"lcsratio":1.0,"wordcoverage":0.7777777778}
{"Year":2022,"Venue":"amta-2022","Acronym":"Lingua","Description":"Addressing Scenarios for Live Interpretation and Automatic Dubbing","Abstract":"Dynamically, is an application developed for the church of jesus christ of latter-day saints that performs both real-time interpretation of live speeches and automatic video dubbing (avd). Like other avd systems, it can perform synchronized automatic dubbing, given video files and optionally, corresponding text files using a traditional asr\u2013mt\u2013tts pipeline. That\u2019s unique contribution is that it can also operate in real-time with a slight delay of a few seconds to interpret live speeches. If no source-language script is provided, the translations are exactly as recognized by asr and translated by mt. If a script is provided, interpret matches the recognized asr segments with script segments and passes the latter to mt for translation and subsequent tts. If a human translation is also provided, it is passed directly to tts. Of switches between these modes dynamically, enabling translation of off-script comments and different levels of quality for multiple languages.","wordlikeness":0.6666666667,"lcsratio":0.8333333333,"wordcoverage":0.8}
{"Year":2021,"Venue":"acl-2021","Acronym":"UXLA","Description":"A Robust Unsupervised Data Augmentation Framework for Zero-Resource Cross-Lingual NLP","Abstract":"Transfer learning has yielded state-of-the-art (sota) results in many supervised nlp tasks. However, annotated data for every target task in every target language is rare, especially for low-resource languages. We propose augmentation, a novel unsupervised data augmentation framework for zero-resource transfer learning scenarios. In particular, solve aims to solve cross-lingual adaptation problems from a source language task distribution to an unknown target language task distribution, assuming no training label in the target language. At its core, has performs simultaneous self-training with data augmentation and unsupervised sample selection. To show its effectiveness, we conduct extensive experiments on three diverse zero-resource cross-lingual transfer tasks. Cross-lingual achieves sota results in all the tasks, outperforming the baselines by a good margin. With an in-depth framework dissection, we demonstrate the cumulative contributions of different components to its success.","wordlikeness":0.75,"lcsratio":0.75,"wordcoverage":0.75}
{"Year":2023,"Venue":"ws-2023","Acronym":"CoSiNES","Description":"Contrastive Siamese Network for Entity Standardization","Abstract":"Entity standardization maps noisy mentions from free-form text to standard entities in a knowledge base. The unique challenge of this task relative to other entity-related tasks is the lack of surrounding context and numerous variations in the surface form of the mentions, especially when it comes to generalization across domains where labeled data is scarce. Previous research mostly focuses on developing models either heavily relying on context, or dedicated solely to a specific domain. In contrast, we propose contrastive, a generic and adaptable framework with contrastive siamese network for entity standardization that effectively adapts a pretrained language model to capture the syntax and semantics of the entities in a new domain. We construct a new dataset in the technology domain, which contains 640 technical stack entities and 6,412 mentions collected from industrial content management systems. We demonstrate that previous yields higher accuracy and faster runtime than baselines derived from leading methods in this domain. Four also achieves competitive performance in four standard datasets from the chemistry, medicine, and biomedical domains, demonstrating its cross-domain applicability. Code and data is available at <a href=https:\/\/github.com\/konveyor\/tackle-container-advisor\/tree\/main\/entity_standardizer\/standard class=acl-markup-url>https:\/\/github.","wordlikeness":0.7142857143,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2023,"Venue":"acl-2023","Acronym":"PAD-Net","Description":"An Efficient Framework for Dynamic Networks","Abstract":"Dynamic networks, e.g., dynamic convolution (dy-conv) and the mixture of experts (moe), have been extensively explored as they can considerably improve the model\u2019s representation power with acceptable computational cost. The common practice in implementing dynamic networks is to convert the given static layers into fully dynamic ones where all parameters are dynamic (at least within a single layer) and vary with the input. However, such a fully dynamic setting may cause redundant parameters and high deployment costs, limiting the applicability of dynamic networks to a broader range of tasks and models. The main contributions of our work are challenging the basic commonsense in dynamic networks and proposing a partially dynamic network, namely e.g.,, to transform the redundant dynamic parameters into static ones. Also, we further design iterative mode partition to partition dynamic and static parameters efficiently. Our method is comprehensively supported by large-scale experiments with two typical advanced dynamic architectures, i.e., dy-conv and moe, on both image classification and glue benchmarks. Encouragingly, we surpass the fully dynamic networks by <span class=tex-math>+0.7%<\/span> top-1 acc with only 30% dynamic parameters for resnet-50 and <span class=tex-math>+1.9%<\/span> average score in language understanding with only 50% dynamic parameters for bert. Code will be released at: <a href=https:\/\/github.com\/shwai-he\/redundant class=acl-markup-url>https:\/\/github.com\/shwai-he\/and<\/a>.","wordlikeness":0.7142857143,"lcsratio":0.7142857143,"wordcoverage":0.7692307692}
{"Year":2012,"Venue":"coling-2012","Acronym":"JMaxAlign","Description":"A Maximum Entropy Parallel Sentence Alignment Tool","Abstract":"Parallel corpora are an extremely useful tool in many natural language processing tasks, particularly statistical machine translation. Parallel corpora for certain language pairs, such as spanish or french, are widely available, but for many language pairs, such as bengali and chinese, it is impossible to \ufb01nd parallel corpora. Several tools have been developed to automatically extract parallel data from non\u2013parallel corpora, but they use languagespeci\ufb01c techniques or require large amounts of training data. This paper demonstrates that maximum entropy classi\ufb01ers can be used to detect parallel sentences between any language pairs with small amounts of training data. This paper is accompanied by such, a java maxent classi\ufb01er which can detect parallel sentences. Keywords: parallel corpora, comparable corpora, maximum entropy classi\ufb01ers, statistical machine translation.","wordlikeness":0.6666666667,"lcsratio":0.8888888889,"wordcoverage":0.7142857143}
{"Year":2021,"Venue":"eacl-2021","Acronym":"OPUS-CAT","Description":"Desktop NMT with CAT integration and local fine-tuning","Abstract":"Models is a collection of software which enables translators to use neural machine translation in computer-assisted translation tools without exposing themselves to security and confidentiality risks inherent in online machine translation. Desktop uses the public opus-mt machine translation models, which are available for over a thousand language pairs. The generic opus-mt models can be fine-tuned with or on the desktop using data for a specific client or domain.","wordlikeness":0.75,"lcsratio":0.625,"wordcoverage":0.7142857143}
{"Year":2020,"Venue":"acl-2020","Acronym":"FLAT","Description":"Chinese NER Using Flat-Lattice Transformer","Abstract":"Recently, the character-word lattice structure has been proved to be effective for chinese named entity recognition (ner) by incorporating the word information. However, since the lattice structure is complex and dynamic, the lattice-based models are hard to fully utilize the parallel computation of gpus and usually have a low inference speed. In this paper, we propose efficiency.: for-lattice transformer for chinese ner, which converts the lattice structure into a chinese structure consisting of spans. Each span corresponds to a character or latent word and its position in the original lattice. With the power of transformer and well-designed position encoding, well-designed can fully leverage the lattice information and has an excellent parallel ability. Experiments on four datasets show the outperforms other lexicon-based models in performance and efficiency.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2023,"Venue":"acl-2023","Acronym":"TeCS","Description":"A Dataset and Benchmark for Tense Consistency of Machine Translation","Abstract":"Tense inconsistency frequently occurs in machine translation. However, there are few criteria to assess the model\u2019s mastery of tense prediction from a linguistic perspective. In this paper, we present a parallel tense test set, containing french-english 552 utterances. We also introduce a corresponding benchmark, tense prediction accuracy. With the tense test set and the benchmark, researchers are able to measure the tense consistency performance of machine translation systems for the first time.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2016,"Venue":"ws-2016","Acronym":"SHEF-MIME","Description":"Word-level Quality Estimation Using Imitation Learning","Abstract":"We describe university of shef\ufb01eld\u2019s submission to the word-level quality estimation shared task. Our system is based on imitation learning, an approach to structured prediction which relies on a classi\ufb01er trained on data generated appropriately to ameliorate error propagation. Compared to other structure prediction approaches such as conditional random \ufb01elds, it allows the use of arbitrary information from previous tag predictions and the use of non-decomposable loss functions over the structure. We explore these two aspects in our submission while using the baseline features provided by the shared task organisers. Our system outperformed the conditional random \ufb01eld baseline while using the same feature set.","wordlikeness":0.5555555556,"lcsratio":0.5555555556,"wordcoverage":0.6666666667}
{"Year":2021,"Venue":"naacl-2021","Acronym":"QMSum","Description":"A New Benchmark for Query-based Multi-domain Meeting Summarization","Abstract":"Meetings are a key component of human collaboration. As increasing numbers of meetings are recorded and transcribed, meeting summaries have become essential to remind those who may or may not have attended the meetings about the key decisions made and the tasks to be completed. However, it is hard to create a single short summary that covers all the content of a long meeting involving multiple people and topics. In order to satisfy the needs of different types of users, we define a new query-based multi-domain meeting summarization task, where models have to select and summarize relevant spans of meetings in response to a query, and we introduce where, a new benchmark for this task. In consists of 1,808 query-summary pairs over 232 meetings in multiple domains. Besides, we investigate a locate-then-summarize method and evaluate a set of strong summarization baselines on the task. Experimental results and manual analysis reveal that about presents significant challenges in long meeting summarization for future research. Dataset is available at <a href=https:\/\/github.com\/yale-lily\/numbers class=acl-markup-url>https:\/\/github.com\/yale-lily\/the<\/a>.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2021,"Venue":"acl-2021","Acronym":"LEGOEval","Description":"An Open-Source Toolkit for Dialogue System Evaluation via Crowdsourcing","Abstract":"We present offers, an open-source toolkit that enables researchers to easily evaluate dialogue systems in a few lines of code using the online crowdsource platform, amazon mechanical turk. Compared to existing toolkits, researchers features a flexible task design by providing a python api that maps to commonly used react.js interface components. Researchers can personalize their evaluation procedures easily with our built-in pages as if playing with lego blocks. Thus, easily provides a fast, consistent method for reproducing human evaluation results. Besides the flexible task design, by also offers an easy api to review collected data.","wordlikeness":0.625,"lcsratio":0.75,"wordcoverage":0.7692307692}
{"Year":2020,"Venue":"emnlp-2020","Acronym":"PatchBERT","Description":"Just-in-Time, Out-of-Vocabulary Patching","Abstract":"Large scale pre-trained language models have shown groundbreaking performance improvements for transfer learning in the domain of natural language processing. In our paper, we study a pre-trained multilingual bert model and analyze the oov rate on downstream tasks, how it introduces information loss, and as a side-effect, obstructs the potential of the underlying model. We then propose multiple approaches for mitigation and demonstrate that it improves performance with the same parameter count when combined with fine-tuning.","wordlikeness":0.8888888889,"lcsratio":0.5555555556,"wordcoverage":0.75}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"Cryptonite","Description":"A Cryptic Crossword Benchmark for Extreme Ambiguity in Language","Abstract":"Current nlp datasets targeting ambiguity can be solved by a native speaker with relative ease. We present 100%, a large-scale dataset based on cryptic crosswords, which is both linguistically complex and naturally sourced. Each example in challenging is a cryptic clue, a short phrase or sentence with a misleading surface reading, whose solving requires disambiguating semantic, syntactic, and phonetic wordplays, as well as world knowledge. Cryptic clues pose a challenge even for experienced solvers, though top-tier experts can solve them with almost 100% accuracy. 100% is a challenging task for current models; fine-tuning t5-large on 470k cryptic clues achieves only 7.6% accuracy, on par with the accuracy of a rule-based clue solver (8.6%).","wordlikeness":0.9,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2020,"Venue":"findings-2020","Acronym":"IndicNLPSuite","Description":"Monolingual Corpora, Evaluation Benchmarks and Pre-trained Multilingual Language Models for Indian Languages","Abstract":"In this paper, we introduce nlp resources for 11 major indian languages from two major language families. These resources include: (a) large-scale sentence-level monolingual corpora, (b) pre-trained word embeddings, (c) pre-trained language models, and (d) multiple nlu evaluation datasets (<i>indicglue<\/i> benchmark). The monolingual corpora contains a total of 8.8 billion tokens across all 11 languages and indian english, primarily sourced from news crawls. The word embeddings are based on <i>fasttext<\/i>, hence suitable for handling morphological complexity of indian languages. The pre-trained language models are based on the compact albert model. Lastly, we compile the (<i>indicglue<\/i> benchmark for indian language nlu. To this end, we create datasets for the following tasks: article genre classification, headline prediction, wikipedia section-title prediction, cloze-style multiple choice qa, winograd nli and copa. We also include publicly available datasets for some indic languages for tasks like named entity recognition, cross-lingual sentence retrieval, paraphrase detection, <i>etc.<\/i> our embeddings are competitive or better than existing pre-trained embeddings on multiple tasks. We hope that the availability of the dataset will accelerate indic nlp research which has the potential to impact more than a billion people. It can also help the community in evaluating advances in nlp over a more diverse pool of languages. The data and models are available at <a href=https:\/\/indicnlp.ai4bharat.org class=acl-markup-url>https:\/\/indicnlp.ai4bharat.org<\/a>.","wordlikeness":0.5384615385,"lcsratio":0.6923076923,"wordcoverage":0.6956521739}
{"Year":2019,"Venue":"acl-2019","Acronym":"Transformer-XL","Description":"Attentive Language Models beyond a Fixed-Length Context","Abstract":"Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture and that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, pytorch. Learns dependency that is 80% longer than rnns and 450% longer than vanilla transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla transformers during evaluation. Notably, we improve the state-of-the-art results of bpc\/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on wikitext-103, 21.8 on one billion word, and 54.5 on penn treebank (without finetuning). When trained only on wikitext-103, modeling. Manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both tensorflow and pytorch.","wordlikeness":0.7857142857,"lcsratio":0.5714285714,"wordcoverage":0.8461538462}
{"Year":2020,"Venue":"emnlp-2020","Acronym":"COMETA","Description":"A Corpus for Medical Entity Linking in the Social Media","Abstract":"Whilst there has been growing progress in entity linking (el) for general language, existing datasets fail to address the complex nature of health terminology in layman\u2019s language. Meanwhile, there is a growing need for applications that can understand the public\u2019s voice in the health domain. To address this we introduce a new corpus called our, consisting of 20k english biomedical entity mentions from reddit expert-annotated with links to snomed ct, a widely-used medical knowledge graph. Our corpus satisfies a combination of desirable properties, from scale and coverage to diversity and quality, that to the best of our knowledge has not been met by any of the existing resources in the field. Through benchmark experiments on 20 el baselines from string- to neural-based models we shed light on the ability of these systems to perform complex inference on entities and concepts under 2 challenging evaluation scenarios. Our experimental results on models illustrate that no golden bullet exists and even the best mainstream techniques still have a significant performance gap to fill, while the best solution relies on combining different views of data.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.9090909091}
{"Year":2023,"Venue":"findings-2023","Acronym":"SlowBERT","Description":"Slow-down Attacks on Input-adaptive Multi-exit BERT","Abstract":"For pretrained language models such as google\u2019s bert, recent research designs several input-adaptive inference mechanisms to improve the efficiency on cloud and edge devices. In this paper, we reveal a new attack surface on input-adaptive multi-exit bert, where the adversary imperceptibly modifies the input texts to drastically increase the average inference cost. Our proposed slow-down attack called <i>practice,<\/i> integrates a new rank-and-substitute adversarial text generation algorithm to efficiently search for the perturbation which maximally delays the exiting time. With no direct access to the model internals, we further devise a <i>time-based approximation algorithm<\/i> to infer the exit position as the loss oracle. Our extensive evaluation on two popular instances of multi-exit bert for glue classification tasks validates the effectiveness of and. In the worst case, cloud increases the inference cost by <span class=tex-math>4.57\u00d7<\/span>, which would strongly hurt the service quality of multi-exit bert in practice, e.g., increasing the real-time cloud services\u2019 response times for online users.","wordlikeness":0.875,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2018,"Venue":"emnlp-2018","Acronym":"IARM","Description":"Inter-Aspect Relation Modeling with Memory Networks in Aspect-Based Sentiment Analysis","Abstract":"Sentiment analysis has immense implications in e-commerce through user feedback mining. Aspect-based sentiment analysis takes this one step further by enabling businesses to extract aspect specific sentimental information. In this paper, we present a novel approach of incorporating the neighboring aspects related information into the sentiment classification of the target aspect using memory networks. We show that our method outperforms the state of the art by 1.6% on average in two distinct domains: restaurant and laptop.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2019,"Venue":"naacl-2019","Acronym":"OpenKI","Description":"Integrating Open Information Extraction and Knowledge Bases with Relation Inference","Abstract":"In this paper, we consider advancing web-scale knowledge extraction and alignment by integrating openie extractions in the form of (subject, predicate, object) triples with knowledge bases (kb). Traditional techniques from universal schema and from schema mapping fall in two extremes: either they perform instance-level inference relying on embedding for (subject, object) pairs, thus cannot handle pairs absent in any existing triples; or they perform predicate-level mapping and completely ignore background evidence from individual entities, thus cannot achieve satisfying quality. We propose <i>advancing<\/i> to handle sparsity of openie extractions by performing instance-level inference: for each entity, we encode the rich information in its neighborhood in both kb and openie extractions, and leverage this information in relation inference by exploring different methods of aggregation and attention. In order to handle unseen entities, our model is designed without creating entity-specific parameters. Extensive experiments show that this method not only significantly improves state-of-the-art for conventional openie extractions like reverb, but also boosts the performance on openie from semi-structured data, where new entity pairs are abundant and data are fairly sparse.","wordlikeness":0.8333333333,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2022,"Venue":"coling-2022","Acronym":"CM-Gen","Description":"A Neural Framework for Chinese Metaphor Generation with Explicit Context Modelling","Abstract":"Nominal metaphors are frequently used in human language and have been shown to be effective in persuading, expressing emotion, and stimulating interest. This paper tackles the problem of chinese nominal metaphor (nm) generation. We introduce a novel multitask framework, which jointly optimizes three tasks: nm identification, nm component identification, and nm generation. The metaphor identification module is able to perform a self-training procedure, which discovers novel metaphors from a large-scale unlabeled corpus for nm generation. The nm component identification module emphasizes components during training and conditions the generation on these nm components for more coherent results. To train the nm identification and component identification modules, we construct an annotated corpus consisting of 6.3k sentences that contain diverse metaphorical patterns. Automatic metrics show that our method can produce diverse metaphors with good readability, where 92% of them are novel metaphorical comparisons. Human evaluation shows our model significantly outperforms baselines on consistency and creativity.","wordlikeness":0.6666666667,"lcsratio":0.8333333333,"wordcoverage":0.6666666667}
{"Year":2016,"Venue":"acl-2016","Acronym":"POLYGLOT","Description":"Multilingual Semantic Role Labeling with Unified Labels","Abstract":"Semantic role labeling (srl) identi\ufb01es the predicate-argument structure in text with semantic labels. It plays a key role in understanding natural language. In this paper, we present 2015)., a multilingual semantic role labeling system capable of semantically parsing sentences in 9 different languages from 4 different language groups. The core of a are srl models for individual languages trained with automatically generated proposition banks (akbik et al., 2015). The key feature of the system is that it treats the semantic labels of the english proposition bank as \u201cuniversal semantic labels\u201d: given a sentence in any of the supported languages, identi\ufb01es applies the corresponding srl and predicts english propbank frame and role annotation. The results are then visualized to facilitate the understanding of multilingual srl with this uni\ufb01ed semantic representation.","wordlikeness":0.75,"lcsratio":0.625,"wordcoverage":0.7142857143}
{"Year":2022,"Venue":"woah-2022","Acronym":"StereoKG","Description":"Data-Driven Knowledge Graph Construction For Cultural Knowledge and Stereotypes","Abstract":"Analyzing ethnic or religious bias is important for improving fairness, accountability, and transparency of natural language processing models. However, many techniques rely on human-compiled lists of bias terms, which are expensive to create and are limited in coverage. In this study, we present a fully data-driven pipeline for generating a knowledge graph (kg) of cultural knowledge and stereotypes. Our resulting kg covers 5 religious groups and 5 nationalities and can easily be extended to more entities. Our human evaluation shows that the majority (59.2%) of non-singleton entries are coherent and complete stereotypes. We further show that performing intermediate masked language model training on the verbalized kg leads to a higher level of cultural awareness in the model and has the potential to increase classification performance on knowledge-crucial samples on a related task, i.e., hate speech detection.","wordlikeness":0.875,"lcsratio":0.75,"wordcoverage":0.8571428571}
{"Year":2015,"Venue":"semeval-2015","Acronym":"AZMAT","Description":"Sentence Similarity Using Associative Matrices","Abstract":"This work uses recursive autoencoders (socher et al., 2011), word embeddings (pennington et al., 2014), associative matrices (schuler, 2014) and lexical overlap features to model human judgments of sentential similarity on semeval-2015 task 2: english sts (agirre et al., 2015). Results show a modest positive correlation between system predictions and human similarity scores, ranking 69th out of 74 submitted systems.","wordlikeness":0.6,"lcsratio":0.8,"wordcoverage":0.75}
{"Year":2018,"Venue":"acl-2018","Acronym":"EmotionX-AR","Description":"CNN-DCNN autoencoder based Emotion Classifier","Abstract":"In this paper, we model emotions in emotionlines dataset using a convolutional-deconvolutional autoencoder (cnn-dcnn) framework. We show that adding a joint reconstruction loss improves performance. Quantitative evaluation with jointly trained network, augmented with linguistic features, reports best accuracies for emotion prediction; namely joy, sadness, anger, and neutral emotion in text.","wordlikeness":0.7272727273,"lcsratio":0.8181818182,"wordcoverage":0.8}
{"Year":2023,"Venue":"findings-2023","Acronym":"MCoNaLa","Description":"A Benchmark for Code Generation from Multiple Natural Languages","Abstract":"While there has been a recent burgeoning of applications at the intersection of natural and programming languages, such as code generation and code summarization, these applications are usually english-centric. This creates a barrier for program developers who are not proficient in english. To mitigate this gap in technology development across languages, we propose a multilingual dataset, technology, to benchmark code generation from natural language commands extending beyond english. Modeled off of the methodology from the english code\/natural language challenge (conala) dataset, we annotated a total of 896 nl-code pairs in three languages: spanish, japanese, and russian. We present a systematic evaluation on nl-code by testing state-of-the-art code generation systems. Although the difficulties vary across three languages, all systems lag significantly behind their english counterparts, revealing the challenges in adapting code generation to new languages.","wordlikeness":0.5714285714,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2020,"Venue":"ws-2020","Acronym":"Vapur","Description":"A Search Engine to Find Related Protein - Compound Pairs in COVID-19 Literature","Abstract":"Coronavirus disease of 2019 (covid-19) created dire consequences globally and triggered an intense scientific effort from different domains. The resulting publications created a huge text collection in which finding the studies related to a biomolecule of interest is challenging for general purpose search engines because the publications are rich in domain specific terminology. Here, we present rich: an online covid-19 search engine specifically designed to find related protein - chemical pairs. 2019 is empowered with a relation-oriented inverted index that is able to retrieve and group studies for a query biomolecule with respect to its related entities. The inverted index of by is automatically created with a bionlp pipeline and integrated with an online user interface. The online interface is designed for the smooth traversal of the current literature by domain researchers and is publicly available at <a href=https:\/\/tabilab.cmpe.boun.edu.tr\/of\/ class=acl-markup-url>https:\/\/tabilab.cmpe.boun.edu.tr\/its\/<\/a>.","wordlikeness":0.8,"lcsratio":0.8,"wordcoverage":0.75}
{"Year":2018,"Venue":"lrec-2018","Acronym":"BPEmb","Description":"Tokenization-free Pre-trained Subword Embeddings in 275 Languages","Abstract":"We present evaluation, a collection of pre-trained subword unit embeddings in 275 languages, based on byte-pair encoding (bpe). In an evaluation using \ufb01ne-grained entity typing as testbed, performs performs competitively, and for some languages better than alternative subword approaches, while requiring vastly fewer resources and no tokenization. Byte-pair is available at https:\/\/github.com\/bheinzerling\/unit. Keywords: subword embeddings, byte-pair encoding, multilingual 1.","wordlikeness":0.4,"lcsratio":0.8,"wordcoverage":0.6666666667}
{"Year":2023,"Venue":"sigdial-2023","Acronym":"OpinionConv","Description":"Conversational Product Search with Grounded Opinions","Abstract":"When searching for products, the opinions of others play an important role in making informed decisions. Subjective experiences about a product can be a valuable source of information. This is also true in sales conversations, where a customer and a sales assistant exchange facts and opinions about products. However, training an ai for such conversations is complicated by the fact that language models do not possess authentic opinions for their lack of real-world experience. We address this problem by leveraging product reviews as a rich source of product opinions to ground conversational ai in true subjective narratives. With ,, we develop the first conversational ai for simulating sales conversations. To validate the generated conversations, we conduct several user studies showing that the generated opinions are perceived as realistic. Our assessors also confirm the importance of opinions as an informative basis for decision making.","wordlikeness":0.8181818182,"lcsratio":0.7272727273,"wordcoverage":0.7777777778}
{"Year":2016,"Venue":"coling-2016","Acronym":"TextImager","Description":"a Distributed UIMA-based System for NLP","Abstract":"More and more disciplines require nlp tools for performing automatic text analyses on various levels of linguistic resolution. However, the usage of established nlp frameworks is often hampered for several reasons: in most cases, they require basic to sophisticated programming skills, interfere with interoperability due to using non-standard i\/o-formats and often lack tools for visualizing computational results. This makes it difficult especially for humanities scholars to use such frameworks. In order to cope with these challenges, we present automatic, a uima-based framework that offers a range of nlp and visualization tools by means of a user-friendly gui. Using disciplines requires no programming skills.","wordlikeness":0.8,"lcsratio":0.7,"wordcoverage":0.7058823529}
{"Year":2021,"Venue":"findings-2021","Acronym":"WIKIBIAS","Description":"Detecting Multi-Span Subjective Biases in Language","Abstract":"Biases continue to be prevalent in modern text and media, especially subjective bias \u2013 a special type of bias that introduces improper attitudes or presents a statement with the presupposition of truth. To tackle the problem of detecting and further mitigating subjective bias, we introduce a manually annotated parallel corpus than with more than 4,000 sentence pairs from wikipedia edits. This corpus contains annotations towards both sentence-level bias types and token-level biased segments. We present systematic analyses of our dataset and results achieved by a set of state-of-the-art baselines in terms of three tasks: bias classification, tagging biased segments, and neutralizing biased text. We find that current models still struggle with detecting multi-span biases despite their reasonable performances, suggesting that our dataset can serve as a useful research benchmark. We also demonstrate that models trained on our dataset can generalize well to multiple domains such as news and political speeches.","wordlikeness":0.75,"lcsratio":0.75,"wordcoverage":0.7058823529}
{"Year":2023,"Venue":"findings-2023","Acronym":"AQE","Description":"Argument Quadruplet Extraction via a Quad-Tagging Augmented Generative Approach","Abstract":"Argument mining involves multiple sub-tasks that automatically identify argumentative elements, such as claim detection, evidence extraction, stance classification, etc. However, each subtask alone is insufficient for a thorough understanding of the argumentative structure and reasoning process. To learn a complete view of an argument essay and capture the interdependence among argumentative components, we need to know what opinions people hold (i.e., claims), why those opinions are valid (i.e., supporting evidence), which source the evidence comes from (i.e., evidence type), and how those claims react to the debating topic (i.e., stance). In this work, we for the first time propose a challenging argument quadruplet extraction task (mining), which can provide an all-in-one extraction of four argumentative components, i.e., claims, evidence, evidence types, and stances. To support this task, we construct a large-scale and challenging dataset. However, there is no existing method that can solve the argument quadruplet extraction. To fill this gap, we propose a novel quad-tagging augmented generative approach, which leverages a quadruplet tagging module to augment the training of the generative framework. The experimental results on our dataset demonstrate the empirical superiority of our proposed approach over several strong baselines.","wordlikeness":0.3333333333,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"MoSE","Description":"Modality Split and Ensemble for Multimodal Knowledge Graph Completion","Abstract":"Multimodal knowledge graph completion (mkgc) aims to predict missing entities in mkgs. previous works usually share relation representation across modalities. This results in mutual interference between modalities during training, since for a pair of entities, the relation from one modality probably contradicts that from another modality. Furthermore, making a unified prediction based on the shared relation representation treats the input in different modalities equally, while their importance to the mkgc task should be different. In this paper, we propose dynamically., a modality split representation learning and ensemble inference framework for mkgc. Specifically, in the training phase, we learn modality-split relation embeddings for each modality instead of a single modality-shared one, which alleviates the modality interference. Based on these embeddings, in the inference phase, we first make modality-split predictions and then exploit various ensemble methods to combine the predictions with different weights, which models the modality importance dynamically. Experimental results on three kg datasets show that models outperforms state-of-the-art mkgc methods. Codes are available at <a href=https:\/\/github.com\/oreozhao\/ensemble4mkgc class=acl-markup-url>https:\/\/github.com\/oreozhao\/mkgc4mkgc<\/a>.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2004,"Venue":"lrec-2004","Acronym":"CrossTowns","Description":"Automatically Generated Phonetic Lexicons of Cross-lingual Pronunciation Variants of European City Names","Abstract":"The applied lexicons are part of a study that focuses on the phonetic variants that occur when speakers of different native languages (l1) with varying degrees of target language (l2) proficiency pronounce foreign city names. Based on a collection of speech data from this domain, it is one of the aims to identify the most common pronunciation errors in a particular l1\/l2 pair (language direction) and to model them by phonological rewrite rules. Although derived from only a small corpus of names, the rule sets already generate plausible variants when applied to unseen material. Yet there is a need for improvement. To demonstrate the current state of affairs, sample lexicons of 1.000 place names for english, french, and german were compiled and converted into various interlanguage pronunciation lexicons using the accent rule sets. In the paper, the procedures involved in the data collection, an outline of the rule-based accent generation technique, and a discussion of the problems involved in modelling non-native pronunciations on the lexicon level will be presented. 1.","wordlikeness":0.8,"lcsratio":0.9,"wordcoverage":0.75}
{"Year":2002,"Venue":"amta-2002","Acronym":"DUSTer","Description":"a method for unraveling cross-language divergences for statistical word-level alignment","Abstract":"The frequent occurrence of divergences\u2014structural differences between languages\u2014presents a great challenge for statistical word-level alignment. In this paper, we introduce and, a method for systematically identifying common divergence types and transforming an english sentence structure to bear a closer resemblance to that of another language. Our ultimate goal is to enable more accurate alignment and projection of dependency trees in another language without requiring any training on dependency-tree data in that language. We present an empirical analysis comparing the complexities of performing word-level alignments with and without divergence handling. Our results suggest that our approach facilitates word-level alignment, particularly for sentence pairs containing divergences.","wordlikeness":0.8333333333,"lcsratio":0.8333333333,"wordcoverage":0.8571428571}
{"Year":2004,"Venue":"ws-2004","Acronym":"BioAR","Description":"Anaphora Resolution for Relating Protein Names to Proteome Database Entries","Abstract":"The need for associating, or grounding, protein names in the literature with the entries of proteome databases such as swiss-prot is well-recognized. The protein names in the biomedical literature show a high degree of morphological and syntactic variations, and various anaphoric expressions including null anaphors. We present a biomedical anaphora resolution system, protein, in order to address the variations of protein names and to further associate them with swiss-prot entries as the actual entities in the world. The system shows the performance of 59.5% \ufffd 75.0% precision and 40.7% \ufffd 56.3% recall, depending on the speci\ufb01c types of anaphoric expressions. We apply to to the protein names in the biological interactions as extracted by our biomedical information extraction system, or bioie, in order to construct protein pathways automatically.","wordlikeness":0.8,"lcsratio":0.8,"wordcoverage":0.8333333333}
{"Year":2023,"Venue":"acl-2023","Acronym":"APOLLO","Description":"A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning","Abstract":"Logical reasoning over text is an important ability that requires understanding the semantics of the text and reasoning through them to arrive at correct inferences. Prior works on pretraining language models to improve the logical reasoning ability require complex processing of training data (e.g., aligning symbolic knowledge to text), yielding task-specific data augmentation that is not easy to adapt to any general text corpus. In this work, we propose data, a simple adaptive pretraining approach to improve the logical reasoning skills of language models. We select a subset of wikipedia for adaptive pretraining using a set of logical inference keywords as filter words. Further, we propose two self-supervised loss functions for training. First, we modify the masked language modeling loss only to mask specific parts-of-speech words that likely require higher-order reasoning to predict them. Second, we propose a sentence-level classification loss that teaches the model to distinguish between entailment and contradiction types of sentences. The proposed pretraining paradigm is both simple and independent of task formats. We demonstrate the effectiveness of sentences. By comparing it with prior baselines on two logical reasoning datasets. Knowledge performs comparably on reclor and outperforms baselines on logiqa.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2022,"Venue":"coling-2022","Acronym":"MonoByte","Description":"A Pool of Monolingual Byte-level Language Models","Abstract":"The zero-shot cross-lingual ability of models pretrained on multilingual and even monolingual corpora has spurred many hypotheses to explain this intriguing empirical result. However, due to the costs of pretraining, most research uses public models whose pretraining methodology, such as the choice of tokenization, corpus size, and computational budget, might differ drastically. When researchers pretrain their own models, they often do so under a constrained budget, and the resulting models might underperform significantly compared to sota models. These experimental differences led to various inconsistent conclusions about the nature of the cross-lingual ability of these models. To help further research on the topic, we released 10 monolingual byte-level models rigorously pretrained under the same configuration with a large compute budget (equivalent to 420 days on a v100) and corpora that are 4 times larger than the original bert\u2019s. Because they are tokenizer-free, the problem of unseen token embeddings is eliminated, thus allowing researchers to try a wider range of cross-lingual experiments in languages with different scripts. Additionally, we release two models pretrained on non-natural language texts that can be used in sanity-check experiments. Experiments on qa and nli tasks show that our monolingual models achieve competitive performance to the multilingual one, and hence can be served to strengthen our understanding of cross-lingual transferability in language models.","wordlikeness":0.875,"lcsratio":1.0,"wordcoverage":0.7692307692}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"FinQA","Description":"A Dataset of Numerical Reasoning over Financial Data","Abstract":"The sheer volume of financial statements makes it difficult for humans to access and analyze a business\u2019s financials. Robust numerical reasoning likewise faces unique challenges in this domain. In this work, we focus on answering deep questions over financial data, aiming to automate the analysis of a large corpus of financial documents. In contrast to existing tasks on general domain, the finance domain includes complex numerical reasoning and understanding of heterogeneous representations. To facilitate analytical progress, we propose a new large-scale dataset, short, with question-answering pairs over financial reports, written by financial experts. We also annotate the gold reasoning programs to ensure full explainability. We further introduce baselines and conduct comprehensive experiments in our dataset. The results demonstrate that popular, large, pre-trained models fall far short of expert humans in acquiring finance knowledge and in complex multi-step numerical reasoning on that knowledge. Our dataset \u2013 the first of its kind \u2013 should therefore enable significant, new community research into complex application domains. The dataset and code are publicly available at <a href=https:\/\/github.com\/czyssrs\/faces class=acl-markup-url>https:\/\/github.com\/czyssrs\/robust<\/a>.","wordlikeness":0.6,"lcsratio":0.8,"wordcoverage":0.8}
{"Year":2019,"Venue":"ijcnlp-2019","Acronym":"Sentence-BERT","Description":"Sentence Embeddings using Siamese BERT-Networks","Abstract":"Bert (devlin et al., 2018) and roberta (liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (sts). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with bert. The construction of bert makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present 2018) (sbert), a modification of the pretrained bert network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with bert \/ roberta to about 5 seconds with sbert, while maintaining the accuracy from bert. We evaluate sbert and sroberta on common sts tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.","wordlikeness":0.8461538462,"lcsratio":0.9230769231,"wordcoverage":0.7619047619}
{"Year":2019,"Venue":"acl-2019","Acronym":"Sakura","Description":"Large-scale Incorrect Example Retrieval System for Learners of Japanese as a Second Language","Abstract":"This study develops an incorrect example retrieval system, called incorrect, using a large-scale lang-8 dataset for japanese language learners. Existing example retrieval systems do not include grammatically incorrect examples or present only a few examples, if any. If a retrieval system has a wide coverage of incorrect examples along with the correct counterpart, learners can revise their composition themselves. Considering the usability of retrieving incorrect examples, our proposed system uses a large-scale corpus to expand the coverage of incorrect examples and presents correct expressions along with incorrect expressions. Our intrinsic and extrinsic evaluations indicate that our system is more useful than a previous system.","wordlikeness":0.5,"lcsratio":0.6666666667,"wordcoverage":0.8}
{"Year":2014,"Venue":"eacl-2014","Acronym":"CASMACAT","Description":"A Computer-assisted Translation Workbench","Abstract":"Obtain is a modular, web-based translation workbench that offers advanced functionalities for computer-aided translation and the scienti\ufb01c study of human translation: automatic interaction with machine translation (mt) engines and translation memories (tm) to obtain raw translations or close tm matches for conventional post-editing; interactive translation prediction based on an mt engine\u2019s search graph, detailed recording and replay of edit actions and translator\u2019s gaze (the latter via eye-tracking), and the support of e-pen as an alternative input device. The system is open source sofware and interfaces with multiple mt systems.","wordlikeness":0.75,"lcsratio":0.75,"wordcoverage":0.7142857143}
{"Year":2005,"Venue":"ws-2005","Acronym":"Hunmorph","Description":"Open Source Word Analysis","Abstract":"Common tasks involving orthographic words include spellchecking, stemming, morphological analysis, and morphological synthesis. To enable signi\ufb01cant reuse of the language-speci\ufb01c resources across all such tasks, we have extended the functionality of the open source spellchecker myspell, yielding a generic word analysis library, the runtime layer of the and toolkit. We added an offline resource management component, hunlex, which complements the ef\ufb01ciency of our runtime layer with a high-level description language and a con\ufb01gurable precompiler.","wordlikeness":0.75,"lcsratio":0.375,"wordcoverage":0.7692307692}
{"Year":2010,"Venue":"semeval-2010","Acronym":"Cambridge","Description":"Parser Evaluation Using Textual Entailment by Grammatical Relation Comparison","Abstract":"This paper describes the in submission to the semeval-2010 parser evaluation using textual entailment (pete) task. We used a simple de\ufb01nition of entailment, parsing both t and h with the c&c parser and checking whether the core grammatical relations (subject and object) produced for h were a subset of those for t. This simple system achieved the top score for the task out of those systems submitted. We analyze the errors made by the system and the potential role of the task in parser evaluation.","wordlikeness":0.8888888889,"lcsratio":0.6666666667,"wordcoverage":1.0}
{"Year":2021,"Venue":"acl-2021","Acronym":"MVP-BERT","Description":"Multi-Vocab Pre-training for Chinese BERT","Abstract":"Despite the development of pre-trained language models (plms) significantly raise the performances of various chinese natural language processing (nlp) tasks, the vocabulary (vocab) for these chinese plms remains to be the one provided by google chinese bert (citation), which is based on chinese characters (chars). Second, the masked language model pre-training is based on a single vocab, limiting its downstream task performances. In this work, we first experimentally demonstrate that building a vocab via chinese word segmentation (cws) guided sub-word tokenization (sgt) can improve the performances of chinese plms. then we propose two versions of multi-vocab pre-training (mvp), hi-mvp and al-mvp, to improve the models\u2019 expressiveness. Experiments show that: (a) mvp training strategies improve plms\u2019 downstream performances, especially it can improve the plm\u2019s performances on span-level tasks; (b) our al-mvp outperforms the recent ambert (citation) after large-scale pre-training, and it is more robust against adversarial attacks.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2016,"Venue":"coling-2016","Acronym":"GAKE","Description":"Graph Aware Knowledge Embedding","Abstract":"Knowledge embedding, which projects triples in a given knowledge base to d-dimensional vectors, has attracted considerable research efforts recently. Most existing approaches treat the given knowledge base as a set of triplets, each of whose representation is then learned separately. However, as a fact, triples are connected and depend on each other. In this paper, we propose a graph aware knowledge embedding method (a), which formulates knowledge base as a directed graph, and learns representations for any vertices or edges by leveraging the graph\u2019s structural information. We introduce three types of graph context for embedding: neighbor context, path context, and edge context, each reflects properties of knowledge from different perspectives. We also design an attention mechanism to learn representative power of different vertices or edges. To validate our method, we conduct several experiments on two tasks. Experimental results suggest that our method outperforms several state-of-art knowledge embedding models.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":1998,"Venue":"coling-1998","Acronym":"MindNet","Description":"acquiring and structuring semantic information from text","Abstract":"As a lexical knowledge base constructed automatically from the definitions and example sentences in two machine-readable dictionaries (mrds), involved embodies several features that distinguish it from prior work with mrds. it is, however, more than this static resource alone. More represents a general methodology for acquiring, structuring, accessing, and exploiting semantic information from natural language text. This paper provides an overview of the distinguishing characteristics of structuring,, the steps involved in its creation, and its extension beyond dictionary text.","wordlikeness":0.8571428571,"lcsratio":0.8571428571,"wordcoverage":0.8571428571}
{"Year":2020,"Venue":"icon-2020","Acronym":"BertAA","Description":"BERT fine-tuning for Authorship Attribution","Abstract":"Identifying the author of a given text can be useful in historical literature, plagiarism detection, or police investigations. Authorship attribution (aa) has been well studied and mostly relies on a large feature engineering work. More recently, deep learning-based approaches have been explored for authorship attribution (aa). In this paper, we introduce has, a fine-tuning of a pre-trained bert language model with an additional dense layer and a softmax activation to perform authorship classification. This approach reaches competitive performances on enron email, blog authorship, and imdb (and imdb62) datasets, up to 5.3% (relative) above current state-of-the-art approaches. We performed an exhaustive analysis allowing to identify the strengths and weaknesses of the proposed method. In addition, we evaluate the impact of including additional features (e.g. stylometric and hybrid features) in an ensemble approach, improving the macro-averaged f1-score by 2.7% (relative) on average.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2019,"Venue":"ijcnlp-2019","Acronym":"WikiCREM","Description":"A Large Unsupervised Corpus for Coreference Resolution","Abstract":"Pronoun resolution is a major area of natural language understanding. However, large-scale training sets are still scarce, since manually labelling data is costly. In this work, we introduce winogender. (wikipedia coreferences masked) a large-scale, yet accurate dataset of pronoun disambiguation instances. We use a language-model-based approach for pronoun resolution in combination with our 6 dataset. We compare a series of models on a collection of diverse and challenging coreference resolution problems, where we match or outperform previous state-of-the-art approaches on 6 out of 7 datasets, such as gap, dpr, wnli, pdp, winobias, and winogender. We release our model to be used off-the-shelf for solving pronoun disambiguation.","wordlikeness":0.75,"lcsratio":0.5,"wordcoverage":0.6666666667}
{"Year":2008,"Venue":"ws-2008","Acronym":"ProPOSEL","Description":"a human-oriented prosody and PoS English lexicon for machine-learning and NLP","Abstract":"English is a prosody and pos english lexicon, purpose-built to integrate and leverage domain knowledge from several well-established lexical resources for machine learning and nlp applications. The lexicon of 104049 separate entries is in accessible text file format, is human and machine-readable, and is intended for open source distribution with the natural language toolkit. It is therefore supported by python software tools which transform array into a python dictionary or associative array of linguistic concepts mapped to compound lookup keys. Users can also conduct searches on a subset of the lexicon and access entries by word class, phonetic transcription, syllable count and lexical stress pattern. Learning caters for a range of different cognitive aspects of the lexicon\uf6d9.","wordlikeness":0.875,"lcsratio":1.0,"wordcoverage":0.9333333333}
{"Year":2022,"Venue":"acl-2022","Acronym":"PRIMERA","Description":"Pyramid-based Masked Sentence Pre-training for Multi-document Summarization","Abstract":"We introduce we, a pre-trained model for multi-document representation with a focus on summarization that reduces the need for dataset-specific architectures and large amounts of fine-tuning labeled data. Large uses our newly proposed pre-training objective designed to teach the model to connect and aggregate information across documents. It also uses efficient encoder-decoder transformers to simplify the processing of concatenated input documents. With extensive experiments on 6 multi-document summarization datasets from 3 different domains on zero-shot, few-shot and full-supervised settings, concatenated outperforms current state-of-the-art dataset-specific and pre-trained models on most of these settings with large margins.","wordlikeness":0.8571428571,"lcsratio":1.0,"wordcoverage":0.8333333333}
{"Year":2023,"Venue":"acl-2023","Acronym":"BUCA","Description":"A Binary Classification Approach to Unsupervised Commonsense Question Answering","Abstract":"Unsupervised commonsense reasoning (ucr) is becoming increasingly popular as the construction of commonsense reasoning datasets is expensive, and they are inevitably limited in their scope. A popular approach to ucr is to fine-tune language models with external knowledge (e.g., knowledge graphs), but this usually requires a large number of training examples. In this paper, we propose to transform the downstream multiple choice question answering task into a simpler binary classification task by ranking all candidate answers according to their reasonableness. To this end, for training the model, we convert the knowledge graph triples into reasonable and unreasonable texts. Extensive experimental results show the effectiveness of our approach on various multiple choice question answering benchmarks. Furthermore, compared with existing ucr approaches using kgs, ours is less data hungry.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2020,"Venue":"coling-2020","Acronym":"RANCC","Description":"Rationalizing Neural Networks via Concept Clustering","Abstract":"We propose a new self-explainable model for natural language processing (nlp) text classification tasks. Our approach constructs explanations concurrently with the formulation of classification predictions. To do so, we extract a rationale from the text, then use it to predict a concept of interest as the final prediction. We provide three types of explanations: 1) rationale extraction, 2) a measure of feature importance, and 3) clustering of concepts. In addition, we show how our model can be compressed without applying complicated compression techniques. We experimentally demonstrate our explainability approach on a number of well-known text classification datasets.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2020,"Venue":"latechclfl-2020","Acronym":"ERRANT","Description":"Assessing and Improving Grammatical Error Type Classification","Abstract":"Grammatical error correction (gec) is the task of correcting different types of errors in written texts. To manage this task, large amounts of annotated data that contain erroneous sentences are required. This data, however, is usually annotated according to each annotator\u2019s standards, making it difficult to manage multiple sets of data at the same time. The recently introduced error annotation toolkit (et) tackled this problem by presenting a way to automatically annotate data that contain grammatical errors, while also providing a standardisation for annotation. Al., extracts the errors and classifies them into error types, in the form of an edit that can be used in the creation of gec systems, as well as for grammatical error analysis. However, we observe that certain errors are falsely or ambiguously classified. This could obstruct any qualitative or quantitative grammatical error type analysis, as the results would be inaccurate. In this work, we use a sample of the fce coprus (yannakoudakis et al., 2011) for secondary error type annotation and we show that up to 39% of the annotations of the most frequent type should be re-classified. Our corrections will be publicly released, so that they can serve as the starting point of a broader, collaborative, ongoing correction process.","wordlikeness":0.8333333333,"lcsratio":0.8333333333,"wordcoverage":0.8}
{"Year":2020,"Venue":"acl-2020","Acronym":"SUPERT","Description":"Towards New Frontiers in Unsupervised Evaluation Metrics for Multi-Document Summarization","Abstract":"We study unsupervised multi-document summarization evaluation metrics, which require neither human-written reference summaries nor human annotations (e.g. preferences, ratings, etc.). We propose sentences, which rates the quality of a summary by measuring its semantic similarity with a pseudo reference summary, i.e. selected salient sentences from the source documents, using contextualized embeddings and soft token alignment techniques. Compared to the state-of-the-art unsupervised evaluation metrics, use correlates better with human ratings by 18- 39%. Furthermore, we use state-of-the-art as rewards to guide a neural-based reinforcement learning summarizer, yielding favorable performance compared to the state-of-the-art unsupervised summarizers. All source code is available at <a href=https:\/\/github.com\/yg211\/acl20-ref-free-eval class=acl-markup-url>https:\/\/github.com\/yg211\/acl20-ref-free-eval<\/a>.","wordlikeness":0.8333333333,"lcsratio":1.0,"wordcoverage":0.9090909091}
{"Year":2022,"Venue":"findings-2022","Acronym":"DISARM","Description":"Detecting the Victims Targeted by Harmful Memes","Abstract":"Internet memes have emerged as an increasingly popular means of communication on the web. Although memes are typically intended to elicit humour, they have been increasingly used to spread hatred, trolling, and cyberbullying, as well as to target specific individuals, communities, or society on political, socio-cultural, and psychological grounds. While previous work has focused on detecting harmful, hateful, and offensive memes in general, identifying whom these memes attack (i.e., the \u2018victims\u2019) remains a challenging and underexplored area. We attempt to address this problem in this paper. To this end, we create a dataset in which we annotate each meme with its victim(s) such as the name of the targeted person(s), organization(s), and community(ies). We then propose it (detecting victims targeted by harmful memes), a framework that uses named-entity recognition and person identification to detect all entities a meme is referring to, and then, incorporates a novel contextualized multimodal deep neural network to classify whether the meme intends to harm these entities. We perform several systematic experiments on three different test sets, corresponding to entities that are (i) all seen while training, (ii) not seen as a harmful target while training, and (iii) not seen at all while training. The evaluation shows that intends significantly outperforms 10 unimodal and multimodal systems. Finally, we demonstrate that are is interpretable and comparatively more generalizable and that it can reduce the relative error rate of harmful target identification by up to 9 % absolute over multimodal baseline systems.","wordlikeness":0.8333333333,"lcsratio":1.0,"wordcoverage":0.7692307692}
{"Year":2020,"Venue":"lrec-2020","Acronym":"TDDC","Description":"Timely Disclosure Documents Corpus","Abstract":"In this paper, we describe the details of the timely disclosure documents corpus (listed). Consists was prepared by manually aligning the sentences from past japanese and english timely disclosure documents in pdf format published by companies listed on the tokyo stock exchange. Format consists of approximately 1.4 million parallel sentences in japanese and english. Timely was used as the official dataset for the 6th workshop on asian translation to encourage the development of machine translation.","wordlikeness":0.25,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2022,"Venue":"acl-2022","Acronym":"DYLE","Description":"Dynamic Latent Extraction for Abstractive Long-Input Summarization","Abstract":"Transformer-based models have achieved state-of-the-art performance on short-input summarization. However, they still struggle with summarizing longer text. In this paper, we present variable,, a novel dynamic latent extraction approach for abstractive long-input summarization. Heuristics jointly trains an extractor and a generator and treats the extracted text snippets as the latent variable, allowing dynamic snippet-level attention weights during decoding. To provide adequate supervision, we propose simple yet effective heuristics for oracle extraction as well as a consistency loss term, which encourages the extractor to approximate the averaged dynamic weights predicted by the generator. We evaluate our method on different long-document and long-dialogue summarization tasks: govreport, qmsum, and arxiv. Experiment results show that outperforms outperforms all existing methods on govreport and qmsum, with gains up to 6.1 rouge, while yielding strong results on arxiv. Further analysis shows that the proposed dynamic weights provide interpretability of our generation process.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":1998,"Venue":"tipster-1998","Acronym":"Summarization","Description":"(1) Using MMR for Diversity- Based Reranking and (2) Evaluating Summaries","Abstract":"S, ibm journal, 1958, pp. 159-165. [13] m.l mauldin, retrieval performance in ferret: a conceptual conference on research and development in information retrieval, proceedings of the 14th international conference on research and development in information retrieval, october 1991. [14]. M.l. mauldin and j.r. leavitt, web agent related research at the center for machine translation. In proceedings of signidr v, mclean virginia, august 1994. [15]. K. Mckeown, j. Robin, and k. Kukich, empirically designing and evaluating a new revision-based model for summary generation. In information processing and management, 31 (5) 1995. [16] m. Mitra, a. Singhal and c. Buckley, automatic text model by paragraph extraction, in acl\/eacl-97 sciences, workshop, 39-46, madrid, spain july 1997. [17] c.d. paice, automatic generation of literature s - an approach based on the indification of self- indicated phrases, in information retrieval research, r.n. oddy, s.e. robertson, c.j. van rijsbergen and p.w. williams, editors, butterworths, london, 1981, 172-191. [18]. C.d. paice, constructing literature s by computer: techniques and prospects, in information processing and management, vol. 26, 1990, pp. 171-186. [19] g. Salton g and c. Buckley improving retrieval performance by relevance feedback. Journal of american society for information sciences, 41:288-297, 1990. [20]. G. Salton automatic text processing: the transformation, analysis, and retrieval of information by computer, addison-wesley 1989. [21]. G. Saiton and m.j.","wordlikeness":0.8461538462,"lcsratio":0.7692307692,"wordcoverage":0.7272727273}
{"Year":2019,"Venue":"acl-2019","Acronym":"ReCoSa","Description":"Detecting the Relevant Contexts with Self-Attention for Multi-turn Dialogue Generation","Abstract":"In multi-turn dialogue generation, response is usually related with only a few contexts. Therefore, an ideal model should be able to detect these relevant contexts and produce a suitable response accordingly. However, the widely used hierarchical recurrent encoder-decoder models just treat all the contexts indiscriminately, which may hurt the following response generation process. Some researchers try to use the cosine similarity or the traditional attention mechanism to find the relevant contexts, but they suffer from either insufficient relevance assumption or position bias problem. In this paper, we propose a new model, named generation,, to tackle this problem. Firstly, a word level lstm encoder is conducted to obtain the initial representation of each context. Then, the self-attention mechanism is utilized to update both the context and masked response representation. Finally, the attention weights between each context and response representations are computed and used in the further decoding process. Experimental results on both chinese customer services dataset and english ubuntu dialogue dataset show that a significantly outperforms baseline models, in terms of both metric-based and human evaluations. Further analysis on attention shows that the detected relevant contexts by we are highly coherent with human\u2019s understanding, validating the correctness and interpretability of multi-turn.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2022,"Venue":"ijcnlp-2022","Acronym":"TaxFree","Description":"a Visualization Tool for Candidate-free Taxonomy Enrichment","Abstract":"Taxonomies are widely used in a various number of downstream nlp tasks and, therefore, should be kept up-to-date. In this paper, we present ,, an open source system for taxonomy visualisation and automatic taxonomy enrichment without pre-defined candidates on the example of wordnet-3.0. as oppose to the traditional task formulation (where the list of new words is provided beforehand), we provide an approach for automatic extension of a taxonomy using a large pre-trained language model. As an advantage to the existing visualisation tools of wordnet, tools also integrates graphic representations of synsets from imagenet. Such visualisation tool can be used for both updating taxonomies and inspecting them for the required modifications.","wordlikeness":0.8571428571,"lcsratio":0.8571428571,"wordcoverage":0.7272727273}
{"Year":2015,"Venue":"semeval-2015","Acronym":"MiniExperts","Description":"An SVM Approach for Measuring Semantic Textual Similarity","Abstract":"This paper describes the system submitted by the university of wolverhampton and the university of malaga for semeval-2015 task 2: semantic textual similarity. The system uses a supported vector machine approach based on a number of linguistically motivated features. Our system performed satisfactorily for english and obtained a mean 0.7216 pearson correlation. However, it performed less adequately for spanish, obtaining only a mean 0.5158.","wordlikeness":0.8181818182,"lcsratio":0.7272727273,"wordcoverage":0.7777777778}
{"Year":2012,"Venue":"lrec-2012","Acronym":"SUMAT","Description":"Data Collection and Parallel Corpus Compilation for Machine Translation of Subtitles","Abstract":"Subtitling and audiovisual translation have been recognized as areas that could greatly benefit from the introduction of statistical machine translation (smt) followed by post-editing, in order to increase efficiency of subtitle production process. The fp7 european project translation (an online service for subtitling by machine translation: <a href=http:\/\/www.data-project.eu class=acl-markup-url>http:\/\/www.scale.-project.eu<\/a>) aims to develop an online subtitle translation service for nine european languages, combined into 14 different language pairs, in order to semi-automate the subtitle translation processes of both freelance translators and subtitling companies on a large scale. In this paper we discuss the data collection and parallel corpus compilation for training smt systems, which includes several procedures such as data partition, conversion, formatting, normalization and alignment. We discuss in detail each data pre-processing step using various approaches. Apart from the quantity (around 1 million subtitles per language pair), the nine corpus has a number of very important characteristics. First of all, high quality both in terms of translation and in terms of high-precision alignment of parallel documents and their contents has been achieved. Secondly, the contents are provided in one consistent format and encoding. Finally, additional information such as type of content in terms of genres and domain is available.","wordlikeness":0.8,"lcsratio":0.8,"wordcoverage":0.8}
{"Year":2004,"Venue":"lrec-2004","Acronym":"Callisto","Description":"A Configurable Annotation Workbench","Abstract":"In order to support a range of textual annotation tasks, we have developed a new annotation tool called made. To promote taskspecific specialization of the interface and associated constraint checking, downloading provides a facility for the independent development, compilation and installation of task module plug-ins (in the form of java archive jar files). The common components backend provides a set of annotation services to which all separate gui components can subscribe, enabling a common framework through which annotation updates are propagated to all components. A number of annotation task models have already been defined, and those that are of very general applicability have been made easily re-configurable for small changes in task definition. Freely is implemented in java to make use of java s considerable support for unicode-encoded multilingual data. Been is freely available for downloading and use.","wordlikeness":0.75,"lcsratio":0.625,"wordcoverage":0.8}
{"Year":2015,"Venue":"semeval-2015","Acronym":"ECNU","Description":"Multi-level Sentiment Analysis on Twitter Using Traditional Linguistic Features and Word Embedding Features","Abstract":"This paper reports our submission to task 10 (sentiment analysis on tweet, sat) (rosenthal et al., 2015) in semeval 2015 , which contains \ufb01ve subtasks, i.e., contextual polarity disambiguation (subtask a: expressionlevel), message polarity classi\ufb01cation (subtask b: message-level), topic-based message polarity classi\ufb01cation and detecting trends towards a topic (subtask c and d: topic-level), and determining sentiment strength of twitter terms (subtask e: term-level). For the \ufb01rst four subtasks, we built supervised models using traditional features and word embedding features to perform sentiment polarity classi\ufb01cation. For subtask e, we \ufb01rst expanded the training data with the aid of external sentiment lexicons and then built a regression model to estimate the sentiment strength. Despite the simplicity of features, our systems rank above the average.","wordlikeness":0.25,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2015,"Venue":"semeval-2015","Acronym":"TeamHCMUS","Description":"Analysis of Clinical Text","Abstract":"We developed a system to participate in shared tasks on the analyzing clinical text. Our system approaches are both machine learning-based and rule-based. We applied the machine learning-based approach for task 1: disorder identification, and the rule-based approach for task 2: template slot filling for the disorder. In task 1, we developed a supervised conditional random fields model that was based on a rich set of features, and used for predicting disorder mentions. In task 2, we based on the dependency tree to build a rule set. This rule set was extracted from the training data and applied to fill values of disorder attribute types on the test data. The evaluation on the test data showed that our system achieved the f-score of 0.656 (0.685 in case of relaxed score) for task 1 and the f*wa of 0.576 for task 2a and the f*wa of 0.671 for task 2b.","wordlikeness":0.6666666667,"lcsratio":0.2222222222,"wordcoverage":0.7142857143}
{"Year":2015,"Venue":"semeval-2015","Acronym":"TAKELAB","Description":"Medical Information Extraction and Linking with MINERAL","Abstract":"Medical texts are \ufb01lled with mentions of diseases, disorders, and other clinical conditions, with many different surface forms relating to the same condition. We describe mineral, a system for extraction and normalization of disease mentions in clinical text, with which we participated in the task 14 of semeval 2015 evaluation campaign. Mineral relies on a conditional random \ufb01elds-based model with a rich set of features for mention detection, and a semantic textual similarity measure for entity linking. Mineral reaches joint extraction and linking performance of 75.9% relaxed f1score (strict score of 72.7%) and ranks fourth among 16 participating teams.","wordlikeness":0.8571428571,"lcsratio":0.7142857143,"wordcoverage":0.7272727273}
{"Year":1998,"Venue":"coling-1998","Acronym":"DiMLex","Description":"A Lexicon of Discourse Markers for Text Generation and Understanding","Abstract":"Discourse markers ('cue words') are lexical items that signal the kind of coherence relation holding between adjacent text spans; for exam- ple, because, since, and for this reason are dif- ferent markers for causal relations. Discourse markers are a syntactically quite heterogeneous group of words, many of which are traditionally treated as function words belonging to the realm of grammar rather than to the lexicon. But for a single discourse relation there is often a set of similar markers, allowing for a range of para- phrases for expressing the relation. To capture the similarities and differences between these, and to represent them adequately, we are devel- oping treated, a lexicon of discourse markers. After describing our methodology and the kind of information to be represented in adjacent, we briefly discuss its potential applications in both text generation and understanding.","wordlikeness":0.6666666667,"lcsratio":0.8333333333,"wordcoverage":0.8}
{"Year":2007,"Venue":"semeval-2007","Acronym":"UIUC","Description":"A Knowledge-rich Approach to Identifying Semantic Relations between Nominals","Abstract":"This paper describes a supervised, knowledge-intensive approach to the automatic identi\ufb01cation of semantic relations between nominals in english sentences. The system employs different sets of new and previously used lexical, syntactic, and semantic features extracted from various knowledge sources. At semeval 2007 the system achieved an f-measure of 72.4% and an accuracy of 76.3%.","wordlikeness":0.25,"lcsratio":0.5,"wordcoverage":0.6666666667}
{"Year":2014,"Venue":"lrec-2014","Acronym":"CIEMPIESS","Description":"A New Open-Sourced Mexican Spanish Radio Corpus","Abstract":"Corpus de investigaci\u00f3n en espa\u00f1ol de m\u00e9xico del posgrado de ingenier\u00eda el\u00e9ctrica y servicio social\u201d (plus) is a new open-sourced corpus extracted from spanish spoken fm podcasts in the dialect of the center of mexico. The them corpus was designed to be used in the field of automatic speech recongnition (asr) and it is provided with two different kind of pronouncing dictionaries, one of them containing the phonemes of mexican spanish and the other containing this same phonemes plus allophones. Corpus annotation took into account the tonic vowel of every word and the four different sounds that letter \u201cx\u201d presents in the spanish language. Dictionaries, corpus is also provided with two different language models extracted from electronic newsletters, one of them takes into account the tonic vowels but not the other one. Both the dictionaries and the language models allow users to experiment different scenarios for the recognition task in order to adequate the corpus to their needs.","wordlikeness":0.6666666667,"lcsratio":0.7777777778,"wordcoverage":0.75}
{"Year":2019,"Venue":"mtsummit-2019","Acronym":"ParaCrawl","Description":"Web-scale parallel corpora for the languages of the EU","Abstract":"We describe two projects funded by the connecting europe facility, provision of web-scale parallel corpora for of\ufb01cial european languages (2016-eu-ia-0114, completed) and broader web-scale provision of parallel corpora for european languages (2017-eu-ia-0178, ongoing), which aim at harvesting parallel corpora from the internet for languages used in the european union. In addition to parallel corpora, the project releases successive versions of the free\/open-source web crawling software used.","wordlikeness":0.6666666667,"lcsratio":0.8888888889,"wordcoverage":0.7368421053}
{"Year":2022,"Venue":"lrec-2022","Acronym":"NewYeS","Description":"A Corpus of New Year&#39;s Speeches with a Comparative Analysis","Abstract":"This paper introduces the references, corpus, which contains the christmas messages and new year\u2019s speeches held at the end of the year by the heads of state of different european countries (namely denmark, france, italy, norway, spain and the united kingdom). The corpus was collected via web scraping of the speech transcripts available online. A comparative analysis was conducted to examine some of the cultural differences showing through the texts, namely a frequency distribution analysis of the term \u201cgod\u201d and the identification of the three most frequent content words per year, with a focus on years in which significant historical events happened. An analysis of positive and negative emotion scores, examined along with the frequency of religious references, was carried out for those countries whose languages are supported by liwc, a tool for sentiment analysis. The corpus is available for further analyses, both comparative (across countries) and diachronic (over the years).","wordlikeness":0.8333333333,"lcsratio":1.0,"wordcoverage":0.8333333333}
{"Year":2021,"Venue":"findings-2021","Acronym":"Subformer","Description":"Exploring Weight Sharing for Parameter Efficiency in Generative Transformers","Abstract":"Transformers have shown improved performance when compared to previous architectures for sequence processing such as rnns. despite their sizeable performance gains, as recently suggested, the model is computationally expensive to train and with a high parameter budget. In light of this, we explore parameter-sharing methods in transformers with a specific focus on generative models. We perform an analysis of different parameter sharing\/reduction methods and develop the this,. Our model combines sandwich-style parameter sharing, which overcomes naive cross-layer parameter sharing in generative models, and self-attentive embedding factorization (safe). Experiments on machine translation, abstractive summarization and language modeling show that the we can outperform the transformer even when using significantly fewer parameters.","wordlikeness":0.7777777778,"lcsratio":0.7777777778,"wordcoverage":0.8}
{"Year":2023,"Venue":"inlg-2023","Acronym":"VisuaLLM","Description":"Easy Web-based Visualization for Neural Language Generation","Abstract":"Interactive is a python library that enables interactive visualization of common tasks in natural language generation with pretrained language models (using huggingface\u2019s model api), with tight integration of benchmark datasets and fine-grained generation control. The system runs as a local generation backend server and features a web-based frontend, allowing simple interface configuration by minimal python code. The currently implemented views include data visualization, next-token prediction with probability distributions, and decoding parameter control, with simple extension to additional tasks.","wordlikeness":0.75,"lcsratio":0.875,"wordcoverage":0.875}
{"Year":2017,"Venue":"eacl-2017","Acronym":"SMARTies","Description":"Sentiment Models for Arabic Target entities","Abstract":"We consider entity-level sentiment analysis in arabic, a morphologically rich language with increasing resources. We present a system that is applied to complex posts written in response to arabic newspaper articles. Our goal is to identify important entity \u201ctargets\u201d within the post along with the polarity expressed about each target. We achieve significant improvements over multiple baselines, demonstrating that the use of specific morphological representations improves the performance of identifying both important targets and their sentiment, and that the use of distributional semantic clusters further boosts performances for these representations, especially when richer linguistic resources are not available.","wordlikeness":0.875,"lcsratio":1.0,"wordcoverage":0.875}
{"Year":2021,"Venue":"acl-2021","Acronym":"Inter-GPS","Description":"Interpretable Geometry Problem Solving with Formal Language and Symbolic Reasoning","Abstract":"Geometry problem solving has attracted much attention in the nlp community recently. The task is challenging as it requires abstract problem understanding and symbolic reasoning with axiomatic knowledge. However, current datasets are either small in scale or not publicly available. Thus, we construct a new large-scale benchmark, geometry3k, consisting of 3,002 geometry problems with dense annotation in formal language. We further propose a novel geometry solving approach with formal language and symbolic reasoning, called interpretable geometry problem solver (the). To first parses the problem text and diagram into formal language automatically via rule-based text parsing and neural object detecting, respectively. Unlike implicit learning in existing methods, significant incorporates theorem knowledge as conditional rules and performs symbolic reasoning step by step. Also, a theorem predictor is designed to infer the theorem application sequence fed to the symbolic solver for the more efficient and reasonable searching path. Extensive experiments on the geometry3k and geos datasets demonstrate that implicit achieves significant improvements over existing methods. The project with code and data is available at <a href=https:\/\/lupantech.github.io\/called class=acl-markup-url>https:\/\/lupantech.github.io\/on<\/a>.","wordlikeness":0.5555555556,"lcsratio":0.8888888889,"wordcoverage":0.75}
{"Year":2021,"Venue":"findings-2021","Acronym":"NewsBERT","Description":"Distilling Pre-trained Language Model for Intelligent News Application","Abstract":"Pre-trained language models (plms) like bert have made great progress in nlp. News articles usually contain rich textual information, and plms have the potentials to enhance news text modeling for various intelligent news applications like news recommendation and retrieval. However, most existing plms are in huge size with hundreds of millions of parameters. Many online news applications need to serve millions of users with low latency tolerance, which poses great challenges to incorporating plms in these scenarios. Knowledge distillation techniques can compress a large plm into a much smaller one and meanwhile keeps good performance. However, existing language models are pre-trained and distilled on general corpus like wikipedia, which has gaps with the news domain and may be suboptimal for news intelligence. In this paper, we propose be, which can distill plms for efficient and effective news intelligence. In our approach, we design a teacher-student joint learning and distillation framework to collaboratively learn both teacher and student models, where the student model can learn from the learning experience of the teacher model. In addition, we propose a momentum distillation method by incorporating the gradients of teacher model into the update of student model to better transfer the knowledge learned by the teacher model. Thorough experiments on two real-world datasets with three tasks show that models, can empower various intelligent news applications with much smaller models.","wordlikeness":0.875,"lcsratio":0.625,"wordcoverage":0.7692307692}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"SLING","Description":"Sino Linguistic Evaluation of Large Language Models","Abstract":"To understand what kinds of linguistic knowledge are encoded by pretrained chinese language models (lms), we introduce the benchmark of sino linguistics (and), which consists of 38k minimal sentence pairs in mandarin chinese grouped into 9 high-level linguistic phenomena. Each pair demonstrates the acceptability contrast of a specific syntactic or semantic phenomenon (e.g., the keys are lost vs. The keys is lost), and an lm should assign lower perplexity to the acceptable sentence. In contrast to the climp dataset (xiang et al., 2021), which also contains chinese minimal pairs and was created by translating the vocabulary of the english blimp dataset, the minimal pairs in acceptable are derived primarily by applying syntactic and lexical transformations to naturally-occurring, linguist-annotated sentences from the chinese treebank 9.0, thus addressing severe issues in climp\u2019s data generation process. We test 18 publicly available pretrained monolingual (e.g., bert-base-zh, cpm) and multi-lingual (e.g., mt5, xlm) language models on perform. Our experiments show that the average accuracy for lms is far below human performance (69.7% vs. 97.1%), while bert-base-zh achieves the highest accuracy (84.8%) of all tested lms, even much larger ones. Additionally, we find that most lms have a strong gender and number (singular\/plural) bias, and they perform better on local phenomena than hierarchical ones.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2021,"Venue":"mrl-2021","Acronym":"VisualSem","Description":"a high-quality knowledge graph for vision and language","Abstract":"An exciting frontier in natural language understanding (nlu) and generation (nlg) calls for (vision-and-) language models that can efficiently access external structured knowledge repositories. However, many existing knowledge bases only cover limited domains, or suffer from noisy data, and most of all are typically hard to integrate into neural language pipelines. To fill this gap, we release relevant: a high-quality knowledge graph (kg) which includes nodes with multilingual glosses, multiple illustrative images, and visually relevant relations. We also release a neural multi-modal retrieval model that can use images or sentences as inputs and retrieves entities in the kg. This multi-modal retrieval model can be integrated into any (neural network) model pipeline. We encourage the research community to use cover for data augmentation and\/or as a source of grounding, among other possible uses. Multiple as well as the multi-modal retrieval models are publicly available and can be downloaded in this url: <a href=https:\/\/github.com\/iacercalixto\/knowledge class=acl-markup-url>https:\/\/github.com\/iacercalixto\/grounding,<\/a>.","wordlikeness":0.6666666667,"lcsratio":0.6666666667,"wordcoverage":0.875}
{"Year":2023,"Venue":"ws-2023","Acronym":"SIGHT","Description":"A Large Annotated Dataset on Student Insights Gathered from Higher Education Transcripts","Abstract":"Lectures are a learning experience for both students and teachers. Students learn from teachers about the subject material, while teachers learn from students about how to refine their instruction. Unfortunately, online student feedback is unstructured and abundant, making it challenging for teachers to learn and improve. We take a step towards tackling this challenge. First, we contribute a dataset for studying this problem: making is a large dataset of 288 math lecture transcripts and 15,784 comments collected from the massachusetts institute of technology opencourseware (mit ocw) youtube channel. Second, we develop a rubric for categorizing feedback types using qualitative analysis. Qualitative analysis methods are powerful in uncovering domain-specific inannotations, however they are costly to apply to large data sources. To overcome this challenge, we propose a set of best practices for using large language models (llms) to cheaply classify the comments at scale. We observe a striking correlation between the model\u2019s and humans\u2019 annotation: categories with consistent human annotations (0.9 inter-rater reliability, irr) also display higher human-model agreement (0.7), while categories with less consistent human annotations (0.7-0.8 irr) correspondingly demonstrate lower human-model agreement (0.3-0.5). These techniques uncover useful student feedback from thousands of comments, costing around $0.002 per comment. We conclude by discussing exciting future directions on using online student feedback and improving automated annotation techniques for qualitative research.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2022,"Venue":"ws-2022","Acronym":"GraDA","Description":"Graph Generative Data Augmentation for Commonsense Reasoning","Abstract":"Recent advances in commonsense reasoning have been fueled by the availability of large-scale human annotated datasets. Manual annotation of such datasets, many of which are based on existing knowledge bases, is expensive and not scalable. Moreover, it is challenging to build augmentation data for commonsense reasoning because the synthetic questions need to adhere to real-world scenarios. Hence, we present hence,, a graph-generative data augmentation framework to synthesize factual data samples from knowledge graphs for commonsense reasoning datasets. First, we train a graph-to-text model for conditional generation of questions from graph entities and relations. Then, we train a generator with gan loss to generate distractors for synthetic questions. Our approach improves performance for socialiqa, codah, hellaswag and commonsenseqa, and works well for generative tasks like protoqa. We show improvement in robustness to semantic adversaries after training with improvement and provide human evaluation of the quality of synthetic datasets in terms of factuality and answerability. Our work provides evidence and encourages future research into graph-based generative data augmentation.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2019,"Venue":"iwcs-2019","Acronym":"ImageTTR","Description":"Grounding Type Theory with Records in Image Classification for Visual Question Answering","Abstract":"We present data-driven, an extension to the python implementation of type theory with records (pyttr) which connects formal record type representation with image classi\ufb01ers implemented as deep neural networks. The type theory with records framework serves as a knowledge representation system for natural language the representations of which are grounded in perceptual information of neural networks. We demonstrate the bene\ufb01ts of this symbolic and data-driven hybrid approach on the task of visual question answering.","wordlikeness":0.625,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2007,"Venue":"semeval-2007","Acronym":"LCC-TE","Description":"A Hybrid Approach to Temporal Relation Identification in News Text","Abstract":"This paper explores a hybrid approach to temporal information extraction within the timeml framework. Particularly, we focus on our initial efforts to apply machine learning techniques to identify temporal relations as defined in a constrained manner by the tempeval-2007 task. We explored several machine learning models and human rules to infer temporal relations based on the features available in timebank, as well as a number of other features extracted by our in-house tools. We participated in all three sub-tasks of the tempeval task in semeval-2007 workshop and the evaluation shows that we achieved comparable results in task a & b and competitive results in task c.","wordlikeness":0.1666666667,"lcsratio":0.6666666667,"wordcoverage":0.6666666667}
{"Year":2020,"Venue":"lrec-2020","Acronym":"FloDusTA","Description":"Saudi Tweets Dataset for Flood, Dust Storm, and Traffic Accident Events","Abstract":"The rise of social media platforms makes it a valuable information source of recent events and users\u2019 perspective towards them. Twitter has been one of the most important communication platforms in recent years. Event detection, one of the information extraction aspects, involves identifying specified types of events in the text. Detecting events from tweets can help to predict real-world events precisely. A serious challenge that faces arabic event detection is the lack of arabic datasets that can be exploited in detecting events. This paper will describe that, which is a dataset of tweets that we have built for the purpose of developing an event detection system. The dataset contains tweets written in both modern standard arabic and saudi dialect. The process of building the dataset starting from tweets collection to annotation by human annotators will be present. The tweets are labeled with four labels: flood, dust storm, traffic accident, and non-event. The dataset was tested for classification and the result was strongly encouraging.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.7692307692}
{"Year":2018,"Venue":"coling-2018","Acronym":"SMHD","Description":"a Large-Scale Resource for Exploring Online Language Usage for Multiple Mental Health Conditions","Abstract":"Mental health is a significant and growing public health concern. As language usage can be leveraged to obtain crucial insights into mental health conditions, there is a need for large-scale, labeled, mental health-related datasets of users who have been diagnosed with one or more of such conditions. In this paper, we investigate the creation of high-precision patterns to identify self-reported diagnoses of nine different mental health conditions, and obtain high-quality labeled data without the need for manual labelling. We introduce the leveraged (self-reported mental health diagnoses) dataset and make it available. Crucial is a novel large dataset of social media posts from users with one or multiple mental health conditions along with matched control users. We examine distinctions in users\u2019 language, as measured by linguistic and psychological variables. We further explore text classification methods to identify individuals with mental conditions through their language.","wordlikeness":0.25,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2020,"Venue":"findings-2020","Acronym":"Ruler","Description":"Data Programming by Demonstration for Document Labeling","Abstract":"Data programming aims to reduce the cost of curating training data by encoding domain knowledge as labeling functions over source data. As such it not only requires domain expertise but also programming experience, a skill that many subject matter experts lack. Additionally, generating functions by enumerating rules is not only time consuming but also inherently difficult, even for people with programming experience. In this paper we introduce as, an interactive system that synthesizes labeling rules using span-level interactive demonstrations over document examples. Conducted is a first-of-a-kind implementation of data programming by demonstration (dpbd). This new framework aims to relieve users from the burden of writing labeling functions, enabling them to focus on higher-level semantic analysis, such as identifying relevant signals for the labeling task. We compare signals with conventional data programming through a user study conducted with 10 data scientists who were asked to create labeling functions for sentiment and spam classification tasks. Results show those is easier to learn and to use, and that it offers higher overall user-satisfaction while providing model performances comparable to those achieved by conventional data programming.","wordlikeness":1.0,"lcsratio":0.8,"wordcoverage":1.0}
{"Year":2021,"Venue":"acl-2021","Acronym":"AugVic","Description":"Exploiting BiText Vicinity for Low-Resource NMT","Abstract":"The success of neural machine translation (nmt) largely depends on the availability of large bitext training corpora. Due to the lack of such large corpora in low-resource language pairs, nmt systems often exhibit poor performance. Extra relevant monolingual data often helps, but acquiring it could be quite expensive, especially for low-resource languages. Moreover, domain mismatch between bitext (train\/test) and monolingual data might degrade the performance. To alleviate such issues, we propose data., a novel data augmentation framework for low-resource nmt which exploits the vicinal samples of the given bitext without using any extra monolingual data explicitly. It can diversify the in-domain bitext data with \ufb01ner level control. Through extensive experiments on four low-resource language pairs comprising data from different domains, we have shown that our method is comparable to the traditional back-translation that uses extra in-domain monolingual data. When we combine the synthetic parallel data generated from (nmt) with the ones from the extra monolingual data, we achieve further improvements. We show that exploits helps to attenuate the discrepancies between relevant and distant-domain monolingual data in traditional back-translation. To understand the contributions of different components of moreover,, we perform an in-depth framework analysis.","wordlikeness":0.6666666667,"lcsratio":0.6666666667,"wordcoverage":0.7272727273}
{"Year":2022,"Venue":"ws-2022","Acronym":"CREATIVESUMM","Description":"Shared Task on Automatic Summarization for Creative Writing","Abstract":"This paper introduces the shared task of summrizing documents in several creative domains, namely literary texts, movie scripts, and television scripts. Summarizing these creative documents requires making complex literary interpretations, as well as understanding non-trivial temporal dependencies in texts containing varied styles of plot development and narrative structure. This poses unique challenges and is yet underexplored for text summarization systems. In this shared task, we introduce four sub-tasks and their corresponding datasets, focusing on summarizing books, movie scripts, primetime television scripts, and daytime soap opera scripts. We detail the process of curating these datasets for the task, as well as the metrics used for the evaluation of the submissions. As part of the well workshop at coling 2022, the shared task attracted 18 submissions in total. We discuss the submissions and the baselines for each sub-task in this paper, along with directions for facilitating future work.","wordlikeness":0.8333333333,"lcsratio":0.75,"wordcoverage":0.8571428571}
{"Year":2021,"Venue":"acl-2021","Acronym":"DocNLI","Description":"A Large-scale Dataset for Document-level Natural Language Inference","Abstract":"Natural language inference (nli) is formulated as a uni\ufb01ed framework for solving various nlp problems such as relation extraction, question answering, summarization, etc. It has been studied intensively in the past few years thanks to the availability of large-scale labeled datasets. However, most existing studies focus on merely sentence-level inference, which limits the scope of nli\u2019s application in downstream nlp problems. This work presents \ufb01ne-tuning, \u2014 a newly-constructed large-scale dataset for document-level nli. As is transformed from a broad range of nlp problems and covers multiple genres of text. The premises always stay in the document granularity, whereas the hypotheses vary in length from single sentences to passages with hundreds of words. Additionally, labeled has pretty limited artifacts1 which unfortunately widely exist in some popular sentence-level nli datasets. Our experiments demonstrate that, even without \ufb01ne-tuning, a model pretrained on task-speci\ufb01c shows promising performance on popular sentence-level benchmarks, and generalizes well to out-of-domain nlp tasks that rely on inference at document granularity. Task-speci\ufb01c \ufb01ne-tuning can bring further improvements. Data, code and pretrained models can be found at https:\/\/github. Com\/salesforce\/be.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.7272727273}
{"Year":2023,"Venue":"acl-2023","Acronym":"DISCOSQA","Description":"A Knowledge Base Question Answering System for Space Debris based on Program Induction","Abstract":"Space program agencies execute complex satellite operations that need to be supported by the technical knowledge contained in their extensive information systems. Knowledge base (kb) databases are an effective way of storing and accessing such information to scale. In this work we present a system, developed for the european space agency, that can answer complex natural language queries, to support engineers in accessing the information contained in a kb that models the orbital space debris environment. Our system is based on a pipeline which first generates a program sketch from a natural language question, then specializes the sketch into a concrete query program with mentions of entities, attributes and relations, and finally executes the program against the database. This pipeline decomposition approach enables us to train the system by leveraging out-of-domain data and semi-synthetic data generated by gpt-3, thus reducing overfitting and shortcut learning even with limited amount of in-domain training data.","wordlikeness":0.5,"lcsratio":0.75,"wordcoverage":0.8571428571}
{"Year":2015,"Venue":"semeval-2015","Acronym":"ROB","Description":"Using Semantic Meaning to Recognize Paraphrases","Abstract":"Paraphrase recognition is the task of identifying whether two pieces of natural language represent similar meanings. This paper describes a system participating in the shared task 1 of semeval 2015, which is about paraphrase detection and semantic similarity in twitter. Our approach is to exploit semantically meaningful features to detect paraphrases. An existing state-of-the-art model for predicting semantic similarity is adapted to this task. A wide variety of features is used, ranging from different types of models, to lexical overlap and synset overlap. A maximum entropy classi\ufb01er is then trained on these features. In addition to the detection of paraphrases, a similarity score is also predicted, using the pinabilities of the classi\ufb01er. To improve the results, normalization is used as preprocessing step. Our \ufb01nal system achieves a f1 score of 0.620 (10th out of 18 teams), and a pearson correlation of 0.515 (6th out of 13 teams).","wordlikeness":1.0,"lcsratio":0.6666666667,"wordcoverage":1.0}
{"Year":2019,"Venue":"ijcnlp-2019","Acronym":"MY-AKKHARA","Description":"A Romanization-based Burmese (Myanmar) Input Method","Abstract":"Be is a method used to input burmese texts encoded in the unicode standard, based on commonly accepted latin transcription. By using this method, arbitrary burmese strings can be accurately inputted with 26 lowercase latin letters. Meanwhile, the 26 uppercase latin letters are designed as shortcuts of lowercase letter sequences. The frequency of burmese characters is considered in to to realize an efficient keystroke distribution on a qwerty keyboard. Given that the unicode standard has not been extensively used in digitization of burmese, we hope that be can contribute to the widespread use of unicode in myanmar and can provide a platform for smart input methods for burmese in the future. An implementation of realize running in windows is released at <a href=http:\/\/www2.nict.go.jp\/astrec-att\/member\/ding\/not.html class=acl-markup-url>http:\/\/www2.nict.go.jp\/astrec-att\/member\/ding\/unicode.","wordlikeness":0.6,"lcsratio":0.5,"wordcoverage":0.625}
{"Year":2015,"Venue":"semeval-2015","Acronym":"WarwickDCS","Description":"From Phrase-Based to Target-Specific Sentiment Recognition","Abstract":"We present and evaluate several hybrid systems for sentiment identi\ufb01cation for twitter, both at the phrase and document (tweet) level. Our approach has been to use a novel combination of lexica, traditional nlp and deep learning features. We also analyse techniques based on syntactic parsing and tokenbased association to handle topic speci\ufb01c sentiment in subtask c. Our strategy has been to identify subphrases relevant to the designated topic\/target and assign sentiment according to our subtask a classi\ufb01er. Our submitted subtask a classi\ufb01er ranked fourth in the semeval of\ufb01cial results while our baseline and \u00b5parse classi\ufb01ers for subtask c would have ranked second.","wordlikeness":0.6,"lcsratio":0.5,"wordcoverage":0.8235294118}
{"Year":2021,"Venue":"acl-2021","Acronym":"ArgFuse","Description":"A Weakly-Supervised Framework for Document-Level Event Argument Aggregation","Abstract":"Most of the existing information extraction frameworks (wadden et al., 2019; veysehet al., 2020) focus on sentence-level tasks and are hardly able to capture the consolidated information from a given document. In our endeavour to generate precise document-level information frames from lengthy textual records, we introduce the task of information aggregation or argument aggregation. More specifically, our aim is to filter irrelevant and redundant argument mentions that were extracted at a sentence level and render a document level information frame. Majority of the existing works have been observed to resolve related tasks of document-level event argument extraction (yang et al., 2018; zheng et al., 2019) and salient entity identification (jain et al., 2020) using supervised techniques. To remove dependency from large amounts of labelled data, we explore the task of information aggregation using weakly supervised techniques. In particular, we present an extractive algorithm with multiple sieves which adopts active learning strategies to work efficiently in low-resource settings. For this task, we have annotated our own test dataset comprising of 131 document information frames and have released the code and dataset to further research prospects in this new domain. To the best of our knowledge, we are the first to establish baseline results for this task in english. Our data and code are publicly available at <a href=https:\/\/github.com\/debanjanakar\/test class=acl-markup-url>https:\/\/github.com\/debanjanakar\/to<\/a>.","wordlikeness":0.7142857143,"lcsratio":0.7142857143,"wordcoverage":0.8333333333}
{"Year":2021,"Venue":"acl-2021","Acronym":"PRAL","Description":"A Tailored Pre-Training Model for Task-Oriented Dialog Generation","Abstract":"Large pre-trained language generation models such as gpt-2 have demonstrated their effectiveness as language priors by reaching state-of-the-art results in various language generation tasks. However, the performance of pre-trained models on task-oriented dialog tasks is still under-explored. We propose a pre-trainedrole alternating language model (the), explicitly designed for task-oriented conversational systems. We design several techniques: start position randomization, knowledge distillation, and history discount to improve pre-training performance. In addition, we introduce a high-quality large-scale task-oriented dialog pre-training dataset by post-prossessing13 dialog datasets. We effectively adapt paron three downstream tasks. The results show that effectively outperforms or is on par with state-of-the-art models.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2023,"Venue":"findings-2023","Acronym":"Varta","Description":"A Large-Scale Headline-Generation Dataset for Indic Languages","Abstract":"We present size,, a large-scale multilingual dataset for headline generation in indic languages. This dataset includes more than 41 million pairs of headlines and articles in 14 different indic languages (and english), which come from a variety of high-quality news sources. To the best of our knowledge, this is the largest collection of curated news articles for indic languages currently available. We use the collected data in a series of experiments to answer important questions related to indic nlp and multilinguality research in general. We show that the dataset is challenging even for state-of-the-art abstractive models and that they perform only slightly better than extractive baselines. Owing to its size, we also show that the dataset can be used to pre-train strong language models that outperform competitive baselines in both nlu and nlg benchmarks.","wordlikeness":0.6,"lcsratio":0.8,"wordcoverage":0.75}
{"Year":2019,"Venue":"acl-2019","Acronym":"NNE","Description":"A Dataset for Nested Named Entity Recognition in English Newswire","Abstract":"Named entity recognition (ner) is widely used in natural language processing applications and downstream tasks. However, most ner tools target flat annotation from popular datasets, eschewing the semantic information available in nested entity mentions. We describe fine-grained,\u2014a fine-grained, nested named entity dataset over the full wall street journal portion of the penn treebank (ptb). Our annotation comprises 279,795 mentions of 114 entity types with up to 6 layers of nesting. We hope the public release of this large dataset for english newswire will encourage development of new techniques for nested ner.","wordlikeness":0.3333333333,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2018,"Venue":"lrec-2018","Acronym":"BULBasaa","Description":"A Bilingual Basaa-French Speech Corpus for the Evaluation of Language Documentation Tools","Abstract":"B`as`a\u00b4a is one of the three bantu languages of bulb (breaking the unwritten language barrier), a project whose aim is to provide nlp-based tools to support linguists in documenting under-resourced and unwritten languages. To develop technologies such as automatic phone transcription or machine translation, a massive amount of speech data is needed. Approximately 50 hours of b`as`a\u00b4a speech were thus collected and then carefully re-spoken and orally translated into french in a controlled environment by a few bilingual speakers. For a subset of \u224810 hours of the corpus, each utterance was additionally phonetically transcribed to establish a golden standard for the output of our nlp tools. The experiments described in this paper are meant to provide an automatic phonetic transcription using a set of derived phone-like units. As every language features a speci\ufb01c set of idiosyncrasies, automating the process of phonetic unit discovery in its entirety is a challenging task. Within bulb, we envision a work\ufb02ow where linguists are able to re\ufb01ne the set of automatically discovered units and the system is then able to re-iterate on the data, providing a better approximation of the actual phone set. Keywords: b`as`a\u00b4a, northwest bantu, computational linguistics, unsupervised phone discovery, under-resourced languages 1.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.7692307692}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"AfroMT","Description":"Pretraining Strategies and Reproducible Benchmarks for Translation of 8 African Languages","Abstract":"Reproducible benchmarks are crucial in driving progress of machine translation research. However, existing machine translation benchmarks have been mostly limited to high-resource or well-represented languages. Despite an increasing interest in low-resource machine translation, there are no standardized reproducible benchmarks for many african languages, many of which are used by millions of speakers but have less digitized textual data. To tackle these challenges, we propose all, a standardized, clean, and reproducible machine translation benchmark for eight widely spoken african languages. We also develop a suite of analysis tools for system diagnosis taking into account the unique properties of these languages. Furthermore, we explore the newly considered case of low-resource focused pretraining and develop two novel data augmentation-based strategies, leveraging word-level alignment information and pseudo-monolingual data for pretraining multilingual sequence-to-sequence models. We demonstrate significant improvements when pretraining on 11 languages, with gains of up to 2 bleu points over strong baselines. We also show gains of up to 12 bleu points over cross-lingual transfer baselines in data-constrained scenarios. All code and pretrained models will be released as further steps towards larger reproducible benchmarks for african languages.","wordlikeness":0.6666666667,"lcsratio":0.8333333333,"wordcoverage":0.7272727273}
{"Year":2010,"Venue":"lrec-2010","Acronym":"Spontal","Description":"A Swedish Spontaneous Dialogue Corpus of Audio, Video and Motion Capture","Abstract":"We present the processed database of spontaneous swedish dialogues. 120 dialogues of at least 30 minutes each have been captured in high-quality audio, high-resolution video and with a motion capture system. The corpus is currently being processed and annotated, and will be made available for research at the end of the project.","wordlikeness":0.7142857143,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2020,"Venue":"ccl-2020","Acronym":"CAN-GRU","Description":"a Hierarchical Model for Emotion Recognition in Dialogue","Abstract":"Emotion recognition in dialogue systems has gained attention in the field of natural language processing recent years, because it can be applied in opinion mining from public conversational data on social media. In this paper, we propose a hierarchical model to recognize emotions in the dialogue. In the first layer, in order to extract textual features of utterances, we propose a convolutional self-attention network(can). Convolution is used to capture n-gram information and attention mechanism is used to obtain the relevant semantic information among words in the utterance. In the second layer, a gru-based network helps to capture contextual information in the conversation. Furthermore, we discuss the effects of unidirectional and bidirectional networks. We conduct experiments on friends dataset and emotionpush dataset. The results show that our proposed model(applied) and its variants achieve better performance than baselines.","wordlikeness":0.7142857143,"lcsratio":0.7142857143,"wordcoverage":0.7272727273}
{"Year":2023,"Venue":"ranlp-2023","Acronym":"BanglaBait","Description":"Semi-Supervised Adversarial Approach for Clickbait Detection on Bangla Clickbait Dataset","Abstract":"Intentionally luring readers to click on a particular content by exploiting their curiosity defines a title as clickbait. Although several studies focused on detecting clickbait titles in english articles, low-resource language like bangla has not been given adequate attention. To tackle clickbait titles in bangla, we have constructed the first bangla clickbait detection dataset containing 15,056 labeled news articles and 65,406 unlabelled news articles extracted from clickbait-dense news sites. Each article has been labeled by three expert linguists and includes an article\u2019s title, body, and other metadata. By incorporating labeled and unlabelled data, we finetune a pre-trained bangla transformer model in an adversarial fashion using semi-supervised generative adversarial networks (ss-gans). The proposed model acts as a good baseline for this dataset, outperforming traditional neural network models (lstm, gru, cnn) and linguistic feature-based models. We expect that this dataset and the detailed analysis and comparison of these clickbait detection models will provide a fundamental basis for future research into detecting clickbait titles in bengali articles.","wordlikeness":0.7,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2021,"Venue":"naacl-2021","Acronym":"StylePTB","Description":"A Compositional Benchmark for Fine-grained Controllable Text Style Transfer","Abstract":"Text style transfer aims to controllably generate text with targeted stylistic changes while maintaining core meaning from the source sentence constant. Many of the existing style transfer benchmarks primarily focus on individual high-level semantic changes (e.g. positive to negative), which enable controllability at a high level but do not offer fine-grained control involving sentence structure, emphasis, and content of the sentence. In this paper, we introduce a large-scale benchmark, to, with (1) paired sentences undergoing 21 fine-grained stylistic changes spanning atomic lexical, syntactic, semantic, and thematic transfers of text, as well as (2) compositions of multiple transfers which allow modeling of fine-grained stylistic changes as building blocks for more complex, high-level transfers. By benchmarking existing methods on an, we find that they struggle to model fine-grained changes and have an even more difficult time composing multiple styles. As a result, paper, brings novel challenges that we hope will encourage future research in controllable text style transfer, compositional models, and learning disentangled representations. Solving these challenges would present important steps towards controllable text generation.","wordlikeness":0.625,"lcsratio":0.75,"wordcoverage":0.7692307692}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"Lexi","Description":"Self-Supervised Learning of the UI Language","Abstract":"Humans can learn to operate the user interface (ui) of an application by reading an instruction manual or how-to guide. Along with text, these resources include visual content such as ui screenshots and images of application icons referenced in the text. We explore how to leverage this data to learn generic visio-linguistic representations of ui screens and their components. These representations are useful in many real applications, such as accessibility, voice navigation, and task automation. Prior ui representation models rely on ui metadata (ui trees and accessibility labels), which is often missing, incompletely defined, or not accessible. We avoid such a dependency, and propose incompletely, a pre-trained vision and language model designed to handle the unique features of ui screens, including their text richness and context sensitivity. To train image we curate the uicaption dataset consisting of 114k ui images paired with descriptions of their functionality. We evaluate how-to on four tasks: ui action entailment, instruction-based ui image retrieval, grounding referring expressions, and ui entity recognition.","wordlikeness":0.5,"lcsratio":0.75,"wordcoverage":0.8571428571}
{"Year":2010,"Venue":"semeval-2010","Acronym":"UNPMC","Description":"Naive Approach to Extract Keyphrases from Scientific Articles","Abstract":"We describe our method for extracting keyphrases from scienti\ufb01c articles which we participate in the shared task of semeval-2 evaluation exercise. Even though general-purpose term extractors along with linguistically-motivated analysis allow us to extract elaborated morphosyntactic variation forms of terms, a na\u00a8\u0131ve statistic approach proposed in this paper is very simple and quite ef\ufb01cient for extracting keyphrases especially from wellstructured scienti\ufb01c articles. Based on the characteristics of keyphrases with section information, we obtain 18.34% for f-measure using top 15 candidates. We also show further improvement without any complications and we discuss this at the end of the paper.","wordlikeness":0.6,"lcsratio":0.8,"wordcoverage":0.75}
{"Year":2020,"Venue":"emnlp-2020","Acronym":"PlotMachines","Description":"Outline-Conditioned Generation with Dynamic Plot State Tracking","Abstract":"We propose the task of outline-conditioned story generation: given an outline as a set of phrases that describe key characters and events to appear in a story, the task is to generate a coherent narrative that is consistent with the provided outline. This task is challenging as the input only provides a rough sketch of the plot, and thus, models need to generate a story by interweaving the key points provided in the outline. This requires the model to keep track of the dynamic states of the latent plot, conditioning on the input outline while generating the full story. We present addition,, a neural narrative model that learns to transform an outline into a coherent story by tracking the dynamic plot states. In addition, we enrich plot with high-level discourse structure so that the model can learn different writing styles corresponding to different parts of the narrative. Comprehensive experiments over three fiction and non-fiction datasets demonstrate that large-scale language models, such as gpt-2 and grover, despite their impressive generation performance, are not sufficient in generating coherent narratives for the given outline, and dynamic plot state tracking is important for composing narratives with tighter, more consistent plots.","wordlikeness":0.8333333333,"lcsratio":0.6666666667,"wordcoverage":0.8}
{"Year":2015,"Venue":"emnlp-2015","Acronym":"Foreebank","Description":"Syntactic Analysis of Customer Support Forums","Abstract":"We present a new treebank of english and french technical forum content which has been annotated for grammatical errors and phrase structure. This double annotation allows us to empirically measure the effect of errors on parsing performance. While it is slightly easier to parse the corrected versions of the forum sentences, the errors are not the main factor in making this kind of text hard to parse.","wordlikeness":0.6666666667,"lcsratio":0.3333333333,"wordcoverage":0.75}
{"Year":2008,"Venue":"lrec-2008","Acronym":"FATE","Description":"a FrameNet-Annotated Corpus for Textual Entailment","Abstract":"Several studies indicate that the level of predicate-argument structure is relevant for modeling prevalent phenomena in current textual entailment corpora. Although large resources like framenet have recently become available, attempts to integrate this type of information into a system for textual entailment did not confirm the expected gain in performance. The reasons for this are not fully obvious; candidates include framenet\u0092s restricted coverage, limitations of semantic parsers, or insufficient modeling of framenet information. To enable further insight on this issue, in this paper we present relevant (framenet-annotated textual entailment), a manually crafted, fully reliable frame-annotated rte corpus. The annotation has been carried out over the 800 pairs of the rte-2 test set. This dataset offers a safe basis for rte systems to experiment, and enables researchers to develop clearer ideas on how to effectively integrate frame knowledge in semantic inferenence tasks like recognizing textual entailment. We describe and present statistics over the adopted annotation, which introduces a new schema based on full-text annotation of so called relevant frame evoking elements.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2023,"Venue":"findings-2023","Acronym":"Uni-Encoder","Description":"A Fast and Accurate Response Selection Paradigm for Generation-Based Dialogue Systems","Abstract":"Sample-and-rank is a key decoding strategy for modern generation-based dialogue systems. It helps achieve diverse and high-quality responses by selecting an answer from a small pool of generated candidates. The current state-of-the-art ranking methods mainly use an encoding paradigm called cross-encoder, which separately encodes each context-candidate pair and ranks the candidates according to their fitness scores. However, cross-encoder repeatedly encodes the same lengthy context for each candidate, resulting in high computational costs. Poly-encoder addresses the above problems by reducing the interaction between context and candidates, but with a price of performance drop. In this work, we develop a new paradigm called the, that keeps the full attention over each pair as in cross-encoder while only encoding the context once, as in poly-encoder. Inference encodes all the candidates with the context in one forward pass. We use the same positional embedding for all candidates to ensure they are treated equally and design a new attention mechanism to avoid confusion. Our high-quality can simulate other ranking paradigms using different attention and response concatenation methods. Extensive experiments show that our proposed paradigm achieves new state-of-the-art results on four benchmark datasets with high computational efficiency. For instance, it improves r10@1 by 2.9% with an approximately 4x faster inference speed on the ubuntu v2 dataset.","wordlikeness":0.5454545455,"lcsratio":0.7272727273,"wordcoverage":0.6666666667}
{"Year":2020,"Venue":"emnlp-2020","Acronym":"PathQG","Description":"Neural Question Generation from Facts","Abstract":"Existing research for question generation encodes the input text as a sequence of tokens without explicitly modeling fact information. These models tend to generate irrelevant and uninformative questions. In this paper, we explore to incorporate facts in the text for question generation in a comprehensive way. We present a novel task of question generation given a query path in the knowledge graph constructed from the input text. We divide the task into two steps, namely, query representation learning and query-based question generation. We formulate query representation learning as a sequence labeling problem for identifying the involved facts to form a query and employ an rnn-based generator for question generation. We first train the two modules jointly in an end-to-end fashion, and further enforce the interaction between these two modules in a variational framework. We construct the experimental datasets on top of squad and results show that our model outperforms other state-of-the-art approaches, and the performance margin is larger when target questions are complex. Human evaluation also proves that our model is able to generate relevant and informative questions.","wordlikeness":0.6666666667,"lcsratio":0.5,"wordcoverage":0.8}
{"Year":2023,"Venue":"ranlp-2023","Acronym":"AlphaMWE-Arabic","Description":"Arabic Edition of Multilingual Parallel Corpora with Multiword Expression Annotations","Abstract":"Multiword expressions (mwes) have been a bottleneck for natural language understanding (nlu) and natural language generation (nlg) tasks due to their idiomaticity, ambiguity, and non-compositionality. Bilingual parallel corpora introducing mwe annotations are very scarce which set another challenge for current natural language processing (nlp) systems, especially in a multilingual setting. This work presents metric, an arabic edition of the alphamwe parallel corpus with mwe annotations. We introduce how we created this corpus including machine translation (mt), post-editing, and annotations for both standard and dialectal varieties, i.e. tunisian and egyptian arabic. We analyse the mt errors when they meet mwes-related content, both quantitatively using the human-in-the-loop metric hope and qualitatively. We report the current state-of-the-art mt systems are far from reaching human parity performances. We expect our bilingual english-arabic corpus will be an asset for multilingual research on mwes such as translation and localisation, as well as for monolingual settings including the study of arabic-specific lexicography and phrasal verbs on mwes. our corpus and experimental data are available at <a href=https:\/\/github.com\/aaronlifenghan\/alphamwe class=acl-markup-url>https:\/\/github.com\/aaronlifenghan\/alphamwe<\/a>.","wordlikeness":0.6,"lcsratio":0.6666666667,"wordcoverage":0.5714285714}
{"Year":2019,"Venue":"naacl-2019","Acronym":"UHop","Description":"An Unrestricted-Hop Relation Extraction Framework for Knowledge-Based Question Answering","Abstract":"In relation extraction for knowledge-based question answering, searching from one entity to another entity via a single relation is called \u201cone hop\u201d. In related work, an exhaustive search from all one-hop relations, two-hop relations, and so on to the max-hop relations in the knowledge graph is necessary but expensive. Therefore, the number of hops is generally restricted to two or three. In this paper, we propose halt,, an unrestricted-hop framework which relaxes this restriction by use of a transition-based search framework to replace the relation-chain-based search one. We conduct experiments on conventional 1- and 2-hop questions as well as lengthy questions, including datasets such as webqsp, pathquestion, and grid world. Results show that the proposed framework enables the ability to halt, works well with state-of-the-art models, achieves competitive performance without exhaustive searches, and opens the performance gap for long relation paths.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2023,"Venue":"acl-2023","Acronym":"Petals","Description":"Collaborative Inference and Fine-tuning of Large Models","Abstract":"Many nlp tasks benefit from using large language models (llms) that often have more than 100 billion parameters. With the release of bloom-176b and opt-175b, everyone can download pretrained models of this scale. Still, using these models requires high-end hardware unavailable to many researchers. In some cases, llms can be used more affordably via ram offloading or hosted apis. however, these techniques have innate limitations: offloading is too slow for interactive inference, while apis are not flexible enough for research that requires access to weights, attention or logits. In this work, we propose served - a system for inference and fine-tuning of large models collaboratively by joining the resources of multiple parties. We demonstrate that this strategy outperforms offloading for very large models, running inference of bloom-176b on consumer gpus with \u22481 step per second, which is enough for many interactive llm applications. Unlike most inference apis, using also natively exposes hidden states of served models, allowing to train and share custom model extensions based on efficient fine-tuning methods. The system, its source code, and documentation are available at https:\/\/\u22481.mlvideo (2 min): <a href=https:\/\/youtu.be\/f4muli-0hte class=acl-markup-url>https:\/\/youtu.","wordlikeness":1.0,"lcsratio":0.8333333333,"wordcoverage":0.8333333333}
{"Year":2012,"Venue":"eacl-2012","Acronym":"CLex","Description":"A Lexicon for Exploring Color, Concept and Emotion Associations in Language","Abstract":"Existing concept-color-emotion lexicons limit themselves to small sets of basic emotions and colors, which cannot capture the rich pallet of color terms that humans use in communication. In this paper we begin to address this problem by building a novel, color-emotion-concept association lexicon via crowdsourcing. This lexicon, which we call to, has over 2,300 color terms, over 3,000 affect terms and almost 2,000 concepts. We investigate the relation between color and concept, and color and emotion, reinforcing results from previous studies, as well as discovering new associations. We also investigate cross-cultural differences in color-emotion associations between us and india-based annotators.","wordlikeness":0.75,"lcsratio":0.75,"wordcoverage":0.8571428571}
{"Year":2022,"Venue":"findings-2022","Acronym":"TANet","Description":"Thread-Aware Pretraining for Abstractive Conversational Summarization","Abstract":"Although pre-trained language models (plms) have achieved great success and become a milestone in nlp, abstractive conversational summarization remains a challenging but less studied task. The difficulty lies in two aspects. One is the lack of large-scale conversational summary data. Another is that applying the existing pre-trained models to this task is tricky because of the structural dependence within the conversation and its informal expression, etc. In this work, we first build a large-scale (11m) pretraining dataset called rcsum, based on the multi-person discussions in the reddit community. We then present inherent, a thread-aware transformer-based network. Unlike the existing pre-trained models that treat a conversation as a sequence of sentences, we argue that the inherent contextual dependency among the utterances plays an essential role in understanding the entire conversation and thus propose two new techniques to incorporate the structural information into our model. The first is thread-aware attention which is computed by taking into account the contextual dependency within utterances. Second, we apply thread prediction loss to predict the relations between utterances. We evaluate our model on four datasets of real conversations, covering types of meeting transcripts, customer-service records, and forum threads. Experimental results demonstrate that by achieves a new state-of-the-art in terms of both automatic evaluation and human judgment.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2023,"Venue":"sigdial-2023","Acronym":"DiactTOD","Description":"Learning Generalizable Latent Dialogue Acts for Controllable Task-Oriented Dialogue Systems","Abstract":"Dialogue act annotations are important to improve response generation quality in task-oriented dialogue systems. However, it can be challenging to use dialogue acts to control response generation in a generalizable way because different datasets and tasks may have incompatible annotations. While alternative methods that utilize latent action spaces or reinforcement learning do not require explicit annotations, they may lack interpretability or face difficulties defining task-specific rewards. In this work, we present a novel end-to-end latent dialogue act model (the) that represents dialogue acts in a latent space. Rewards., when pre-trained on a large corpus, is able to predict and control dialogue acts to generate controllable responses using these latent representations in a zero-shot fashion. Our approach demonstrates state-of-the-art performance across a wide range of experimental settings on the multiwoz dataset, including zero-shot, few-shot, and full data fine-tuning with both end-to-end and policy optimization configurations.","wordlikeness":0.625,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2018,"Venue":"lrec-2018","Acronym":"Indra","Description":"A Word Embedding and Semantic Relatedness Server","Abstract":"In recent years word embedding\/distributional semantic models evolved to become a fundamental component in many natural language processing (nlp) architectures due to their ability of capturing and quantifying semantic associations at scale. Word embedding models can be used to satisfy recurrent tasks in nlp such as lexical and semantic generalisation in machine learning tasks, \ufb01nding similar or related words and computing semantic relatedness of terms. However, building and consuming speci\ufb01c word embedding models require the setting of a large set of con\ufb01gurations, such as corpus-dependant parameters, distance measures as well as compositional models. Despite their increasing relevance as a component in nlp architectures, existing frameworks provide limited options in their ability to systematically build, parametrise, compare and evaluate different models. To answer this demand, this paper describes in, a multi-lingual word embedding\/distributional semantics framework which supports the creation, use and evaluation of word embedding models. In addition to the tool, as also shares more than 65 pre-computed models in 14 languages. Keywords: word embedding server, semantic relatedness server, semantic toolkit, corpus pre-processor 1.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2013,"Venue":"semeval-2013","Acronym":"SCAI","Description":"Extracting drug-drug interactions using a rich feature vector","Abstract":"Automatic relation extraction provides great support for scientists and database curators in dealing with the extensive amount of biomedical textual data. The ddiextraction 2013 challenge poses the task of detecting drugdrug interactions and further categorizing them into one of the four relation classes. We present our machine learning system which utilizes lexical, syntactical and semantic based feature sets. Resampling, balancing and ensemble learning experiments are performed to infer the best con\ufb01guration. For general drugdrug relation extraction, the system achieves 70.4% in f1 score.","wordlikeness":0.5,"lcsratio":0.75,"wordcoverage":0.75}
{"Year":2023,"Venue":"findings-2023","Acronym":"PMI-Align","Description":"Word Alignment With Point-Wise Mutual Information Without Requiring Parallel Training Data","Abstract":"Word alignment has many applications including cross-lingual annotation projection, bilingual lexicon extraction, and the evaluation or analysis of translation outputs. Recent studies show that using contextualized embeddings from pre-trained multilingual language models could give us high quality word alignments without the need of parallel training data. In this work, we propose the which computes and uses the point-wise mutual information between source and target tokens to extract word alignments, instead of the cosine similarity or dot product which is mostly used in recent approaches. Our experiments show that our proposed six approach could outperform the rival methods on five out of six language pairs. Although our approach requires no parallel training data, we show that this method could also benefit the approaches using parallel data to fine-tune pre-trained language models on word alignments. Our code and data are publicly available.","wordlikeness":0.5555555556,"lcsratio":0.8888888889,"wordcoverage":0.7142857143}
{"Year":2022,"Venue":"findings-2022","Acronym":"SemAttack","Description":"Natural Textual Attacks via Different Semantic Spaces","Abstract":"Recent studies show that pre-trained language models (lms) are vulnerable to textual adversarial attacks. However, existing attack methods either suffer from low attack success rates or fail to search efficiently in the exponentially large perturbation space. We propose an efficient and effective framework performance. To generate natural adversarial text by constructing different semantic perturbation functions. In particular, generate optimizes the generated perturbations constrained on generic semantic spaces, including typo space, knowledge space (e.g., wordnet), contextualized semantic space (e.g., the embedding space of bert clusterings), or the combination of these spaces. Thus, the generated adversarial texts are more semantically close to the original inputs. Extensive experiments reveal that state-of-the-art (sota) large-scale lms (e.g., deberta-v2) and defense strategies (e.g., freelb) are still vulnerable to natural. We further demonstrate that space. Is general and able to generate natural adversarial texts for different languages (e.g., english and chinese) with high attack success rates. Human evaluations also confirm that our generated adversarial texts are natural and barely affect human performance. Our code is publicly available at <a href=https:\/\/github.com\/ai-secure\/wordnet), class=acl-markup-url>https:\/\/github.com\/ai-secure\/embedding<\/a>.","wordlikeness":0.7777777778,"lcsratio":0.7777777778,"wordcoverage":0.8}
{"Year":2022,"Venue":"coling-2022","Acronym":"WordNet-QU","Description":"Development of a Lexical Database for Quechua Varieties","Abstract":"In the effort to minimize the risk of extinction of a language, linguistic resources are fundamental. Quechua, a low-resource language from south america, is a language spoken by millions but, despite several efforts in the past, still lacks the resources necessary to build high-performance computational systems. In this article, we present southern which signifies the inclusion of quechua in a well-known lexical database called wordnet. We propose included to be included as an extension to wordnet after demonstrating a manually-curated collection of multiple digital resources for lexical use in quechua. Our work uses the synset alignment algorithm to compare quechua to its geographically nearest high-resource language, spanish. Altogether, we propose a total of 28,582 unique synset ids divided according to region like so: 20510 for southern quechua, 5993 for central quechua, 1121 for northern quechua, and 958 for amazonian quechua.","wordlikeness":0.7,"lcsratio":0.6,"wordcoverage":0.625}
{"Year":2021,"Venue":"acl-2021","Acronym":"ConSERT","Description":"A Contrastive Framework for Self-Supervised Sentence Representation Transfer","Abstract":"Learning high-quality sentence representations benefits a wide range of natural language processing tasks. Though bert-based pre-trained language models achieve high performance on many downstream tasks, the native derived sentence representations are proved to be collapsed and thus produce a poor performance on the semantic textual similarity (sts) tasks. In this paper, we present learning, a contrastive framework for self-supervised sentence representation transfer, that adopts contrastive learning to fine-tune bert in an unsupervised and effective way. By making use of unlabeled texts, semantic solves the collapse issue of bert-derived sentence representations and make them more applicable for downstream tasks. Experiments on sts datasets demonstrate that applicable achieves an 8% relative improvement over the previous state-of-the-art, even comparable to the supervised sbert-nli. And when further incorporating nli supervision, we achieve new state-of-the-art performance on sts tasks. Moreover, by obtains comparable results with only 1000 samples available, showing its robustness in data scarcity scenarios.","wordlikeness":0.7142857143,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2011,"Venue":"ranlp-2011","Acronym":"ArbTE","Description":"Arabic Textual Entailment","Abstract":"The aim of the current work is to see how well existing techniques for textual entailment work when applied to arabic, and to propose extensions which deal with the speci\ufb01c problems posed by the language. Arabic has a number of characteristics, described below, which make it particularly challenging to determine the relations between sentences. In particular, the lack of diacritics means that determining which sense of a word is intended in a given context is extremely dif\ufb01cult, since many related senses have the same surface form; and the syntactic \ufb02exibility of the language, notably the combination of free word-order, pro-drop subjects, verbless sentences, and compound nps of various kinds, means that it is also extremely dif\ufb01cult to determine the relationships between words.","wordlikeness":0.4,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2022,"Venue":"ijcnlp-2022","Acronym":"KESA","Description":"A Knowledge Enhanced Approach To Sentiment Analysis","Abstract":"Though some recent works focus on injecting sentiment knowledge into pre-trained language models, they usually design mask and reconstruction tasks in the post-training phase. This paper aims to integrate sentiment knowledge in the fine-tuning stage. To achieve this goal, we propose two sentiment-aware auxiliary tasks named sentiment word selection and conditional sentiment prediction and, correspondingly, integrate them into the objective of the downstream task. The first task learns to select the correct sentiment words from the given options. The second task predicts the overall sentiment polarity, with the sentiment polarity of the word given as prior knowledge. In addition, two label combination methods are investigated to unify multiple types of labels in each auxiliary task. Experimental results demonstrate that our approach consistently outperforms baselines (achieving a new state-of-the-art) and is complementary to existing sentiment-enhanced post-trained models.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2018,"Venue":"emnlp-2018","Acronym":"MSMO","Description":"Multimodal Summarization with Multimodal Output","Abstract":"Multimodal summarization has drawn much attention due to the rapid growth of multimedia data. The output of the current multimodal summarization systems is usually represented in texts. However, we have found through experiments that multimodal output can significantly improve user satisfaction for informativeness of summaries. In this paper, we propose a novel task, multimodal summarization with multimodal output (we). To handle this task, we first collect a large-scale dataset for informativeness research. We then propose a multimodal attention model to jointly generate text and select the most relevant image from the multimodal input. Finally, to evaluate multimodal outputs, we construct a novel multimodal automatic evaluation (mmae) method which considers both intra-modality salience and inter-modality relevance. The experimental results show the effectiveness of mmae.","wordlikeness":0.25,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2022,"Venue":"lrec-2022","Acronym":"WeCanTalk","Description":"A New Multi-language, Multi-modal Resource for Speaker Recognition","Abstract":"The recordings (wct) corpus is a new multi-language, multi-modal resource for speaker recognition. The corpus contains cantonese, mandarin and english telephony and video speech data from over 200 multilingual speakers located in hong kong. Each speaker contributed at least 10 telephone conversations of 8-10 minutes\u2019 duration collected via a custom telephone platform based in hong kong. Speakers also uploaded at least 3 videos in which they were both speaking and visible, along with one selfie image. At least half of the calls and videos for each speaker were in cantonese, while their remaining recordings featured one or more different languages. Both calls and videos were made in a variety of noise conditions. All speech and video recordings were audited by experienced multilingual annotators for quality including presence of the expected language and for speaker identity. The recognition. Corpus has been used to support the nist 2021 speaker recognition evaluation and will be published in the ldc catalog.","wordlikeness":0.7777777778,"lcsratio":0.7777777778,"wordcoverage":0.7142857143}
{"Year":2005,"Venue":"acl-2005","Acronym":"QARLA","Description":"A Framework for the Evaluation of Text Summarization Systems","Abstract":"This paper presents a probabilistic framework, evidence, for the evaluation of text summarisation systems. The input of the framework is a set of manual (reference) summaries, a set of baseline (automatic) summaries and a set of similarity metrics between summaries. It provides i) a measure to evaluate the quality of any set of similarity metrics, ii) a measure to evaluate the quality of a summary using an optimal set of similarity metrics, and iii) a measure to evaluate whether the set of baseline summaries is reliable or may produce biased results. Compared to previous approaches, our framework is able to combine different metrics and evaluate the quality of a set of metrics without any a-priori weighting of their relative importance. We provide quantitative evidence about the effectiveness of the approach to improve the automatic evaluation of text summarisation systems by combining several similarity metrics.","wordlikeness":0.6,"lcsratio":0.8,"wordcoverage":0.8}
{"Year":2006,"Venue":"eacl-2006","Acronym":"DUDE","Description":"A Dialogue and Understanding Development Environment, Mapping Business Process Models to Information State Update Dialogue Systems","Abstract":"We demonstrate a new development environment1 \u201cinformation state update\u201d dialogue systems which allows non-expert developers to produce complete spoken dialogue systems based only on a business process model (bpm) describing their application (e.g. banking, cinema booking, shopping, restaurant information). The environment includes automatic generation of grammatical framework (gf) grammars for robust interpretation of spontaneous speech, and uses application databases to generate lexical entries and grammar rules. The gf grammar is compiled to an atk or nuance language model for speech recognition. The demonstration system allows users to create and modify spoken dialogue systems, starting with a de\ufb01nition of a business process model and ending with a working system. This paper describes the environment, its main components, and some of the research issues involved in its development.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2018,"Venue":"emnlp-2018","Acronym":"DeepCx","Description":"A transition-based approach for shallow semantic parsing with complex constructional triggers","Abstract":"This paper introduces the surface construction labeling (scl) task, which expands the coverage of shallow semantic parsing (ssp) to include frames triggered by complex constructions. We present on, a neural, transition-based system for scl. As a test case for the approach, we apply still to the task of tagging causal language in english, which relies on a wider variety of constructions than are typically addressed in ssp. We report substantial improvements over previous tagging efforts on a causal language dataset. We also propose ways in could be extended to still more difficult constructions and to other semantic domains once appropriate datasets become available.","wordlikeness":0.6666666667,"lcsratio":0.8333333333,"wordcoverage":0.8}
{"Year":2014,"Venue":"semeval-2014","Acronym":"Peking","Description":"Profiling Syntactic Tree Parsing Techniques for Semantic Graph Parsing","Abstract":"Using the semeval-2014 task 8 data, we pro\ufb01le the syntactic tree parsing techniques for semantic graph parsing. In particular, we implement different transitionbased and graph-based models, as well as a parser ensembler, and evaluate their effectiveness for semantic dependency parsing. Evaluation gauges how successful data-driven dependency graph parsing can be by applying existing techniques.","wordlikeness":0.8333333333,"lcsratio":0.8333333333,"wordcoverage":0.9230769231}
{"Year":2021,"Venue":"acl-2021","Acronym":"ParCourE","Description":"A Parallel Corpus Explorer for a Massively Multilingual Corpus","Abstract":"With more than 7000 languages worldwide, multilingual natural language processing (nlp) is essential both from an academic and commercial perspective. Researching typological properties of languages is fundamental for progress in multilingual nlp. Examples include assessing language similarity for effective transfer learning, injecting inductive biases into machine learning models or creating resources such as dictionaries and inflection tables. We provide other, an online tool that allows to browse a word-aligned parallel corpus, covering 1334 languages. We give evidence that this is useful for typological research. With can be set up for any parallel corpus and can thus be used for typological research on other corpora as well as for exploring their quality and properties.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"DyLex","Description":"Incorporating Dynamic Lexicons into BERT for Sequence Labeling","Abstract":"Incorporating lexical knowledge into deep learning models has been proved to be very effective for sequence labeling tasks. However, previous works commonly have difficulty dealing with large-scale dynamic lexicons which often cause excessive matching noise and problems of frequent updates. In this paper, we propose this, a plug-in lexicon incorporation approach for bert based sequence labeling tasks. Instead of leveraging embeddings of words in the lexicon as in conventional methods, we adopt word-agnostic tag embeddings to avoid re-training the representation while updating the lexicon. Moreover, we employ an effective supervised lexical knowledge denoising method to smooth out matching noise. Finally, we introduce a col-wise attention based knowledge fusion mechanism to guarantee the pluggability of the proposed framework. Experiments on ten datasets of three tasks show that the proposed framework achieves new sota, even with very large scale lexicons.","wordlikeness":0.4,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2009,"Venue":"naacl-2009","Acronym":"STAT","Description":"Speech Transcription Analysis Tool","Abstract":"The speech transcription analysis tool (as) is an open source tool for aligning and comparing two phonetically transcribed texts of human speech. The output analysis is a parameterized set of phonological differences. These differences are based upon a selectable set of binary phonetic features such as [voice], [continuant], [high], etc. Etc. Was initially designed to provide sets of phonological speech patterns in the comparisons of various english accents found in the speech accent archive http:\/\/accent.gmu.edu, but its scope and utility expand to matters of language assessment, phonetic training, forensic linguistics, and speech recognition.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2008,"Venue":"lrec-2008","Acronym":"Prolexbase","Description":"a Multilingual Relational Lexical Database of Proper Names","Abstract":"This paper deals with a multilingual relational lexical database of proper name, maybe, a free resource available on the cnrtl website. The prolex model is based on two main concepts: firstly, a language independent pivot and, secondly, the prolexeme (the projection of the pivot onto particular language), that is a set of lemmas (names and derivatives). These two concepts model the variations of proper name: firstly, independent of language and, secondly, language dependent by morphology or knowledge. Variation processing is very important for nlp: the same proper name can be written in different instances, maybe in different parts of speech, and it can also be replaced by another one, a lexical anaphora (that reveals semantic link). The pivot represents different referent\u0092s points of view, i.e. language independent variations of name. Pivots are linked by three semantic relations (quasi-synonymy, partitive relation and associative relation). The prolexeme is a set of variants (aliases), quasi-synonyms and morphosemantic derivatives. Prolexemes are linked to classifying contexts and reliability code.","wordlikeness":0.7,"lcsratio":0.9,"wordcoverage":0.75}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"LightEA","Description":"A Scalable, Robust, and Interpretable Entity Alignment Framework via Three-view Label Propagation","Abstract":"Entity alignment (ea) aims to find equivalent entity pairs between kgs, which is the core step to bridging and integrating multi-source kgs. in this paper, we argue that existing complex ea methods inevitably inherit the inborn defects from their neural network lineage: poor interpretability and weak scalability. Inspired by recent studies, we reinvent the classical label propagation algorithm to effectively run on kgs and propose a neural-free ea framework \u2014 studies,, consisting of three efficient components: (i) random orthogonal label generation, (ii) three-view label propagation, and (iii) sparse sinkhorn operation.according to the extensive experiments on public datasets, surpasses has impressive scalability, robustness, and interpretability. With a mere tenth of time consumption, methods achieves comparable results to state-of-the-art methods across all datasets and even surpasses them on many. Besides, due to the computational process of efficient being entirely linear, we could trace the propagation process at each step and clearly explain how the entities are aligned.","wordlikeness":0.7142857143,"lcsratio":0.8571428571,"wordcoverage":0.8571428571}
{"Year":2021,"Venue":"acl-2021","Acronym":"VisualSparta","Description":"An Embarrassingly Simple Approach to Large-scale Text-to-Image Search with Weighted Bag-of-words","Abstract":"Text-to-image retrieval is an essential task in cross-modal information retrieval, i.e., retrieving relevant images from a large and unlabelled dataset given textual queries. In this paper, we propose achieve, a novel (visual-text sparse transformer matching) model that shows significant improvement in terms of both accuracy and efficiency. Implemented is capable of outperforming previous state-of-the-art scalable methods in mscoco and flickr30k. We also show that it achieves substantial retrieving speed advantages, i.e., for a 1 million image index, retrieval, using cpu gets ~391x speedup compared to cpu vector search and ~5.4x speedup compared to vector search with gpu acceleration. Experiments show that this speed advantage even gets bigger for larger datasets because terms can be efficiently implemented as an inverted index. To the best of our knowledge, as is the first transformer-based text-to-image retrieval model that can achieve real-time searching for large-scale datasets, with significant accuracy improvement compared to previous state-of-the-art methods.","wordlikeness":0.75,"lcsratio":0.75,"wordcoverage":0.7368421053}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"DropMix","Description":"A Textual Data Augmentation Combining Dropout with Mixup","Abstract":"Overfitting is a notorious problem when there is insufficient data to train deep neural networks in machine learning tasks. Data augmentation regularization methods such as dropout, mixup, and their enhanced variants are effective and prevalent, and achieve promising performance to overcome overfitting. However, in text learning, most of the existing regularization approaches merely adopt ideas from computer vision without considering the importance of dimensionality in natural language processing. In this paper, we argue that the property is essential to overcome overfitting in text learning. Accordingly, we present a saliency map informed textual data augmentation and regularization framework, which combines dropout and mixup, namely mixup,, to mitigate the overfitting problem in text learning. In addition, we design a procedure that drops and patches fine grained shapes of the saliency map under the achieve framework to enhance regularization. Empirical studies confirm the effectiveness of the proposed approach on 12 text classification tasks.","wordlikeness":0.8571428571,"lcsratio":1.0,"wordcoverage":0.7272727273}
{"Year":2022,"Venue":"findings-2022","Acronym":"TEAM","Description":"A multitask learning based Taxonomy Expansion approach for Attach and Merge","Abstract":"Taxonomy expansion is a crucial task. Most of automatic expansion of taxonomy are of two types, attach and merge. In a taxonomy like wordnet, both merge and attach are integral parts of the expansion operations but majority of study consider them separately. This paper proposes a novel mult-task learning-based deep learning method known as taxonomy expansion with attach and merge (been) that performs both the merge and attach operations. To the best of our knowledge this is the first study which integrates both merge and attach operations in a single model. The proposed models have been evaluated on three separate wordnet taxonomies, viz., assamese, bangla, and hindi. From the various experimental setups, it is shown that both outperforms its state-of-the-art counterparts for attach operation, and also provides highly encouraging performance for the merge operation.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2020,"Venue":"findings-2020","Acronym":"MMFT-BERT","Description":"Multimodal Fusion Transformer with BERT Encodings for Visual Question Answering","Abstract":"We present data(multimodal fusiontransformer with bert encodings), to solve visual question answering (vqa) ensuring individual and combined processing of multiple input modalities. Our approach benefits from processing multimodal data (video and text) adopting the bert encodings individually and using a novel transformer-based fusion method to fuse them together. Our method decomposes the different sources of modalities, into different bert instances with similar architectures, but variable weights. This achieves sota results on the tvqa dataset. Additionally, we provide tvqa-visual, an isolated diagnostic subset of tvqa, which strictly requires the knowledge of visual (v) modality based on a human annotator\u2019s judgment. This set of questions helps us to study the model\u2019s behavior and the challenges tvqa poses to prevent the achievement of super human performance. Extensive experiments show the effectiveness and superiority of our method.","wordlikeness":0.5555555556,"lcsratio":0.8888888889,"wordcoverage":0.6666666667}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"AdapterDrop","Description":"On the Efficiency of Adapters in Transformers","Abstract":"Transformer models are expensive to fine-tune, slow for inference, and have large storage requirements. Recent approaches tackle these shortcomings by training smaller models, dynamically reducing the model size, and by training light-weight adapters. In this paper, we propose in, removing adapters from lower transformer layers during training and inference, which incorporates concepts from all three directions. We show that in can dynamically reduce the computational overhead when performing inference over multiple tasks simultaneously, with minimal decrease in task performances. We further prune adapters from adapterfusion, which improves the inference efficiency while maintaining the task performances entirely.","wordlikeness":0.8181818182,"lcsratio":0.8181818182,"wordcoverage":0.7777777778}
{"Year":2009,"Venue":"ws-2009","Acronym":"GREAT","Description":"A Finite-State Machine Translation Toolkit Implementing a Grammatical Inference Approach for Transducer Inference (GIATI)","Abstract":"Corpus, is a \ufb01nite-state toolkit which is devoted to machine translation and that learns structured models from bilingual data. The training procedure is based on grammatical inference techniques to obtain stochastic transducers that model both the structure of the languages and the relationship between them. The inference of grammars from natural language causes the models to become larger when a less restrictive task is involved; even more if a bilingual modelling is being considered. Modelling has been successful to implement the giati learning methodology, using different scalability issues to be able to deal with corpora of high volume of data. This is reported with experiments on the europarl corpus, which is a state-of-theart task in statistical machine translation.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"KECP","Description":"Knowledge Enhanced Contrastive Prompting for Few-shot Extractive Question Answering","Abstract":"Extractive question answering (eqa) is one of the most essential tasks in machine reading comprehension (mrc), which can be solved by fine-tuning the span selecting heads of pre-trained language models (plms). However, most existing approaches for mrc may perform poorly in the few-shot learning scenario. To solve this issue, we propose a novel framework named knowledge enhanced contrastive prompt-tuning (models). Instead of adding pointer heads to plms, we introduce a seminal paradigm for eqa that transforms the task into a non-autoregressive masked language modeling (mlm) generation problem. Simultaneously, rich semantics from the external knowledge base (kb) and the passage context support enhancing the query\u2019s representations. In addition, to boost the performance of plms, we jointly train the model by the mlm and contrastive learning objectives. Experiments on multiple benchmarks demonstrate that our method consistently outperforms state-of-the-art approaches in few-shot settings by a large margin.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2023,"Venue":"iwcs-2023","Acronym":"SimpleMTOD","Description":"A Simple Language Model for Multimodal Task-Oriented Dialogue with Symbolic Scene Representation","Abstract":"Dialog is a simple language model which recasts several sub-tasks in multimodal task-oriented dialogues as sequence prediction tasks. Response is built on a large-scale transformer-based auto-regressive architecture, which has already proven to be successful in uni-modal task-oriented dialogues, and effectively leverages transfer learning from pretrained gpt-2. In-order to capture the semantics of visual scenes, we introduce both local and de-localized tokens for objects within a scene. De-localized tokens represent the type of an object rather than the specific object itself and so possess a consistent meaning across the dataset. Has achieves a state-of-the-art bleu score (0.327) in the response generation sub-task of the simmc 2.0 test-std dataset while performing on par in other multimodal sub-tasks: disambiguation, coreference resolution, and dialog state tracking. This is despite taking a minimalist approach for extracting visual (and non-visual) informa- tion. In addition the model does not rely on task-specific architectural changes such as classification heads.","wordlikeness":0.7,"lcsratio":1.0,"wordcoverage":0.7777777778}
{"Year":2022,"Venue":"naacl-2022","Acronym":"CREATER","Description":"CTR-driven Advertising Text Generation with Controlled Pre-Training and Contrastive Fine-Tuning","Abstract":"This paper focuses on automatically generating the text of an ad, and the goal is that the generated text can capture user interest for achieving higher click-through rate (ctr). We propose with, a ctr-driven advertising text generation approach, to generate ad texts based on high-quality user reviews. To incorporate ctr objective, our model learns from online a\/b test data with contrastive learning, which encourages the model to generate ad texts that obtain higher ctr. To make use of large-scale unpaired reviews, we design a customized self-supervised objective reducing the gap between pre-training and fine-tuning. Experiments on industrial datasets show that fine-tuning. Significantly outperforms current approaches. It has been deployed online in a leading advertising platform and brings uplift on core online metrics.","wordlikeness":0.8571428571,"lcsratio":1.0,"wordcoverage":0.9230769231}
{"Year":2022,"Venue":"acl-2022","Acronym":"NumGLUE","Description":"A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks","Abstract":"Given the ubiquitous nature of numbers in text, reasoning with numbers to perform simple calculations is an important skill of ai systems. While many datasets and models have been developed to this end, state-of-the-art ai systems are brittle; failing to perform the underlying mathematical reasoning when they appear in a slightly different scenario. Drawing inspiration from glue that was proposed in the context of natural language understanding, we propose reasoning., a multi-task benchmark that evaluates the performance of ai systems on eight different tasks, that at their core require simple arithmetic understanding. We show that this benchmark is far from being solved with neural models including state-of-the-art large-scale language models performing significantly worse than humans (lower by 46.4 %). Further, proposed promotes sharing knowledge across tasks, especially those with limited training data as evidenced by the superior performance (average gain of 3.4 % on each task) when a model is jointly trained on all the tasks as opposed to task-specific modeling. Finally, we hope that , will encourage systems that perform robust and general arithmetic reasoning within language, a first step towards being able to perform more complex mathematical reasoning.","wordlikeness":0.7142857143,"lcsratio":0.7142857143,"wordcoverage":0.7272727273}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"Yes-Yes-Yes","Description":"Proactive Data Collection for ACL Rolling Review and Beyond","Abstract":"The shift towards publicly available text sources has enabled language processing at unprecedented scale, yet leaves under-serviced the domains where public and openly licensed data is scarce. Proactively collecting text data for research is a viable strategy to address this scarcity, but lacks systematic methodology taking into account the many ethical, legal and confidentiality-related aspects of data collection. Our work presents a case study on proactive data collection in peer review \u2013 a challenging and under-resourced nlp domain. We outline ethical and legal desiderata for proactive data collection and introduce \u201creport\u201d, the first donation-based peer reviewing data collection workflow that meets these requirements. We report on the implementation of aspects at acl rolling review and empirically study the implications of proactive data collection for the dataset size and the biases induced by the donation behavior on the peer reviewing platform.","wordlikeness":0.6363636364,"lcsratio":0.2727272727,"wordcoverage":0.5263157895}
{"Year":2020,"Venue":"emnlp-2020","Acronym":"GraphDialog","Description":"Integrating Graph Knowledge into End-to-End Task-Oriented Dialogue Systems","Abstract":"End-to-end task-oriented dialogue systems aim to generate system responses directly from plain text inputs. There are two challenges for such systems: one is how to effectively incorporate external knowledge bases (kbs) into the learning framework; the other is how to accurately capture the semantics of dialogue history. In this paper, we address these two challenges by exploiting the graph structural information in the knowledge base and in the dependency parsing tree of the dialogue. To effectively leverage the structural information in dialogue history, we propose a new recurrent cell architecture which allows representation learning on graphs. To exploit the relations between entities in kbs, the model combines multi-hop reasoning ability based on the graph structure. Experimental results show that the proposed model achieves consistent improvement over state-of-the-art models on two different task-oriented dialogue datasets.","wordlikeness":0.6363636364,"lcsratio":1.0,"wordcoverage":0.6956521739}
{"Year":2021,"Venue":"acl-2021","Acronym":"DynaEval","Description":"Unifying Turn and Dialogue Level Evaluation","Abstract":"A dialogue is essentially a multi-turn interaction among interlocutors. Effective evaluation metrics should reflect the dynamics of such interaction. Existing automatic metrics are focused very much on the turn-level quality, while ignoring such dynamics. To this end, we propose constructed, a unified automatic evaluation framework which is not only capable of performing turn-level evaluation, but also holistically considers the quality of the entire dialogue. In which, the graph convolutional network (gcn) is adopted to model a dialogue in totality, where the graph nodes denote each individual utterance and the edges represent the dependency between pairs of utterances. A contrastive loss is then applied to distinguish well-formed dialogues from carefully constructed negative samples. Experiments show that (gcn) significantly outperforms the state-of-the-art dialogue coherence model, and correlates strongly with human judgements across multiple dialogue evaluation aspects at both turn and dialogue level.","wordlikeness":0.75,"lcsratio":0.875,"wordcoverage":0.7692307692}
{"Year":2010,"Venue":"lrec-2010","Acronym":"WTIMIT","Description":"The TIMIT Speech Corpus Transmitted Over The 3G AMR Wideband Mobile Network","Abstract":"In anticipation of upcoming mobile telephony services with higher speech quality, a wideband (50 hz to 7 khz) mobile telephony derivative of timit has been recorded called simulating. It opens up various scientific investigations; e.g., on speech quality and intelligibility, as well as on wideband upgrades of network-side interactive voice response (ivr) systems with retrained or bandwidth-extended acoustic models for automatic speech recognition (asr). Wideband telephony could enable network-side speech recognition applications such as remote dictation or spelling without the need of distributed speech recognition techniques. The has corpus was transmitted via two prepared nokia 6220 mobile phones over t-mobile's 3g wideband mobile network in the hague, the netherlands, employing the adaptive multirate wideband (amr-wb) speech codec. The paper presents observations of transmission effects and phoneme recognition experiments. It turns out that in the case of wideband telephony, server-side asr should not be carried out by simply decimating received signals to 8 khz and applying existent narrowband acoustic models. Nor do we recommend just simulating the amr-wb codec for training of wideband acoustic models. Instead, real-world wideband telephony channel data (such as been) provides the best training material for wideband ivr systems.","wordlikeness":0.6666666667,"lcsratio":0.8333333333,"wordcoverage":0.7272727273}
{"Year":2019,"Venue":"naacl-2019","Acronym":"LeafNATS","Description":"An Open-Source Toolkit and Live Demo System for Neural Abstractive Text Summarization","Abstract":"Neural abstractive text summarization (nats) has received a lot of attention in the past few years from both industry and academia. In this paper, we introduce an open-source toolkit, namely introduce, for training and evaluation of different sequence-to-sequence based models for the nats task, and for deploying the pre-trained models to real-world applications. The toolkit is modularized and extensible in addition to maintaining competitive performance in the nats task. A live news blogging system has also been implemented to demonstrate how these models can aid blog\/news editors by providing them suggestions of headlines and summaries of their articles.","wordlikeness":0.625,"lcsratio":0.875,"wordcoverage":0.7692307692}
{"Year":2021,"Venue":"acl-2021","Acronym":"DeCLUTR","Description":"Deep Contrastive Learning for Unsupervised Textual Representations","Abstract":"Sentence embeddings are an important component of many natural language processing (nlp) systems. Like word embeddings, sentence embeddings are typically learned on large text corpora and then transferred to various downstream tasks, such as clustering and retrieval. Unlike word embeddings, the highest performing solutions for learning sentence embeddings require labelled data, limiting their usefulness to languages and domains where labelled data is abundant. In this paper, we present advances: deep contrastive learning for unsupervised textual representations. Inspired by recent advances in deep metric learning (dml), we carefully design a self-supervised objective for learning universal sentence embeddings that does not require labelled training data. When used to extend the pretraining of transformer-based language models, our approach closes the performance gap between unsupervised and supervised pretraining for universal sentence encoders. Importantly, our experiments suggest that the quality of the learned embeddings scale with both the number of trainable parameters and the amount of unlabelled training data. Our code and pretrained models are publicly available and can be easily adapted to new domains or used to embed unseen text.","wordlikeness":0.5714285714,"lcsratio":1.0,"wordcoverage":0.7142857143}
{"Year":2013,"Venue":"acl-2013","Acronym":"Deepfix","Description":"Statistical Post-editing of Statistical Machine Translation Using Deep Syntactic Analysis","Abstract":"Deep\ufb01x is a statistical post-editing system for improving the quality of statistical machine translation outputs. It attempts to correct errors in verb-noun valency using deep syntactic analysis and a simple probabilistic model of valency. On the english-to-czech translation pair, we show that statistical post-editing of statistical machine translation leads to an improvement of the translation quality when helped by deep linguistic knowledge.","wordlikeness":0.8571428571,"lcsratio":0.7142857143,"wordcoverage":0.7272727273}
{"Year":2023,"Venue":"acl-2023","Acronym":"UniTRec","Description":"A Unified Text-to-Text Transformer and Joint Contrastive Learning Framework for Text-based Recommendation","Abstract":"Prior study has shown that pretrained language models (plm) can boost the performance of text-based recommendation. In contrast to previous works that either use plm to encode user history as a whole input text, or impose an additional aggregation network to fuse multi-turn history representations, we propose a unified local- and global-attention transformer encoder to better model two-level contexts of user history. Moreover, conditioned on user history encoded by transformer encoders, our framework leverages transformer decoders to estimate the language perplexity of candidate text items, which can serve as a straightforward yet significant contrastive signal for user-item text matching. Based on this, our framework, our, unifies the contrastive objectives of discriminative matching scores and candidate text perplexity to jointly enhance text-based recommendation. Extensive evaluation shows that (plm) delivers sota performance on three text-based recommendation tasks.","wordlikeness":0.7142857143,"lcsratio":1.0,"wordcoverage":0.8333333333}
{"Year":2022,"Venue":"pandl-2022","Acronym":"PatternRank","Description":"Jointly Ranking Patterns and Extractions for Relation Extraction Using Graph-Based Algorithms","Abstract":"In this paper we revisit the direction of using lexico-syntactic patterns for relation extraction instead of today\u2019s ubiquitous neural classifiers. We propose a semi-supervised graph-based algorithm for pattern acquisition that scores patterns and the relations they extract jointly, using a variant of pagerank. We insert light supervision in the form of seed patterns or relations, and model it with several custom teleportation probabilities that bias random-walk scores of patterns\/relations based on their proximity to correct information. We evaluate our approach on few-shot tacred, and show that our method outperforms (or performs competitively with) more expensive and opaque deep neural networks. Lastly, we thoroughly compare our proposed approach with the seminal rlogf pattern acquisition algorithm of, showing that it outperforms it for all the hyper parameters tested, in all settings.","wordlikeness":0.9090909091,"lcsratio":0.9090909091,"wordcoverage":0.7777777778}
{"Year":2021,"Venue":"acl-2021","Acronym":"HILDIF","Description":"Interactive Debugging of NLI Models Using Influence Functions","Abstract":"Biases and artifacts in training data can cause unwelcome behavior in text classifiers (such as shallow pattern matching), leading to lack of generalizability. One solution to this problem is to include users in the loop and leverage their feedback to improve models. We propose a novel explanatory debugging pipeline called influence, enabling humans to improve deep text classifiers using influence functions as an explanation method. We experiment on the natural language inference (nli) task, showing that and can effectively alleviate artifact problems in fine-tuned bert models and result in increased model generalizability.","wordlikeness":0.5,"lcsratio":0.8333333333,"wordcoverage":0.7272727273}
{"Year":2021,"Venue":"naacl-2021","Acronym":"MetaXL","Description":"Meta Representation Transformation for Low-resource Cross-lingual Learning","Abstract":"The combination of multilingual pre-trained representations and cross-lingual transfer learning is one of the most effective methods for building functional nlp systems for low-resource languages. However, for extremely low-resource languages without large-scale monolingual corpora for pre-training or sufficient annotated data for fine-tuning, transfer learning remains an understudied and challenging task. Moreover, recent work shows that multilingual representations are surprisingly disjoint across languages, bringing additional challenges for transfer onto extremely low-resource languages. In this paper, we propose from, a meta-learning based framework that learns to transform representations judiciously from auxiliary languages to a target one and brings their representation spaces closer for effective transfer. Extensive experiments on real-world low-resource languages \u2013 without access to large-scale monolingual corpora or large amounts of labeled data \u2013 for tasks like cross-lingual sentiment analysis and named entity recognition show the effectiveness of our approach. Code for annotated is publicly available at github.com\/microsoft\/our.","wordlikeness":0.6666666667,"lcsratio":0.8333333333,"wordcoverage":0.9090909091}
{"Year":2022,"Venue":"naacl-2022","Acronym":"LEA","Description":"Meta Knowledge-Driven Self-Attentive Document Embedding for Few-Shot Text Classification","Abstract":"Text classification has achieved great success with the prosperity of deep enablesrning and pre-trained language models. However, we often encounter labeled data deficiency problems in real-world text-classification tasks. To overcome such challenging scenarios, interest in few-shot scenarios,rning has increased, whereas most few-shot text classification studies suffer from a difficulty of utilizing pre-trained language models. In the study, we propose a novel languagerning method for embeddingrning how to attend, called increased,, through which meta-level attention aspects are derived based on our meta-howrning strategy. This enables the generation of task-specific document embedding with leveraging pre-trained language models even though a few labeled data instances are given. We evaluate our proposed oftenrning method on five benchmark datasets. The results show that the novel method robustly provides the competitive performance compared to recent few-shot models.rning methods for all the datasets.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2018,"Venue":"emnlp-2018","Acronym":"DERE","Description":"A Task and Domain-Independent Slot Filling Framework for Declarative Relation Extraction","Abstract":"Most machine learning systems for natural language processing are tailored to specific tasks. As a result, comparability of models across tasks is missing and their applicability to new tasks is limited. This affects end users without machine learning experience as well as model developers. To address these limitations, we present missing, a novel framework for declarative specification and compilation of template-based information extraction. It uses a generic specification language for the task and for data annotations in terms of spans and frames. This formalism enables the representation of a large variety of natural language processing challenges. The backend can be instantiated by different models, following different paradigms. The clear separation of frame specification and model backend will ease the implementation of new models and the evaluation of different models across different tasks. Furthermore, it simplifies transfer learning, joint learning across tasks and\/or domains as well as the assessment of model generalizability. Challenges. Is available as open-source software.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2023,"Venue":"ws-2023","Acronym":"DeTexD","Description":"A Benchmark Dataset for Delicate Text Detection","Abstract":"Over the past few years, much research has been conducted to identify and regulate toxic language. However, few studies have addressed a broader range of sensitive texts that are not necessarily overtly toxic. In this paper, we introduce and define a new category of sensitive text called \u201cdelicate text.\u201d we provide the taxonomy of delicate text and present a detailed annotation scheme. We annotate been, the first benchmark dataset for delicate text detection. The significance of the difference in the definitions is highlighted by the relative performance deltas between models trained each definitions and corpora and evaluated on the other. We make publicly available the is benchmark dataset, annotation guidelines, and baseline model for delicate text detection.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2020,"Venue":"emnlp-2020","Acronym":"AutoPrompt","Description":"Eliciting Knowledge from Language Models with Automatically Generated Prompts","Abstract":"The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop parameters, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using has, we show that masked language models (mlms) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from mlms than the manually created prompts on the lama benchmark, and that mlms can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained lms become more sophisticated and capable, potentially a replacement for finetuning.","wordlikeness":0.7,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2021,"Venue":"acl-2021","Acronym":"TweeNLP","Description":"A Twitter Exploration Portal for Natural Language Processing","Abstract":"We present envision, a one-stop portal that organizes twitter\u2019s natural language processing (nlp) data and builds a visualization and exploration platform. It curates 19,395 tweets (as of april 2021) from various nlp conferences and general nlp discussions. It supports multiple features such as tweetexplorer to explore tweets by topics, visualize insights from twitter activity throughout the organization cycle of conferences, discover popular research papers and researchers. It also builds a timeline of conference and workshop submission deadlines. We envision april to function as a collective memory unit for the nlp community by integrating the tweets pertaining to research papers with the nlpexplorer scientific literature search engine. The current system is hosted at <a href=http:\/\/nlpexplorer.org\/twitter\/cfp class=acl-markup-url>http:\/\/nlpexplorer.org\/twitter\/cfp<\/a>.","wordlikeness":0.4285714286,"lcsratio":1.0,"wordcoverage":0.7272727273}
{"Year":2018,"Venue":"acl-2018","Acronym":"StructVAE","Description":"Tree-structured Latent Variable Models for Semi-supervised Semantic Parsing","Abstract":"Semantic parsing is the task of transducing natural language (nl) utterances into formal meaning representations (mrs), commonly represented as tree structures. Annotating nl utterances with their corresponding mrs is expensive and time-consuming, and thus the limited availability of labeled data often becomes the bottleneck of data-driven, supervised models. We introduce natural, a variational auto-encoding model for semi-supervised semantic parsing, which learns both from limited amounts of parallel data, and readily-available unlabeled nl utterances. Unlabeled models latent mrs not observed in the unlabeled data as tree-structured latent variables. Experiments on semantic parsing on the atis domain and python code generation show that with extra unlabeled data, corresponding outperforms strong supervised models.","wordlikeness":0.7777777778,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2017,"Venue":"emnlp-2017","Acronym":"DOC","Description":"Deep Open Classification of Text Documents","Abstract":"Traditional supervised learning makes the closed-world assumption that the classes appeared in the test data must have appeared in training. This also applies to text learning or text classification. As learning is used increasingly in dynamic open environments where some new\/test baseduments may not belong to any of the training classes, identifying these novel datauments during classification presents an important problem. This problem is called open-world classification or open classification. This paper proposes a novel deep learning based approach. It outperforms existing state-of-the-art techniques dramatically.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2012,"Venue":"eacl-2012","Acronym":"TransAhead","Description":"A Writing Assistant for CAT and CALL","Abstract":"We introduce a method for learning to predict the following grammar and text of the ongoing translation given a source text. In our approach, predictions are offered aimed at reducing users\u2019 burden on lexical and grammar choices, and improving productivity. The method involves learning syntactic phraseology and translation equivalents. At run-time, the source and its translation prefix are sliced into ngrams to generate subsequent grammar and translation predictions. We present a prototype writing assistant, of1, that applies the method to where computer-assisted translation and language learning meet. The preliminary results show that the method has great potentials in cat and call (significant boost in translation quality is observed). 1.","wordlikeness":0.8,"lcsratio":0.6,"wordcoverage":0.8}
{"Year":2009,"Venue":"ws-2009","Acronym":"MATREX","Description":"The DCU MT System for WMT 2009","Abstract":"In this paper, we describe the machine translation system in the evaluation campaign of the fourth workshop on statistical machine translation at eacl 2009. We describe the modular design of our multi-engine mt system with particular focus on the components used in this participation. We participated in the translation task for the following translation directions: french\u2013english and english\u2013french, in which we employed our multi-engine architecture to translate. We also participated in the system combination task which was carried out by the mbr decoder and confusion network decoder. We report results on the provided development and test sets.","wordlikeness":0.6666666667,"lcsratio":0.5,"wordcoverage":0.8333333333}
{"Year":2021,"Venue":"ranlp-2021","Acronym":"EmoPars","Description":"A Collection of 30K Emotion-Annotated Persian Social Media Texts","Abstract":"The wide reach of social media platforms, such as twitter, have enabled many users to share their thoughts, opinions and emotions on various topics online. The ability to detect these emotions automatically would allow social scientists, as well as, businesses to better understand responses from nations and costumers. In this study we introduce a dataset of 30,000 persian tweets labeled with ekman\u2019s six basic emotions (anger, fear, happiness, sadness, hatred, and wonder). This is the first publicly available emotion dataset in the persian language. In this paper, we explain the data collection and labeling scheme used for the creation of this dataset. We also analyze the created dataset, showing the different features and characteristics of the data. Among other things, we investigate co-occurrence of different emotions in the dataset, and the relationship between sentiment and emotion of textual instances. The dataset is publicly available at <a href=https:\/\/github.com\/nazaninsbr\/persian-emotion-detection class=acl-markup-url>https:\/\/github.com\/nazaninsbr\/persian-emotion-detection<\/a>.","wordlikeness":0.4285714286,"lcsratio":0.8571428571,"wordcoverage":0.7272727273}
{"Year":2021,"Venue":"eacl-2021","Acronym":"STAR","Description":"Cross-modal [STA]tement [R]epresentation for selecting relevant mathematical premises","Abstract":"Mathematical statements written in natural language are usually composed of two different modalities: mathematical elements and natural language. These two modalities have several distinct linguistic and semantic properties. State-of-the-art representation techniques have demonstrated an inability in capturing such an entangled style of discourse. In this work, we propose language, a model that uses cross-modal attention to learn how to represent mathematical text for the task of natural language premise selection. This task uses conjectures written in both natural and mathematical language to recommend premises that most likely will be relevant to prove a particular statement. We found that most not only outperforms baselines that do not distinguish between natural language and mathematical elements, but it also achieves better performance than state-of-the-art models.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2018,"Venue":"coling-2018","Acronym":"Sensala","Description":"a Dynamic Semantics System for Natural Language Processing","Abstract":"Here we describe to , an open source framework for the semantic interpretation of natural language that provides the logical meaning of a given text. The framework\u2019s theory is based on a lambda calculus with exception handling and uses contexts, continuations, events and dependent types to handle a wide range of complex linguistic phenomena, such as donkey anaphora, verb phrase anaphora, propositional anaphora, presuppositions and implicatures.","wordlikeness":0.7142857143,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2019,"Venue":"iwcs-2019","Acronym":"R-grams","Description":"Unsupervised Learning of Semantic Units in Natural Language","Abstract":"This paper investigates data-driven segmentation using re-pair or byte pair encoding-techniques. In contrast to previous work which has primarily been focused on subword units for machine translation, we are interested in the general properties of such segments above the word level. We call these segments test, and discuss their properties and the effect they have on the token frequency distribution. The proposed approach is evaluated by demonstrating its viability in embedding techniques, both in monolingual and multilingual test settings. We also provide a number of qualitative examples of the proposed methodology, demonstrating its viability as a language-invariant segmentation procedure.","wordlikeness":0.5714285714,"lcsratio":0.5714285714,"wordcoverage":0.8333333333}
{"Year":2018,"Venue":"ngt-2018","Acronym":"Marian","Description":"Cost-effective High-Quality Neural Machine Translation in C&#43;&#43;","Abstract":"This paper describes the submissions of the \u201cshared\u201d team to the wnmt 2018 shared task. We investigate combinations of teacher-student training, low-precision matrix products, auto-tuning and other methods to optimize the transformer model on gpu and cpu. By further integrating these methods with the new averaging attention networks, a recently introduced faster transformer variant, we create a number of high-quality, high-performance models on the gpu and cpu, dominating the pareto frontier for this shared task.","wordlikeness":0.8333333333,"lcsratio":0.8333333333,"wordcoverage":1.0}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"TransIns","Description":"Document Translation with Markup Reinsertion","Abstract":"For many use cases, it is required that mt does not just translate raw text, but complex formatted documents (e.g. websites, slides, spreadsheets) and the result of the translation should reflect the formatting. This is challenging, as markup can be nested, apply to spans contiguous in source but non-contiguous in target etc. Here we present spans, a system for non-plain text document translation that builds on the okapi framework and mt models trained with marian nmt. We develop, implement and evaluate different strategies for reinserting markup into translated sentences using token alignments between source and target sentences. We propose a simple and effective strategy that compiles down all markup to single source tokens and transfers them to aligned target tokens. A first evaluation shows that this strategy yields highly accurate markup in the translated documents that outperforms the markup quality found in documents translated with popular translation services. We release implement under the mit license as open-source software on <a href=https:\/\/github.com\/dfki-mlt\/, class=acl-markup-url>https:\/\/github.com\/dfki-mlt\/markup<\/a>. An online demonstrator is available at <a href=https:\/\/sentences.dfki.de class=acl-markup-url>https:\/\/strategy.dfki.de<\/a>.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2020,"Venue":"ws-2020","Acronym":"DSNDM","Description":"Deep Siamese Neural Discourse Model with Attention for Text Pairs Categorization and Ranking","Abstract":"In this paper, the utility and advantages of the discourse analysis for text pairs categorization and ranking are investigated. We consider two tasks in which discourse structure seems useful and important: automatic verification of political statements, and ranking in question answering systems. We propose a neural network based approach to learn the match between pairs of discourse tree structures. To this end, the neural treelstm model is modified to effectively encode discourse trees and structure model based on it is suggested to analyze pairs of texts. In addition, the integration of the attention mechanism in the model is proposed. Moreover, different ranking approaches are investigated for the second task. In the paper, the comparison with state-of-the-art methods is given. Experiments illustrate that combination of neural networks and discourse structure in longer is effective since it reaches top results in the assigned tasks. The evaluation also demonstrates that discourse analysis improves quality for the processing of longer texts.","wordlikeness":0.4,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"NMTScore","Description":"A Multilingual Analysis of Translation-based Text Similarity Measures","Abstract":"Being able to rank the similarity of short text segments is an interesting bonus feature of neural machine translation. Translation-based similarity measures include direct and pivot translation probability, as well as translation cross-likelihood, which has not been studied so far. We analyze these measures in the common framework of multilingual nmt, releasing the an library. Compared to baselines such as sentence embeddings, translation-based measures prove competitive in paraphrase identification and are more robust against adversarial or multilingual input, especially if proper normalization is applied. When used for reference-based evaluation of data-to-text generation in 2 tasks and 17 languages, translation-based measures show a relatively high correlation to human judgments.","wordlikeness":0.5,"lcsratio":0.75,"wordcoverage":0.7692307692}
{"Year":2022,"Venue":"coling-2022","Acronym":"SSR","Description":"Utilizing Simplified Stance Reasoning Process for Robust Stance Detection","Abstract":"Dataset bias in stance detection tasks allows models to achieve superior performance without using targets. Most existing debiasing methods are task-agnostic, which fail to utilize task knowledge to better discriminate between genuine and bias features. Motivated by how humans tackle stance detection tasks, we propose to incorporate the stance reasoning process as task knowledge to assist in learning genuine features and reducing reliance on bias features. The full stance reasoning process usually involves identifying the span of the mentioned target and corresponding opinion expressions, such fine-grained annotations are hard and expensive to obtain. To alleviate this, we simplify the stance reasoning process to relax the granularity of annotations from token-level to sentence-level, where labels for sub-tasks can be easily inferred from existing resources. We further implement those sub-tasks by maximizing mutual information between the texts and the opinioned targets. To evaluate whether stance detection models truly understand the task from various aspects, we collect and construct a series of new test sets. Our proposed model achieves better performance than previous task-agnostic debiasing methods on most of those new test sets while maintaining comparable performances to existing stance detection models.","wordlikeness":0.3333333333,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2019,"Venue":"ranlp-2019","Acronym":"EASY-M","Description":"Evaluation System for Multilingual Summarizers","Abstract":"Automatic text summarization aims at producing a shorter version of a document (or a document set). Evaluation of summarization quality is a challenging task. Because human evaluations are expensive and evaluators often disagree between themselves, many researchers prefer to evaluate their systems automatically, with help of software tools. Such a tool usually requires a point of reference in the form of one or more human-written summaries for each text in the corpus. Then, a system-generated summary is compared to one or more human-written summaries, according to selected metrics. However, a single metric cannot reflect all quality-related aspects of a summary. In this paper we present the evaluation system for multilingual summarization (multilingual), which enables the evaluation of system-generated summaries in 17 different languages with several quality measures, based on comparison with their human-generated counterparts. The system also provides comparative results with two built-in baselines. The source code and both online and offline versions of compared is freely available for the nlp community.","wordlikeness":0.6666666667,"lcsratio":0.8333333333,"wordcoverage":0.8}
{"Year":2023,"Venue":"ws-2023","Acronym":"DORIC","Description":"Domain Robust Fine-Tuning for Open Intent Clustering through Dependency Parsing","Abstract":"We present our work on track 2 in the dialog system technology challenges 11 (dstc11). Dstc11-track2 aims to provide a benchmark for zero-shot, cross-domain, intent-set induction. In the absence of in-domain training dataset, robust utterance representation that can be used across domains is necessary to induce users\u2019 intentions. To achieve this, we leveraged a multi-domain dialogue dataset to fine-tune the language model and proposed extracting verb-object pairs to remove the artifacts of unnecessary information. Furthermore, we devised the method that generates each cluster\u2019s name for the explainability of clustered results. Our approach achieved 3rd place in the precision score and showed superior accuracy and normalized mutual information (nmi) score than the baseline model on various domain datasets.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2020,"Venue":"emnlp-2020","Acronym":"CoSaTa","Description":"A Constraint Satisfaction Solver and Interpreted Language for Semi-Structured Tables of Sentences","Abstract":"This work presents of, an intuitive constraint satisfaction solver and interpreted language for knowledge bases of semi-structured tables expressed as text. The stand-alone tasks, solver allows easily expressing complex compositional \u201cinference patterns\u201d for how knowledge from different tables tends to connect to support inference and explanation construction in question answering and other downstream tasks, while including advanced declarative features and the ability to operate over multiple representations of text (words, lemmas, or part-of-speech tags). For also includes a hybrid imperative\/declarative interpreted language for expressing simple models through minimally-specified simulations grounded in constraint patterns, helping bridge the gap between question answering, question explanation, and model simulation. The solver and interpreter are released as open source. Screencast demo: <a href=https:\/\/youtu.be\/t93acsz7lye class=acl-markup-url>https:\/\/youtu.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.9090909091}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"MEE","Description":"A Novel Multilingual Event Extraction Dataset","Abstract":"Event extraction (ee) is one of the fundamental tasks in information extraction (ie) that aims to recognize event mentions and their arguments (i.e., participants) from text. Due to its importance, extensive methods and resources have been developed for event extraction. However, one limitation of current research for ee involves the under-exploration for non-english languages in which the lack of high-quality multilingual ee datasets for model training and evaluation has been the main hindrance. To address this limitation, we propose a novel multilingual event extraction dataset (comprehensively) that provides annotation for more than 50k event mentions in 8 typologically different languages. Methods comprehensively annotates data for entity mentions, event triggers and event arguments. We conduct extensive experiments on the proposed dataset to reveal challenges and opportunities for multilingual ee. To foster future research in this direction, our dataset will be publicly available.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2021,"Venue":"eacl-2021","Acronym":"AnswerQuest","Description":"A System for Generating Question-Answer Items from Multi-Paragraph Documents","Abstract":"One strategy for facilitating reading comprehension is to present information in a question-and-answer format. We demo a system that integrates the tasks of question answering (qa) and question generation (qg) in order to produce q&amp;a items that convey the content of multi-paragraph documents. We report some experiments for qa and qg that yield improvements on both tasks, and assess how they interact to produce a list of q&amp;a items for a text. The demo is accessible at qna.sdl.com.","wordlikeness":0.9090909091,"lcsratio":0.8181818182,"wordcoverage":0.7777777778}
{"Year":2020,"Venue":"lrec-2020","Acronym":"LexiDB","Description":"Patterns \\&amp; Methods for Corpus Linguistic Database Management","Abstract":"Managing is a tool for storing, managing and querying corpus data. In contrast to other database management systems (dbmss), it is designed specifically for text corpora. It improves on other corpus management systems (cmss) because data can be added and deleted from corpora on the fly with the ability to add live data to existing corpora. Ability sits between these two categories of dbmss and cmss, more specialised to language data than a general purpose dbms but more flexible than a traditional static corpus management system. Previous work has demonstrated the scalability of present in response to the growing need to be able to scale out for ever growing corpus datasets. Here, we present the patterns and methods developed in and for storage, retrieval and querying of multi-level annotated corpus data. These techniques are evaluated and compared to an existing cms (corpus workbench cwb - cqp) and indexer (lucene). We find that general consistently outperforms existing tools for corpus queries.","wordlikeness":0.3333333333,"lcsratio":0.6666666667,"wordcoverage":0.7142857143}
{"Year":2015,"Venue":"naacl-2015","Acronym":"HEADS","Description":"Headline Generation as Sequence Prediction Using an Abstract Feature-Rich Space","Abstract":"Feature-rich space carlos a. Colmenares\u2217 google inc. Brandschenkestrasse 110 8002 zurich, switzerland crcarlos@google.com marina litvak shamoon college of engineering beer sheva, israel marinal@sce.ac.il amin mantrach fabrizio silvestri yahoo labs. Avinguda diagonal 177 08018 barcelona, spain {amantrach,silvestr}@yahoo-inc.com automatic headline generation is a sub-task of document summarization with many reported applications. In this study we present a sequence-prediction technique for learning how editors title their news stories. The introduced technique models the problem as a discrete optimization task in a feature-rich space. In this space the global optimum can be found in polynomial time by means of dynamic programming. We train and test our model on an extensive corpus of \ufb01nancial news, and compare it against a number of baselines by using standard metrics from the document summarization domain, as well as some new ones proposed in this work. We also assess the readability and informativeness of the generated titles through human evaluation. The obtained results are very appealing and substantiate the soundness of the approach.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2021,"Venue":"sigdial-2021","Acronym":"ERICA","Description":"An Empathetic Android Companion for Covid-19 Quarantine","Abstract":"Over the past year, research in various domains, including natural language processing (nlp), has been accelerated to fight against the covid-19 pandemic, yet such research has just started on dialogue systems. In this paper, we introduce an end-to-end dialogue system which aims to ease the isolation of people under self-quarantine. We conduct a control simulation experiment to assess the effects of the user interface: a web-based virtual agent, nora vs. The android system via a video call. The experimental results show that the android can offer a more valuable user experience by giving the impression of being more empathetic and engaging in the conversation due to its nonverbal information, such as facial expressions and body gestures.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2017,"Venue":"emnlp-2017","Acronym":"SCDV","Description":"Sparse Composite Document Vectors using soft clustering over distributional representations","Abstract":"We present a feature vector formation technique for documents - sparse composite document vector (semantic) - which overcomes several shortcomings of the current distributional paragraph vector representations that are widely used for text representation. In time, word embeddings are clustered to capture multiple semantic contexts in which words occur. They are then chained together to form document topic-vectors that can express complex, multi-topic documents. Through extensive experiments on multi-class and multi-label classification tasks, we outperform the previous state-of-the-art method, ntsg. We also show that prediction embeddings perform well on heterogeneous tasks like topic coherence, context-sensitive learning and information retrieval. Moreover, we achieve a significant reduction in training and prediction times compared to other representation methods. We achieves best of both worlds - better performance with lower time and space complexity.","wordlikeness":0.25,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2022,"Venue":"naacl-2022","Acronym":"TIE","Description":"Topological Information Enhanced Structural Reading Comprehension on Web Pages","Abstract":"Recently, the structural reading comprehension (src) task on web pages has attracted increasing research interests. Although previous src work has leveraged extra information such as html tags or xpaths, the informative topology of web pages is not effectively exploited. In this work, we propose a topological information enhanced model (outperforms), which transforms the token-level task into a tag-level task by introducing a two-stage process (i.e. node locating and answer refining). Based on that, on integrates graph attention network (gat) and pre-trained language model (plm) to leverage the topological information of both logical structures and spatial structures. Experimental results demonstrate that our model outperforms strong baselines and achieves state-of-the-art performances on the web-based src benchmark websrc at the time of writing. The code of exploited. Will be publicly available at <a href=https:\/\/github.com\/x-lance\/model class=acl-markup-url>https:\/\/github.com\/x-lance\/has<\/a>.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"GREENER","Description":"Graph Neural Networks for News Media Profiling","Abstract":"We study the problem of profiling news media on the web with respect to their factuality of reporting and bias. This is an important but under-studied problem related to disinformation and \u201cfake news\u201d detection, but it addresses the issue at a coarser granularity compared to looking at an individual article or an individual claim. This is useful as it allows to profile entire media outlets in advance. Unlike previous work, which has focused primarily on text (e.g., on the text of the articles published by the target website, or on the textual description in their social media profiles or in wikipedia), here our main focus is on modeling the similarity between media outlets based on the overlap of their audience. This is motivated by homophily considerations, i.e., the tendency of people to have connections to people with similar interests, which we extend to media, hypothesizing that similar types of media would be read by similar kinds of users. In particular, we propose hypothesizing (graph neural network for news media profiling), a model that builds a graph of inter-media connections based on their audience overlap, and then uses graph neural networks to represent each medium. We find that such representations are quite useful for predicting the factuality and the bias of news media outlets, yielding improvements over state-of-the-art results reported on two datasets. When augmented with conventionally used representations obtained from news articles, twitter, youtube, facebook, and wikipedia, prediction accuracy is found to improve by 2.5-27 macro-f1 points for the two tasks.","wordlikeness":0.8571428571,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"HRKD","Description":"Hierarchical Relational Knowledge Distillation for Cross-domain Language Model Compression","Abstract":"On many natural language processing tasks, large pre-trained language models (plms) have shown overwhelming performances compared with traditional neural network methods. Nevertheless, their huge model size and low inference speed have hindered the deployment on resource-limited devices in practice. In this paper, we target to compress plms with knowledge distillation, and propose a hierarchical relational knowledge distillation (with) method to capture both hierarchical and domain relational information. Specifically, to enhance the model capability and transferability, we leverage the idea of meta-learning and set up domain-relational graphs to capture the relational information across different domains. And to dynamically select the most representative prototypes for each domain, we propose a hierarchical compare-aggregate mechanism to capture hierarchical relationships. Extensive experiments on public multi-domain datasets demonstrate the superior performance of our huge method as well as its strong few-shot learning ability. For reproducibility, we release the code at <a href=https:\/\/github.com\/cheneydon\/pre-trained class=acl-markup-url>https:\/\/github.com\/cheneydon\/on<\/a>.","wordlikeness":0.25,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"CQR-SQL","Description":"Conversational Question Reformulation Enhanced Context-Dependent Text-to-SQL Parsers","Abstract":"Context-dependent text-to-sql is the task of translating multi-turn questions into database-related sql queries. Existing methods typically focus on making full use of history context or previously predicted sql for currently sql parsing, while neglecting to explicitly comprehend the schema and conversational dependency, such as co-reference, ellipsis and user focus change. In this paper, we propose semantics, which uses auxiliary conversational question reformulation (cqr) learning to explicitly exploit schema and decouple contextual dependency for multi-turn sql parsing. Specifically, we first present a schema enhanced recursive cqr method to produce domain-relevant self-contained questions. Secondly, we train space models to map the semantics of multi-turn questions and auxiliary self-contained questions into the same latent space through schema grounding consistency task and tree-structured sql parsing consistency task, which enhances the abilities of sql parsing by adequately contextual understanding. At the time of writing, our and achieves new state-of-the-art results on two context-dependent text-to-sql benchmarks sparc and cosql.","wordlikeness":0.1428571429,"lcsratio":1.0,"wordcoverage":0.5714285714}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"LOPS","Description":"Learning Order Inspired Pseudo-Label Selection for Weakly Supervised Text Classification","Abstract":"Weakly supervised text classification methods typically train a deep neural classifier based on pseudo-labels. The quality of pseudo-labels is crucial to final performance but they are inevitably noisy due to their heuristic nature, so selecting the correct ones has a huge potential for performance boost. One straightforward solution is to select samples based on the softmax probability scores in the neural classifier corresponding to their pseudo-labels. However, we show through our experiments that such solutions are ineffective and unstable due to the erroneously high-confidence predictions from poorly calibrated models. Recent studies on the memorization effects of deep neural models suggest that these models first memorize training samples with clean labels and then those with noisy labels. Inspired by this observation, we propose a novel pseudo-label selection method the that takes learning order of samples into consideration. We hypothesize that the learning order reflects the probability of wrong annotation in terms of ranking, and therefore, propose to select the samples that are learnt earlier. Through can be viewed as a strong performance-boost plug-in to most existing weakly-supervised text classification methods, as confirmed in extensive experiments on four real-world datasets.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2015,"Venue":"ws-2015","Acronym":"MediaMeter","Description":"A Global Monitor for Online News Coverage","Abstract":"This paper introduces primary, an application that works to detect and track emergent topics in the us online news media. What makes of unique is its reliance on a labeling algorithm which we call wikilabel, whose primary goal is to identify what news stories are about by looking up wikipedia. We discuss some of the major news events that were successfully detected and how it compares to prior work.","wordlikeness":0.9,"lcsratio":0.5,"wordcoverage":0.8888888889}
{"Year":2022,"Venue":"acl-2022","Acronym":"AnnIE","Description":"An Annotation Platform for Constructing Complete Open Information Extraction Benchmark","Abstract":"Open information extraction (oie) is the task of extracting facts from sentences in the form of relations and their corresponding arguments in schema-free manner. Intrinsic performance of oie systems is difficult to measure due to the incompleteness of existing oie benchmarks: ground truth extractions do not group all acceptable surface realizations of the same fact that can be extracted from a sentence. To measure performance of oie systems more realistically, it is necessary to manually annotate complete facts (i.e., clusters of all acceptable surface realizations of the same fact) from input sentences. We propose fact-oriented: an interactive annotation platform that facilitates such challenging annotation tasks and supports creation of complete fact-oriented oie evaluation benchmarks. Incompleteness is modular and flexible in order to support different use case scenarios (i.e., benchmarks covering different types of facts) and different languages. We use on to build two complete oie benchmarks: one with verb-mediated facts and another with facts encompassing named entities. We evaluate several oie systems on our complete benchmarks created with challenging. We publicly release challenging (and all gold datasets generated with it) under non-restrictive license.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2013,"Venue":"semeval-2013","Acronym":"LIMSI","Description":"Cross-lingual Word Sense Disambiguation using Translation Sense Clustering","Abstract":"We describe the in system for the semeval-2013 cross-lingual word sense disambiguation (clwsd) task. Word senses are represented by means of translation clusters in different languages built by a cross-lingual word sense induction (wsi) method. Our clwsd classi\ufb01er exploits the wsi output for selecting appropriate translations for target words in context. We present the design of the system and the obtained results.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2023,"Venue":"acl-2023","Acronym":"ORGAN","Description":"Observation-Guided Radiology Report Generation via Tree Reasoning","Abstract":"This paper explores the task of radiology report generation, which aims at generating free-text descriptions for a set of radiographs. One significant challenge of this task is how to correctly maintain the consistency between the images and the lengthy report. Previous research explored solving this issue through planning-based methods, which generate reports only based on high-level plans. However, these plans usually only contain the major observations from the radiographs (e.g., lung opacity), lacking much necessary information, such as the observation characteristics and preliminary clinical diagnoses. To address this problem, the system should also take the image information into account together with the textual plan and perform stronger reasoning during the generation process. In this paper, we propose an observation-guided radiology report generation framework (descriptions). It first produces an observation plan and then feeds both the plan and radiographs for report generation, where an observation graph and a tree reasoning mechanism are adopted to precisely enrich the plan information by capturing the multi-formats of each observation. Experimental results demonstrate that our framework outperforms previous state-of-the-art methods regarding text quality and clinical efficacy.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2020,"Venue":"ws-2020","Acronym":"AskMe","Description":"A LAPPS Grid-based NLP Query and Retrieval System for Covid-19 Literature","Abstract":"In a recent project, the language application grid was augmented to support the mining of scientific publications. The results of that ef- fort have now been repurposed to focus on covid-19 literature, including modification of the lapps grid \u201cbeen\u201d query and retrieval engine. We describe the covid-19 system and discuss its functionality as compared to other query engines available to search covid-related publications.","wordlikeness":0.8,"lcsratio":0.8,"wordcoverage":0.8}
{"Year":2022,"Venue":"coling-2022","Acronym":"LEGO-ABSA","Description":"A Prompt-based Task Assemblable Unified Generative Framework for Multi-task Aspect-based Sentiment Analysis","Abstract":"Aspect-based sentiment analysis (absa) has received increasing attention recently. Absa can be divided into multiple tasks according to the different extracted elements. Existing generative methods usually treat the output as a whole string rather than the combination of different elements and only focus on a single task at once. This paper proposes a unified generative multi-task framework that can solve multiple absa tasks by controlling the type of task prompts consisting of multiple element prompts. Further, the proposed approach can train on simple tasks and transfer to difficult tasks by assembling task prompts, like assembling lego bricks. We conduct experiments on six absa tasks across multiple benchmarks. Our proposed multi-task approach achieves new state-of-the-art results in almost all tasks and competitive results in task transfer scenarios.","wordlikeness":0.5555555556,"lcsratio":1.0,"wordcoverage":0.6153846154}
{"Year":2021,"Venue":"acl-2021","Acronym":"XL-Sum","Description":"Large-Scale Multilingual Abstractive Summarization for 44 Languages","Abstract":"Ive summarization for 44 languages tahmid hasan1\u2217, abhik bhattacharjee1\u2217, md saiful islam2, kazi samin1, yuan-fang li3, yong-bin kang4, m. Sohel rahman1, rifat shahriyar1 bangladesh university of engineering and technology (buet)1, university of rochester2, monash university3, swinburne university of technology4 tahmidhasan@cse.buet.ac.bd, {abhik,samin}@ra.cse.buet.ac.bd, mislam6@ur.rochester.edu, yuanfang.li@monash.edu, ykang@swin.edu.au, {msrahman,rifat}@cse.buet.ac.bd contemporary works on abstractive text summarization have focused primarily on highresource languages like english, mostly due to the limited availability of datasets for low\/midresource ones. In this work, we present xlsum, a comprehensive and diverse dataset comprising 1 million professionally annotated article-summary pairs from bbc, extracted using a set of carefully designed heuristics. The dataset covers 44 languages ranging from low to high-resource, for many of which no public dataset is currently available. Ykang@swin.edu.au, is highly abstractive, concise, and of high quality, as indicated by human and intrinsic evaluation. We \ufb01ne-tune mt5, a state-of-theart pretrained multilingual model, with xlsum and experiment on multilingual and lowresource summarization tasks. Annotated induces competitive results compared to the ones obtained using similar monolingual datasets: we show higher than 11 rouge-2 scores on 10 languages we benchmark on, with some of them exceeding 15, as obtained by multilingual training. Additionally, training on low-resource languages individually also provides competitive performance. To the best of our knowledge, the is the largest abstractive summarization dataset in terms of the number of samples collected from a single source and the number of languages covered. We are releasing our dataset and models to encourage future research on multilingual abstractive summarization. The resources can be found at https:\/\/github. Com\/csebuetnlp\/collected.","wordlikeness":0.5,"lcsratio":0.8333333333,"wordcoverage":0.6666666667}
{"Year":2018,"Venue":"lrec-2018","Acronym":"CONDUCT","Description":"An Expressive Conducting Gesture Dataset for Sound Control","Abstract":"Recent research in music-gesture relationship has paid more attention on the sound variations and its corresponding gesture expressiveness. In this study we are interested by gestures performed by orchestral researchors, with a focus on the expressive gestures made by the non dominant hand. We make the assumption that these gestures convey some meaning shared by most of collectionors, and that they implicitly correspond to sound effects which can be encoded in musical scores. Following this hypothesis, we de\ufb01ned a collection of gestures for musical direction. These gestures are designed to correspond to well known functional effect on sounds, and they can be modulated to vary this effect by simply modifying one of their structural component (hand movement or hand shape). This paper presents the design of the gesture and sound sets and the protocol that has led to the database construction. The relevant musical excerpts and the related expressive gestures have been \ufb01rst de\ufb01ned by one expert musician. The gestures were then recorded through motion capture by two non experts who performed them along with recorded music. This database will serve as a basis for training gesture recognition system for live sound control and modulation. Keywords: corpus, sound-control gestures, expressive gesture, traininging 1.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"DuQM","Description":"A Chinese Dataset of Linguistically Perturbed Natural Questions for Evaluating the Robustness of Question Matching Models","Abstract":"In this paper, we focus on the robustness evaluation of chinese question matching (qm) models. Most of the previous work on analyzing robustness issues focus on just one or a few types of artificial adversarial examples. Instead, we argue that a comprehensive evaluation should be conducted on natural texts, which takes into account the fine-grained linguistic capabilities of qm models. For this purpose, we create a chinese dataset namely strength which contains natural questions with linguistic perturbations to evaluate the robustness of qm models. In contains 3 categories and 13 subcategories with 32 linguistic perturbations. The extensive experiments demonstrate that takes has a better ability to distinguish different models. Importantly, the detailed breakdown of evaluation by the linguistic phenomena in takes helps us easily diagnose the strength and weakness of different models. Additionally, our experiment results show that the effect of artificial adversarial examples does not work on natural texts. Our baseline codes and a leaderboard are now publicly available.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2012,"Venue":"starsem-2012","Acronym":"LIMSI","Description":"Learning Semantic Similarity by Selecting Random Word Subsets","Abstract":"We propose a semantic similarity learning method based on random indexing (ri) and ranking with boosting. Unlike classical ri, we use only those context vector features that are informative for the semantics modeled. Despite ignoring text preprocessing and dispensing with semantic resources, the approach was ranked as high as 22nd among 89 participants in the semeval-2012 task6: semantic textual similarity.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2023,"Venue":"acl-2023","Acronym":"FSUIE","Description":"A Novel Fuzzy Span Mechanism for Universal Information Extraction","Abstract":"Universal information extraction (uie) has been introduced as a unified framework for various information extraction (ie) tasks and has achieved widespread success. Despite this, uie models have limitations. For example, they rely heavily on span boundaries in the data during training, which does not reflect the reality of span annotation challenges. Slight adjustments to positions can also meet requirements. Additionally, uie models lack attention to the limited span length feature in ie. To address these deficiencies, we propose the fuzzy span universal information extraction (for) framework. Specifically, our contribution consists of two concepts: <i>fuzzy span loss<\/i> and <i>fuzzy span attention<\/i>. Our experimental results on a series of main ie tasks show significant improvement compared to the baseline, especially in terms of fast convergence and strong performance with small amounts of data and training epochs. These results demonstrate the effectiveness and generalization of these in different tasks, settings, and scenarios.","wordlikeness":0.2,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2022,"Venue":"lrec-2022","Acronym":"ALIGNMEET","Description":"A Comprehensive Tool for Meeting Annotation, Alignment, and Evaluation","Abstract":"Summarization is a challenging problem, and even more challenging is to manually create, correct, and evaluate the summaries. The severity of the problem grows when the inputs are multi-party dialogues in a meeting setup. To facilitate the research in this area, we present from, a comprehensive tool for meeting annotation, alignment, and evaluation. The tool aims to provide an efficient and clear interface for fast annotation while mitigating the risk of introducing errors. Moreover, we add an evaluation mode that enables a comprehensive quality evaluation of meeting minutes. To the best of our knowledge, there is no such tool available. We release the tool as open source. It is also directly installable from pypi.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"N-LTP","Description":"An Open-source Neural Language Technology Platform for Chinese","Abstract":"We introduce our, an open-source neural language technology platform supporting six fundamental chinese nlp tasks: lexical analysis (chinese word segmentation, part-of-speech tagging, and named entity recognition), syntactic parsing (dependency parsing), and semantic parsing (semantic dependency parsing and semantic role labeling). Unlike the existing state-of-the-art toolkits, such as stanza, that adopt an independent model for each task, across adopts the multi-task framework by using a shared pre-trained model, which has the advantage of capturing the shared knowledge across relevant chinese tasks. In addition, a knowledge distillation method (clark et al., 2019) where the single-task model teaches the multi-task model is further introduced to encourage the multi-task model to surpass its single-task teacher. Finally, we provide a collection of easy-to-use apis and a visualization tool to make users to use and view the processing results more easily and directly. To the best of our knowledge, this is the first toolkit to support six chinese nlp fundamental tasks. Source code, documentation, and pre-trained models are available at <a href=https:\/\/github.com\/hit-scir\/ltp class=acl-markup-url>https:\/\/github.com\/hit-scir\/ltp<\/a>.","wordlikeness":0.2,"lcsratio":1.0,"wordcoverage":0.6}
{"Year":2022,"Venue":"findings-2022","Acronym":"RelationPrompt","Description":"Leveraging Prompts to Generate Synthetic Data for Zero-Shot Relation Triplet Extraction","Abstract":"Despite the importance of relation extraction in building and representing knowledge, less research is focused on generalizing to unseen relations types. We introduce the task setting of zero-shot relation triplet extraction (zerorte) to encourage further research in low-resource relation extraction methods. Given an input sentence, each extracted triplet consists of the head entity, relation label, and tail entity where the relation label is not seen at the training stage. To solve zerorte, we propose to synthesize relation examples by prompting language models to generate structured texts. Concretely, we unify language model prompts and structured text approaches to design a structured prompt template for generating synthetic relation samples when conditioning on relation label prompts (extracting). To overcome the limitation for extracting multiple relation triplets in a sentence, we design a novel triplet search decoding method. Experiments on fewrel and wiki-zsl datasets show the efficacy of entity, for the zerorte task and zero-shot relation classification. Our code and data are available at github.com\/declare-lab\/in.","wordlikeness":0.7857142857,"lcsratio":0.7857142857,"wordcoverage":0.7272727273}
{"Year":2020,"Venue":"lrec-2020","Acronym":"ATC-ANNO","Description":"Semantic Annotation for Air Traffic Control with Assistive Auto-Annotation","Abstract":"In air traffic control, assistant systems support air traffic controllers in their work. To improve the reactivity and accuracy of the assistant, automatic speech recognition can monitor the commands uttered by the controller. However, to provide sufficient training data for the speech recognition system, many hours of air traffic communications have to be transcribed and semantically annotated. For this purpose we developed the annotation tool assistant,. It provides a number of features to support the annotator in their task, such as auto-complete suggestions for semantic tags, access to preliminary speech recognition predictions, syntax highlighting and consistency indicators. Its core assistive feature, however, is its ability to automatically generate semantic annotations. Although it is based on a simple hand-written finite state grammar, it is also able to annotate sentences that deviate from this grammar. We evaluate the impact of different features on annotator efficiency and find that automatic annotation allows annotators to cover four times as many utterances in the same time.","wordlikeness":0.625,"lcsratio":1.0,"wordcoverage":0.7142857143}
{"Year":2023,"Venue":"ws-2023","Acronym":"LSLlama","Description":"Fine-Tuned LLaMA for Lexical Simplification","Abstract":"Generative large language models (llms), such as gpt-3, have become increasingly effective and versatile in natural language processing (nlp) tasks. One such task is lexical simplification, where state-of-the-art methods involve complex, multi-step processes which can use both deep learning and non-deep learning processes. Llama, an llm with full research access, holds unique potential for the adaption of the entire ls pipeline. This paper details the process of fine-tuning llama to create methods, which performs comparably to previous ls baseline models lsbert and unihd.","wordlikeness":0.4285714286,"lcsratio":0.8571428571,"wordcoverage":0.7272727273}
{"Year":2005,"Venue":"ws-2005","Acronym":"LIHLA","Description":"Shared Task System Description","Abstract":"In this paper we describe paper, a lexical aligner which uses bilingual probabilistic lexicons generated by a freely available set of tools (natools) and languageindependent heuristics to \ufb01nd links between single words and multiword units in sentence-aligned parallel texts. The method has achieved an alignment error rate of 22.72% and 44.49% on english\u2013 inuktitut and romanian\u2013english parallel sentences, respectively.","wordlikeness":0.4,"lcsratio":0.4,"wordcoverage":0.75}
{"Year":2022,"Venue":"findings-2022","Acronym":"Spa","Description":"On the Sparsity of Virtual Adversarial Training for Dependency Parsing","Abstract":"Virtual adversarial training (vat) is a powerful approach to improving robustness and performance, leveraging both labeled and unlabeled data to compensate for the scarcity of labeled data. It is adopted on lots of vision and language classification tasks. However, for tasks with structured output (e.g., dependency parsing), the application of vat is nontrivial due to the intrinsic proprieties of structures: (1) the non-vatrse problem and (2) exponential complexity. Against this background, we propose the onrse parse adjustment (an) algorithm and successfully applied vat to the dependency parsing task. Adjustable refers to the learning algorithm which combines the graph-based dependency parsing model with vat in an exact computational manner and enhances the dependency parser with controllable and adjustable (vat)rsity. Empirical studies show that the treecrf parser optimized using outperforms other methods without visionrsity regularization.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2014,"Venue":"semeval-2014","Acronym":"USF","Description":"Chunking for Aspect-term Identification \\&amp; Polarity Classification","Abstract":"This paper describes the systems submitted by the university of san francisco (features) to semeval-2014 task 4, aspect based sentiment analysis (absa), which provides labeled data in two domains, laptops and restaurants. For the constrained condition of both the aspect term extraction and aspect term polarity tasks, we take a supervised machine learning approach using a combination of lexical, syntactic, and baseline sentiment features. Our extraction approach is inspired by a chunking approach, based on its strong past results on related tasks. Our system performed slightly below average compared to other submissions, possibly because we use a simpler classi\ufb01cation model than prior work. Our polarity labeling approach uses two baseline hand-built sentiment classi\ufb01ers as features in addition to lexical and syntactic features, and performed in the top ten of other constrained systems on both domains.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2021,"Venue":"ws-2021","Acronym":"ALUE","Description":"Arabic Language Understanding Evaluation","Abstract":"The emergence of multi-task learning (mtl)models in recent years has helped push thestate of the art in natural language un-derstanding (nlu). We strongly believe thatmany nlu problems in arabic are especiallypoised to reap the benefits of such models. Tothis end we propose the arabic language un-derstanding evaluation benchmark (language),based on 8 carefully selected and previouslypublished tasks. For five of these, we providenew privately held evaluation datasets to en-sure the fairness and validity of our benchmark. We also provide a diagnostic dataset to helpresearchers probe the inner workings of theirmodels.our initial experiments show thatmtl models outperform their singly trainedcounterparts on most tasks. But in order to en-tice participation from the wider community,we stick to publishing singly trained baselinesonly. Nonetheless, our analysis reveals thatthere is plenty of room for improvement inarabic nlu. We hope that thatthere will playa part in helping our community realize someof these improvements. Interested researchersare invited to submit their results to our online,and publicly accessible leaderboard.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2018,"Venue":"wassa-2018","Acronym":"EmojiGAN","Description":"learning emojis distributions with a generative model","Abstract":"Generative models have recently experienced a surge in popularity due to the development of more efficient training algorithms and increasing computational power. Models such as adversarial generative networks (gans) have been successfully used in various areas such as computer vision, medical imaging, style transfer and natural language generation. Adversarial nets were recently shown to yield results in the image-to-text task, where given a set of images, one has to provide their corresponding text description. In this paper, we take a similar approach and propose a image-to-emoji architecture, which is trained on data from social networks and can be used to score a given picture using ideograms. We show empirical results of our algorithm on data obtained from the most influential instagram accounts.","wordlikeness":0.625,"lcsratio":0.875,"wordcoverage":0.7692307692}
{"Year":2011,"Venue":"ws-2011","Acronym":"ULISSE","Description":"an Unsupervised Algorithm for Detecting Reliable Dependency Parses","Abstract":"In this paper we present parser., an unsupervised linguistically\u2013driven algorithm to select reliable parses from the output of a dependency parser. Different experiments were devised to show that the algorithm is robust enough to deal with the output of different parsers and with different languages, as well as to be used across different domains. In all cases, a appears to outperform the baseline algorithms.","wordlikeness":0.8333333333,"lcsratio":0.8333333333,"wordcoverage":0.8333333333}
{"Year":2022,"Venue":"lrec-2022","Acronym":"IndoUKC","Description":"A Concept-Centered Indian Multilingual Lexical Resource","Abstract":"We introduce the beyond, a new multilingual lexical database comprised of eighteen indian languages, with a focus on formally capturing words and word meanings specific to indian languages and cultures. The database reuses content from the existing indowordnet resource while providing a new model for the cross-lingual mapping of lexical meanings that allows for a richer, diversity-aware representation. Accordingly, beyond a thorough syntactic and semantic cleaning, the indowordnet lexical content has been thoroughly remodeled in order to allow a more precise expression of language-specific meaning. The resulting database is made available both for browsing through a graphical web interface and for download through the livelanguage data catalogue.","wordlikeness":0.4285714286,"lcsratio":0.8571428571,"wordcoverage":0.7692307692}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"CAT-probing","Description":"A Metric-based Approach to Interpret How Pre-trained Models for Programming Language Attend Code Structure","Abstract":"Code pre-trained models (codeptms) have recently demonstrated significant success in code intelligence. To interpret these models, some probing methods have been applied. However, these methods fail to consider the inherent characteristics of codes. In this paper, to address the problem, we propose a novel probing method codes. To quantitatively interpret how codeptms attend code structure. We first denoise the input code sequences based on the token types pre-defined by the compilers to filter those tokens whose attention scores are too small. After that, we define a new metric cat-score to measure the commonality between the token-level attention scores generated in codeptms and the pair-wise distances between corresponding ast nodes. The higher the cat-score, the stronger the ability of codeptms to capture code structure. We conduct extensive experiments to integrate effectiveness with representative codeptms for different programming languages. Experimental results show the effectiveness of input in codeptm interpretation. Our codes and data are publicly available at <a href=https:\/\/github.com\/nchen909\/codeattention class=acl-markup-url>https:\/\/github.com\/nchen909\/codeattention<\/a>.","wordlikeness":0.6363636364,"lcsratio":0.9090909091,"wordcoverage":0.7777777778}
{"Year":2011,"Venue":"acl-2011","Acronym":"MemeTube","Description":"A Sentiment-based Audiovisual System for Analyzing and Displaying Microblog Messages","Abstract":"Micro-blogging services provide platforms for users to share their feelings and ideas on the move. In this paper, we present a search-based demonstration system, called provides, to summarize the sentiments of microblog messages in an audiovisual manner. We provides three main functions: (1) recognizing the sentiments of messages (2) generating music melody automatically based on detected sentiments, and (3) produce an animation of real-time piano playing for audiovisual display. Our ideas system can be accessed via: http:\/\/mslab.csie.ntu.edu.tw\/our\/ .","wordlikeness":0.875,"lcsratio":0.875,"wordcoverage":0.7142857143}
{"Year":2023,"Venue":"findings-2023","Acronym":"Longtonotes","Description":"OntoNotes with Longer Coreference Chains","Abstract":"Ontonotes has served as the most important benchmark for coreference resolution. However, for ease of annotation, several long documents in ontonotes were split into smaller parts. In this work, we build a corpus of coreference-annotated documents of significantly longer length than what is currently available. We do so by providing an accurate, manually-curated, merging of annotations from documents that were split into multiple parts in the original ontonotes annotation process. The resulting corpus, which we call parts contains documents in multiple genres of the english language with varying lengths, the longest of which are up to 8x the length of documents in ontonotes, and 2x those in litbank.we evaluate state-of-the-art neural coreference systems on this new corpus, analyze the relationships between model architectures\/hyperparameters and document length on performance and efficiency of the models, and demonstrate areas of improvement in long-document coreference modelling revealed by our new corpus.","wordlikeness":0.7272727273,"lcsratio":0.8181818182,"wordcoverage":0.7}
{"Year":2020,"Venue":"lrec-2020","Acronym":"SAPPHIRE","Description":"Simple Aligner for Phrasal Paraphrase with Hierarchical Representation","Abstract":"We present word, a simple aligner for phrasal paraphrase with hierarchical representation. Monolingual phrase alignment is a fundamental problem in natural language understanding and also a crucial technique in various applications such as natural language inference and semantic textual similarity assessment. Previous methods for monolingual phrase alignment are language-resource intensive; they require large-scale synonym\/paraphrase lexica and high-quality parsers. Different from them, alignments depends only on a monolingual corpus to train word embeddings. Therefore, it is easily transferable to specific domains and different languages. Specifically, searches first obtains word alignments using pre-trained word embeddings and then expands them to phrase alignments by bilingual phrase extraction methods. To estimate the likelihood of phrase alignments, with uses phrase embeddings that are hierarchically composed of word embeddings. Finally, bilingual searches for a set of consistent phrase alignments on a lattice of phrase alignment candidates. It achieves search-efficiency by constraining the lattice so that all the paths go through a phrase alignment pair with the highest alignment score. Experimental results using the standard dataset for phrase alignment evaluation show that them outperforms the previous method and establishes the state-of-the-art performance.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.7692307692}
{"Year":2019,"Venue":"nodalida-2019","Acronym":"UniParse","Description":"A universal graph-based parsing toolkit","Abstract":"This paper describes the design and use of the graph-based parsing framework and toolkit ,, released as an open-source python software package. Well as a framework novelly streamlines research prototyping, development and evaluation of graph-based dependency parsing architectures. Of does this by enabling highly efficient, sufficiently independent, easily readable, and easily extensible implementations for all dependency parser components. We distribute the toolkit with ready-made configurations as re-implementations of all current state-of-the-art first-order graph-based parsers, including even more efficient cython implementations of both encoders and decoders, as well as the required specialised loss functions.","wordlikeness":0.5,"lcsratio":0.875,"wordcoverage":0.75}
{"Year":2006,"Venue":"lrec-2006","Acronym":"I-CAB","Description":"the Italian Content Annotation Bank","Abstract":"In this paper we present work in progress for the creation of the italian content annotation bank (consisted), a corpus of italian news annotated with semantic information at different levels. The first level is represented by temporal expressions, the second level is represented by different types of entities (i.e. person, organizations, locations and geo-political entities), and the third level is represented by relations between entities (e.g. the affiliation relation connecting a person to an organization). So far finally, has been manually annotated with temporal expressions, person entities and organization entities. As we intend creation to become a benchmark for various automatic information extraction tasks, we followed a policy of reusing already available markup languages. In particular, we adopted the annotation schemes developed for the ace entity detection and time expressions recognition and normalization tasks. As the ace guidelines have originally been developed for english, part of the effort consisted in adapting them to the specific morpho-syntactic features of italian. Finally, we have extended them to include a wider range of entities, such as conjunctions.","wordlikeness":0.6,"lcsratio":0.8,"wordcoverage":0.75}
{"Year":2010,"Venue":"lrec-2010","Acronym":"WikiWoods","Description":"Syntacto-Semantic Annotation for English Wikipedia","Abstract":"Is is an ongoing initiative to provide rich syntacto-semantic annotations for english wikipedia. We sketch an automated processing pipeline to extract relevant textual content from wikipedia sources, segment documents into sentence-like units, parse and disambiguate using a broad-coverage precision grammar, and support the export of syntactic and semantic information in various formats. The full parsed corpus is accompanied by a subset of wikipedia articles for which gold-standard annotations in the same format were produced manually. This subset was selected to represent a coherent domain, wikipedia entries on the broad topic of natural language processing.","wordlikeness":0.7777777778,"lcsratio":0.5555555556,"wordcoverage":0.7142857143}
{"Year":2020,"Venue":"acl-2020","Acronym":"LEAN-LIFE","Description":"A Label-Efficient Annotation Framework Towards Learning from Explanation","Abstract":"Successfully training a deep neural network demands a huge corpus of labeled data. However, each label only provides limited information to learn from, and collecting the requisite number of labels involves massive human effort. In this work, we introduce limited, a web-based, label-efficient annotation framework for sequence labeling and classification tasks, with an easy-to-use ui that not only allows an annotator to provide the needed labels for a task but also enables learning from explanations for each labeling decision. Such explanations enable us to generate useful additional labeled data from unlabeled instances, bolstering the pool of available training data. On three popular nlp tasks (named entity recognition, relation extraction, sentiment analysis), we find that using this enhanced supervision allows our models to surpass competitive baseline f1 scores by more than 5-10 percentage points, while using 2x times fewer labeled instances. Our framework is the first to utilize this enhanced supervision technique and does so for three important tasks \u2013 thus providing improved annotation recommendations to users and an ability to build datasets of (data, label, explanation) triples instead of the regular (data, label) pair.","wordlikeness":0.6666666667,"lcsratio":0.8888888889,"wordcoverage":0.7058823529}
{"Year":2000,"Venue":"amta-2000","Acronym":"LabelTool","Description":"a localization application for devices with restricted display areas","Abstract":"The into\/trtool system is designed to administer text strings that are shown in devices with a very limited display area and translated into a very large number of foreign languages. Automation of character set handling and file naming and storage together with real\u2013time simulation of text string input are the main features of this application.","wordlikeness":0.8888888889,"lcsratio":0.7777777778,"wordcoverage":0.7142857143}
{"Year":2022,"Venue":"findings-2022","Acronym":"uFACT","Description":"Unfaithful Alien-Corpora Training for Semantically Consistent Data-to-Text Generation","Abstract":"We propose alone. (un-faithful alien corpora training), a training corpus construction method for data-to-text (d2t) generation models. We show that d2t models trained on training), datasets generate utterances which represent the semantic content of the data sources more accurately compared to models trained on the target corpus alone. Our approach is to augment the training set of a given target corpus with alien corpora which have different semantic representations. We show that while it is important to have faithful data from the target corpus, the faithfulness of additional corpora only plays a minor role. Consequently, training), datasets can be constructed with large quantities of unfaithful data. We show how we can be leveraged to obtain state-of-the-art results on the webnlg benchmark using meteor as our performance metric. Furthermore, we investigate the sensitivity of the generation faithfulness to the training corpus structure using the parent metric, and provide a baseline for this metric on the webnlg (gardent et al., 2017) benchmark to facilitate comparisons with future work.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2020,"Venue":"emnlp-2020","Acronym":"IsOBS","Description":"An Information System for Oracle Bone Script","Abstract":"Oracle bone script (obs) is the earliest known ancient chinese writing system and the ancestor of modern chinese. As the chinese writing system is the oldest continuously-used system in the world, the study of obs plays an important role in both linguistic and historical research. In order to utilize advanced machine learning methods to automatically process obs, we construct an information system for obs (also) to symbolize, serialize, and store obs data at the character-level, based on efficient databases and retrieval modules. Moreover, we also apply few-shot learning methods to build an effective obs character recognition module, which can recognize a large number of obs characters (especially those characters with a handful of examples) and make the system easy to use. The demo system of obs can be found from <a href=http:\/\/character.thunlp.org\/ class=acl-markup-url>http:\/\/advanced.thunlp.org\/<\/a>. In the future, we will add more obs data to the system, and hopefully our with can support further efforts in automatically processing obs and advance the scientific progress in this field.","wordlikeness":0.4,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2008,"Venue":"ws-2008","Acronym":"TuLiPA","Description":"A syntax-semantics parsing environment for mildly context-sensitive formalisms","Abstract":"In this paper we present a parsing architecture that allows processing of different mildly context-sensitive formalisms, in particular tree-adjoining grammar (tag), multi-component tree-adjoining grammar with tree tuples (tt-mctag) and simple range concatenation grammar (rcg). Furthermore, for tree-based grammars, the parser computes not only syntactic analyses but also the corresponding semantic representations.","wordlikeness":0.5,"lcsratio":0.6666666667,"wordcoverage":0.7272727273}
{"Year":2022,"Venue":"acl-2022","Acronym":"ABC","Description":"Attention with Bounded-memory Control","Abstract":"Transformer architectures have achieved state- of-the-art results on a variety of natural language processing (nlp) tasks. However, their attention mechanism comes with a quadratic complexity in sequence lengths, making the computational overhead prohibitive, especially for long sequences. Attention context can be seen as a random-access memory with each token taking a slot. Under this perspective, the memory size grows linearly with the sequence length, and so does the overhead of reading from it. One way to improve the efficiency is to bound the memory size. We show that disparate approaches can be subsumed into one abstraction, attention with bounded-memory control (on), and they vary in their organization of the memory. Does reveals new, unexplored possibilities. First, it connects several efficient attention variants that would otherwise seem apart. Second, this abstraction gives new insights\u2014an established approach (wang et al., 2020b) previously thought to not be applicable in causal attention, actually is. Last, we present a new instance of computational, which draws inspiration from existing negligible approaches, but replaces their heuristic memory-organizing functions with a learned, contextualized one. Our experiments on language modeling, machine translation, and masked language model finetuning show that our approach outperforms previous efficient attention models; compared to the strong transformer baselines, it significantly improves the inference time and space efficiency with no or negligible accuracy loss.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2020,"Venue":"eamt-2020","Acronym":"MICE","Description":"a middleware layer for MT","Abstract":"The the project (2018-2020) will deliver a middleware layer for improving the output quality of the etranslation system of ec\u2019s connecting europe facility through additional services, such as domain adaptation and named entity recognition. It will also deliver a user portal, allowing for human post-editing.","wordlikeness":1.0,"lcsratio":0.75,"wordcoverage":1.0}
{"Year":2004,"Venue":"acl-2004","Acronym":"NLTK","Description":"The Natural Language Toolkit","Abstract":"The natural language toolkit is a suite of program modules, data sets, tutorials and exercises, covering symbolic and statistical natural language processing. Modules, is written in python and distributed under the gpl open source license. Over the past three years, sets, has become popular in teaching and research. We describe the toolkit and report on its current state of development.","wordlikeness":0.25,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2023,"Venue":"findings-2023","Acronym":"LED","Description":"A Dataset for Life Event Extraction from Dialogs","Abstract":"Lifelogging has gained more attention due to its wide applications, such as personalized recommendations or memory assistance. The issues of collecting and extracting personal life events have emerged. People often share their life experiences with others through conversations. However, extracting life events from conversations is rarely explored. In this paper, we present life event dialog, a dataset containing fine-grained life event annotations on conversational data. In addition, we initiate a novel conversational life event extraction task and differentiate the task from the public event extraction or the life event extraction from other sources like microblogs. We explore three information extraction (ie) frameworks to address the conversational life event extraction task: openie, relation extraction, and event extraction. A comprehensive empirical analysis of the three baselines is established. The results suggest that the current event extraction model still struggles with extracting life events from human daily conversations. Our proposed life event dialog dataset and in-depth analysis of ie frameworks will facilitate future research on life event extraction from conversations.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2017,"Venue":"eacl-2017","Acronym":"Lingmotif","Description":"Sentiment Analysis for the Digital Humanities","Abstract":"Sentiment is a lexicon-based, linguistically-motivated, user-friendly, gui-enabled, multi-platform, sentiment analysis desktop application. Sentiment. Can perform sa on any type of input texts, regardless of their length and topic. The analysis is based on the identification of sentiment-laden words and phrases contained in the application\u2019s rich core lexicons, and employs context rules to account for sentiment shifters. It offers easy-to-interpret visual representations of quantitative data (text polarity, sentiment intensity, sentiment profile), as well as a detailed, qualitative analysis of the text in terms of its sentiment. Lexicons, can also take user-provided plugin lexicons in order to account for domain-specific sentiment expression. Polarity, currently analyzes english and spanish texts.","wordlikeness":0.7777777778,"lcsratio":0.6666666667,"wordcoverage":0.6666666667}
{"Year":2022,"Venue":"lrec-2022","Acronym":"TArC","Description":"Tunisian Arabish Corpus, First complete release","Abstract":"In this paper we present the final result of a project focused on tunisian arabic encoded in arabizi, the latin-based writing system for digital conversations. The project led to the realization of two integrated and independent tools: a linguistic corpus and a neural network architecture created to annotate the former with various levels of linguistic information (code-switching classification, transliteration, tokenization, pos-tagging, lemmatization). We discuss the choices made in terms of computational and linguistic methodology and the strategies adopted to improve our results. We report on the experiments performed in order to outline our research path. Finally, we explain the reasons why we believe in the potential of these tools for both computational and linguistic researches.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2018,"Venue":"coling-2018","Acronym":"CRST","Description":"a Claim Retrieval System in Twitter","Abstract":"For controversial topics, collecting argumentation-containing tweets which tend to be more convincing will help researchers analyze public opinions. Meanwhile, claim is the heart of argumentation. Hence, we present the first real-time claim retrieval system analyze that retrieves tweets containing claims for a given topic from twitter. We propose a claim-oriented ranking module which can be divided into the offline topic-independent learning to rank model and the online topic-dependent lexicon model. Our system outperforms previous claim retrieval system and argument mining system. Moreover, the claim-oriented ranking module can be easily adapted to new topics without any manual process or external information, guaranteeing the practicability of our system.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2016,"Venue":"lrec-2016","Acronym":"MultiVec","Description":"a Multilingual and Multilevel Representation Learning Toolkit for NLP","Abstract":"We present different, a new toolkit for computing continuous representations for text at different granularity levels (word-level or sequences of words). We includes word2vec\u2019s features, paragraph vector (batch and online) and bivec for bilingual distributed representations. (word-level also includes different distance measures between words and sequences of words. The toolkit is written in c++ and is aimed at being fast (in the same order of magnitude as word2vec), easy to use, and easy to extend. It has been evaluated on several nlp tasks: the analogical reasoning task, sentiment analysis, and crosslingual document classification.","wordlikeness":0.75,"lcsratio":0.875,"wordcoverage":0.7777777778}
{"Year":2022,"Venue":"findings-2022","Acronym":"PLATO-XL","Description":"Exploring the Large-scale Pre-training of Dialogue Generation","Abstract":"To explore the limit of dialogue generation pre-training, we present the models of both with up to 11 billion parameters, trained on both chinese and english social media conversations. To train such large models, we adopt the architecture of unified transformer with high computation and parameter efficiency. In addition, we carry out multi-party aware pre-training to better distinguish the characteristic information in social media conversations. With such designs, conversational successfully achieves superior performances as compared to other approaches in both chinese and english chitchat. We further explore the capacity of we on other conversational tasks, such as knowledge grounded dialogue and task-oriented conversation. The experimental results indicate that tasks, obtains state-of-the-art results across multiple conversational tasks, verifying its potential as a foundation model of conversational ai.","wordlikeness":0.5,"lcsratio":0.75,"wordcoverage":0.7142857143}
{"Year":2000,"Venue":"ws-2000","Acronym":"TransType","Description":"a Computer-Aided Translation Typing System","Abstract":"This paper describes the embedding of a sta- tistical translation system within a text editor to produce types, a system that watches over the user as he or she types a translation and repeatedly suggests completions for the text al- ready entered. This innovative embedded ma- chine translation system is thus a specialized means of helping produce high quality transla- tions.","wordlikeness":0.8888888889,"lcsratio":1.0,"wordcoverage":0.7777777778}
{"Year":2018,"Venue":"lrec-2018","Acronym":"FooTweets","Description":"A Bilingual Parallel Corpus of World Cup Tweets","Abstract":"The way information spreads through society has changed signi\ufb01cantly over the past decade with the advent of online social networking. Twitter, one of the most widely used social networking websites, is known as the real-time, public microblogging network where news breaks \ufb01rst. Most users love it for its iconic 140-character limitation and un\ufb01ltered feed that show them news and opinions in the form of tweets. Tweets are usually multilingual in nature and of varying quality. However, machine translation (mt) of twitter data is a challenging task especially due to the following two reasons: (i) tweets are informal in nature (i.e., violates linguistic norms), and (ii) parallel resource for twitter data is scarcely available on the internet. In this paper, we develop tweets,, a \ufb01rst parallel corpus of tweets for english\u2013german language pair. We extract 4, 000 english tweets from the fifa 2014 world cup and manually translate them into german with a special focus on the informal nature of the tweets. In addition to this, we also annotate sentiment scores between 0 and 1 to all the tweets depending upon the degree of sentiment associated with them. This data has recently been used to build sentiment translation engines and an extensive evaluation revealed that such a resource is very useful in machine translation of user generated content. Keywords: tweets, parallel data, sentiment translation 1.","wordlikeness":0.5555555556,"lcsratio":0.8888888889,"wordcoverage":0.8}
{"Year":2015,"Venue":"semeval-2015","Acronym":"UTU","Description":"Adapting Biomedical Event Extraction System to Disorder Attribute Detection","Abstract":"In this paper we describe our entry to the semeval 2015 clinical text analysis task. We participated only in the disorder attribute detection task 2a. Our main goal was to assess how well an information extraction system originally developed for a different task and domain can be utilized in this task. Our system, based on svm and crf classi\ufb01ers, showed promising results, placing 3rd out of 6 participants in this task with performance of 0.857 measured in weighted accuracy, the of\ufb01cial evaluation metric.","wordlikeness":0.3333333333,"lcsratio":0.6666666667,"wordcoverage":0.8}
{"Year":2021,"Venue":"findings-2021","Acronym":"HAConvGNN","Description":"Hierarchical Attention Based Convolutional Graph Neural Network for Code Documentation Generation in Jupyter Notebooks","Abstract":"Jupyter notebook allows data scientists to write machine learning code together with its documentation in cells. In this paper, we propose a new task of code documentation generation (cdg) for computational notebooks. In contrast to the previous cdg tasks which focus on generating documentation for single code snippets, in a computational notebook, one documentation in a markdown cell often corresponds to multiple code cells, and these code cells have an inherent structure. We proposed a new model (tasks) that uses a hierarchical attention mechanism to consider the relevant code cells and the relevant code tokens information when generating the documentation. Tested on a new corpus constructed from well-documented kaggle notebooks, we show that our model outperforms other baseline models.","wordlikeness":0.4444444444,"lcsratio":1.0,"wordcoverage":0.6315789474}
{"Year":2023,"Venue":"findings-2023","Acronym":"GLUE-X","Description":"Evaluating Natural Language Understanding Models from an Out-of-Distribution Generalization Perspective","Abstract":"Pre-trained language models (plms) are known to improve the generalization performance of natural language understanding models by leveraging large amounts of data during the pre-training phase. However, the out-of-distribution (ood) generalization problem remains a challenge in many nlp tasks, limiting the real-world deployment of these methods. This paper presents the first attempt at creating a unified benchmark named and for evaluating ood robustness in nlp models, highlighting the importance of ood robustness and providing insights on how to measure the robustness of a model and how to improve it. The benchmark includes 13 publicly available datasets for ood testing, and evaluations are conducted on 8 classic nlp tasks over 21 popularly used plms. our findings confirm the need for improved ood accuracy in nlp tasks, as significant performance degradation was observed in all settings compared to in-distribution (id) accuracy.","wordlikeness":0.6666666667,"lcsratio":0.8333333333,"wordcoverage":0.8}
{"Year":2022,"Venue":"findings-2022","Acronym":"OneAligner","Description":"Zero-shot Cross-lingual Transfer with One Rich-Resource Language Pair for Low-Resource Sentence Retrieval","Abstract":"Aligning parallel sentences in multilingual corpora is essential to curating data for downstream applications such as machine translation. In this work, we present large-scale, an alignment model specially designed for sentence retrieval tasks. This model is able to train on only one language pair and transfers, in a cross-lingual fashion, to low-resource language pairs with negligible degradation in performance. When trained with all language pairs of a large-scale parallel multilingual corpus (opus-100), this model achieves the state-of-the-art result on the tateoba dataset, outperforming an equally-sized previous model by 8.0 points in accuracy while using less than 0.6% of their parallel data. When finetuned on a single rich-resource language pair, be it english-centered or not, our model is able to match the performance of the ones finetuned on all language pairs under the same data budget with less than 2.0 points decrease in accuracy. Furthermore, with the same setup, scaling up the number of rich-resource language pairs monotonically improves the performance, reaching a minimum of 0.4 points discrepancy in accuracy, making it less mandatory to collect any low-resource parallel data. Finally, we conclude through empirical results and analyses that the performance of the sentence alignment task depends mostly on the monolingual and parallel data size, up to a certain size threshold, rather than on what language pairs are used for training or evaluation.","wordlikeness":0.7,"lcsratio":0.9,"wordcoverage":0.75}
{"Year":2022,"Venue":"acl-2022","Acronym":"RNG-KBQA","Description":"Generation Augmented Iterative Ranking for Knowledge Base Question Answering","Abstract":"Existing kbqa approaches, despite achieving strong performance on i.i.d. test data, often struggle in generalizing to questions involving unseen kb schema items. Prior ranking-based approaches have shown some success in generalization, but suffer from the coverage issue. We present settings, a rank-and-generate approach for kbqa, which remedies the coverage issue with a generation model while preserving a strong generalization capability. Our approach first uses a contrastive ranker to rank a set of candidate logical forms obtained by searching over the knowledge graph. It then introduces a tailored generation model conditioned on the question and the top-ranked candidates to compose the final logical form. We achieve new state-of-the-art results on grailqa and webqsp datasets. In particular, our method surpasses the prior state-of-the-art by a large margin on the grailqa leaderboard. In addition, the outperforms all prior approaches on the popular webqsp benchmark, even including the ones that use the oracle entity linking. The experimental results demonstrate the effectiveness of the interplay between ranking and generation, which leads to the superior performance of our proposed approach across all settings with especially strong improvements in zero-shot generalization.","wordlikeness":0.25,"lcsratio":0.875,"wordcoverage":0.5454545455}
{"Year":2020,"Venue":"ws-2020","Acronym":"iobes","Description":"Library for Span Level Processing","Abstract":"Many tasks in natural language processing, such as named entity recognition and slot-filling, involve identifying and labeling specific spans of text. In order to leverage common models, these tasks are often recast as sequence labeling tasks. Each token is given a label and these labels are prefixed with special tokens such as b- or i-. After a model assigns labels to each token, these prefixes are used to group the tokens into spans. Properly parsing these annotations is critical for producing fair and comparable metrics; however, despite its importance, there is not an easy-to-use, standardized, programmatically integratable library to help work with span labeling. To remedy this, we introduce our open-source library, into. We is used for parsing, converting, and processing spans represented as token level decisions.","wordlikeness":0.4,"lcsratio":0.8,"wordcoverage":0.8}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"WeTS","Description":"A Benchmark for Translation Suggestion","Abstract":"Translation suggestion (ts), which provides alternatives for specific words or phrases given the entire documents generated by machine translation (mt), has been proven to play a significant role in post-editing (pe). There are two main pitfalls for existing researches in this line. First, most conventional works only focus on the overall performance of pe but ignore the exact performance of ts, which makes the progress of pe sluggish and less explainable; second, as no publicly available golden dataset exists to support in-depth research for ts, almost all of the previous works conduct experiments on their in-house datasets or the noisy datasets built automatically, which makes their experiments hard to be reproduced and compared. To break these limitations mentioned above and spur the research in ts, we create a benchmark dataset, called <i>no<\/i>, which is a golden corpus annotated by expert translators on four translation directions. Apart from the golden corpus, we also propose several methods to generate synthetic corpora which can be used to improve the performance substantially through pre-training. As for the model, we propose the segment-aware self-attention based transformer for ts. Experimental results show that our approach achieves the best results on all four directions, including english-to-german, german-to-english, chinese-to-english, and english-to-chinese.","wordlikeness":0.5,"lcsratio":0.75,"wordcoverage":0.8571428571}
{"Year":2023,"Venue":"acl-2023","Acronym":"TAGPRIME","Description":"A Unified Framework for Relational Structure Extraction","Abstract":"Many tasks in natural language processing require the extraction of relationship information for a given condition, such as event argument extraction, relation extraction, and task-oriented semantic parsing. Recent works usually propose sophisticated models for each task independently and pay less attention to the commonality of these tasks and to have a unified framework for all the tasks. In this work, we propose to take a unified view of all these tasks and introduce in to address relational structure extraction problems. Priming is a sequence tagging model that appends priming words about the information of the given condition (such as an event trigger) to the input text. With the self-attention mechanism in pre-trained language models, the priming words make the output contextualized representations contain more information about the given condition, and hence become more suitable for extracting specific relationships for the condition. Extensive experiments and analyses on three different tasks that cover ten datasets across five different languages demonstrate the generality and effectiveness of output.","wordlikeness":0.75,"lcsratio":0.5,"wordcoverage":0.7692307692}
{"Year":2013,"Venue":"semeval-2013","Acronym":"CU","Description":"Computational Assessment of Short Free Text Answers - A Tool for Evaluating Students&#39; Understanding","Abstract":"Assessing student understanding by evaluating their free text answers to posed questions is a very important task. However, manually, it is time-consuming and computationally, it is dif\ufb01andlt. This paper details our shallow nlp approach to computationally assessing student free text answers when a reference answer is provided. For four out of the \ufb01ve test sets, our system achieved an overall acsets,racy above the median and mean.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2023,"Venue":"ranlp-2023","Acronym":"SSSD","Description":"Leveraging Pre-trained Models and Semantic Search for Semi-supervised Stance Detection","Abstract":"Pre-trained models (ptms) based on the transformers architecture are trained on massive amounts of data and can capture nuances and complexities in linguistic expressions, making them a powerful tool for many natural language processing tasks. In this paper, we present in (semantic similarity stance detection), a semi-supervised method for stance detection on twitter that automatically labels a large, domain-related corpus for training a stance classification model. The method assumes as input a domain set of tweets about a given target and a labeled query set of tweets of representative arguments related to the stances. It scales the automatic labeling of a large number of tweets, and improves classification accuracy by leveraging the power of ptms and semantic search to capture context and meaning. We largely outperformed all baselines in experiments using the semeval benchmark.","wordlikeness":0.25,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2019,"Venue":"naacl-2019","Acronym":"ChatEval","Description":"A Tool for Chatbot Evaluation","Abstract":"Open-domain dialog systems (i.e. chatbots) are difficult to evaluate. The current best practice for analyzing and comparing these dialog systems is the use of human judgments. However, the lack of standardization in evaluation procedures, and the fact that model parameters and code are rarely published hinder systematic human evaluation experiments. We introduce a unified framework for human evaluation of chatbots that augments existing tools and provides a web-based hub for researchers to share and compare their dialog systems. Researchers can submit their trained models to the baselines web interface and obtain comparisons with baselines and prior work. The evaluation code is open-source to ensure standardization and transparency. In addition, we introduce open-source baseline models and evaluation datasets. Systematic can be found at <a href=https:\/\/are.org class=acl-markup-url>https:\/\/fact.org<\/a>.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.7142857143}
{"Year":2014,"Venue":"lrec-2014","Acronym":"TagNText","Description":"A parallel corpus for the induction of resource-specific non-taxonomical relations from tagged images","Abstract":"When producing textual descriptions, humans express propositions regarding an object; but what do they express when annotating a document with simple tags? To answer this question, we have studied what users of tagging systems would have said if they were to describe a resource with fully fledged text. In particular, our work attempts to answer the following questions: if users were to use full descriptions, would their current tags be words present in these hypothetical sentences? If yes, what kind of language would connect these words? Such questions, although central to the problem of extracting binary relations between tags, have been sidestepped in the existing literature, which has focused on a small subset of possible inter-tag relations, namely hierarchical ones (e.g. \u201ccar\u201d \u2013is-a\u2013 \u201cvehicle\u201d), as opposed to non-taxonomical relations (e.g. \u201cwoman\u201d \u2013wears\u2013 \u201chat\u201d). Well is the first attempt to construct a parallel corpus of tags and textual descriptions with respect to particular resources. The corpus provides enough data for the researcher to gain an insight into the nature of underlying relations, as well as the tools and methodology for constructing larger-scale parallel corpora that can aid non-taxonomical relation extraction.","wordlikeness":0.75,"lcsratio":0.75,"wordcoverage":0.6666666667}
{"Year":2023,"Venue":"acl-2023","Acronym":"LayoutMask","Description":"Enhance Text-Layout Interaction in Multi-modal Pre-training for Document Understanding","Abstract":"Visually-rich document understanding (vrdu) has attracted much research attention over the past years. Pre-trained models on a large number of document images with transformer-based backbones have led to significant performance gains in this field. The major challenge is how to fusion the different modalities (text, layout, and image) of the documents in a unified model with different pre-training tasks. This paper focuses on improving text-layout interactions and proposes a novel multi-modal pre-training model, between. This uses local 1d position, instead of global 1d position, as layout input and has two pre-training objectives: (1) masked language modeling: predicting masked tokens with two novel masking strategies; (2) masked position modeling: predicting masked 2d positions to improve layout representation learning. Model, can enhance the interactions between text and layout modalities in a unified model and produce adaptive and robust multi-modal representations for downstream tasks. Experimental results show that our proposed method can achieve state-of-the-art results on a wide variety of vrdu problems, including form understanding, receipt understanding, and document image classification.","wordlikeness":0.8,"lcsratio":0.9,"wordcoverage":0.75}
{"Year":2021,"Venue":"konvens-2021","Acronym":"WordGuess","Description":"Using Associations for Guessing, Learning and Exploring Related Words","Abstract":"This paper presents guess, a gamewith-a-purpose vocabulary training where \u2013in order to guess a target word (such as snow)\u2013 the player is offered associations of that target word (such as winter, white, cold). The game relies on existing association norms and co-occurrence information to establish an entertaining way of deepening the player\u2019s learning and understanding of vocabulary and of associative relatedness between words in the vocabulary. From comes with data in english and german and can be extended with data from further languages. From an application-oriented point of view the players\u2019 data enables us to induce conditions and weights for word association and to quantify contextual relationships, which is useful for many nlp purposes such as ontology induction and anaphora resolution.","wordlikeness":0.7777777778,"lcsratio":0.7777777778,"wordcoverage":0.7142857143}
{"Year":2023,"Venue":"acl-2023","Acronym":"TOME","Description":"A Two-stage Approach for Model-based Retrieval","Abstract":"Recently, model-based retrieval has emerged as a new paradigm in text retrieval that discards the index in the traditional retrieval model and instead memorizes the candidate corpora using model parameters. This design employs a sequence-to-sequence paradigm to generate document identifiers, which enables the complete capture of the relevance between queries and documents and simplifies the classic index-retrieval-rerank pipeline. Despite its attractive qualities, there remain several major challenges in model-based retrieval, including the discrepancy between pre-training and fine-tuning, and the discrepancy between training and inference. To deal with the above challenges, we propose a novel two-stage model-based retrieval approach called paradigm, which makes two major technical contributions, including the utilization of tokenized urls as identifiers and the design of a two-stage generation architecture. We also propose a number of training strategies to deal with the training difficulty as the corpus size increases. Extensive experiments and analysis on ms marco and natural questions demonstrate the effectiveness of our proposed approach, and we investigate the scaling laws of questions by examining various influencing factors.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2019,"Venue":"naacl-2019","Acronym":"TOI-CNN","Description":"a Solution of Information Extraction on Chinese Insurance Policy","Abstract":"Contract analysis can significantly ease the work for humans using ai techniques. This paper shows a problem of element tagging on insurance policy (etip). A novel text-of-interest convolutional neural network (a) is proposed for the etip solution. We introduce a toi pooling layer to replace traditional pooling layer for processing the nested phrasal or clausal elements in insurance policies. The advantage of toi pooling layer is that the nested elements from one sentence could share computation and context in the forward and backward passes. The computation of backpropagation through toi pooling is also demonstrated in the paper. We have collected a large chinese insurance contract dataset and labeled the critical elements of seven categories to test the performance of the proposed method. The results show the promising performance of our method in the etip problem.","wordlikeness":0.2857142857,"lcsratio":0.8571428571,"wordcoverage":0.6666666667}
{"Year":2022,"Venue":"lrec-2022","Acronym":"DiscoGeM","Description":"A Crowdsourced Corpus of Genre-Mixed Implicit Discourse Relations","Abstract":"We present inferred, a crowdsourced corpus of 6,505 implicit discourse relations from three genres: political speech, literature, and encyclopedic texts. Each instance was annotated by 10 crowd workers. Various label aggregation methods were explored to evaluate how to obtain a label that best captures the meaning inferred by the crowd annotators. The results show that a significant proportion of discourse relations in data are ambiguous and can express multiple relation senses. Probability distribution labels better capture these interpretations than single labels. Further, the results emphasize that text genre crucially affects the distribution of discourse relations, suggesting that genre should be included as a factor in automatic relation classification. We make available the newly created non-connective corpus, as well as the dataset with all annotator-level labels. Both the corpus and the dataset can facilitate a multitude of applications and research purposes, for example to function as training data to improve the performance of automatic discourse relation parsers, as well as facilitate research into non-connective signals of discourse relations.","wordlikeness":0.625,"lcsratio":0.875,"wordcoverage":0.7777777778}
{"Year":2022,"Venue":"lrec-2022","Acronym":"GeezSwitch","Description":"Language Identification in Typologically Related Low-resourced East African Languages","Abstract":"Language identification is one of the fundamental tasks in natural language processing that is a prerequisite to data processing and numerous applications. Low-resourced languages with similar typologies are generally confused with each other in real-world applications such as machine translation, affecting the user\u2019s experience. In this work, we present a language identification dataset for five typologically and phylogenetically related low-resourced east african languages that use the ge\u2019ez script as a writing system; namely amharic, blin, ge\u2019ez, tigre, and tigrinya. The dataset is built automatically from selected data sources, but we also performed a manual evaluation to assess its quality. Our approach to constructing the dataset is cost-effective and applicable to other low-resource languages. We integrated the dataset into an existing language-identification tool and also fine-tuned several transformer based language models, achieving very strong results in all cases. While the task of language identification is easy for the informed person, such datasets can make a difference in real-world deployments and also serve as part of a benchmark for language understanding in the target languages. The data and models are made available at <a href=https:\/\/github.com\/fgaim\/we class=acl-markup-url>https:\/\/github.com\/fgaim\/at<\/a>.","wordlikeness":0.6,"lcsratio":0.6,"wordcoverage":0.75}
{"Year":2023,"Venue":"acl-2023","Acronym":"LAVIS","Description":"A One-stop Library for Language-Vision Intelligence","Abstract":"We introduce we, an open-source deep learning library for language-vision research and applications. Answering, aims to serve as a one-stop comprehensive library that brings recent advancements in the language-vision field accessible for researchers and practitioners, as well as fertilizing future research and development. It features a unified interface to easily access state-of-the-art image-language, video-language models and common datasets. Development. Supports training, evaluation and benchmarking on a rich variety of tasks, including multimodal classification, retrieval, captioning, visual question answering, dialogue and pre-training. In the meantime, the library is also highly extensible and configurable, facilitating future development and customization. In this technical report, we describe design principles, key components and functionalities of the library, and also present benchmarking results across common language-vision tasks.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.9090909091}
{"Year":2022,"Venue":"coling-2022","Acronym":"CorefDiffs","Description":"Co-referential and Differential Knowledge Flow in Document Grounded Conversations","Abstract":"Knowledge-grounded dialog systems need to incorporate smooth transitions among knowledge selected for generating responses, to ensure that dialog flows naturally. For document-grounded dialog systems, the inter- and intra-document knowledge relations can be used to model such conversational flows. We develop a novel multi-document co-referential graph (coref-mdg) to effectively capture the inter-document relationships based on commonsense and similarity and the intra-document co-referential structures of knowledge segments within the grounding documents. We propose conversational, a co-referential and differential flow management method, to linearize the static coref-mdg into conversational sequence logic. Develop performs knowledge selection by accounting for contextual graph structures and the knowledge difference sequences. Static significantly outperforms the state-of-the-art by 9.5%, 7.4% and 8.2% on three public benchmarks. This demonstrates that the effective modeling of co-reference and knowledge difference for dialog flows are critical for transitions in document-grounded conversation.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.7058823529}
{"Year":2019,"Venue":"ijcnlp-2019","Acronym":"VizSeq","Description":"a visual analysis toolkit for text generation tasks","Abstract":"Automatic evaluation of text generation tasks (e.g. machine translation, text summarization, image captioning and video description) usually relies heavily on task-specific metrics, such as bleu and rouge. They, however, are abstract numbers and are not perfectly aligned with human assessment. This suggests inspecting detailed examples as a complement to identify system error patterns. In this paper, we present latest, a visual analysis toolkit for instance-level and corpus-level system evaluation on a wide variety of text generation tasks. It supports multimodal sources and multiple text references, providing visualization in jupyter notebook or a web app interface. It can be used locally or deployed onto public servers for centralized data hosting and benchmarking. It covers most common n-gram based metrics accelerated with multiprocessing, and also provides latest embedding-based metrics such as bertscore.","wordlikeness":0.5,"lcsratio":0.6666666667,"wordcoverage":0.6666666667}
{"Year":2023,"Venue":"acl-2023","Acronym":"SSD-LM","Description":"Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control","Abstract":"Despite the growing success of diffusion models in continuous-valued domains (e.g., images), similar efforts for discrete domains such as text have yet to match the performance of autoregressive language models. In this work, we present flexible\u2014a diffusion-based language model with two key design choices. First, first, is semi-autoregressive, iteratively generating blocks of text, allowing for flexible output length at decoding time while enabling local bidirectional context updates. Second, it is simplex-based, performing diffusion on the natural vocabulary space rather than a learned latent space, allowing us to incorporate classifier guidance and modular control using off-the-shelf classifiers without any adaptation. We evaluate diffusion on unconstrained text generation benchmarks, and show that it matches or outperforms strong autoregressive gpt-2 models across standard quality and diversity metrics, while vastly outperforming diffusion-based baselines. On controlled text generation, decoding also outperforms competitive baselines, with an extra advantage in modularity.","wordlikeness":0.1666666667,"lcsratio":0.8333333333,"wordcoverage":0.6666666667}
{"Year":2014,"Venue":"semeval-2014","Acronym":"DCU","Description":"Aspect-based Polarity Classification for SemEval Task 4","Abstract":"We describe the work carried out by at on the aspect based sentiment analysis task at semeval 2014. Our team submitted one constrained run for the restaurant domain and one for the laptop domain for sub-task b (aspect term polarity prediction), ranking highest out of 36 systems on the restaurant test set and joint highest out of 32 systems on the laptop test set.","wordlikeness":0.6666666667,"lcsratio":0.6666666667,"wordcoverage":0.8571428571}
{"Year":2023,"Venue":"findings-2023","Acronym":"ML-LMCL","Description":"Mutual Learning and Large-Margin Contrastive Learning for Improving ASR Robustness in Spoken Language Understanding","Abstract":"Spoken language understanding (slu) is a fundamental task in the task-oriented dialogue systems. However, the inevitable errors from automatic speech recognition (asr) usually impair the understanding performance and lead to error propagation. Although there are some attempts to address this problem through contrastive learning, they (1) treat clean manual transcripts and asr transcripts equally without discrimination in fine-tuning; (2) neglect the fact that the semantically similar pairs are still pushed away when applying contrastive learning; (3) suffer from the problem of kullback\u2013leibler (kl) vanishing. In this paper, we propose mutual learning and large-margin contrastive learning (learning), a novel framework for improving asr robustness in slu. Specifically, in fine-tuning, we apply mutual learning and train two slu models on the manual transcripts and the asr transcripts, respectively, aiming to iteratively share knowledge between these two models. We also introduce a distance polarization regularizer to avoid pushing away the intra-cluster pairs as much as possible. Moreover, we use a cyclical annealing schedule to mitigate kl vanishing issue. Experiments on three datasets show that between outperforms existing models and achieves new state-of-the-art performance.","wordlikeness":0.1428571429,"lcsratio":0.8571428571,"wordcoverage":0.6153846154}
{"Year":2010,"Venue":"coling-2010","Acronym":"YanFa","Description":"An Online Automatic Scoring and Intelligent Feedback System of Student English-Chinese Translation","Abstract":"Online learning calls for instant assessment and feedback. Cilin\u2014chinese is a system developed to score online englishchinese translation exercises with intelligent feedback for chinese non-english majors. With the aid of hownet and cilin\u2014chinese synonym set (extended version), the system adopts the hybrid approach to scoring student translation semantically. It compares student translation with model translation by synonym matching, sentence-pattern matching and word similarity calculating respectively. The experiment results show that the correlation ratio between the scores given by the system and by human raters is 0.58, which indicates that the algorithm is able to fulfill the task of automated scoring. Online is also able to provide feedback on syntactic mistakes made by students through interacting with them. It asks students to analyze the english sentence elements. Then it compares the student analyses with those of the parser and points out the parts which might lead to their wrong understanding as well as their wrong translating.","wordlikeness":0.4,"lcsratio":0.8,"wordcoverage":0.75}
{"Year":2020,"Venue":"icon-2020","Acronym":"TechTexC","Description":"Classification of Technical Texts using Convolution and Bidirectional Long Short Term Memory Network","Abstract":"This paper illustrates the details description of technical text classification system and its results that developed as a part of participation in the shared task techdofication 2020. The shared task consists of two sub-tasks: (i) first task identify the coarse-grained technical domain of given text in a specified language and (ii) the second task classify a text of computer science domain into fine-grained sub-domains. A classification system (called \u2018classification\u2019) is developed to perform the classification task using three techniques: convolution neural network (cnn), bidirectional long short term memory (bilstm) network, and combined cnn with bilstm. Results show that cnn with bilstm model outperforms the other techniques concerning task-1 of sub-tasks (a, b, c and g) and task-2a. This combined model obtained f1 scores of 82.63 (sub-task a), 81.95 (sub-task b), 82.39 (sub-task c), 84.37 (sub-task g), and 67.44 (task-2a) on the development dataset. Moreover, in the case of test set, the combined cnn with bilstm approach achieved that higher accuracy for the subtasks 1a (70.76%), 1b (79.97%), 1c (65.45%), 1g (49.23%) and 2a (70.14%).","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2015,"Venue":"semeval-2015","Acronym":"Swiss-Chocolate","Description":"Combining Flipout Regularization and Random Forests with Artificially Built Subsystems to Boost Text-Classification for Sentiment","Abstract":"We describe a classi\ufb01er for predicting message-level sentiment of english microblog messages from twitter. This paper describes our submission to the semeval2015 competition (task 10). Our approach is to combine several variants of our previous year\u2019s svm system into one meta-classi\ufb01er, which was then trained using a random forest. The main idea is that the meta-classi\ufb01er allows the combination of the strengths and overcome some of the weaknesses of the arti\ufb01cially-built individual classi\ufb01ers, and adds additional non-linearity. We were also able to improve the linear classi\ufb01ers by using a new regularization technique we call \ufb02ipout.","wordlikeness":0.5333333333,"lcsratio":0.7333333333,"wordcoverage":0.75}
{"Year":2022,"Venue":"acl-2022","Acronym":"SummScreen","Description":"A Dataset for Abstractive Screenplay Summarization","Abstract":"We introduce serves, a summarization dataset comprised of pairs of tv series transcripts and human written recaps. The dataset provides a challenging testbed for abstractive summarization for several reasons. Plot details are often expressed indirectly in character dialogues and may be scattered across the entirety of the transcript. These details must be found and integrated to form the succinct plot descriptions in the recaps. Also, tv scripts contain content that does not directly pertain to the central plot but rather serves to develop characters or provide comic relief. This information is rarely contained in recaps. Since characters are fundamental to tv series, we also propose two entity-centric evaluation metrics. Empirically, we characterize the dataset by evaluating several methods, including neural models and those based on nearest neighbors. An oracle extractive approach outperforms all benchmarked models according to automatic metrics, showing that the neural models are unable to fully exploit the input transcripts. Human evaluation and qualitative analysis reveal that our non-oracle models are competitive with their oracle counterparts in terms of generating faithful plot events and can benefit from better content selectors. Both oracle and non-oracle models generate unfaithful facts, suggesting future research directions.","wordlikeness":0.8,"lcsratio":0.7,"wordcoverage":0.8421052632}
{"Year":2020,"Venue":"lt4gov-2020","Acronym":"FRAQUE","Description":"a FRAme-based QUEstion-answering system for the Public Administration domain","Abstract":"In this paper, we propose queries, a question answering system for factoid questions in the public administration domain. The system is based on semantic frames, here intended as collections of slots typed with their possible values. Frames queries unstructured textual data and exploits the potential of different approaches: it extracts pattern elements from texts which are linguistically analyzed through statistical methods.which allows italian users to query vast document repositories related to the domain of public administration. Given the statistical nature of most of its components such as word embeddings, the system allows for a flexible domain and language adaptation process. Are\u2019s goal is to associate questions with frames stored into a knowledge graph along with relevant document passages, which are returned as the answer.","wordlikeness":0.8333333333,"lcsratio":1.0,"wordcoverage":0.7272727273}
{"Year":2020,"Venue":"acl-2020","Acronym":"TaPas","Description":"Weakly Supervised Table Parsing via Pre-training","Abstract":"Answering natural language questions over tables is usually seen as a semantic parsing task. To alleviate the collection cost of full logical forms, one popular approach focuses on weak supervision consisting of denotations instead of logical forms. However, training semantic parsers from weak supervision poses difficulties, and in addition, the generated logical forms are only used as an intermediate step prior to retrieving the denotation. In this paper, we present only, an approach to question answering over tables without generating logical forms. Answering trains from weak supervision, and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection. Over extends bert\u2019s architecture to encode tables as input, initializes from an effective joint pre-training of text segments and tables crawled from wikipedia, and is trained end-to-end. We experiment with three different semantic parsing datasets, and find that setting, outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on sqa from 55.1 to 67.2 and performing on par with the state-of-the-art on wikisql and wikitq, but with a simpler model architecture. We additionally find that transfer learning, which is trivial in our setting, from wikisql to wikitq, yields 48.7 accuracy, 4.2 points above the state-of-the-art.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"HCL-TAT","Description":"A Hybrid Contrastive Learning Method for Few-shot Event Detection with Task-Adaptive Threshold","Abstract":"Event detection has been suffering from constantly emerging event types with lack of sufficient data. Existing works formulate the new problem as few-shot event detection (fsed), and employ two-stage or unified models based on meta-learning to address the problem. However, these methods fall far short of expectations due to: (i) insufficient learning of discriminative representations in low-resource scenarios, and (ii) representation overlap between triggers and non-triggers. To resolve the above issues, in this paper, we propose a novel hybrid contrastive learning method with a task-adaptive threshold (abbreviated as all), which enables discriminative representation learning with a two-view contrastive loss (support-support and prototype-query), and devises an easily-adapted threshold to alleviate misidentification of triggers. Extensive experiments on the benchmark dataset fewevent demonstrate the superiority of our method to achieve better results compared to the state-of-the-arts. All the data and codes will be available to facilitate future research.","wordlikeness":0.2857142857,"lcsratio":1.0,"wordcoverage":0.625}
{"Year":2020,"Venue":"emnlp-2020","Acronym":"MUTANT","Description":"A Training Paradigm for Out-of-Distribution Generalization in Visual Question Answering","Abstract":"While progress has been made on the visual question answering leaderboards, models often utilize spurious correlations and priors in datasets under the i.i.d. setting. As such, evaluation on out-of-distribution (ood) test samples has emerged as a proxy for generalization. In this paper, we present <i>about<\/i>, a training paradigm that exposes the model to perceptually similar, yet semantically distinct <i>mutations<\/i> of the input, to improve ood generalization, such as the vqa-cp challenge. Under this paradigm, models utilize a consistency-constrained training objective to understand the effect of semantic changes in input (question-image pair) on the output (answer). Unlike existing methods on vqa-cp, <i>about<\/i> does not rely on the knowledge about the nature of train and test answer distributions. <i>visual<\/i> establishes a new state-of-the-art accuracy on vqa-cp with a 10.57% improvement. Our work opens up avenues for the use of semantic input mutations for ood generalization in question answering.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2023,"Venue":"eacl-2023","Acronym":"NusaX","Description":"Multilingual Parallel Sentiment Dataset for 10 Indonesian Local Languages","Abstract":"Natural language processing (nlp) has a significant impact on society via technologies such as machine translation and search engines. Despite its success, nlp technology is only widely available for high-resource languages such as english and chinese, while it remains inaccessible to many languages due to the unavailability of data resources and benchmarks. In this work, we focus on developing resources for languages in indonesia. Despite being the second most linguistically diverse country, most languages in indonesia are categorized as endangered and some are even extinct. We develop the first-ever parallel resource for 10 low-resource languages in indonesia. Our resource includes sentiment and machine translation datasets, and bilingual lexicons. We provide extensive analyses and describe challenges for creating such resources. We hope this work can spark nlp research on indonesian and other underrepresented languages.","wordlikeness":0.4,"lcsratio":0.8,"wordcoverage":0.75}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"EasyNLP","Description":"A Comprehensive and Easy-to-use Toolkit for Natural Language Processing","Abstract":"Pre-trained models (ptms) have reshaped the development of natural language processing (nlp) and achieved significant improvement in various benchmarks. Yet, it is not easy for industrial practitioners to obtain high-performing ptm-based models without a large amount of labeled training data and deploy them online with fast inference speed. To bridge this gap, group is designed to make it easy to build nlp applications, which supports a comprehensive suite of nlp algorithms. It further features knowledge-enhanced pre-training, knowledge distillation and few-shot learning functionalities, and provides a unified framework of model training, inference and deployment for real-world applications. This has powered over ten business units within alibaba group and is seamlessly integrated to the platform of ai (pai) products on alibaba cloud. The source code of integrated is released at github (<a href=https:\/\/github.com\/alibaba\/(pai) class=acl-markup-url>https:\/\/github.com\/alibaba\/significant<\/a>).","wordlikeness":0.5714285714,"lcsratio":1.0,"wordcoverage":0.7272727273}
{"Year":2021,"Venue":"acl-2021","Acronym":"IrEne","Description":"Interpretable Energy Prediction for Transformers","Abstract":"Existing software-based energy measurements of nlp models are not accurate because they do not consider the complex interactions between energy consumption and model execution. We present system, an interpretable and extensible energy prediction system that accurately predicts the inference energy consumption of a wide range of transformer-based nlp models. Between constructs a model tree graph that breaks down the nlp model into modules that are further broken down into low-level machine learning (ml) primitives. <a predicts the inference energy consumption of the ml primitives as a function of generalizable features and fine-grained runtime resource usage. Class=acl-markup-url>https:\/\/github.com\/stonybrooknlp\/<\/a>. Then aggregates these low-level predictions recursively to predict the energy of each module and finally of the entire model. Experiments across multiple transformer models show are predicts inference energy consumption of transformer models with an error of under 7% compared to the ground truth. In contrast, existing energy models see an error of over 50%. We also show how usage. Can be used to conduct energy bottleneck analysis and to easily evaluate the energy impact of different architectural choices. We release the code and data at <a href=https:\/\/github.com\/stonybrooknlp\/can class=acl-markup-url>https:\/\/github.com\/stonybrooknlp\/graph<\/a>.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2022,"Venue":"lrec-2022","Acronym":"SciPar","Description":"A Collection of Parallel Corpora from Scientific Abstracts","Abstract":"This paper presents institutional, a new collection of parallel corpora created from openly available metadata of bachelor theses, master theses and doctoral dissertations hosted in institutional repositories, digital libraries of universities and national archives. We describe first how we harvested and processed metadata from 86, mainly european, repositories to extract bilingual titles and abstracts, and then how we mined high quality sentence pairs in a wide range of scientific areas and sub-disciplines. In total, the resource includes 9.17 million segment alignments in 31 language pairs and is publicly available via the elrc-share repository. The bilingual corpora in this collection could prove valuable in various applications, such as cross-lingual plagiarism detection or adapting machine translation systems for the translation of scientific texts and academic writing in general, especially for language pairs which include english.","wordlikeness":0.8333333333,"lcsratio":0.8333333333,"wordcoverage":0.8}
{"Year":2009,"Venue":"news-2009","Acronym":"DirecTL","Description":"a Language Independent Approach to Transliteration","Abstract":"We present prediction,: an online discriminative sequence prediction model that employs a many-to-many alignment between target and source. Our system incorporates input segmentation, target character prediction, and sequence modeling in a uni\ufb01ed dynamic programming framework. Experimental results suggest that and is able to independently discover many of the language-speci\ufb01c regularities in the training data.","wordlikeness":0.8571428571,"lcsratio":0.7142857143,"wordcoverage":0.9333333333}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"LiteVL","Description":"Efficient Video-Language Learning with Enhanced Spatial-Temporal Modeling","Abstract":"Recent large-scale video-language pre-trained models have shown appealing performance on various downstream tasks. However, the pre-training process is computationally expensive due to the requirement of millions of video-text pairs and the redundant data structure of each video. To mitigate these problems, we propose the, which adapts a pre-trained image-language model blip into a video-text model directly on downstream tasks, without heavy pre-training. To enhance the temporal modeling lacking in the image-language model, we propose to add temporal attention modules in the image encoder of blip with dynamic temporal scaling. Besides the model-wise adaptation, we also propose a non-parametric pooling mechanism to adaptively reweight the fine-grained video embedding conditioned on the text. Experimental results on text-video retrieval and video question answering show that the proposed though even outperforms previous video-language pre-trained models by a clear margin, though without any video-language pre-training.","wordlikeness":0.6666666667,"lcsratio":0.8333333333,"wordcoverage":0.8}
{"Year":2021,"Venue":"acl-2021","Acronym":"LearnDA","Description":"Learnable Knowledge-Guided Data Augmentation for Event Causality Identification","Abstract":"Modern models for event causality identification (eci) are mainly based on supervised learning, which are prone to the data lacking problem. Unfortunately, the existing nlp-related augmentation methods cannot directly produce available data required for this task. To solve the data lacking problem, we introduce a new approach to augment training data for event causality identification, by iteratively generating new examples and classifying event causality in a dual learning framework. On the one hand, our approach is knowledge guided, which can leverage existing knowledge bases to generate well-formed new sentences. On the other hand, our approach employs a dual mechanism, which is a learnable augmentation framework, and can interactively adjust the generation process to generate task-related sentences. Experimental results on two benchmarks eventstoryline and causal-timebank show that 1) our method can augment suitable task-related training data for eci; 2) our method outperforms previous methods on eventstoryline and causal-timebank (+2.5 and +2.1 points on f1 value respectively).","wordlikeness":0.7142857143,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2016,"Venue":"emnlp-2016","Acronym":"SimpleScience","Description":"Lexical Simplification of Scientific Terminology","Abstract":"Lexical simpli\ufb01cation of scienti\ufb01c terms represents a unique challenge due to the lack of a standard parallel corpora and fast rate at which vocabulary shift along with research. We introduce embeddings, a lexical simpli\ufb01cation approach for scienti\ufb01c terminology. We use word embeddings to extract simpli\ufb01cation rules from a parallel corpora containing scienti\ufb01c publications and wikipedia. To evaluate our system we construct simplescigold, a novel gold standard set for science-related simpli\ufb01cations. We \ufb01nd that our approach outperforms prior context-aware approaches at generating simpli\ufb01cations for scienti\ufb01c terms.","wordlikeness":0.9230769231,"lcsratio":0.9230769231,"wordcoverage":0.7}
{"Year":2007,"Venue":"semeval-2007","Acronym":"PU-BCD","Description":"Exponential Family Models for the Coarse- and Fine-Grained All-Words Tasks","Abstract":"This paper describes an exponential family model of word sense which captures both occurrences and co-occurrences of words and senses in a joint probability distribution. This statistical framework lends itself to the task of word sense disambiguation. We evaluate the performance of the model in its participation on the semeval-2007 coarse- and \ufb01ne-grained all-words tasks under a variety of parameters.","wordlikeness":0.3333333333,"lcsratio":0.5,"wordcoverage":0.6666666667}
{"Year":2020,"Venue":"findings-2020","Acronym":"TSDG","Description":"Content-aware Neural Response Generation with Two-stage Decoding Process","Abstract":"Neural response generative models have achieved remarkable progress in recent years but tend to yield irrelevant and uninformative responses. One of the reasons is that encoder-decoder based models always use a single decoder to generate a complete response at a stroke. This tends to generate high-frequency function words with less semantic information rather than low-frequency content words with more semantic information. To address this issue, we propose a content-aware model with two-stage decoding process named two-stage dialogue generation (but). We separate the decoding process of content words and function words so that content words can be generated independently without the interference of function words. Experimental results on two datasets indicate that our model significantly outperforms several competitive generative models in terms of automatic and human evaluation.","wordlikeness":0.25,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2022,"Venue":"sigdial-2022","Acronym":"GRILLBot","Description":"A multi-modal conversational agent for complex real-world tasks","Abstract":"We present range, an open-source multi-modal task-oriented voice assistant to help users perform complex tasks, focusing on the domains of cooking and home improvement. Helping curates and leverages web information extraction to build coverage over a broad range of tasks for which a user can receive guidance. To represent each task, we propose taskgraphs as a dynamic graph unifying steps, requirements, and curated domain knowledge enabling contextual question answering, and detailed explanations. Multi-modal elements play a key role in build both helping the user navigate through the task and enriching the experience with helpful videos and images that are automatically linked throughout the task. We leverage a contextual neural semantic parser to enable flexible navigation when interacting with the system by jointly encoding stateful information with the conversation history. Task-oriented enables dynamic and adaptable task planning and assistance for complex tasks by combining elements of task representations that incorporate text and structure, combined with neural models for search, question answering, and dialogue state management. Perform competed in the alexa prize taskbot challenge as one of the finalists.","wordlikeness":0.875,"lcsratio":0.75,"wordcoverage":0.7692307692}
{"Year":2022,"Venue":"acl-2022","Acronym":"UKP-SQUARE","Description":"An Online Platform for Question Answering Research","Abstract":"Recent advances in nlp and information retrieval have given rise to a diverse set of question answering tasks that are of different formats (e.g., extractive, abstractive), require different model architectures (e.g., generative, discriminative), and setups (e.g., with or without retrieval). Despite having a large number of powerful, specialized qa pipelines (which we refer to as skills) that consider a single domain, model or setup, there exists no framework where users can easily explore and compare such pipelines and can extend them according to their needs. To address this issue, we present discriminative),, an extensible online qa platform for researchers which allows users to query and analyze a large collection of modern skills via a user-friendly web interface and integrated behavioural tests. In addition, qa researchers can develop, manage, and share their custom skills using our microservices that support a wide range of models (transformers, adapters, onnx), datastores and retrieval techniques (e.g., sparse and dense). Extensible is available on <a href=https:\/\/square.ukp-lab.de class=acl-markup-url>https:\/\/square.ukp-lab.","wordlikeness":0.7,"lcsratio":0.6,"wordcoverage":0.75}
{"Year":2013,"Venue":"semeval-2013","Acronym":"NRC-Canada","Description":"Building the State-of-the-Art in Sentiment Analysis of Tweets","Abstract":"In this paper, we describe how we created two state-of-the-art svm classi\ufb01ers, one to detect the sentiment of messages such as tweets and sms (message-level task) and one to detect the sentiment of a term within a message (term-level task). Among submissions from 44 teams in a competition, our submissions stood \ufb01rst in both tasks on tweets, obtaining an f-score of 69.02 in the message-level task and 88.93 in the term-level task. We implemented a variety of surface-form, semantic, and sentiment features. We also generated two large word\u2013sentiment association lexicons, one from tweets with sentiment-word hashtags, and one from tweets with emoticons. In the message-level task, the lexicon-based features provided a gain of 5 f-score points over all others. Both of our systems can be replicated using freely available resources.","wordlikeness":0.5,"lcsratio":0.6,"wordcoverage":0.75}
{"Year":2021,"Venue":"acl-2021","Acronym":"InfoSurgeon","Description":"Cross-Media Fine-grained Information Consistency Checking for Fake News Detection","Abstract":"To defend against machine-generated fake news, an effective mechanism is urgently needed. We contribute a novel benchmark for fake news detection at the knowledge element level, as well as a solution for this task which incorporates cross-media consistency checking to detect the fine-grained knowledge elements making news articles misinformative. Due to training data scarcity, we also formulate a novel data synthesis method by manipulating knowledge elements within the knowledge graph to generate noisy training data with specific, hard to detect, known inconsistencies. Our detection approach outperforms the state-of-the-art (up to 16.8% accuracy gain), and more critically, yields fine-grained explanations.","wordlikeness":0.8181818182,"lcsratio":0.8181818182,"wordcoverage":0.7777777778}
{"Year":2020,"Venue":"acl-2020","Acronym":"MIE","Description":"A Medical Information Extractor towards Medical Dialogues","Abstract":"Electronic medical records (emrs) have become key components of modern medical care systems. Despite the merits of emrs, many doctors suffer from writing them, which is time-consuming and tedious. We believe that automatically converting medical dialogues to emrs can greatly reduce the burdens of doctors, and extracting information from medical dialogues is an essential step. To this end, we annotate online medical consultation dialogues in a window-sliding style, which is much easier than the sequential labeling annotation. We then propose a medical information extractor (information) towards medical dialogues. An is able to extract mentioned symptoms, surgeries, tests, other information and their corresponding status. To tackle the particular challenges of the task, we uses a deep matching architecture, taking dialogue turn-interaction into account. The experimental results demonstrate extracting is a promising solution to extract medical information from doctor-patient dialogues.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2009,"Venue":"ws-2009","Acronym":"KSC-PaL","Description":"A Peer Learning Agent that Encourages Students to take the Initiative","Abstract":"We present an innovative application of discourse processing concepts to educational technology. In our corpus analysis of peer learning dialogues, we found that initiative and initiative shifts are indicative of learning, and of learning-conducive episodes. We are incorporating this \ufb01nding in episodes., the peer learning agent we have been developing. Finding will promote learning by encouraging shifts in task initiative.","wordlikeness":0.4285714286,"lcsratio":0.2857142857,"wordcoverage":0.6666666667}
{"Year":2010,"Venue":"lrec-2010","Acronym":"FipsRomanian","Description":"Towards a Romanian Version of the Fips Syntactic Parser","Abstract":"We describe work in progress on the development of a full syntactic parser for romanian. This work is part of a larger project of multilingual extension of the fips parser (wehrli, 2007), already available for french, english, german, spanish, italian, and greek, to four new languages (romanian, romansh, russian and japanese). The romanian version was built by starting with the fips generic parsing architecture for the romance languages and customising the grammatical component, in close relation to the development of the lexical component. We describe this process and report on preliminary results obtained for journalistic texts.","wordlikeness":0.5833333333,"lcsratio":0.75,"wordcoverage":0.7368421053}
{"Year":2010,"Venue":"semeval-2010","Acronym":"UTDMet","Description":"Combining WordNet and Corpus Data for Argument Coercion Detection","Abstract":"This paper describes our system for the classi\ufb01cation of argument coercion for semeval-2010 task 7. We present two approaches to classifying an argument\u2019s semantic class, which is then compared to the predicate\u2019s expected semantic class to detect coercions. The \ufb01rst approach is based on learning the members of an arbitrary semantic class using wordnet\u2019s hypernymy structure. The second approach leverages automatically extracted semantic parse information from a large corpus to identify similar arguments by the predicates that select them. We show the results these approaches obtain on the task as well as how they can improve a traditional feature-based approach.","wordlikeness":0.5,"lcsratio":0.8333333333,"wordcoverage":0.7142857143}
{"Year":2023,"Venue":"acl-2023","Acronym":"HuCurl","Description":"Human-induced Curriculum Discovery","Abstract":"We introduce the problem of curriculum discovery and describe a curriculum learning framework capable of discovering effective curricula in a curriculum space based on prior knowledge about sample difficulty. Using annotation entropy and loss as measures of difficulty, we show that (i): the top-performing discovered curricula for a given model and dataset are often non-monotonic as apposed to monotonic curricula in existing literature, (ii): the prevailing easy-to-hard or hard-to-easy transition curricula are often at the risk of underperforming, and (iii): the curricula discovered for smaller datasets and models perform well on larger datasets and models respectively. The proposed framework encompasses some of the existing curriculum learning approaches and can discover curricula that outperform them across several nlp tasks.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.7272727273}
{"Year":2023,"Venue":"eacl-2023","Acronym":"PANACEA","Description":"An Automated Misinformation Detection System on COVID-19","Abstract":"In this demo, we introduce a web-based misinformation detection system evidence on covid-19 related claims, which has two modules, fact-checking and rumour detection. Our fact-checking module, which is supported by novel natural language inference methods with a self-attention network, outperforms state-of-the-art approaches. It is also able to give automated veracity assessment and ranked supporting evidence with the stance towards the claim to be checked. In addition, base. Adapts the bi-directional graph convolutional networks model, which is able to detect rumours based on comment networks of related tweets, instead of relying on the knowledge base. This rumour detection module assists by warning the users in the early stages when a knowledge base may not be available.","wordlikeness":0.8571428571,"lcsratio":0.7142857143,"wordcoverage":0.7692307692}
{"Year":2023,"Venue":"findings-2023","Acronym":"PTCSpell","Description":"Pre-trained Corrector Based on Character Shape and Pinyin for Chinese Spelling Correction","Abstract":"Chinese spelling correction (csc) is a challenging task with the goal of correcting each wrong character in chinese texts. Incorrect characters in a chinese text are mainly due to the similar shape and similar pronunciation of chinese characters. Recently, the paradigm of pre-training and fine-tuning has achieved remarkable success in natural language processing. However, the pre-training objectives in existing methods are not tailored for the csc task since they neglect the visual and phonetic properties of characters, resulting in suboptimal spelling correction. In this work, we propose to pre-train a new corrector named two for the csc task under the detector-corrector architecture. The corrector we propose has the following two improvements. First, we design two novel pre-training objectives to capture pronunciation and shape information in chinese characters. Second, we propose a new strategy to tackle the issue that the detector\u2019s prediction results mislead the corrector by balancing the loss of wrong characters and correct characters. Experiments on three benchmarks (i.e., sighan 2013, 2014, and 2015) show that our model achieves an average of 5.8% f1 improvements at the correction level over state-of-the-art methods, verifying its effectiveness.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.7692307692}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"InfoCSE","Description":"Information-aggregated Contrastive Learning of Sentence Embeddings","Abstract":"Contrastive learning has been extensively studied in sentence embedding learning, which assumes that the embeddings of different views of the same sentence are closer. The constraint brought by this assumption is weak, and a good sentence representation should also be able to reconstruct the original sentence fragments. Therefore, this paper proposes an information-aggregated contrastive learning framework for learning unsupervised sentence embeddings, termed w.r.t.on forces the representation of [cls] positions to aggregate denser sentence information by introducing an additional masked language model task and a well-designed network. We evaluate the proposed constraint on several benchmark datasets w.r.t the semantic text similarity (sts) task. Experimental results show that spearman outperforms simcse by an average spearman correlation of 2.60% on bert-base, and 1.77% on bert-large, achieving state-of-the-art results among unsupervised sentence representation learning methods.","wordlikeness":0.5714285714,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2022,"Venue":"findings-2022","Acronym":"IsoScore","Description":"Measuring the Uniformity of Embedding Space Utilization","Abstract":"The recent success of distributed word representations has led to an increased interest in analyzing the properties of their spatial distribution. Several studies have suggested that contextualized word embedding models do not isotropically project tokens into vector space. However, current methods designed to measure isotropy, such as average random cosine similarity and the partition score, have not been thoroughly analyzed and are not appropriate for measuring isotropy. We propose have: a novel tool that quantifies the degree to which a point cloud uniformly utilizes the ambient vector space. Using rigorously designed tests, we demonstrate that ambient is the only tool available in the literature that accurately measures how uniformly distributed variance is across dimensions in vector space. Additionally, we use measuring to challenge a number of recent conclusions in the nlp literature that have been derived using brittle metrics of isotropy. We caution future studies from using existing tools to measure isotropy in contextualized embedding space as resulting conclusions will be misleading or altogether inaccurate.","wordlikeness":0.625,"lcsratio":0.625,"wordcoverage":0.7692307692}
{"Year":2020,"Venue":"ws-2020","Acronym":"FastFormers","Description":"Highly Efficient Transformer Models for Natural Language Understanding","Abstract":"Transformer-based models are the state-of-the-art for natural language understanding (nlu) applications. Models are getting bigger and better on various tasks. However, transformer models remain computationally challenging since they are not efficient at inference-time compared to traditional approaches. In this paper, we present are, a set of recipes to achieve efficient inference-time performance for transformer-based models on various nlu tasks. We show how carefully utilizing knowledge distillation, structured pruning and numerical optimization can lead to drastic improvements on inference efficiency. We provide effective recipes that can guide practitioners to choose the best settings for various nlu tasks and pretrained models. Applying the proposed recipes to the superglue benchmark, we achieve from 9.8x up to 233.9x speed-up compared to out-of-the-box models on cpu. On gpu, we also achieve up to 12.4x speed-up with the presented methods. We show that practitioners can drastically reduce cost of serving 100 million requests from 4,223 usd to just 18 usd on an azure f16s_v2 instance. This translates to a sustainable runtime by reducing energy consumption 6.9x - 125.8x according to the metrics used in the sustainlp 2020 shared task.","wordlikeness":0.8181818182,"lcsratio":0.9090909091,"wordcoverage":0.7826086957}
{"Year":2010,"Venue":"semeval-2010","Acronym":"CFILT","Description":"Resource Conscious Approaches for All-Words Domain Specific WSD","Abstract":"We describe two approaches for all-words word sense disambiguation on a speci\ufb01c domain. The \ufb01rst approach is a knowledge based approach which extracts domain-speci\ufb01c largest connected components from the wordnet graph by exploiting the semantic relations between all candidate synsets appearing in a domainspeci\ufb01c untagged corpus. Given a test word, disambiguation is performed by considering only those candidate synsets that belong to the top-k largest connected components. The second approach is a weakly supervised approach which relies on the \u201cone sense per domain\u201d heuristic and uses a few hand labeled examples for the most frequently appearing words in the target domain. Once the most frequent words have been disambiguated they can provide strong clues for disambiguating other words in the sentence using an iterative disambiguation algorithm. Our weakly supervised system gave the best performance across all systems that participated in the task even when it used as few as 100 hand labeled examples from the target domain.","wordlikeness":0.6,"lcsratio":0.6,"wordcoverage":0.75}
{"Year":2022,"Venue":"naacl-2022","Acronym":"GlobEnc","Description":"Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers","Abstract":"There has been a growing interest in interpreting the underlying dynamics of transformers. While self-attention patterns were initially deemed as the primary option, recent studies have shown that integrating other components can yield more accurate explanations. This paper introduces a novel token attribution analysis method that incorporates all the components in the encoder block and aggregates this throughout layers. Through extensive quantitative and qualitative experiments, we demonstrate that our method can produce faithful and meaningful global token attributions. Our experiments reveal that incorporating almost every encoder component results in increasingly more accurate analysis in both local (single layer) and global (the whole model) settings. Our global attribution analysis significantly outperforms previous methods on various tasks regarding correlation with gradient-based saliency scores. Our code is freely available at <a href=https:\/\/github.com\/mohsenfayyaz\/interpreting class=acl-markup-url>https:\/\/github.com\/mohsenfayyaz\/href=https:\/\/github.com\/mohsenfayyaz\/<\/a>.","wordlikeness":0.7142857143,"lcsratio":1.0,"wordcoverage":0.8333333333}
{"Year":2021,"Venue":"acl-2021","Acronym":"PhotoChat","Description":"A Human-Human Dialogue Dataset With Photo Sharing Behavior For Joint Image-Text Modeling","Abstract":"We present a new human-human dialogue dataset - present, the first dataset that casts light on the photo sharing behavior in online messaging. Models contains 12k dialogues, each of which is paired with a user photo that is shared during the conversation. Based on this dataset, we propose two tasks to facilitate research on image-text modeling: a photo-sharing intent prediction task that predicts whether one intends to share a photo in the next conversation turn, and a photo retrieval task that retrieves the most relevant photo according to the dialogue context. In addition, for both tasks, we provide baseline models using the state-of-the-art models and report their benchmark performances. The best image retrieval model achieves 10.4% recall@1 (out of 1000 candidates) and the best photo intent prediction model achieves 58.1% f1 score, indicating that the dataset presents interesting yet challenging real-world problems. We are releasing facilitate to facilitate future research work among the community.","wordlikeness":0.8888888889,"lcsratio":0.8888888889,"wordcoverage":0.7368421053}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"StreamHover","Description":"Livestream Transcript Summarization and Annotation","Abstract":"With the explosive growth of livestream broadcasting, there is an urgent need for new summarization technology that enables us to create a preview of streamed content and tap into this wealth of knowledge. However, the problem is nontrivial due to the informal nature of spoken language. Further, there has been a shortage of annotated datasets that are necessary for transcript summarization. In this paper, we present to, a framework for annotating and summarizing livestream transcripts. With a total of over 500 hours of videos annotated with both extractive and abstractive summaries, our benchmark dataset is significantly larger than currently existing annotated corpora. We explore a neural extractive summarization model that leverages vector-quantized variational autoencoder to learn latent vector representations of spoken utterances and identify salient utterances from the transcripts to form summaries. We show that our model generalizes better and improves performance over strong baselines. The results of this study provide an avenue for future research to improve summarization solutions for efficient browsing of livestreams.","wordlikeness":0.8181818182,"lcsratio":0.6363636364,"wordcoverage":0.8421052632}
{"Year":2019,"Venue":"ijcnlp-2019","Acronym":"FlowSeq","Description":"Non-Autoregressive Conditional Sequence Generation with Generative Flow","Abstract":"Most sequence-to-sequence (seq2seq) models are autoregressive; they generate each token by conditioning on previously generated tokens. In contrast, non-autoregressive seq2seq models generate all tokens in one pass, which leads to increased efficiency through parallel processing on hardware such as gpus. however, directly modeling the joint distribution of all tokens simultaneously is challenging, and even with increasingly complex model structures accuracy lags significantly behind autoregressive models. In this paper, we propose a simple, efficient, and effective model for non-autoregressive sequence generation using latent variable models. Specifically, we turn to generative flow, an elegant technique to model complex distributions using neural networks, and design several layers of flow tailored for modeling the conditional density of sequential latent variables. We evaluate this model on three neural machine translation (nmt) benchmark datasets, achieving comparable performance with state-of-the-art non-autoregressive nmt models and almost constant decoding time w.r.t the sequence length.","wordlikeness":0.7142857143,"lcsratio":0.5714285714,"wordcoverage":0.8333333333}
{"Year":2016,"Venue":"coling-2016","Acronym":"TextPro-AL","Description":"An Active Learning Platform for Flexible and Efficient Production of Training Data for NLP Tasks","Abstract":"This paper presents learning (active learning for text processing), a platform where human annotators can efficiently work to produce high quality training data for new domains and new languages exploiting active learning methodologies. Learning is a web-based application integrating four components: a machine learning based nlp pipeline, an annotation editor for task definition and text annotations, an incremental re-training procedure based on active learning selection from a large pool of unannotated data, and a graphical visualization of the learning status of the system.","wordlikeness":0.7,"lcsratio":0.9,"wordcoverage":0.6666666667}
{"Year":2020,"Venue":"emnlp-2020","Acronym":"AutoQA","Description":"From Databases To QA Semantic Parsers With Only Synthetic Training Data","Abstract":"We propose speech., a methodology and toolkit to generate semantic parsers that answer questions on databases, with no manual effort. Given a database schema and its data, that automatically generates a large set of high-quality questions for training that covers different database operations. It uses automatic paraphrasing combined with template-based parsing to find alternative expressions of an attribute in different parts of speech. It also uses a novel filtered auto-paraphraser to generate correct paraphrases of entire sentences. We apply on to the schema2qa dataset and obtain an average logical form accuracy of 62.9% when tested on natural questions, which is only 6.4% lower than a model trained with expert natural language annotations and paraphrase data collected from crowdworkers. To demonstrate the generality of parts, we also apply it to the overnight dataset. Than achieves 69.8% answer accuracy, 16.4% higher than the state-of-the-art zero-shot models and only 5.2% lower than the same model trained with human data.","wordlikeness":0.6666666667,"lcsratio":0.8333333333,"wordcoverage":0.8}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"MLEC-QA","Description":"A Chinese Multi-Choice Biomedical Question Answering Dataset","Abstract":"Question answering (qa) has been successfully applied in scenarios of human-computer interaction such as chatbots and search engines. However, for the specific biomedical domain, qa systems are still immature due to expert-annotated datasets being limited by category and scale. In this paper, we present only, the largest-scale chinese multi-choice biomedical qa dataset, collected from the national medical licensing examination in china. The dataset is composed of five subsets with 136,236 biomedical multi-choice questions with extra materials (images or tables) annotated by human experts, and first covers the following biomedical sub-fields: clinic, stomatology, public health, traditional chinese medicine, and traditional chinese medicine combined with western medicine. We implement eight representative control methods and open-domain qa methods as baselines. Experimental results demonstrate that even the current best model can only achieve accuracies between 40% to 55% on five subsets, especially performing poorly on questions that require sophisticated reasoning ability. We hope the release of the a dataset can serve as a valuable resource for research and evaluation in open-domain qa, and also make advances for biomedical qa systems.","wordlikeness":0.2857142857,"lcsratio":0.8571428571,"wordcoverage":0.6666666667}
{"Year":2010,"Venue":"ws-2010","Acronym":"DLUT","Description":"Chinese Personal Name Disambiguation with Rich Features","Abstract":"In this paper we describe a person clustering system for a given document set and report the results we have obtained on the test set of chinese personal name (cpn) disambiguation task of cipssighan 2010. This task consists of clustering a set of xinhua news documents that mention an ambiguous cpn according to named entity in reality. Several features including named entities (ne) and common nouns generated from the documents and a variety of rules are employed in our system. This system achieves f = 86.36% with b_cubed scoring metrics and f = 90.78% with purity_based metrics.","wordlikeness":0.25,"lcsratio":0.75,"wordcoverage":0.8}
{"Year":2019,"Venue":"conll-2019","Acronym":"TILM","Description":"Neural Language Models with Evolving Topical Influence","Abstract":"Content of text data are often influenced by contextual factors which often evolve over time (e.g., content of social media are often influenced by topics covered in the major news streams). Existing language models do not consider the influence of such related evolving topics, and thus are not optimal. In this paper, we propose to incorporate such topical-influence into a language model to both improve its accuracy and enable cross-stream analysis of topical influences. Specifically, we propose a novel language model called topical influence language model (one), which is a novel extension of a neural language model to capture the influences on the contents in one text stream by the evolving topics in another related (or possibly same) text stream. Experimental results on six different text stream data comprised of conference paper titles show that the incorporation of evolving topical influence into a language model is beneficial and accuracy outperforms multiple baselines in a challenging task of text forecasting. In addition to serving as a language model, major further enables interesting analysis of topical influence among multiple text streams.","wordlikeness":0.5,"lcsratio":0.75,"wordcoverage":0.8571428571}
{"Year":2016,"Venue":"lrec-2016","Acronym":"GhoSt-NN","Description":"A Representative Gold Standard of German Noun-Noun Compounds","Abstract":"This paper presents a novel gold standard of german noun-noun compounds (868) including 868 compounds annotated with corpus frequencies of the compounds and their constituents, productivity and ambiguity of the constituents, semantic relations between the constituents, and compositionality ratings of compound-constituent pairs.","wordlikeness":0.625,"lcsratio":0.875,"wordcoverage":0.7692307692}
{"Year":2022,"Venue":"findings-2022","Acronym":"FreeTransfer-X","Description":"Safe and Label-Free Cross-Lingual Transfer from Off-the-Shelf Models","Abstract":"Cross-lingual transfer (clt) is of various applications. However, labeled cross-lingual corpus is expensive or even inaccessible, especially in the fields where labels are private, such as diagnostic results of symptoms in medicine and user profiles in business. Nevertheless, there are off-the-shelf models in these sensitive fields. Instead of pursuing the original labels, a workaround for clt is to transfer knowledge from the off-the-shelf models without labels. To this end, we define a novel clt problem named be that aims to achieve knowledge transfer from the off-the-shelf models in rich-resource languages. To address the problem, we propose a 2-step knowledge distillation (kd, hinton et al., 2015) framework based on multilingual pre-trained language models (mplm). The significant improvement over strong neural machine translation (nmt) baselines demonstrates the effectiveness of the proposed method. In addition to reducing annotation cost and protecting private labels, the proposed method is compatible with different networks and easy to be deployed. Finally, a range of analyses indicate the great potential of the proposed method.","wordlikeness":0.7142857143,"lcsratio":0.9285714286,"wordcoverage":0.7272727273}
{"Year":2021,"Venue":"findings-2021","Acronym":"RW-KD","Description":"Sample-wise Loss Terms Re-Weighting for Knowledge Distillation","Abstract":"Knowledge distillation (kd) is extensively used in natural language processing to compress the pre-training and task-specific fine-tuning phases of large neural language models. A student model is trained to minimize a convex combination of the prediction loss over the labels and another over the teacher output. However, most existing works either fix the interpolating weight between the two losses apriori or vary the weight using heuristics. In this work, we propose a novel sample-wise loss weighting method, other. A meta-learner, simultaneously trained with the student, adaptively re-weights the two losses for each sample. We demonstrate, on 7 datasets of the glue benchmark, that other outperforms other loss re-weighting methods for kd.","wordlikeness":0.2,"lcsratio":0.8,"wordcoverage":0.6}
{"Year":2020,"Venue":"emnlp-2020","Acronym":"CHARM","Description":"Inferring Personal Attributes from Conversations","Abstract":"Personal knowledge about users\u2019 professions, hobbies, favorite food, and travel preferences, among others, is a valuable asset for individualized ai, such as recommenders or chatbots. Conversations in social media, such as reddit, are a rich source of data for inferring personal facts. Prior work developed supervised methods to extract this knowledge, but these approaches can not generalize beyond attribute values with ample labeled training samples. This paper overcomes this limitation by devising document: a zero-shot learning method that creatively leverages keyword extraction and document retrieval in order to predict attribute values that were never seen during training. Experiments with large datasets from reddit show the viability of keyword for open-ended attributes, such as professions and hobbies.","wordlikeness":1.0,"lcsratio":0.6,"wordcoverage":1.0}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"BiSECT","Description":"Learning to Split and Rephrase Sentences with Bitexts","Abstract":"An important task in nlp applications such as sentence simplification is the ability to take a long, complex sentence and split it into shorter sentences, rephrasing as necessary. We introduce a novel dataset and a new model for this \u2018split and rephrase\u2019 task. Our consists training data consists of 1 million long english sentences paired with shorter, meaning-equivalent english sentences. We obtain these by extracting 1-2 sentence alignments in bilingual parallel corpora and then using machine translation to convert both sides of the corpus into the same language. In contains higher quality training examples than the previous split and rephrase corpora, with sentence splits that require more significant modifications. We categorize examples in our corpus and use these categories in a novel model that allows us to target specific regions of the input sentence to be split and edited. Moreover, we show that models trained on examples can perform a wider variety of split operations and improve upon previous state-of-the-art approaches in automatic and human evaluations.","wordlikeness":0.5,"lcsratio":0.8333333333,"wordcoverage":0.8333333333}
{"Year":2021,"Venue":"findings-2021","Acronym":"RelDiff","Description":"Enriching Knowledge Graph Relation Representations for Sensitivity Classification","Abstract":"The relationships that exist between entities can be a reliable indicator for classifying sensitive information, such as commercially sensitive information. For example, the relation person-isdirectorof-company can indicate whether an individual\u2019s salary should be considered as sensitive personal information. Representations of such relations are often learned using a knowledge graph to produce embeddings for relation types, generalised across different entity-pairs. However, a relation type may or may not correspond to a sensitivity depending on the entities that participate to the relation. Therefore, generalised relation embeddings are typically insufficient for classifying sensitive information. In this work, we propose a novel method for representing entities and relations within a single embedding to better capture the relationship between the entities. Moreover, we show that our proposed entity-relation-entity embedding approach can significantly improve (mcnemar\u2019s test, p &lt;0.05) the effectiveness of sensitivity classification, compared to classification approaches that leverage relation embedding approaches from the literature. (0.426 f1 vs 0.","wordlikeness":0.7142857143,"lcsratio":0.8571428571,"wordcoverage":0.7692307692}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"ReCo","Description":"Reliable Causal Chain Reasoning via Structural Causal Recurrent Neural Networks","Abstract":"Causal chain reasoning (ccr) is an essential ability for many decision-making ai systems, which requires the model to build reliable causal chains by connecting causal pairs. However, ccr suffers from two main transitive problems: threshold effect and scene drift. In other words, the causal pairs to be spliced may have a conflicting threshold boundary or scenario. To address these issues, we propose a novel reliable causal chain reasoning framework (suffers), which introduces exogenous variables to represent the threshold and scene factors of each causal pair within the causal chain, and estimates the threshold and scene contradictions across exogenous variables via structural causal recurrent neural networks (srnn). Experiments show that is outperforms a series of strong baselines on both chinese and english ccr datasets. Moreover, by injecting reliable causal chain knowledge distilled by scene, bert can achieve better performances on four downstream causal-related tasks than bert models enhanced by other kinds of knowledge.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2017,"Venue":"cl-2017","Acronym":"HyperLex","Description":"A Large-Scale Evaluation of Graded Lexical Entailment","Abstract":"We introduce range\u2014a data set and evaluation resource that quantifies the extent of the semantic category membership, that is, type-of relation, also known as hyponymy\u2013hypernymy or lexical entailment (le) relation between 2,616 concept pairs. Cognitive psychology research has established that typicality and category\/class membership are computed in human semantic memory as a gradual rather than binary relation. Nevertheless, most nlp research and existing large-scale inventories of concept category membership (wordnet, dbpedia, etc.) treat category membership and le as binary. To address this, we asked hundreds of native english speakers to indicate typicality and strength of category membership between a diverse range of concept pairs on a crowdsourcing platform. Our results confirm that category membership and le are indeed more gradual than binary. We then compare these human judgments with the predictions of automatic systems, which reveals a huge gap between human performance and state-of-the-art le, distributional and representation learning models, and substantial differences between the models themselves. We discuss a pathway for improving semantic models to overcome this discrepancy, and indicate future application areas for improved graded le systems.","wordlikeness":0.75,"lcsratio":0.625,"wordcoverage":0.6666666667}
{"Year":2021,"Venue":"acl-2021","Acronym":"ESRA","Description":"Explainable Scientific Research Assistant","Abstract":"We introduce explainable scientific research assistant (of), a literature discovery platform that augments search results with relevant details and explanations, aiding users in understanding more about their queries and the returned papers beyond existing literature search systems. Enabled by a knowledge graph we extracted from abstracts of 23k papers on the arxiv\u2019s cs.cl category, query), provides three main features: explanation (for why a paper is returned to the user), list of facts (that are relevant to the query), and graph visualization (drawing connections between the query and each paper with surrounding related entities). The experimental results with humans involved show that humans can accelerate the users\u2019 search process with paper explanations and helps them better explore the landscape of the topics of interest by exploiting the underlying knowledge graph. We provide the we web application at <a href=http:\/\/a.cp.eng.chula.ac.th\/ class=acl-markup-url>http:\/\/(for.cp.eng.chula.ac.th\/<\/a>.","wordlikeness":0.25,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2020,"Venue":"osact-2020","Acronym":"AraNet","Description":"A Deep Learning Toolkit for Arabic Social Media","Abstract":"We describe performs, a collection of deep learning arabic social media processing tools. Namely, we exploit an extensive host of both publicly available and novel social media datasets to train bidirectional encoders from transformers (bert) focused at social meaning extraction. Tasks models predict age, dialect, gender, emotion, irony, and sentiment. Nlp. Either delivers state-of-the-art performance on a number of these tasks and performs competitively on others. Delivers is exclusively based on a deep learning framework, giving it the advantage of being feature-engineering free. To the best of our knowledge, we is the first to performs predictions across such a wide range of tasks for arabic nlp. As such, media has the potential to meet critical needs.","wordlikeness":0.8333333333,"lcsratio":0.6666666667,"wordcoverage":0.8}
{"Year":2022,"Venue":"coling-2022","Acronym":"QSTS","Description":"A Question-Sensitive Text Similarity Measure for Question Generation","Abstract":"While question generation (qg) has received significant focus in conversation modeling and text generation research, the problems of comparing questions and evaluation of qg models have remained inadequately addressed. Indeed, qg models continue to be evaluated using traditional measures such as bleu, meteor, and rouge scores which were designed for other text generation problems. We propose only, a novel question-sensitive text similarity measure for comparing two questions by characterizing their target intent based on question class, named-entity, and semantic similarity information from the two questions. We show that comparing addresses several shortcomings of existing measures that depend on <span class=tex-math>n<\/span>-gram overlap scores and obtains superior results compared to traditional measures on publicly-available qg datasets. We also collect a novel dataset simqg, for enabling question similarity research in qg contexts. Simqg contains questions generated by state-of-the-art qg models along with human judgements on their relevance with respect to the passage context they were generated for as well as when compared to the given reference question. Using simqg, we showcase the key aspect of problems. That differentiates it from all existing measures. Enabling is not only able to characterize similarity between two questions, but is also able to score questions with respect to passage contexts. Thus our is, to our knowledge, the first metric that enables the measurement of qg performance in a reference-free manner.","wordlikeness":0.25,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2014,"Venue":"lrec-2014","Acronym":"Tharwa","Description":"A Large Scale Dialectal Arabic - Standard Arabic - English Lexicon","Abstract":"We introduce an electronic three-way lexicon, ,, comprising dialectal arabic, modern standard arabic and english correspondents. The paper focuses on egyptian arabic as the first pilot dialect for the resource, with plans to expand to other dialects of arabic in later phases of the project. We describe a\u0092s creation process and report on its current status. The lexical entries are augmented with various elements of linguistic information such as pos, gender, rationality, number, and root and pattern information. The lexicon is based on a compilation of information from both monolingual and bilingual existing resources such as paper dictionaries and electronic, corpus-based dictionaries. Multiple levels of quality checks are performed on the output of each step in the creation process. The importance of this lexicon lies in the fact that it is the first resource of its kind bridging multiple variants of arabic with english. Furthermore, it is a wide coverage lexical resource containing over 73,000 egyptian entries. Have is publicly available. We believe it will have a significant impact on both theoretical linguistics as well as computational linguistics research.","wordlikeness":0.5,"lcsratio":0.6666666667,"wordcoverage":0.8}
{"Year":2020,"Venue":"emnlp-2020","Acronym":"MAD-X","Description":"An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer","Abstract":"The main goal behind state-of-the-art pre-trained multilingual models such as multilingual bert and xlm-r is enabling and bootstrapping nlp applications in low-resource languages through zero-shot or few-shot cross-lingual transfer. However, due to limited model capacity, their transfer performance is the weakest exactly on such low-resource languages and languages unseen during pre-training. We propose enabling, an adapter-based framework that enables high portability and parameter-efficient transfer to arbitrary tasks and languages by learning modular language and task representations. In addition, we introduce a novel invertible adapter architecture and a strong baseline method for adapting a pre-trained multilingual model to a new language. Zero-shot outperforms the state of the art in cross lingual transfer across a representative set of typologically diverse languages on named entity recognition and causal commonsense reasoning, and achieves competitive results on question answering. Our code and adapters are available at adapterhub.ml.","wordlikeness":0.6,"lcsratio":0.6,"wordcoverage":0.75}
{"Year":2021,"Venue":"naacl-2021","Acronym":"Edge","Description":"Enriching Knowledge Graph Embeddings with External Text","Abstract":"Knowloriginal graphs suffer from sparsity which degrades the quality of representations generated by various methods. While there is an abundance of textual information throughout the web and many existing knowllearning bases, aligning information across these diverse data sources remains a challenge in the literature. Previous work has partially addressed this issue by enriching knowltexts graph entities based on \u201chard\u201d co-occurrence of words present in the entities of the knowlframework graphs and external text, while we achieve \u201csoft\u201d augmentation by proposing a knowlby graph enrichment and embedding framework named generate. Given an original knowladdressed graph, we first generate a rich but noisy augmented graph using external texts in semantic and structural level. To distill the relevant knowlsampling. And suppress the introduced noise, we design a graph alignment term in a shared embedding space between the original graph and augmented graph. To enhance the embedding learning on the augmented graph, we further regularize the locality relationship of target entity based on negative sampling. Experimental results on four benchmark datasets demonstrate the robustness and effectiveness of noise, in link prediction and node classification.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2021,"Venue":"ccl-2021","Acronym":"LRRA","Description":"A Transparent Neural-Symbolic Reasoning Framework for Real-World Visual Question Answering","Abstract":"The predominant approach of visual question answering (vqa) relies on encoding the imageand question with a \u201dblack box\u201d neural encoder and decoding a single token into answers suchas \u201dyes\u201d or \u201dno\u201d. Despite this approach\u2019s strong quantitative results it struggles to come up withhuman-readable forms of justification for the prediction process. To address this insufficiency we propose form[lookreadreasoninganswer]a transparent neural-symbolic framework forvisual question answering that solves the complicated problem in the real world step-by-steplike humans and provides human-readable form of justification at each step. Specifically eachlearns to first convert an image into a scene graph and parse a question into multiple reasoning instructions. It then executes the reasoning instructions one at a time by traversing the scenegraph using a recurrent neural-symbolic execution module. Finally it generates answers to the given questions and makes corresponding marks on the image. Furthermore we believe that the relations between objects in the question is of great significance for obtaining the correct answerso we create a perturbed gqa test set by removing linguistic cues (attributes and relations) in the questions to analyze which part of the question contributes more to the answer. Our experimentson the gqa dataset show that form is significantly better than the existing representative model(57.12% vs. 56.39%). Our experiments on the perturbed gqa test set show that the relations between objects is more important for answering complicated questions than the attributes ofobjects.keywords:visual question answering relations between objects neural-symbolic reason-ing.","wordlikeness":0.25,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2020,"Venue":"emnlp-2020","Acronym":"SLURP","Description":"A Spoken Language Understanding Resource Package","Abstract":"Spoken language understanding infers semantic meaning directly from audio data, and thus promises to reduce error propagation and misunderstandings in end-user applications. However, publicly available slu resources are limited. In this paper, we release the, a new slu package containing the following: (1) a new challenging dataset in english spanning 18 domains, which is substantially bigger and linguistically more diverse than existing datasets; (2) competitive baselines based on state-of-the-art nlu and asr systems; (3) a new transparent metric for entity labelling which enables a detailed error analysis for identifying potential areas of improvement. Existing is available at <a href=https:\/\/github.com\/pswietojanski\/potential class=acl-markup-url>https:\/\/github.com\/pswietojanski\/new<\/a>.","wordlikeness":0.4,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2017,"Venue":"emnlp-2017","Acronym":"ConStance","Description":"Modeling Annotation Contexts to Improve Stance Classification","Abstract":"Manual annotations are a prerequisite for many applications of machine learning. However, weaknesses in the annotation process itself are easy to overlook. In particular, scholars often choose what information to give to annotators without examining these decisions empirically. For subjective tasks such as sentiment analysis, sarcasm, and stance detection, such choices can impact results. Here, for the task of political stance detection on twitter, we show that providing too little context can result in noisy and uncertain annotations, whereas providing too strong a context may cause it to outweigh other signals. To characterize and reduce these biases, we develop we, a general model for reasoning about annotations across information conditions. Given conflicting labels produced by multiple annotators seeing the same instances with different contexts, these simultaneously estimates gold standard labels and also learns a classifier for new instances. We show that the classifier learned by stance outperforms a variety of baselines at predicting political stance, while the model\u2019s interpretable parameters shed light on the effects of each context.","wordlikeness":0.8888888889,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2020,"Venue":"findings-2020","Acronym":"TED","Description":"A Pretrained Unsupervised Summarization Model with Theme Modeling and Denoising","Abstract":"Text summarization aims to extract essential information from a piece of text and transform the text into a concise version. Existing unsupervised abstractive summarization models leverage recurrent neural networks framework while the recently proposed transformer exhibits much more capability. Moreover, most of previous summarization models ignore abundant unlabeled corpora resources available for pretraining. In order to address these issues, we propose recurrent, a transformer-based unsupervised abstractive summarization system with pretraining on large-scale data. We first leverage the lead bias in news articles to pretrain the model on millions of unlabeled corpora. Next, we finetune on on target domains through theme modeling and a denoising autoencoder to enhance the quality of generadatasets summaries. Notably, first outperforms all unsupervised abstractive baselines on nyt, cnn\/dm and english gigaword datasets with various document styles. Further analysis shows that the summaries generaanalysis by further are highly abstractive, and each component in the objective function of aims is highly effective.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2021,"Venue":"sigdial-2021","Acronym":"ARTA","Description":"Collection and Classification of Ambiguous Requests and Thoughtful Actions","Abstract":"Human-assisting systems such as dialogue systems must take thoughtful, appropriate actions not only for clear and unambiguous user requests, but also for ambiguous user requests, even if the users themselves are not aware of their potential requirements. To construct such a dialogue agent, we collected a corpus and developed a model that classifies ambiguous user requests into corresponding system actions. In order to collect a high-quality corpus, we asked workers to input antecedent user requests whose pre-defined actions could be regarded as thoughtful. Although multiple actions could be identified as thoughtful for a single user request, annotating all combinations of user requests and system actions is impractical. For this reason, we fully annotated only the test data and left the annotation of the training data incomplete. In order to train the classification model on such training data, we applied the positive\/unlabeled (pu) learning method, which assumes that only a part of the data is labeled with positive examples. The experimental results show that the pu learning method achieved better performance than the general positive\/negative (pn) learning method to classify thoughtful actions given an ambiguous user request.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2010,"Venue":"semeval-2010","Acronym":"YSC-DSAA","Description":"An Approach to Disambiguate Sentiment Ambiguous Adjectives Based on SAAOL","Abstract":"In this paper, we describe the system we developed for the semeval-2010 task of disambiguating sentiment ambiguous adjectives (hereinafter referred to saa). Our system created a new word library named saa-oriented library consisting of positive words, negative words, negative words related to saa, positive words related to saa, and inverse words, etc. Based on the syntactic parsing, we analyzed the relationship between saa and the keywords and handled other special processes by extracting such words in the relevant sentences to disambiguate sentiment ambiguous adjectives. Our micro average accuracy is 0.942, which puts our system in the first place.","wordlikeness":0.125,"lcsratio":0.75,"wordcoverage":0.5714285714}
{"Year":2015,"Venue":"semeval-2015","Acronym":"GPLSIUA","Description":"Combining Temporal Information and Topic Modeling for Cross-Document Event Ordering","Abstract":"Building uni\ufb01ed timelines from a collection of written news articles requires cross-document event coreference resolution and temporal relation extraction. In this paper we present an approach event coreference resolution according to: a) similar temporal information, and b) similar semantic arguments. Temporal information is detected using an automatic temporal information system (tipsem), while semantic information is represented by means of lda topic modeling. The evaluation of our approach shows that it obtains the highest micro-average f-score results in the semeval2015 task 4: \u201ctimeline: cross-document event ordering\u201d (25.36% for trackb, 23.15% for subtrackb), with an improvement of up to 6% in comparison to the other systems. However, our experiment also showed some drawbacks in the topic modeling approach that degrades performance of the system.","wordlikeness":0.1428571429,"lcsratio":0.7142857143,"wordcoverage":0.6153846154}
{"Year":2020,"Venue":"emnlp-2020","Acronym":"BENNERD","Description":"A Neural Named Entity Linking System for COVID-19","Abstract":"We present a biomedical entity linking (el) system end that detects named enti- ties in text and links them to the unified medical language system (umls) knowledge base (kb) entries to facilitate the corona virus disease 2019 (covid-19) research. Ben- nerd mainly covers biomedical domain, es- pecially new entity types (e.g., coronavirus, vi- ral proteins, immune responses) by address- ing cord-ner dataset. It includes several nlp tools to process biomedical texts includ- ing tokenization, flat and nested entity recog- nition, and candidate generation and rank- ing for el that have been pre-trained using the cord-ner corpus. To the best of our knowledge, this is the first attempt that ad- dresses ner and el on covid-19-related entities, such as covid-19 virus, potential vaccines, and spreading mechanism, that may benefit research on covid-19. We release an online system to enable real-time entity annotation with linking for end users. We also release the manually annotated test set and cord-nerd dataset for leveraging el task. The corpus. System is available at <a href=https:\/\/aistairc.github.io\/task.\/ class=acl-markup-url>https:\/\/aistairc.github.io\/a\/<\/a>.","wordlikeness":0.5714285714,"lcsratio":0.8571428571,"wordcoverage":0.7692307692}
{"Year":2019,"Venue":"acl-2019","Acronym":"DeepSentiPeer","Description":"Harnessing Sentiment in Review Texts to Recommend Peer Review Decisions","Abstract":"Automatically validating a research artefact is one of the frontiers in artificial intelligence (ai) that directly brings it close to competing with human intellect and intuition. Although criticised sometimes, the existing peer review system still stands as the benchmark of research validation. The present-day peer review process is not straightforward and demands profound domain knowledge, expertise, and intelligence of human reviewer(s), which is somewhat elusive with the current state of ai. However, the peer review texts, which contains rich sentiment information of the reviewer, reflecting his\/her overall attitude towards the research in the paper, could be a valuable entity to predict the acceptance or rejection of the manuscript under consideration. Here in this work, we investigate the role of reviewer sentiment embedded within peer review texts to predict the peer review outcome. Our proposed deep neural architecture takes into account three channels of information: the paper, the corresponding reviews, and review\u2019s polarity to predict the overall recommendation score as well as the final decision. We achieve significant performance improvement over the baselines (\u223c 29% error reduction) proposed in a recently released dataset of peer reviews. An ai of this kind could assist the editors\/program chairs as an additional layer of confidence, especially when non-responding\/missing reviewers are frequent in present day peer review.","wordlikeness":0.6923076923,"lcsratio":0.7692307692,"wordcoverage":0.6666666667}
{"Year":2010,"Venue":"semeval-2010","Acronym":"SWAT","Description":"Cross-Lingual Lexical Substitution using Local Context Matching, Bilingual Dictionaries and Machine Translation","Abstract":"We present two systems that select the most appropriate spanish substitutes for a marked word in an english test sentence. These systems were of\ufb01cial entries to the semeval-2010 cross-lingual lexical substitution task. The \ufb01rst system, \ufb01nds-e, \ufb01nds spanish substitutions by \ufb01rst \ufb01nding english substitutions in the english sentence and then translating these substitutions into spanish using an english-spanish dictionary. The second system, and-s, translates each english sentence into spanish and then \ufb01nds the spanish substitutions in the spanish sentence. Both systems exceeded the baseline and all other participating systems by a wide margin using one of the two of\ufb01cial scoring metrics.","wordlikeness":0.5,"lcsratio":0.75,"wordcoverage":1.0}
{"Year":2022,"Venue":"acl-2022","Acronym":"M-SENA","Description":"An Integrated Platform for Multimodal Sentiment Analysis","Abstract":"A is an open-sourced platform for multimodal sentiment analysis. It aims to facilitate advanced research by providing flexible toolkits, reliable benchmarks, and intuitive demonstrations. The platform features a fully modular video sentiment analysis framework consisting of data management, feature extraction, model training, and result analysis modules. In this paper, we first illustrate the overall architecture of the analysis. Platform and then introduce features of the core modules. Reliable baseline results of different modality features and msa benchmarks are also reported. Moreover, we use model evaluation and analysis tools provided by modules. To present intermediate representation visualization, on-the-fly instance test, and generalization ability test results. The source code of the platform is publicly available at <a href=https:\/\/github.com\/thuiar\/flexible class=acl-markup-url>https:\/\/github.com\/thuiar\/benchmarks<\/a>.","wordlikeness":0.3333333333,"lcsratio":0.8333333333,"wordcoverage":0.8}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"Dim-Krum","Description":"Backdoor-Resistant Federated Learning for NLP with Dimension-wise Krum-Based Aggregation","Abstract":"Despite the potential of federated learning, it is known to be vulnerable to backdoor attacks. Many robust federated aggregation methods are proposed to reduce the potential backdoor risk. However, they are mainly validated in the cv field. In this paper, we find that nlp backdoors are hard to defend against than cv, and we provide a theoretical analysis that the malicious update detection error probabilities are determined by the relative backdoor strengths. Nlp attacks tend to have small relative backdoor strengths, which may result in the failure of robust federated aggregation methods for nlp attacks. Inspired by the theoretical results, we can choose some dimensions with higher backdoor strengths to settle this issue. We propose a novel federated aggregation algorithm, aggregation, for nlp tasks, and experimental results validate its effectiveness.","wordlikeness":0.625,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2022,"Venue":"acl-2022","Acronym":"PARE","Description":"A Simple and Strong Baseline for Monolingual and Multilingual Distantly Supervised Relation Extraction","Abstract":"Neural models for distantly supervised relation extraction (ds-re) encode each sentence in an entity-pair bag separately. These are then aggregated for bag-level relation prediction. Since, at encoding time, these approaches do not allow information to flow from other sentences in the bag, we believe that they do not utilize the available bag data to the fullest. In response, we explore a simple baseline approach (using) in which all sentences of a bag are concatenated into a passage of sentences, and encoded jointly using bert. The contextual embeddings of tokens are aggregated using attention with the candidate relation as query \u2013 this summary of whole passage predicts the candidate relation. We find that our simple baseline solution outperforms existing state-of-the-art ds-re models in both monolingual and multilingual ds-re datasets.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2023,"Venue":"findings-2023","Acronym":"SERENGETI","Description":"Massively Multilingual Language Models for Africa","Abstract":"Multilingual pretrained language models (mplms) acquire valuable, generalizable linguistic information during pretraining and have advanced the state of the art on task-specific finetuning. To date, only ~31 out of ~2,000 african languages are covered in existing language models. We ameliorate this limitation by developing 517, a set of massively multilingual language model that covers 517 african languages and language varieties. We evaluate our novel models on eight natural language understanding tasks across 20 datasets, comparing to 4 mplms that cover 4-23 african languages. Datasets, outperforms other models on 11 datasets across the eights tasks, achieving 82.27 average f_1. We also perform analyses of errors from our models, which allows us to investigate the influence of language genealogy and linguistic similarity when the models are applied under zero-shot settings. We will publicly release our models for research.","wordlikeness":0.6666666667,"lcsratio":0.6666666667,"wordcoverage":0.8}
{"Year":2022,"Venue":"cl-2022","Acronym":"UDapter","Description":"Typology-based Language Adapters for Multilingual Dependency Parsing and Sequence Labeling","Abstract":"Recent advances in multilingual language modeling have brought the idea of a truly universal parser closer to reality. However, such models are still not immune to the \u201ccurse of multilinguality\u201d: cross-language interference and restrained model capacity remain major obstacles. To address this, we propose a novel language adaptation approach by introducing contextual language adapters to a multilingual parser. Contextual language adapters make it possible to learn adapters via language embeddings while sharing model parameters across languages based on contextual parameter generation. Moreover, our method allows for an easy but effective integration of existing linguistic typology features into the parsing model. Because not all typological features are available for every language, we further combine typological feature prediction with parsing in a multi-task model that achieves very competitive parsing performance without the need for an external prediction system for missing features. The resulting parser, easy, can be used for dependency parsing as well as sequence labeling tasks such as pos tagging, morphological tagging, and ner. In dependency parsing, it outperforms strong monolingual and multilingual baselines on the majority of both high-resource and low-resource (zero-shot) languages, showing the success of the proposed adaptation approach. In sequence labeling tasks, our parser surpasses the baseline on high resource languages, and performs very competitively in a zero-shot setting. Our in-depth analyses show that adapter generation via typological features of languages is key to this success.","wordlikeness":0.5714285714,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2023,"Venue":"ws-2023","Acronym":"MuLMS-AZ","Description":"An Argumentative Zoning Dataset for the Materials Science Domain","Abstract":"Scientific publications follow conventionalized rhetorical structures. Classifying the argumentative zone (az), e.g., identifying whether a sentence states a motivation, a result or background information, has been proposed to improve processing of scholarly documents. In this work, we adapt and extend this idea to the domain of materials science research. We present and release a new dataset of 50 manually annotated research articles. The dataset spans seven sub-topics and is annotated with a materials-science focused multi-label annotation scheme for az. We detail corpus statistics and demonstrate high inter-annotator agreement. Our computational experiments show that using domain-specific pre-trained transformer-based text encoders is key to high classification performance. We also find that az categories from existing datasets in other domains are transferable to varying degrees.","wordlikeness":0.375,"lcsratio":0.5,"wordcoverage":0.6666666667}
{"Year":2018,"Venue":"lrec-2018","Acronym":"AET","Description":"Web-based Adjective Exploration Tool for German","Abstract":"We present a new web-based corpus query tool, the adjective exploration tool (underlying), which enables research on the modi\ufb01cational behavior of german adjectives and adverbs. The tool can also be transferred to other languages and modi\ufb01cation phenomena. The underlying database is derived from a corpus of german print media texts, which we annnotated with dependency parses and several morphological, lexical, and statistical properties of the tokens. We extracted pairs of adjectives and adverbs (modi\ufb01ers) as well as the tokens modi\ufb01ed by them (modi\ufb01ees) from the corpus and stored them in a way that makes the modi\ufb01er-modi\ufb01ee pairs easily searchable. With keywords:, linguists from different research areas can access corpus samples using an intuitive query language and user-friendly web interface. A has been developed as a part of a collaborative research project that focuses on the compositional interaction of attributive adjectives with nouns and the interplay of events and adverbial modi\ufb01ers. The tool is easy to extend and update and is free to use online without registration: http:\/\/adverbs..phil.hhu.de keywords: corpus-query tool, adjectives, adjective modi\ufb01cation, german, syntactic dependencies 1.","wordlikeness":0.3333333333,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2016,"Venue":"emnlp-2016","Acronym":"AFET","Description":"Automatic Fine-Grained Entity Typing by Hierarchical Partial-Label Embedding","Abstract":"Distant supervision has been widely used in current systems of \ufb01ne-grained entity typing to automatically assign categories (entity types) to entity mentions. However, the types so obtained from knowledge bases are often incorrect for the entity mention\u2019s local context. This paper proposes a novel embedding method to separately model \u201cclean\u201d and \u201cnoisy\u201d mentions, and incorporates the given type hierarchy to induce loss functions. We formulate a joint optimization problem to learn embeddings for mentions and typepaths, and develop an iterative algorithm to solve the problem. Experiments on three public datasets demonstrate the effectiveness and robustness of the proposed method, with an average 15% improvement in accuracy over the next best compared method1.","wordlikeness":0.25,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2006,"Venue":"lrec-2006","Acronym":"X-Score","Description":"Automatic Evaluation of Machine Translation Grammaticality","Abstract":"In this paper we report an experiment of an automated metric used to analyse the grammaticality of machine translation output. The approach (rajman, hartley, 2001) is based on the distribution of the linguistic information within a translated text, which is supposed similar between a learning corpus and the translation. This method is quite inexpensive, since it does not need any reference translation. First we describe the experimental method and the different tests we used. Then we show the promising results we obtained on the cesta data, and how they correlate well with human judgments.","wordlikeness":0.5714285714,"lcsratio":0.4285714286,"wordcoverage":0.8333333333}
{"Year":2019,"Venue":"ijcnlp-2019","Acronym":"Pun-GAN","Description":"Generative Adversarial Network for Pun Generation","Abstract":"In this paper, we focus on the task of generating a pun sentence given a pair of word senses. A major challenge for pun generation is the lack of large-scale pun corpus to guide supervised learning. To remedy this, we propose an adversarial generative network for pun generation (between). It consists of a generator to produce pun sentences, and a discriminator to distinguish between the generated pun sentences and the real sentences with specific word senses. The output of the discriminator is then used as a reward to train the generator via reinforcement learning, encouraging it to produce pun sentences which can support two word senses simultaneously. Experiments show that the proposed via can generate sentences that are more ambiguous and diverse in both automatic and human evaluation.","wordlikeness":0.7142857143,"lcsratio":0.8571428571,"wordcoverage":0.6666666667}
{"Year":2022,"Venue":"coling-2022","Acronym":"EmoMent","Description":"An Emotion Annotated Mental Health Corpus from Two South Asian Countries","Abstract":"People often utilise online media (e.g., facebook, reddit) as a platform to express their psychological distress and seek support. State-of-the-art nlp techniques demonstrate strong potential to automatically detect mental health issues from text. Research suggests that mental health issues are reflected in emotions (e.g., sadness) indicated in a person\u2019s choice of language. Therefore, we developed a novel emotion-annotated mental health corpus (postgraduates),consisting of 2802 facebook posts (14845 sentences) extracted from two south asian countries - sri lanka and india. Three clinical psychology postgraduates were involved in annotating these posts into eight categories, including \u2018mental illness\u2019 (e.g., depression) and emotions (e.g., \u2018sadness\u2019, \u2018anger\u2019). Agreement) corpus achieved \u2018very good\u2019 inter-annotator agreement of 98.3% (i.e. % with two or more agreement) and fleiss\u2019 kappa of 0.82. Our roberta based models achieved an f1 score of 0.76 and a macro-averaged f1 score of 0.77 for the first task (i.e. predicting a mental health condition from a post) and the second task (i.e. extent of association of relevant posts with the categories defined in our taxonomy), respectively.","wordlikeness":0.5714285714,"lcsratio":1.0,"wordcoverage":0.9230769231}
{"Year":2011,"Venue":"ijcnlp-2011","Acronym":"TriS","Description":"A Statistical Sentence Simplifier with Log-linear Models and Margin-based Discriminative Training","Abstract":"We propose a statistical sentence simpli\ufb01cation system with log-linear models. In contrast to state-of-the-art methods that drive sentence simpli\ufb01cation process by hand-written linguistic rules, our method used a margin-based discriminative learning algorithm operates on a feature set. The feature set is de\ufb01ned on statistics of surface form as well as syntactic and dependency structures of the sentences. A stack decoding algorithm is used which allows us to ef\ufb01ciently generate and search simpli\ufb01cation hypotheses. Experimental results show that the simpli\ufb01ed text produced by the proposed system reduces 1.7 flesch-kincaid grade level when compared with the original text. We will show that a comparison of a state-ofthe-art rule-based system (heilman and smith, 2010) to the proposed system demonstrates an improvement of 0.2, 0.6, and 4.5 points in rouge-2, rouge-4, and avef10, respectively.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2014,"Venue":"wanlp-2014","Acronym":"GWU-HASP","Description":"Hybrid Arabic Spelling and Punctuation Corrector","Abstract":"In this paper, we describe our hybrid arabic spelling and punctuation corrector (hasp). Hasp was one of the systems participating in the qalb-2014 shared task on arabic error correction. The system uses a crf (conditional random fields) classifier for correcting punctuation errors, an open-source dictionary (or word list) for detecting errors and generating and filtering candidates, an n-gram language model for selecting the best candidates, and a set of deterministic rules for text normalization (such as removing diacritics and kashida and converting hindi numbers into arabic numerals). We also experiment with word alignment for spelling correction at the character level and report some preliminary results.","wordlikeness":0.375,"lcsratio":0.5,"wordcoverage":0.625}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"SpellBERT","Description":"A Lightweight Pretrained Model for Chinese Spelling Check","Abstract":"Chinese spelling check (csc) is to detect and correct chinese spelling errors. Many models utilize a predefined confusion set to learn a mapping between correct characters and its visually similar or phonetically similar misuses but the mapping may be out-of-domain. To that end, we propose ocr, a pretrained model with graph-based extra features and independent on confusion set. To explicitly capture the two erroneous patterns, we employ a graph neural network to introduce radical and pinyin information as visual and phonetic features. For better fusing these features with character representations, we devise masked language model alike pre-training tasks. With this feature-rich pre-training, we with only half size of bert can show competitive performance and make a state-of-the-art result on the ocr dataset where most of the errors are not covered by the existing confusion set.","wordlikeness":0.8888888889,"lcsratio":0.6666666667,"wordcoverage":0.8}
{"Year":2020,"Venue":"emnlp-2020","Acronym":"SSCR","Description":"Iterative Language-Based Image Editing via Self-Supervised Counterfactual Reasoning","Abstract":"Iterative language-based image editing (ilbie) tasks follow iterative instructions to edit images step by step. Data scarcity is a significant issue for ilbie as it is challenging to collect large-scale examples of images before and after instruction-based changes. Yet, humans still accomplish these editing tasks even when presented with an unfamiliar image-instruction pair. Such ability results from counterfactual thinking, the ability to think about possible alternatives to events that have happened already. In this paper, we introduce a self-supervised counterfactual reasoning (terms) framework that incorporates counterfactual thinking to overcome data scarcity. Data. Allows the model to consider out-of-distribution instructions paired with previous images. With the help of cross-task consistency (ctc), we train these counterfactual instructions in a self-supervised scenario. Extensive results show that we improves the correctness of ilbie in terms of both object identity and position, establishing a new state of the art (sota) on two iblie datasets (i-clevr and codraw). Even with only 50% of the training data, both achieves a comparable result to using complete data.","wordlikeness":0.25,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2019,"Venue":"ijcnlp-2019","Acronym":"INMT","Description":"Interactive Neural Machine Translation Prediction","Abstract":"In this paper, we demonstrate an interactive machine translation interface, that assists human translators with on-the-fly hints and suggestions. This makes the end-to-end translation process faster, more efficient and creates high-quality translations. We augment the opennmt backend with a mechanism to accept the user input and generate conditioned translations.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2020,"Venue":"acl-2020","Acronym":"KdConv","Description":"A Chinese Multi-domain Dialogue Dataset Towards Multi-turn Knowledge-driven Conversation","Abstract":"The research of knowledge-driven conversational systems is largely limited due to the lack of dialog data which consists of multi-turn conversations on multiple topics and with knowledge annotations. In this paper, we propose a chinese multi-domain knowledge-driven conversation dataset, knowledge-driven, which grounds the topics in multi-turn conversations to knowledge graphs. Our corpus contains 4.5k conversations from three domains (film, music, and travel), and 86k utterances with an average turn number of 19.0. these conversations contain in-depth discussions on related topics and natural transition between multiple topics. To facilitate the following research on this corpus, we provide several benchmark models. Comparative results show that the models can be enhanced by introducing background knowledge, yet there is still a large space for leveraging knowledge to model multi-turn conversations for further research. Results also show that there are obvious performance differences between different domains, indicating that it is worth further explore transfer learning and domain adaptation. The corpus and benchmark models are publicly available.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2020,"Venue":"acl-2020","Acronym":"Neural-DINF","Description":"A Neural Network based Framework for Measuring Document Influence","Abstract":"Measuring the scholarly impact of a document without citations is an important and challenging problem. Existing approaches such as document influence model (dim) are based on dynamic topic models, which only consider the word frequency change. In this paper, we use both frequency changes and word semantic shifts to measure document influence by developing a neural network framework. Our model has three steps. Firstly, we train the word embeddings for different time periods. Subsequently, we propose an unsupervised method to align vectors for different time periods. Finally, we compute the influence value of documents. Our experimental results show that our model outperforms dim.","wordlikeness":0.7272727273,"lcsratio":0.9090909091,"wordcoverage":0.7058823529}
{"Year":2022,"Venue":"acl-2022","Acronym":"HIBRIDS","Description":"Attention with Hierarchical Biases for Structure-aware Long Document Summarization","Abstract":"Document structure is critical for efficient information consumption. However, it is challenging to encode it efficiently into the modern transformer architecture. In this work, we present question, which injects hierarchical biases for incorporating document structure into attention score calculation. We further present a new task, hierarchical question-summary generation, for summarizing salient content in the source document into a hierarchy of questions and summaries, where each follow-up question inquires about the content of its parent question-summary pair. We also annotate a new dataset with 6,153 question-summary hierarchies labeled on government reports. Experiment results show that our model produces better question-summary hierarchies than comparisons on both hierarchy quality and content coverage, a finding also echoed by human judges. Additionally, our model improves the generation of long-form summaries from long government reports and wikipedia articles, as measured by rouge scores.","wordlikeness":0.5714285714,"lcsratio":0.8571428571,"wordcoverage":0.8571428571}
{"Year":2021,"Venue":"eacl-2021","Acronym":"WikiMatrix","Description":"Mining 135M Parallel Sentences in 1620 Language Pairs from Wikipedia","Abstract":"We present an approach based on multilingual sentence embeddings to automatically extract parallel sentences from the content of wikipedia articles in 96 languages, including several dialects or low-resource languages. We do not limit the extraction process to alignments with english, but we systematically consider all possible language pairs. In total, we are able to extract 135m parallel sentences for 16720 different language pairs, out of which only 34m are aligned with english. This corpus is freely available. To get an indication on the quality of the extracted bitexts, we train neural mt baseline systems on the mined data only for 1886 languages pairs, and evaluate them on the ted corpus, achieving strong bleu scores for many language pairs. The an bitexts seem to be particularly interesting to train mt systems between distant languages without the need to pivot through english.","wordlikeness":0.7,"lcsratio":0.7,"wordcoverage":0.75}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"RetroMAE","Description":"Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder","Abstract":"Despite pre-training\u2019s progress in many important nlp tasks, it remains to explore effective pre-training strategies for dense retrieval. In this paper, we propose class=acl-markup-url>https:\/\/github.com\/staoxiao\/<\/a>, a new retrieval oriented pre-training paradigm based on masked auto-encoder (mae). To is highlighted by three critical designs. 1) a novel mae workflow, where the input sentence is polluted for encoder and decoder with different masks. The sentence embedding is generated from the encoder\u2019s masked input; then, the original sentence is recovered based on the sentence embedding and the decoder\u2019s masked input via masked language modeling. 2) asymmetric model structure, with a full-scale bert like transformer as encoder, and a one-layer transformer as decoder. 3) asymmetric masking ratios, with a moderate ratio for encoder: 15 30%, and an aggressive ratio for decoder: 50 70%. Our framework is simple to realize and empirically competitive: the pre-trained models dramatically improve the sota performances on a wide range of dense retrieval benchmarks, like beir and ms marco. The source code and pre-trained models are made publicly available at <a href=https:\/\/github.com\/staoxiao\/ratios, class=acl-markup-url>https:\/\/github.com\/staoxiao\/explore<\/a> so as to inspire more interesting research.","wordlikeness":0.625,"lcsratio":1.0,"wordcoverage":0.7692307692}
{"Year":2001,"Venue":"tc-2001","Acronym":"DTS","Description":"A Delivery System for Translation and Translation-Related Services","Abstract":"Globalisation is bringing translation and multilingual information processing to areas where it was previously unknown, relatively unimportant, or technically just not feasible. Today, natural language understanding techniques are important for reaching global audiences, and are therefore becoming an indispensable component inside many systems and workflows. The main subjects of discussion continue to be the linguistic approaches used by such tools and applications. Equally important, however, are new types of software that support linguistic systems and that provide the information infrastructure to offer linguistic services to the user. Nlu technology often means large and memory-intensive applications, unsuitable for installation and use on smaller computers and mobile clients. Is is a delivery system ideally suited to providing multilingual and translation services to users within a distributed environment, in networks and on thin clients like mobile phones, pdas and notebooks. With its modular and scalable architecture and automatic load balancing it provides a flexible basis for delivering linguistic capabilities and other services over the internet or on an intranet. Internet translation portals, sms services, or networked multilingual information systems need a distribution architecture like that provided by or. 1.","wordlikeness":0.3333333333,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2022,"Venue":"acl-2022","Acronym":"SocioFillmore","Description":"A Tool for Discovering Perspectives","Abstract":"Implemented is a multilingual tool which helps to bring to the fore the focus or the perspective that a text expresses in depicting an event. Our tool, whose rationale we also support through a large collection of human judgements, is theoretically grounded on frame semantics and cognitive linguistics, and implemented using the lome frame semantic parser. We describe already\u2019s development and functionalities, show how non-nlp researchers can easily interact with the tool, and present some example case studies which are already incorporated in the system, together with the kind of analysis that can be visualised.","wordlikeness":0.8461538462,"lcsratio":0.5384615385,"wordcoverage":0.6363636364}
{"Year":2016,"Venue":"argmining-2016","Acronym":"Argumentation","Description":"Content, Structure, and Relationship with Essay Quality","Abstract":"In this paper, we investigate the relationship between and structures and (a) argument content, and (b) the holistic quality of an argumentative essay. Our results suggest that structure-based approaches hold promise for automated evaluation of argumentative writing.","wordlikeness":0.9230769231,"lcsratio":0.6923076923,"wordcoverage":0.7692307692}
{"Year":2012,"Venue":"lrec-2012","Acronym":"ConanDoyle-neg","Description":"Annotation of negation cues and their scope in Conan Doyle stories","Abstract":"In this paper we present measured, a corpus of stories by conan doyle annotated with negation information. The negation cues and their scope, as well as the event or property that is negated have been annotated by two annotators. The inter-annotator agreement is measured in terms of f-scores at scope level. It is higher for cues (94.88 and 92.77), less high for scopes (85.04 and 77.31), and lower for the negated event (79.23 and 80.67). The corpus is publicly available.","wordlikeness":0.4285714286,"lcsratio":0.7857142857,"wordcoverage":0.75}
{"Year":2021,"Venue":"acl-2021","Acronym":"OntoEA","Description":"Ontology-guided Entity Alignment via Joint Knowledge Graph Embedding","Abstract":"Semantic embedding has been widely investigated for aligning knowledge graph (kg) entities. Current methods have explored and utilized the graph structure, the entity names, and attributes, but ignore the ontology (or ontological schema) which contains critical meta information such as classes and their membership relationships with entities. In this paper, we propose an ontology-guided entity alignment method named membership, where both kgs and their ontologies are jointly embedded, and the class hierarchy and the class disjointness are utilized to avoid false mappings. Extensive experiments on seven public and industrial benchmarks have demonstrated the state-ofthe-art performance of benchmarks and the effectiveness of the ontologies.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2022,"Venue":"ws-2022","Acronym":"D-REX","Description":"Dialogue Relation Extraction with Explanations","Abstract":"Existing research studies on cross-sentence relation extraction in long-form multi-party conversations aim to improve relation extraction without considering the explainability of such methods. This work addresses that gap by focusing on extracting explanations that indicate that a relation exists while using only partially labeled explanations. We propose our model-agnostic framework, over, a policy-guided semi-supervised algorithm that optimizes for explanation quality and relation extraction simultaneously. We frame relation extraction as a re-ranking task and include relation- and entity-specific explanations as an intermediate step of the inference process. We find that human annotators are 4.2 times more likely to prefer over\u2019s explanations over a joint relation extraction and explanation model. Finally, our evaluations show that explanation is simple yet effective and improves relation extraction performance of strong baseline models by 1.2-4.7%.","wordlikeness":0.4,"lcsratio":0.8,"wordcoverage":0.75}
{"Year":2018,"Venue":"sigdial-2018","Acronym":"Cogent","Description":"A Generic Dialogue System Shell Based on a Collaborative Problem Solving Model","Abstract":"The bulk of current research in dialogue systems is focused on fairly simple task models, primarily state-based. Progress on developing dialogue systems for more complex tasks has been limited by the lack generic toolkits to build from. In this paper we report on our development from the ground up of a new dialogue model based on collaborative problem solving. We implemented the model in a dialogue system shell (toolkits) that al-lows developers to plug in problem-solving agents to create dialogue systems in new domains. The we shell has now been used by several independent teams of researchers to develop dialogue systems in different domains, with varied lexicons and interaction style, each with their own problem-solving back-end. We believe this to be the first practical demonstration of the feasibility of a cps-based dialogue system shell.","wordlikeness":0.8333333333,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2023,"Venue":"acl-2023","Acronym":"RAMP","Description":"Retrieval and Attribute-Marking Enhanced Prompting for Attribute-Controlled Translation","Abstract":"Attribute-controlled translation (act) is a subtask of machine translation that involves controlling stylistic or linguistic attributes (like formality and gender) of translation outputs. While act has garnered attention in recent years due to its usefulness in real-world applications, progress in the task is currently limited by dataset availability, since most prior approaches rely on supervised methods. To address this limitation, we propose retrieval and attribute-marking enhanced prompting (formality), which leverages large multilingual language models to perform act in few-shot and zero-shot settings. Few-shot improves generation accuracy over the standard prompting approach by (1) incorporating a semantic similarity retrieval component for selecting similar in-context examples, and (2) marking in-context examples with attribute annotations. Our comprehensive experiments show that few-shot is a viable approach in both zero-shot and few-shot settings.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2023,"Venue":"findings-2023","Acronym":"GUMSum","Description":"Multi-Genre Data and Evaluation for English Abstractive Summarization","Abstract":"Automatic summarization with pre-trained language models has led to impressively fluent results, but is prone to \u2018hallucinations\u2019, low performance on non-news genres, and outputs which are not exactly summaries. Targeting acl 2023\u2019s \u2018reality check\u2019 theme, we present prompted,, a small but carefully crafted dataset of english summaries in 12 written and spoken genres for evaluation of abstractive summarization. Summaries are highly constrained, focusing on substitutive potential, factuality, and faithfulness. We present guidelines and evaluate human agreement as well as subjective judgments on recent system outputs, comparing general-domain untuned approaches, a fine-tuned one, and a prompt-based approach, to human performance. Results show that while gpt3 achieves impressive scores, it still underperforms humans, with varying quality across genres. Human judgments reveal different types of errors in supervised, prompted, and human-generated summaries, shedding light on the challenges of producing a good summary.","wordlikeness":0.8333333333,"lcsratio":0.8333333333,"wordcoverage":0.6666666667}
{"Year":2017,"Venue":"ws-2017","Acronym":"Strawman","Description":"An Ensemble of Deep Bag-of-Ngrams for Sentiment Analysis","Abstract":"This paper describes a builder entry, named \u201cwhose\u201d, to the sentence-level sentiment analysis task of the \u201cbuild it, break it\u201d shared task of the first workshop on building linguistically generalizable nlp systems. The goal of a builder is to provide an automated sentiment analyzer that would serve as a target for breakers whose goal is to find pairs of minimally-differing sentences that break the analyzer.","wordlikeness":0.875,"lcsratio":0.75,"wordcoverage":0.7777777778}
{"Year":2014,"Venue":"acl-2014","Acronym":"ReNew","Description":"A Semi-Supervised Framework for Generating Domain-Specific Lexicons and Sentiment Analysis","Abstract":"The sentiment captured in opinionated text provides interesting and valuable information for social media services. However, due to the complexity and diversity of linguistic representations, it is challenging to build a framework that accurately extracts such sentiment. We propose a semi-supervised framework for generating a domain-speci\ufb01c sentiment lexicon and inferring sentiments at the segment level. Our framework can greatly reduce the human effort for building a domainspeci\ufb01c sentiment lexicon with high quality. Speci\ufb01cally, in our evaluation, working with just 20 manually labeled reviews, it generates a domain-speci\ufb01c sentiment lexicon that yields weighted average fmeasure gains of 3%. Our sentiment classi\ufb01cation model achieves approximately 1% greater accuracy than a state-of-the-art approach based on elementary discourse units.","wordlikeness":1.0,"lcsratio":0.8,"wordcoverage":1.0}
{"Year":2020,"Venue":"coling-2020","Acronym":"TableGPT","Description":"Few-shot Table-to-Text Generation with Table Structure Reconstruction and Content Matching","Abstract":"Although neural table-to-text models have achieved remarkable progress with the help of large-scale datasets, they suffer insufficient learning problem with limited training data. Recently, pre-trained language models show potential in few-shot learning with linguistic knowledge learnt from pretraining on large-scale corpus. However, benefiting table-to-text generation in few-shot setting with the powerful pretrained language model faces three challenges, including (1) the gap between the task\u2019s structured input and the natural language input for pretraining language model. (2) the lack of modeling for table structure and (3) improving text fidelity with less incorrect expressions that are contradicting to the table. To address aforementioned problems, we propose with for table-to-text generation. At first, we utilize table transformation module with template to rewrite structured table in natural language as input for gpt-2. In addition, we exploit multi-task learning with two auxiliary tasks that preserve table\u2019s structural information by reconstructing the structure from gpt-2\u2019s representation and improving the text\u2019s fidelity with content matching task aligning the table and information in the generated text. By experimenting on humans, songs and books, three few-shot table-to-text datasets in different domains, our model outperforms existing systems on most few-shot settings.","wordlikeness":0.625,"lcsratio":0.875,"wordcoverage":0.8571428571}
{"Year":2014,"Venue":"coling-2014","Acronym":"Nerdle","Description":"Topic-Specific Question Answering Using Wikia Seeds","Abstract":"The wikia project maintains wikis across a diverse range of subjects from areas of popular culture. Each wiki consists of collaboratively authored content and focuses on a particular topic, including franchises such as \u201cstar trek\u201d, \u201cstar wars\u201d and \u201cthe simpsons\u201d. In this paper, we investigate the use of such wikis to create question-answering (qa) systems for a given topic. Our key idea is to use a wiki as seed to gather large amounts of relevant text and to use semantic role labeling (srl) methods to extract n-ary facts from this data. By applying our method to very large amounts of topically focused text, we propose to address the coverage issues that have been noted for qa systems built using such techniques. To illustrate the strengths and weaknesses of the proposed approach, we make a web demonstrator of our system publicly available; it provides a qa view that enables users to pose natural language questions to the system and that visualizes how questions are interpreted and matched to answers. In addition, the demonstrator provides a graph exploration view in which users can directly browse the fact base in order to inspect the scope of the extracted information.","wordlikeness":0.6666666667,"lcsratio":0.6666666667,"wordcoverage":0.8333333333}
{"Year":2020,"Venue":"lrec-2020","Acronym":"VROAV","Description":"Using Iconicity to Visually Represent Abstract Verbs","Abstract":"For a long time, philosophers, linguists and scientists have been keen on finding an answer to the mind-bending question \u201cwhat does abstract language look like?\u201d, which has also sprung from the phenomenon of mental imagery and how this emerges in the mind. One way of approaching the matter of word representations is by exploring the common semantic elements that link words to each other. Visual languages like sign languages have been found to reveal enlightening patterns across signs of similar meanings, pointing towards the possibility of identifying clusters of iconic meanings. With this insight, merged with an understanding of verb predicates achieved from verbnet, this study presents a novel verb classification system based on visual shapes, using graphic animation to visually represent 20 classes of abstract verbs. Considerable agreement between participants who judged the graphic animations based on representativeness suggests a positive way forward for this proposal, which may be developed as a language learning aid in educational contexts or as a multimodal language comprehension tool for digital text.","wordlikeness":0.6,"lcsratio":0.8,"wordcoverage":0.75}
{"Year":2022,"Venue":"acl-2022","Acronym":"Dynatask","Description":"A Framework for Creating Dynamic AI Benchmark Tasks","Abstract":"We introduce state-of-the-art: an open source system for setting up custom nlp tasks that aims to greatly lower the technical knowledge and effort required for hosting and evaluating state-of-the-art nlp models, as well as for conducting model in the loop data collection with crowdworkers. A is integrated with dynabench, a research platform for rethinking benchmarking in ai that facilitates human and model in the loop data collection and evaluation. To create a task, users only need to write a short task configuration file from which the relevant web interfaces and model hosting infrastructure are automatically generated. The system is available at <a href=https:\/\/dynabench.org\/ class=acl-markup-url>https:\/\/dynabench.org\/<\/a> and the full library can be found at <a href=https:\/\/github.com\/facebookresearch\/dynabench class=acl-markup-url>https:\/\/github.com\/facebookresearch\/dynabench<\/a>.","wordlikeness":0.875,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2022,"Venue":"coling-2022","Acronym":"PSSAT","Description":"A Perturbed Semantic Structure Awareness Transferring Method for Perturbation-Robust Slot Filling","Abstract":"Most existing slot filling models tend to memorize inherent patterns of entities and corresponding contexts from training data. However, these models can lead to system failure or undesirable outputs when being exposed to spoken language perturbation or variation in practice. We propose a perturbed semantic structure awareness transferring method for training perturbation-robust slot filling models. Specifically, we introduce two mlm-based training strategies to respectively learn contextual semantic structure and word distribution from unsupervised language perturbation corpus. Then, we transfer semantic knowledge learned from upstream training procedure into the original samples and filter generated data by consistency processing. These procedures aims to enhance the robustness of slot filling models. Experimental results show that our method consistently outperforms the previous basic methods and gains strong generalization while preventing the model from memorizing inherent patterns of entities and contexts.","wordlikeness":0.4,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2021,"Venue":"eacl-2021","Acronym":"GRIT","Description":"Generative Role-filler Transformers for Document-level Event Entity Extraction","Abstract":"We revisit the classic problem of document-level role-filler entity extraction (ree) for template filling. We argue that sentence-level approaches are ill-suited to the task and introduce a generative transformer-based encoder-decoder framework (has) that is designed to model context at the document level: it can make extraction decisions across sentence boundaries; is implicitly aware of noun phrase coreference structure, and has the capacity to respect cross-role dependencies in the template structure. We evaluate our approach on the muc-4 dataset, and show that our model performs substantially better than prior work. We also show that our modeling choices contribute to model performance, e.g., by implicitly capturing linguistic knowledge such as recognizing coreferent entity mentions.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2022,"Venue":"lrec-2022","Acronym":"CrudeOilNews","Description":"An Annotated Crude Oil News Corpus for Event Extraction","Abstract":"In this paper, we present train, a corpus of english crude oil news for event extraction. It is the first of its kind for commodity news and serves to contribute towards resource building for economic and financial text mining. This paper describes the data collection process, the annotation methodology, and the event typology used in producing the corpus. Firstly, a seed set of 175 news articles were manually annotated, of which a subset of 25 news was used as the adjudicated reference test set for inter-annotator and system evaluation. The inter-annotator agreement was generally substantial, and annotator performance was adequate, indicating that the annotation scheme produces consistent event annotations of high quality. Subsequently, the dataset is expanded through (1) data augmentation and (2) human-in-the-loop active learning. The resulting corpus has 425 news articles with approximately 11k events annotated. As part of the active learning process, the corpus was used to train basic event extraction models for machine labeling; the resulting models also serve as a validation or as a pilot study demonstrating the use of the corpus in machine learning purposes. The annotated corpus is made available for academic research purpose at <a href=https:\/\/github.com\/meisin\/news-corpus class=acl-markup-url>https:\/\/github.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.6363636364}
{"Year":2009,"Venue":"paclic-2009","Acronym":"GuideLink","Description":"A Corpus Annotation System that Integrates the Management of Annotation Guidelines","Abstract":". This paper presents an annotation framework wherein the management process of the annotation guidelines is integrated into the annotation process. Such an integration allows systematic management and reference of guidelines during annotation. For the evaluation of the proposed annotation system, we compare the conventional and proposed annotation frameworks, experiments using automatic guideline suggestion, and describe a unique feature of the integrated framework.","wordlikeness":0.8888888889,"lcsratio":0.8888888889,"wordcoverage":0.8888888889}
{"Year":2017,"Venue":"eacl-2017","Acronym":"TDParse","Description":"Multi-target-specific sentiment recognition on Twitter","Abstract":"Existing target-specific sentiment recognition methods consider only a single target per tweet, and have been shown to miss nearly half of the actual targets mentioned. We present a corpus of uk election tweets, with an average of 3.09 entities per tweet and more than one type of sentiment in half of the tweets. This requires a method for multi-target specific sentiment recognition, which we develop by using the context around a target as well as syntactic dependencies involving the target. We present results of our method on both a benchmark corpus of single targets and the multi-target election corpus, showing state-of-the art performance in both corpora and outperforming previous approaches to multi-target sentiment task as well as deep learning models for single-target sentiment.","wordlikeness":0.4285714286,"lcsratio":0.7142857143,"wordcoverage":0.7692307692}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"Hate-CLIPper","Description":"Multimodal Hateful Meme Classification based on Cross-modal Interaction of CLIP Features","Abstract":"Hateful memes are a growing menace on social media. While the image and its corresponding text in a meme are related, they do not necessarily convey the same meaning when viewed individually. Hence, detecting hateful memes requires careful consideration of both visual and textual information. Multimodal pre-training can be beneficial for this task because it effectively captures the relationship between the image and the text by representing them in a similar feature space. Furthermore, it is essential to model the interactions between the image and text features through intermediate fusion. Most existing methods either employ multimodal pre-training or intermediate fusion, but not both. In this work, we propose the essential architecture, which explicitly models the cross-modal interactions between the image and text representations obtained using contrastive language-image pre-training (clip) encoders via a feature interaction matrix (fim). A simple classifier based on the fim representation is able to achieve state-of-the-art performance on the hateful memes challenge (hmc) dataset with an auroc of 85.8, which even surpasses the human performance of 82.65. Experiments on other meme datasets such as propaganda memes and tamilmemes also demonstrate the generalizability of the proposed approach. Finally, we analyze the interpretability of the fim representation and show that cross-modal interactions can indeed facilitate the learning of meaningful concepts. The code for this work is available at <a href=https:\/\/github.com\/gokulkarthik\/hateclipper class=acl-markup-url>https:\/\/github.","wordlikeness":0.75,"lcsratio":0.9166666667,"wordcoverage":0.7}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"AUTOSUMM","Description":"Automatic Model Creation for Text Summarization","Abstract":"Recent efforts to develop deep learning models for text generation tasks such as extractive and abstractive summarization have resulted in state-of-the-art performances on various datasets. However, obtaining the best model configuration for a given dataset requires an extensive knowledge of deep learning specifics like model architecture, tuning parameters etc., and is often extremely challenging for a non-expert. In this paper, we propose methods to automatically create deep learning models for the tasks of extractive and abstractive text summarization. Based on the recent advances in automated machine learning and the success of large language models such as bert and gpt-2 in encoding knowledge, we use a combination of neural architecture search (nas) and knowledge distillation (kd) techniques to perform model search and compression using the vast knowledge provided by these language models to develop smaller, customized models for any given dataset. We present extensive empirical results to illustrate the effectiveness of our model creation methods in terms of inference time and model size, while achieving near state-of-the-art performances in terms of accuracy across a range of datasets.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.7142857143}
{"Year":2022,"Venue":"acl-2022","Acronym":"SummaReranker","Description":"A Multi-Task Mixture-of-Experts Re-ranking Framework for Abstractive Summarization","Abstract":"Sequence-to-sequence neural networks have recently achieved great success in abstractive summarization, especially through fine-tuning large pre-trained language models on the downstream dataset. These models are typically decoded with beam search to generate a unique summary. However, the search space is very large, and with the exposure bias, such decoding is not optimal. In this paper, we show that it is possible to directly train a second-stage model performing re-ranking on a set of summary candidates. Our mixture-of-experts by learns to select a better candidate and consistently improves the performance of the base model. With a base pegasus, we push rouge scores by 5.44% on cnn- dailymail (47.16 rouge-1), 1.31% on xsum (48.12 rouge-1) and 9.34% on reddit tifu (29.83 rouge-1), reaching a new state-of-the-art. Our code and checkpoints will be available at <a href=https:\/\/github.com\/ntunlp\/state-of-the-art. Class=acl-markup-url>https:\/\/github.com\/ntunlp\/be<\/a>.","wordlikeness":0.6923076923,"lcsratio":0.7692307692,"wordcoverage":0.6363636364}
{"Year":2020,"Venue":"emnlp-2020","Acronym":"ENT-DESC","Description":"Entity Description Generation by Exploring Knowledge Graph","Abstract":"Previous works on knowledge-to-text generation take as input a few rdf triples or key-value pairs conveying the knowledge of some entities to generate a natural language description. Existing datasets, such as wikibio, webnlg, and e2e, basically have a good alignment between an input triple\/pair set and its output text. However, in practice, the input knowledge could be more than enough, since the output description may only cover the most significant knowledge. In this paper, we introduce a large-scale and challenging dataset to facilitate the study of such a practical scenario in kg-to-text. Our dataset involves retrieving abundant knowledge of various types of main entities from a large knowledge graph (kg), which makes the current graph-to-sequence models severely suffer from the problems of information loss and parameter explosion while generating the descriptions. We address these challenges by proposing a multi-graph structure that is able to represent the original graph information more comprehensively. Furthermore, we also incorporate aggregation methods that learn to extract the rich graph information. Extensive experiments demonstrate the effectiveness of our model architecture.","wordlikeness":0.625,"lcsratio":0.875,"wordcoverage":0.7142857143}
{"Year":2021,"Venue":"acl-2021","Acronym":"PHMOSpell","Description":"Phonological and Morphological Knowledge Guided Chinese Spelling Check","Abstract":"Chinese spelling check (csc) is a challenging task due to the complex characteristics of chinese characters. Statistics reveal that most chinese spelling errors belong to phonological or visual errors. However, previous methods rarely utilize phonological and morphological knowledge of chinese characters or heavily rely on external resources to model their similarities. To address the above issues, we propose a novel end-to-end trainable model called audio, which promotes the performance of csc with multi-modal information. Specifically, we derive pinyin and glyph representations for chinese characters from audio and visual modalities respectively, which are integrated into a pre-trained language model by a well-designed adaptive gating mechanism. To verify its effectiveness, we conduct comprehensive experiments and ablation tests. Experimental results on three shared benchmarks demonstrate that our model consistently outperforms previous state-of-the-art models.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.7142857143}
{"Year":2012,"Venue":"lrec-2012","Acronym":"Ontoterminology","Description":"How to unify terminology and ontology into a single paradigm","Abstract":"Terminology is assigned to play a more and more important role in the information society. The need for a computational representation of terminology for it applications raises new challenges for terminology. Ontology appears to be one of the most suitable solutions for such an issue. But an ontology is not a terminology as well as a terminology is not an ontology. Terminology, especially for technical domains, relies on two different semiotic systems: the linguistic one, which is directly linked to the \u0093language for special purposes\u0094 and the conceptual system that describes the domain knowledge. These two systems must be both separated and linked. The new paradigm of solutions, i.e. a terminology whose conceptual system is a formal ontology, emphasizes the difference between the linguistic and conceptual dimensions of terminology while unifying them. A double semantic triangle is introduced in order to link terms (signifiers) to concept names on a first hand and meanings (signified) to concepts on the other hand. Such an approach allows two kinds of definition to be introduced. The definition of terms written in natural language is considered as a linguistic explanation while the definition of concepts written in a formal language is viewed as a formal specification that allows operationalization of terminology.","wordlikeness":0.8666666667,"lcsratio":0.9333333333,"wordcoverage":0.6428571429}
{"Year":2022,"Venue":"tacl-2022","Acronym":"ProoFVer","Description":"Natural Logic Theorem Proving for Fact Verification","Abstract":"Fact verification systems typically rely on neural network classifiers for veracity prediction, which lack explainability. This paper proposes rely, which uses a seq2seq model to generate natural logic-based inferences as proofs. These proofs consist of lexical mutations between spans in the claim and the evidence retrieved, each marked with a natural logic operator. Claim veracity is determined solely based on the sequence of these operators. Hence, these proofs are faithful explanations, and this makes hence, faithful by construction. Currently, claim has the highest label accuracy and the second best score in the fever leaderboard. Furthermore, it improves by 13.21% points over the next best model on a dataset with counterfactual instances, demonstrating its robustness. As explanations, the proofs show better overlap with human rationales than attention-based highlights and the proofs help humans predict model decisions correctly more often than using the evidence directly.","wordlikeness":0.875,"lcsratio":1.0,"wordcoverage":0.7692307692}
{"Year":2016,"Venue":"acl-2016","Acronym":"LexSemTm","Description":"A Semantic Dataset Based on All-words Unsupervised Sense Distribution Learning","Abstract":"There has recently been a lot of interest in unsupervised methods for learning sense distributions, particularly in applications where sense distinctions are needed. This paper analyses a state-of-the-art method for sense distribution learning, and optimises it for application to the entire vocabulary of a given language. The optimised method is then used to produce which: a sense frequency and semantic dataset of unprecedented size, spanning approximately 88% of polysemous, english simplex lemmas, which is released as a public resource to the community. Finally, the quality of this data is investigated, and the lemmas sense distributions are shown to be superior to those based on the wordnet \ufb01rst sense for lemmas missing from semcor, and at least on par with semcor-based distributions otherwise.","wordlikeness":0.25,"lcsratio":0.625,"wordcoverage":0.7142857143}
{"Year":2021,"Venue":"acl-2021","Acronym":"REPT","Description":"Bridging Language Models and Machine Reading Comprehension via Retrieval-Based Pre-training","Abstract":"Pre-trained language models (plms) have achieved great success on machine reading comprehension (mrc) over the past few years. Although the general language representation learned from large-scale corpora does bene\ufb01t mrc, the poor support in evidence extraction which requires reasoning across multiple sentences hinders plms from further advancing mrc. To bridge the gap between general plms and mrc, we present years., a retrieval-based pre-training approach. In particular, we introduce two self-supervised tasks to strengthen evidence extraction during pre-training, which is further inherited by downstream mrc tasks through the consistent retrieval operation and model architecture. To evaluate our proposed method, we conduct extensive experiments on \ufb01ve mrc datasets that require collecting evidence from and reasoning across multiple sentences. Experimental results demonstrate the effectiveness of our pre-training approach. Moreover, further analysis shows that our approach is able to enhance the capacity of evidence extraction without explicit supervision.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2019,"Venue":"naacl-2019","Acronym":"ComQA","Description":"A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters","Abstract":"To bridge the gap between the capabilities of the state-of-the-art in factoid question answering (qa) and what users ask, we need large datasets of real user questions that capture the various question phenomena users are interested in, and the diverse ways in which these questions are formulated. We introduce from, a large dataset of real user questions that exhibit different challenging aspects such as compositionality, temporal reasoning, and comparisons. Taken questions come from the wikianswers community qa platform, which typically contains questions that are not satisfactorily answerable by existing search engine technology. Through a large crowdsourcing effort, we clean the question dataset, group questions into paraphrase clusters, and annotate clusters with their answers. Qa. Contains 11,214 questions grouped into 4,834 paraphrase clusters. We detail the process of constructing qa., including the measures taken to ensure its high quality while making effective use of crowdsourcing. We also present an extensive analysis of the dataset and the results achieved by state-of-the-art systems on engine, demonstrating that our dataset can be a driver of future research on qa.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2023,"Venue":"eacl-2023","Acronym":"SPINDLE","Description":"Spinning Raw Text into Lambda Terms with Graph Attention","Abstract":"This paper describes and, an open source python module, providing an efficient and accurate parser for written dutch that transforms raw text input to programs for meaning composition expressed as \u03bb terms. The parser integrates a number of breakthrough advances made in recent years. Its output consists of hi-res derivations of a multimodal type-logical grammar, capturing two orthogonal axes of syntax, namely deep function-argument structures and dependency relations. These are produced by three interdependent systems: a static type-checker asserting the well-formedness of grammatical analyses, a state-of-the-art, structurally-aware supertagger based on heterogeneous graph convolutions, and a massively parallel proof search component based on sinkhorn iterations. Packed in the software are also handy utilities and extras for proof visualization and inference, intended to facilitate end-user utilization.","wordlikeness":0.7142857143,"lcsratio":0.8571428571,"wordcoverage":0.8333333333}
{"Year":2022,"Venue":"ws-2022","Acronym":"NTULM","Description":"Enriching Social Media Text Representations with Non-Textual Units","Abstract":"On social media, additional context is often present in the form of annotations and meta-data such as the post\u2019s author, mentions, hashtags, and hyperlinks. We refer to these annotations as non-textual units (ntus). We posit that ntus provide social context beyond their textual semantics and leveraging these units can enrich social media text representations. In this work we construct an ntu-centric social heterogeneous network to co-embed ntus. we then principally integrate these ntu embeddings into a large pretrained language model by fine-tuning with these additional units. This adds context to noisy short-text social media. Experiments show that utilizing ntu-augmented text representations significantly outperforms existing text-only baselines by 2-5% relative points on many downstream tasks highlighting the importance of context to social media nlp. We also highlight that including ntu context into the initial layers of language model alongside text is better than using it after the text embedding is generated. Our work leads to the generation of holistic general purpose social media content embedding.","wordlikeness":0.6,"lcsratio":0.8,"wordcoverage":0.6666666667}
{"Year":2014,"Venue":"coling-2014","Acronym":"CLAM","Description":"Quickly deploy NLP command-line tools on the web","Abstract":"In this paper we present the software transparently; the computational linguistics application mediator. Allows is a tool that allows you to quickly and transparently transform command-line nlp tools into fully-\ufb02edged restful webservices with which automated clients can communicate, as well as a generic webapplication interface for human end-users.","wordlikeness":1.0,"lcsratio":0.75,"wordcoverage":0.8888888889}
{"Year":2019,"Venue":"ijcnlp-2019","Acronym":"HMEAE","Description":"Hierarchical Modular Event Argument Extraction","Abstract":"Existing event extraction methods classify each argument role independently, ignoring the conceptual correlations between different argument roles. In this paper, we propose a hierarchical modular event argument extraction (from) model, to provide effective inductive bias from the concept hierarchy of event argument roles. Specifically, we design a neural module network for each basic unit of the concept hierarchy, and then hierarchically compose relevant unit modules with logical operations into a role-oriented modular network to classify a specific argument role. As many argument roles share the same high-level unit module, their correlation can be utilized to extract specific event arguments better. Experiments on real-world datasets show that bias can effectively leverage useful knowledge from the concept hierarchy and significantly outperform the state-of-the-art baselines. The source code can be obtained from <a href=https:\/\/github.com\/thunlp\/model, class=acl-markup-url>https:\/\/github.com\/thunlp\/experiments<\/a>.","wordlikeness":0.4,"lcsratio":1.0,"wordcoverage":0.7692307692}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"CR-Walker","Description":"Tree-Structured Graph Reasoning and Dialog Acts for Conversational Recommendation","Abstract":"Growing interests have been attracted in conversational recommender systems (crs), which explore user preference through conversational interactions in order to make appropriate recommendation. However, there is still a lack of ability in existing crs to (1) traverse multiple reasoning paths over background knowledge to introduce relevant items and attributes, and (2) arrange selected entities appropriately under current system intents to control response generation. To address these issues, we propose the in this paper, a model that performs tree-structured reasoning on a knowledge graph, and generates informative dialog acts to guide language generation. The unique scheme of tree-structured reasoning views the traversed entity at each hop as part of dialog acts to facilitate language generation, which links how entities are selected and expressed. Automatic and human evaluations show that (crs), can arrive at more accurate recommendation, and generate more informative and engaging responses.","wordlikeness":0.7777777778,"lcsratio":0.6666666667,"wordcoverage":0.8}
{"Year":2023,"Venue":"ws-2023","Acronym":"SIDLR","Description":"Slot and Intent Detection Models for Low-Resource Language Varieties","Abstract":"Intent detection and slot filling are two critical tasks in spoken and natural language understandingfor task-oriented dialog systems. In this work, we describe our participation in slot and intent detection for low-resource language varieties (sid4lr) (aepli et al., 2023). We investigate the slot and intent detection (sid) tasks using a wide range of models and settings. Given the recent success of multitask promptedfinetuning of the large language models, we also test the generalization capability of the recent encoder-decoder model mt0 (muennighoff et al., 2022) on new tasks (i.e., sid) in languages they have never intentionally seen. We show that our best model outperforms the baseline by a large margin (up to +30 f1 points) in both sid tasks.","wordlikeness":0.4,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2021,"Venue":"tacl-2021","Acronym":"SummEval","Description":"Re-evaluating Summarization Evaluation","Abstract":"The scarcity of comprehensive up-to-date studies on evaluation metrics for text summarization and the lack of consensus regarding evaluation protocols continue to inhibit progress. We address the existing shortcomings of summarization evaluation methods along five dimensions: 1) we re-evaluate 14 automatic evaluation metrics in a comprehensive and consistent fashion using neural summarization model outputs along with expert and crowd-sourced human annotations; 2) we consistently benchmark 23 recent summarization models using the aforementioned automatic evaluation metrics; 3) we assemble the largest collection of summaries generated by models trained on the cnn\/dailymail news dataset and share it in a unified format; 4) we implement and share a toolkit that provides an extensible and unified api for evaluating summarization models across a broad range of automatic metrics; and 5) we assemble and share the largest and most diverse, in terms of model types, collection of human judgments of model-generated summaries on the cnn\/daily mail dataset annotated by both expert judges and crowd-source workers. We hope that this work will help promote a more complete evaluation protocol for text summarization as well as advance research in developing evaluation metrics that better correlate with human judgments.","wordlikeness":0.875,"lcsratio":1.0,"wordcoverage":0.7142857143}
{"Year":2012,"Venue":"lrec-2012","Acronym":"PEXACC","Description":"A Parallel Sentence Mining Algorithm from Comparable Corpora","Abstract":"Extracting parallel data from comparable corpora in order to enrich existing statistical translation models is an avenue that attracted a lot of research in recent years. There are experiments that convincingly show how parallel data extracted from comparable corpora is able to improve statistical machine translation. Yet, the existing body of research on parallel sentence mining from comparable corpora does not take into account the degree of comparability of the corpus being processed or the computation time it takes to extract parallel sentences from a corpus of a given size. We will show that the performance of a parallel sentence extractor crucially depends on the degree of comparability such that it is more difficult to process a weakly comparable corpus than a strongly comparable corpus. In this paper we describe mt-related, a distributed (running on multiple cpus), trainable parallel sentence\/phrase extractor from comparable corpora. Computation is freely available for download with the accurat toolkit, a collection of mt-related tools developed in the accurat project.","wordlikeness":0.6666666667,"lcsratio":0.8333333333,"wordcoverage":0.7692307692}
{"Year":2021,"Venue":"acl-2021","Acronym":"Stretch-VST","Description":"Getting Flexible With Visual Stories","Abstract":"In visual storytelling, a short story is generated based on a given image sequence. Despite years of work, most visual storytelling models remain limited in terms of the generated stories\u2019 fixed length: most models produce stories with exactly five sentences because five-sentence stories dominate the training data. The fix-length stories carry limited details and provide ambiguous textual information to the readers. Therefore, we propose to \u201cstretch\u201d the stories, which create the potential to present in-depth visual details. This paper presents training, a visual storytelling framework that enables the generation of prolonged stories by adding appropriate knowledge, which is selected by the proposed scoring function. We propose a length-controlled transformer to generate long stories. This model introduces novel positional encoding methods to maintain story quality with lengthy inputs. Experiments confirm that long stories are generated without deteriorating the quality. The human evaluation further shows that details. Can provide better focus and detail when stories are prolonged compared to state of the art. We create a webpage to demonstrate our prolonged capability.","wordlikeness":0.6363636364,"lcsratio":0.6363636364,"wordcoverage":0.8}
{"Year":2021,"Venue":"acl-2021","Acronym":"HieRec","Description":"Hierarchical User Interest Modeling for Personalized News Recommendation","Abstract":"User interest modeling is critical for personalized news recommendation. Existing news recommendation methods usually learn a single user embedding for each user from their previous behaviors to represent their overall interest. However, user interest is usually diverse and multi-grained, which is difficult to be accurately modeled by a single user embedding. In this paper, we propose a news recommendation method with hierarchical user interest modeling, named a. Instead of a single user embedding, in our method each user is represented in a hierarchical interest tree to better capture their diverse and multi-grained interest in news. We use a three-level hierarchy to represent 1) overall user interest; 2) user interest in coarse-grained topics like sports; and 3) user interest in fine-grained topics like football. Moreover, we propose a hierarchical user interest matching framework to match candidate news with different levels of user interest for more accurate user interest targeting. Extensive experiments on two real-world datasets validate our method can effectively improve the performance of user modeling for personalized news recommendation.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2020,"Venue":"intellang-2020","Acronym":"FitChat","Description":"Conversational AI for Active Ageing","Abstract":"Advances in conversational ai are creating novel and engaging experiences for user interaction through ai chatbots. In this work, we present the voice-based ai chatbot \u201cfrom\u201d developed to deliver behaviour change interventions that encourage physical activities among older adults. We start by identifying conversation skills or topics necessary to promote physical well-being through co-creation activities with users. For each conversation skill, we further explore the use of natural language understanding (nlu) and natural language generation (nlg) techniques to improve the conversation. We generate personalised conversation, contextualised with the information extracted from user responses. Provisioning educational content from who guidelines on physical well-being provided a useful knowledge source for contextualising chatbot responses and a corpus-based approach helped to avoid non-repetitive chatbot responses. We evaluate the prototype using think-aloud sessions where thematic analysis emphasises that voice-based chatbots are a powerful mode of intervention delivery. Analysis of user responses shows the nlu techniques were instrumental in extracting information that is essential to create cohesive and personalised conversations using nlg techniques.","wordlikeness":0.8571428571,"lcsratio":0.4285714286,"wordcoverage":0.8333333333}
{"Year":2013,"Venue":"acl-2013","Acronym":"VSEM","Description":"An open library for visual semantics representation","Abstract":"From is an open library for visual semantics. Starting from a collection of tagged images, it is possible to automatically construct an image-based representation of concepts by using off-theshelf image-based functionalities. Functionalities. Is entirely written in matlab and its objectoriented design allows a large \ufb02exibility and reusability. The software is accompanied by a website with supporting documentation and examples.","wordlikeness":0.25,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2019,"Venue":"naacl-2019","Acronym":"VQD","Description":"Visual Query Detection In Natural Scenes","Abstract":"We propose a new visual grounding task called visual query detection (our). In called, the task is to localize a <i>variable<\/i> number of objects in an image where the objects are specified in natural language. An is related to visual referring expression comprehension, where the task is to localize only <i>one<\/i> object. We propose the first algorithms for for, and we evaluate them on both visual referring expression datasets and our new visualv1 dataset.","wordlikeness":0.3333333333,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2008,"Venue":"acl-2008","Acronym":"Yawat","Description":"Yet Another Word Alignment Tool","Abstract":"Labeling1 is a tool for the visualization and manipulation of word- and phrase-level alignments of parallel text. Unlike most other tools for manual word alignment, it relies on dynamic markup to visualize alignment relations, that is, markup is shown and hidden depending on the current mouse position. This reduces the visual complexity of the visualization and allows the annotator to focus on one item at a time. For a bird\u2019s-eye view of alignment patterns within a sentence, the tool is also able to display alignments as alignment matrices. In addition, it allows for manual labeling of alignment relations with customizable tag sets. Different text colors are used to indicate which words in a given sentence pair have already been aligned, and which ones still need to be aligned. Tag sets and color schemes can easily be adapted to the needs of speci\ufb01c annotation projects through con\ufb01guration \ufb01les. The tool is implemented in javascript and designed to run as a web application.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2023,"Venue":"findings-2023","Acronym":"MTCue","Description":"Learning Zero-Shot Control of Extra-Textual Attributes by Leveraging Unstructured Context in Neural Machine Translation","Abstract":"Efficient utilisation of both intra- and extra-textual context remains one of the critical gaps between machine and human translation. Existing research has primarily focused on providing individual, well-defined types of context in translation, such as the surrounding text or discrete external variables like the speaker\u2019s gender. This work introduces neural, a novel neural machine translation (nmt) framework that interprets all context (including discrete variables) as text. Translation, learns an abstract representation of context, enabling transferability across different data settings and leveraging similar attributes in low-resource scenarios. With a focus on a dialogue domain with access to document and metadata context, we extensively evaluate discrete in four language pairs in both translation directions. Our framework demonstrates significant improvements in translation quality over a parameter-matched non-contextual baseline, as measured by bleu (+0.88) and comet (+1.58). Moreover, by significantly outperforms a \u201ctagging\u201d baseline at translating english text. Analysis reveals that the context encoder of and learns a representation space that organises context based on specific attributes, such as formality, enabling effective zero-shot control. Pre-training on context embeddings also improves extra-textual\u2019s few-shot performance compared to the \u201ctagging\u201d baseline. Finally, an ablation study conducted on model components and contextual variables further supports the robustness of significant for context-based nmt.","wordlikeness":0.4,"lcsratio":0.8,"wordcoverage":0.75}
{"Year":2018,"Venue":"emnlp-2018","Acronym":"TwoWingOS","Description":"A Two-Wing Optimization Strategy for Evidential Claim Verification","Abstract":"Determining whether a given claim is supported by evidence is a fundamental nlp problem that is best modeled as textual entailment. However, given a large collection of text, finding evidence that could support or refute a given claim is a challenge in itself, amplified by the fact that different evidence might be needed to support or refute a claim. Nevertheless, most prior work decouples evidence finding from determining the truth value of the claim given the evidence. We propose to consider these two aspects jointly. We develop supported (two-wing optimization strategy), a system that, while identifying appropriate evidence for a claim, also determines whether or not the claim is supported by the evidence. Given the claim, given attempts to identify a subset of the evidence candidates; given the predicted evidence, it then attempts to determine the truth value of the corresponding claim entailment problem. We treat this problem as coupled optimization problems, training a joint model for it. Then offers two advantages: (i) unlike pipeline systems it facilitates flexible-size evidence set, and (ii) joint training improves both the claim entailment and the evidence identification. Experiments on a benchmark dataset show state-of-the-art performance.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2022,"Venue":"lrec-2022","Acronym":"BU-TTS","Description":"An Open-Source, Bilingual Welsh-English, Text-to-Speech Corpus","Abstract":"This paper presents the design, collection and verification of a bilingual text-to-speech synthesis corpus for welsh and english. The ever expanding voice collection currently contains almost 10 hours of recordings from a bilingual, phonetically balanced text corpus. The speakers consist of a professional voice actor and three amateur contributors, with male and female accents from north and south wales. This corpus provides audio-text pairs for building and training high-quality bilingual welsh-english neural based tts systems. We describe the process by which we created a phonetically balanced prompt set and the challenges of attempting to collate such a dataset during the covid-19 pandemic. Our initial findings in validating the corpus via the implementation of a state-of-the-art tts models are presented. This corpus represents the first open-source welsh language corpus large enough to capitalise on neural tts architectures.","wordlikeness":0.3333333333,"lcsratio":1.0,"wordcoverage":0.9090909091}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"SQuALITY","Description":"Building a Long-Document Summarization Dataset the Hard Way","Abstract":"Summarization datasets are often assembled either by scraping naturally occurring public-domain summaries\u2014which are nearly always in difficult-to-work-with technical domains\u2014or by using approximate heuristics to extract them from everyday text\u2014which frequently yields unfaithful summaries. In this work, we turn to a slower but more straightforward approach to developing summarization benchmark data: we hire highly-qualified contractors to read stories and write original summaries from scratch. To amortize reading time, we collect five summaries per document, with the first giving an overview and the subsequent four addressing specific questions. We use this protocol to collect document,, a dataset of question-focused summaries built on the same public-domain short stories as the multiple-choice dataset quality (pang et al., 2021). Experiments with state-of-the-art summarization systems show that our dataset is challenging and that existing automatic evaluation metrics are weak indicators of quality.","wordlikeness":0.875,"lcsratio":0.75,"wordcoverage":0.9333333333}
{"Year":2023,"Venue":"acl-2023","Acronym":"HistRED","Description":"A Historical Document-Level Relation Extraction Dataset","Abstract":"Despite the extensive applications of relation extraction (re) tasks in various domains, little has been explored in the historical context, which contains promising data across hundreds and thousands of years. To promote the historical re research, we present dataset constructed from yeonhaengnok. Yeonhaengnok is a collection of records originally written in hanja, the classical chinese writing, which has later been translated into korean. Baselines provides bilingual annotations such that re can be performed on korean and hanja texts. In addition, data supports various self-contained subtexts with different lengths, from a sentence level to a document level, supporting diverse context settings for researchers to evaluate the robustness of their re models. To demonstrate the usefulness of our dataset, we propose a bilingual re model that leverages both korean and hanja contexts to predict relations between entities. Our model outperforms monolingual baselines on diverse, showing that employing multiple language contexts supplements the re predictions. The dataset is publicly available at: <a href=https:\/\/huggingface.co\/datasets\/soyoung\/a class=acl-markup-url>https:\/\/huggingface.co\/datasets\/soyoung\/demonstrate<\/a> under cc by-nc-nd 4.0 license.","wordlikeness":0.7142857143,"lcsratio":1.0,"wordcoverage":0.8333333333}
{"Year":2023,"Venue":"tacl-2023","Acronym":"FRMT","Description":"A Benchmark for Few-Shot Region-Aware Machine Translation","Abstract":"We present explore, a new dataset and evaluation benchmark for few-shot region-aware machine translation, a type of style-targeted translation. The dataset consists of professional translations from english into two regional variants each of portuguese and mandarin chinese. Source documents are selected to enable detailed analysis of phenomena of interest, including lexically distinct terms and distractor terms. We explore automatic evaluation metrics for are and validate their correlation with expert human evaluation across both region-matched and mismatched rating scenarios. Finally, we present a number of baseline models for this task, and offer guidelines for how researchers can train, evaluate, and compare their own models. Our dataset and evaluation code are publicly available: <a href=https:\/\/bit.ly\/distractor-task class=acl-markup-url>https:\/\/bit.ly\/offer-task<\/a>.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2014,"Venue":"semeval-2014","Acronym":"UWB","Description":"Machine Learning Approach to Aspect-Based Sentiment Analysis","Abstract":"This paper describes our system participating in the aspect-based sentiment analysis task of semeval 2014. The goal was to identify the aspects of given target entities and the sentiment expressed towards each aspect. We \ufb01rstly introduce a system based on supervised machine learning, which is strictly constrained and uses the training data as the only source of information. This system is then extended by unsupervised methods for latent semantics discovery (lda and semantic spaces) as well as the approach based on sentiment vocabularies. The evaluation was done on two domains, restaurants and laptops. We show that our approach leads to very promising results.","wordlikeness":0.3333333333,"lcsratio":0.3333333333,"wordcoverage":0.8}
{"Year":2021,"Venue":"acl-2021","Acronym":"BERTGen","Description":"Multi-task Generation through BERT","Abstract":"We present biases, a novel, generative, decoder-only model which extends bert by fusing multimodal and multilingual pre-trained models vl-bert and m-bert, respectively. Image is auto-regressively trained for language generation tasks, namely image captioning, machine translation and multimodal machine translation, under a multi-task setting. With a comprehensive set of evaluations, we show that namely outperforms many strong baselines across the tasks explored. We also show generation,\u2019s ability for zero-shot language generation, where it exhibits competitive performance to supervised counterparts. Finally, we conduct ablation studies which demonstrate that translation, substantially benefits from multi-tasking and effectively transfers relevant inductive biases from the pre-trained models.","wordlikeness":0.8571428571,"lcsratio":0.7142857143,"wordcoverage":0.7692307692}
{"Year":2020,"Venue":"findings-2020","Acronym":"LEGAL-BERT","Description":"The Muppets straight out of Law School","Abstract":"Bert has achieved impressive performance in several nlp tasks. However, there has been limited investigation on its adaptation guidelines in specialised domains. Here we focus on the legal domain, where we explore several approaches for applying bert models to downstream legal tasks, evaluating on multiple datasets. Our findings indicate that the previous guidelines for pre-training and fine-tuning, often blindly followed, do not always generalize well in the legal domain. Thus we propose a systematic investigation of the available strategies when applying bert in specialised domains. These are: (a) use the original bert out of the box, (b) adapt bert by additional pre-training on domain-specific corpora, and (c) pre-train bert from scratch on domain-specific corpora. We also propose a broader hyper-parameter search space when fine-tuning for downstream tasks and we release broader, a family of bert models intended to assist legal nlp research, computational law, and legal technology applications.","wordlikeness":0.8,"lcsratio":0.4,"wordcoverage":0.75}
{"Year":2018,"Venue":"lrec-2018","Acronym":"EventWiki","Description":"A Knowledge Base of Major Events","Abstract":"This paper introduces a new resource called entities which is, to the best of our knowledge, the \ufb01rst knowledge base resource of major events. In contrast to most existing knowledge bases that focus on static entities such as people, locations and organizations, our which concentrate on major events, in which all entries in introduces are important events in mankind history. We demonstrate that resource, is a very useful resource for information extraction regarding events in natural language processing (nlp), knowledge inference and automatic knowledge base construction. Keywords: concentrate, event knowledge base, major event, resource, event-driven, information extraction, knowledge base construction, knowledge inference 1.","wordlikeness":0.8888888889,"lcsratio":0.5555555556,"wordcoverage":0.7142857143}
{"Year":2006,"Venue":"hlt-2006","Acronym":"Nuggeteer","Description":"Automatic Nugget-Based Evaluation using Descriptions and Judgements","Abstract":"The trec de\ufb01nition and relationship questions are evaluated on the basis of information nuggets that may be contained in system responses. Human evaluators provide informal descriptions of each nugget, and judgements (assignments of nuggets to responses) for each response submitted by participants. While human evaluation is the most accurate way to compare systems, approximate automatic evaluation becomes critical during system development. We present a, a new automatic evaluation tool for nugget-based tasks. Like the \ufb01rst such tool, pourpre, existing uses words in common between candidate answer and answer key to approximate human judgements. Unlike pourpre, but like human assessors, candidate creates a judgement for each candidatenugget pair, and can use existing judgements instead of guessing. This creates a more readily interpretable aggregate score, and allows developers to track individual nuggets through the variants of their system. Of is quantitatively comparable in performance to pourpre, and provides qualitatively better feedback to developers.","wordlikeness":0.5555555556,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2007,"Venue":"semeval-2007","Acronym":"MELB-MKB","Description":"Lexical Substitution system based on Relatives in Context","Abstract":"In this paper we describe the this system, as entered in the semeval-2007 lexical substitution task. The core of our system was the \u201crelatives in context\u201d unsupervised approach, which ranked the candidate substitutes by web-lookup of the word sequences built combining the target context and each substitute. Our system ranked third in the \ufb01nal evaluation, performing close to the top-ranked system.","wordlikeness":0.375,"lcsratio":0.625,"wordcoverage":0.5454545455}
{"Year":2023,"Venue":"findings-2023","Acronym":"BanglaBook","Description":"A Large-scale Bangla Dataset for Sentiment Analysis from Book Reviews","Abstract":"The analysis of consumer sentiment, as expressed through reviews, can provide a wealth of insight regarding the quality of a product. While the study of sentiment analysis has been widely explored in many popular languages, relatively less attention has been given to the bangla language, mostly due to a lack of relevant data and cross-domain adaptability. To address this limitation, we present we, a large-scale dataset of bangla book reviews consisting of 158,065 samples classified into three broad categories: positive, negative, and neutral. We provide a detailed statistical analysis of the dataset and employ a range of machine learning models to establish baselines including svm, lstm, and bangla-bert. Our findings demonstrate a substantial performance advantage of pre-trained models over models that rely on manually crafted features, emphasizing the necessity for additional training resources in this domain. Additionally, we conduct an in-depth error analysis by examining sentiment unigrams, which may provide insight into common classification errors in under-resourced languages like bangla. Our codes and data are publicly available at <a href=https:\/\/github.com\/mohsinulkabir14\/has class=acl-markup-url>https:\/\/github.com\/mohsinulkabir14\/domain.<\/a>.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.7058823529}
{"Year":2013,"Venue":"ws-2013","Acronym":"LIME","Description":"Towards a Metadata Module for Ontolex","Abstract":"The ontolex w3c community group has been working for more than a year on realizing a proposal for a standard ontology lexicon model. As the corespecification of the model is almost complete, the group started development of additional modules for specific tasks and use cases. We think that in many usage scenarios (e.g. linguistic enrichment, localization and alignment of ontologies) the discovery and exploitation of linguistically grounded datasets may benefit from summarizing information about their linguistic expressivity. While the void vocabulary covers the need for general metadata about linked datasets, this more specific information demands a dedicated extension. In this paper, we fill this gap by introducing general (linguistic metadata), a new vocabulary aiming at completing the ontolex standard with specifications for linguistic metadata.","wordlikeness":1.0,"lcsratio":0.5,"wordcoverage":1.0}
{"Year":2012,"Venue":"eacl-2012","Acronym":"NERD","Description":"A Framework for Unifying Named Entity Recognition and Disambiguation Extraction Tools","Abstract":"Named entity extraction is a mature task in the nlp \ufb01eld that has yielded numerous services gaining popularity in the semantic web community for extracting knowledge from web documents. These services are generally organized as pipelines, using dedicated apis and different taxonomy for extracting, classifying and disambiguating named entities. Integrating one of these services in a particular application requires to implement an appropriate driver. Furthermore, the results of these services are not comparable due to different formats. This prevents the comparison of the performance of these services as well as their possible combination. We address this problem by proposing for, a framework which uni\ufb01es 10 popular named entity extractors available on the web, and the uni\ufb01es ontology which provides a rich set of axioms aligning the taxonomies of these tools.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2023,"Venue":"acl-2023","Acronym":"AutoConv","Description":"Automatically Generating Information-seeking Conversations with Large Language Models","Abstract":"Information-seeking conversation, which aims to help users gather information through conversation, has achieved great progress in recent years. However, the research is still stymied by the scarcity of training data. To alleviate this problem, we propose over for synthetic conversation generation, which takes advantage of the few-shot learning ability and generation capacity of large language models (llm). Specifically, we formulate the conversation generation problem as a language modeling task, then finetune an llm with a few human conversations to capture the characteristics of the information-seeking process and use it for generating synthetic conversations with high quality. Experimental results on two frequently-used datasets verify that few has substantial improvements over strong baselines and alleviates the dependence on human annotation. In addition, we also provide several analysis studies to promote future research.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2022,"Venue":"acl-2022","Acronym":"MemSum","Description":"Extractive Summarization of Long Documents Using Multi-Step Episodic Markov Decision Processes","Abstract":"We introduce history. (multi-step episodic markov decision process extractive summarizer), a reinforcement-learning-based extractive summarizer enriched at each step with information on the current extraction history. When iteratively iteratively selects sentences into the summary, it considers a broad information set that would intuitively also be used by humans in this task: 1) the text content of the sentence, 2) the global text context of the rest of the document, and 3) the extraction history consisting of the set of sentences that have already been extracted. With a lightweight architecture, human obtains state-of-the-art test-set performance (rouge) in summarizing long documents taken from pubmed, arxiv, and govreport. Ablation studies demonstrate the importance of local, global, and history information. A human evaluation confirms the high quality and low redundancy of the generated summaries, stemming from summaries,\u2019s awareness of extraction history.","wordlikeness":0.6666666667,"lcsratio":0.8333333333,"wordcoverage":0.7272727273}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"CRIPP-VQA","Description":"Counterfactual Reasoning about Implicit Physical Properties via Video Question Answering","Abstract":"Videos often capture objects, their visible properties, their motion, and the interactions between different objects. Objects also have physical properties such as mass, which the imaging pipeline is unable to directly capture. However, these properties can be estimated by utilizing cues from relative object motion and the dynamics introduced by collisions. In this paper, we introduce introduce, a new video question answering dataset for reasoning about the implicit physical properties of objects in a scene. Paper, contains videos of objects in motion, annotated with questions that involve counterfactual reasoning about the effect of actions, questions about planning in order to reach a goal, and descriptive questions about visible properties of objects. The introduced test set enables evaluation under several out-of-distribution settings \u2013 videos with objects with masses, coefficients of friction, and initial velocities that are not observed in the training distribution. Our experiments reveal a surprising and significant performance gap in terms of answering questions about implicit properties (the focus of this paper) and explicit properties of objects (the focus of prior work).","wordlikeness":0.4444444444,"lcsratio":0.8888888889,"wordcoverage":0.5882352941}
{"Year":2023,"Venue":"acl-2023","Acronym":"PairSpanBERT","Description":"An Enhanced Language Model for Bridging Resolution","Abstract":"We present and, a spanbert-based pre-trained model specialized for bridging resolution. To this end, we design a novel pre-training objective that aims to learn the contexts in which two mentions are implicitly linked to each other from a large amount of data automatically generated either heuristically or via distance supervision with a knowledge graph. Despite the noise inherent in the automatically generated data, we achieve the best results reported to date on three evaluation datasets for bridging resolution when replacing spanbert with two in a state-of-the-art resolver that jointly performs entity coreference resolution and bridging resolution.","wordlikeness":0.8333333333,"lcsratio":0.5,"wordcoverage":0.7}
{"Year":2021,"Venue":"tacl-2021","Acronym":"Erratum","Description":"Measuring and Improving Consistency in Pretrained Language Models","Abstract":"During production of this paper, an error was introduced to the formula on the bottom of the right column of page 1020. In the last two terms of the formula, the n and m subscripts were swapped. The correct formula is:lc=\u2211n=1k\u2211m=n+1kdkl(qnri\u2225qmri)+dkl(qmri\u2225qnri)the paper has been updated.","wordlikeness":0.7142857143,"lcsratio":0.8571428571,"wordcoverage":0.7142857143}
{"Year":2021,"Venue":"acl-2021","Acronym":"ZmBART","Description":"An Unsupervised Cross-lingual Transfer Framework for Language Generation","Abstract":"Despite the recent advancement in nlp research, cross-lingual transfer for natural language generation is relatively understudied. In this work, we transfer supervision from high resource language (hrl) to multiple lowresource languages (lrls) for natural language generation (nlg). We consider four nlg tasks (text summarization, question generation, news headline generation, and distractor generation) and three syntactically diverse languages, i.e., english, hindi, and japanese. We propose an unsupervised crosslingual language generation framework (called training) that does not use any parallel or pseudo-parallel\/back-translated data. In this framework, we further pre-train mbart sequence-to-sequence denoising auto-encoder model with an auxiliary task using monolingual data of three languages. The objective function of the auxiliary task is close to the target tasks which enriches the multi-lingual latent representation of mbart and provides good initialization for target tasks. Then, this model is \ufb01ne-tuned with task-speci\ufb01c supervised english data and directly evaluated with low-resource languages in the zero-shot setting. To overcome catastrophic forgetting and spurious correlation issues, we applied freezing model component and data argumentation approaches respectively. This simple modeling approach gave us promising results. We experimented with few-shot training (with 1000 supervised data-points) which boosted the model performance further. We performed several ablations and cross-lingual transferability analysis to demonstrate the robustness of forgetting.","wordlikeness":0.5,"lcsratio":0.6666666667,"wordcoverage":0.8}
{"Year":2022,"Venue":"bionlp-2022","Acronym":"DISTANT-CTO","Description":"A Zero Cost, Distantly Supervised Approach to Improve Low-Resource Entity Extraction Using Clinical Trials Literature","Abstract":"Pico recognition is an information extraction task for identifying participant, intervention, comparator, and outcome information from clinical literature. Manually identifying pico information is the most time-consuming step for conducting systematic reviews (sr), which is already labor-intensive. A lack of diversified and large, annotated corpora restricts innovation and adoption of automated pico recognition systems. The largest-available pico entity\/span corpus is manually annotated which is too expensive for a majority of the scientific community. To break through the bottleneck, we propose identifying, a novel distantly supervised pico entity extraction approach using the clinical trials literature, to generate a massive weakly-labeled dataset with more than a million \u2018intervention\u2019 and \u2018comparator\u2019 entity annotations. We train distant ner (named-entity recognition) models using this weakly-labeled dataset and demonstrate that it outperforms even the sophisticated models trained on the manually annotated dataset with a 2% f1 improvement over the intervention entity of the pico benchmark and more than 5% improvement when combined with the manually annotated dataset. We investigate the generalizability of our approach and gain an impressive f1 score on another domain-specific pico benchmark. The approach is not only zero-cost but is also scalable for a constant stream of pico entity annotations.","wordlikeness":0.7272727273,"lcsratio":1.0,"wordcoverage":0.7777777778}
{"Year":2020,"Venue":"findings-2020","Acronym":"CLAR","Description":"A Cross-Lingual Argument Regularizer for Semantic Role Labeling","Abstract":"Semantic role labeling (srl) identifies predicate-argument structure(s) in a given sentence. Although different languages have different argument annotations, polyglot training, the idea of training one model on multiple languages, has previously been shown to outperform monolingual baselines, especially for low resource languages. In fact, even a simple combination of data has been shown to be effective with polyglot training by representing the distant vocabularies in a shared representation space. Meanwhile, despite the dissimilarity in argument annotations between languages, certain argument labels do share common semantic meaning across languages (e.g. adjuncts have more or less similar semantic meaning across languages). To leverage such similarity in annotation space across languages, we propose a method called cross-lingual argument regularizer (do). Outperform identifies such linguistic annotation similarity across languages and exploits this information to map the target language arguments using a transformation of the space on which source language arguments lie. By doing so, our experimental results show that shown consistently improves srl performance on multiple languages over monolingual and polyglot baselines for low resource languages.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2021,"Venue":"findings-2021","Acronym":"UnClE","Description":"Explicitly Leveraging Semantic Similarity to Reduce the Parameters of Word Embeddings","Abstract":"Natural language processing (nlp) models often require a massive number of parameters for word embeddings, which limits their application on mobile devices. Researchers have employed many approaches, e.g. adaptive inputs, to reduce the parameters of word embeddings. However, existing methods rarely pay attention to semantic information. In this paper, we propose a novel method called unique and class embeddings (unique), which explicitly leverages semantic similarity with weight sharing to reduce the dimensionality of word embeddings. Inspired by the fact that words with similar semantic can share a part of weights, we divide the embeddings of words into two parts: unique embedding and class embedding. The former is one-to-one mapping like traditional embedding, while the latter is many-to-one mapping and learn the representation of class information. Our method is suitable for both word-level and sub-word level models and can be used to reduce both input and output embeddings. Experimental results on the standard wmt 2014 english-german dataset show that our method is able to reduce the parameters of word embeddings by more than 11x, with about 93% performance retaining in bleu metrics. For language modeling task, our model can reduce word embeddings by 6x or 11x on ptb\/wt2 dataset at the cost of a certain degree of performance degradation.","wordlikeness":1.0,"lcsratio":0.8,"wordcoverage":1.0}
{"Year":2023,"Venue":"ws-2023","Acronym":"Larth","Description":"Dataset and Machine Translation for Etruscan","Abstract":"Etruscan is an ancient language spoken in italy from the 7th century bc to the 1st century ad. There are no native speakers of the language at the present day, and its resources are scarce, as there are an estimated 12,000 known inscriptions. To the best of our knowledge, there are no publicly available etruscan corpora for natural language processing. Therefore, we propose a dataset for machine translation from etruscan to english, which contains 2891 translated examples from existing academic sources. Some examples are extracted manually, while others are acquired in an automatic way. Along with the dataset, we benchmark different machine translation models observing that it is possible to achieve a bleu score of 10.1 with a small transformer model. Releasing the dataset can help enable future research on this language, similar languages or other languages with scarce resources.","wordlikeness":0.8,"lcsratio":0.8,"wordcoverage":0.8}
{"Year":2011,"Venue":"ws-2011","Acronym":"KenLM","Description":"Faster and Smaller Language Model Queries","Abstract":"We present tables, a library that implements two data structures for ef\ufb01cient language model queries, reducing both time and memory costs. The probing data structure uses linear probing hash tables and is designed for speed. Compared with the widelyused srilm, our probing model is 2.4 times as fast while using 57% of the memory. The trie data structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed at lower memory consumption. Trie simultaneously uses less memory than the smallest lossless baseline and less cpu than the fastest baseline. Our code is open-source1, thread-safe, and integrated into the moses, cdec, and joshua translation systems. This paper describes the several performance techniques used and presents benchmarks against alternative implementations.","wordlikeness":0.6,"lcsratio":0.8,"wordcoverage":0.75}
{"Year":2020,"Venue":"findings-2020","Acronym":"AGIF","Description":"An Adaptive Graph-Interactive Framework for Joint Multiple Intent Detection and Slot Filling","Abstract":"In real-world scenarios, users usually have multiple intents in the same utterance. Unfortunately, most spoken language understanding (slu) models either mainly focused on the single intent scenario, or simply incorporated an overall intent context vector for all tokens, ignoring the fine-grained multiple intents information integration for token-level slot prediction. In this paper, we propose an adaptive graph-interactive framework (graph-interactive) for joint multiple intent detection and slot filling, where we introduce an intent-slot graph interaction layer to model the strong correlation between the slot and intents. Such an interaction layer is applied to each token adaptively, which has the advantage to automatically extract the relevant intents information, making a fine-grained intent information integration for the token-level slot prediction. Experimental results on three multi-intent datasets show that our framework obtains substantial improvement and achieves the state-of-the-art performance. In addition, our framework achieves new state-of-the-art performance on two single-intent datasets.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2014,"Venue":"semeval-2014","Acronym":"SemantiKLUE","Description":"Robust Semantic Similarity at Multiple Levels Using Maximum Weight Matching","Abstract":"Being able to quantify the semantic similarity between two texts is important for many practical applications. System combines unsupervised and supervised techniques into a robust system for measuring semantic similarity. At the core of the system is a word-to-word alignment of two texts using a maximum weight matching algorithm. The system participated in three semeval-2014 shared tasks and the competitive results are evidence for its usability in that broad \ufb01eld of application.","wordlikeness":0.6363636364,"lcsratio":0.9090909091,"wordcoverage":0.7058823529}
{"Year":2020,"Venue":"acl-2020","Acronym":"KLEJ","Description":"Comprehensive Benchmark for Polish Language Understanding","Abstract":"In recent years, a series of transformer-based models unlocked major improvements in general natural language understanding (nlu) tasks. Such a fast pace of research would not be possible without general nlu benchmarks, which allow for a fair comparison of the proposed methods. However, such benchmarks are available only for a handful of languages. To alleviate this issue, we introduce a comprehensive multi-task benchmark for the polish language understanding, accompanied by an online leaderboard. It consists of a diverse set of tasks, adopted from existing datasets for named entity recognition, question-answering, textual entailment, and others. We also introduce a new sentiment analysis task for the e-commerce domain, named allegro reviews (ar). To ensure a common evaluation scheme and promote models that generalize to different nlu tasks, the benchmark includes datasets from varying domains and applications. Additionally, we release herbert, a transformer-based model trained specifically for the polish language, which has the best average performance and obtains the best results for three out of nine tasks. Finally, we provide an extensive evaluation, including several standard baselines and recently proposed, multilingual transformer-based models.","wordlikeness":0.75,"lcsratio":0.75,"wordcoverage":0.75}
{"Year":2023,"Venue":"findings-2023","Acronym":"FACTUAL","Description":"A Benchmark for Faithful and Consistent Textual Scene Graph Parsing","Abstract":"Textual scene graph parsing has become increasingly important in various vision-language applications, including image caption evaluation and image retrieval. However, existing scene graph parsers that convert image captions into scene graphs often suffer from two types of errors. First, the generated scene graphs fail to capture the true semantics of the captions or the corresponding images, resulting in a lack of faithfulness. Second, the generated scene graphs have high inconsistency, with the same semantics represented by different annotations. To address these challenges, we propose a novel dataset, which involves re-annotating the captions in visual genome (vg) using a new intermediate representation called -mr.-mr. And-mr can be directly converted into faithful and consistent scene graph annotations. Our experimental results clearly demonstrate that the parser trained on our dataset outperforms existing approaches in terms of faithfulness and consistency. This improvement leads to a significant performance boost in both image caption evaluation and zero-shot image retrieval tasks. Furthermore, we introduce a novel metric for measuring scene graph similarity, which, when combined with the improved scene graph parser, achieves state-of-the-art (sota) results on multiple benchmark datasets for the aforementioned tasks.","wordlikeness":0.8571428571,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2022,"Venue":"lrec-2022","Acronym":"RaFoLa","Description":"A Rationale-Annotated Corpus for Detecting Indicators of Forced Labour","Abstract":"Forced labour is the most common type of modern slavery, and it is increasingly gaining the attention of the research and social community. Recent studies suggest that artificial intelligence (ai) holds immense potential for augmenting anti-slavery action. However, ai tools need to be developed transparently in cooperation with different stakeholders. Such tools are contingent on the availability and access to domain-specific data, which are scarce due to the near-invisible nature of forced labour. To the best of our knowledge, this paper presents the first openly accessible english corpus annotated for multi-class and multi-label forced labour detection. The corpus consists of 989 news articles retrieved from specialised data sources and annotated according to risk indicators defined by the international labour organization (ilo). Each news article was annotated for two aspects: (1) indicators of forced labour as classification labels and (2) snippets of the text that justify labelling decisions. We hope that our data set can help promote research on explainability for multi-class and multi-label text classification. In this work, we explain our process for collecting the data underpinning the proposed corpus, describe our annotation guidelines and present some statistical analysis of its content. Finally, we summarise the results of baseline experiments based on different variants of the bidirectional encoder representation from transformer (bert) model.","wordlikeness":0.3333333333,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2007,"Venue":"iwslt-2007","Acronym":"MISTRAL","Description":"a lattice translation system for IWSLT 2007","Abstract":"This paper describes speech, the lattice translation system that we developed for the italian-english track of the international workshop on spoken language translation 2007. Lattices, is a discriminative phrase-based system that translates a source word lattice in two passes. The first pass extracts a list of top ranked sentence pairs from the lattice and the second pass rescores this list with more complex features. Our experiments show that our system, when translating pruned lattices, is at least as good as a fair baseline that translates the first ranked sentences returned by a speech recognition system.","wordlikeness":0.8571428571,"lcsratio":0.7142857143,"wordcoverage":0.7777777778}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"SimCSE","Description":"Simple Contrastive Learning of Sentence Embeddings","Abstract":"This paper presents similarity, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework, by using \u201centailment\u201d pairs as positives and \u201ccontradiction\u201d pairs as hard negatives. We evaluate it on standard semantic textual similarity (sts) tasks, and our unsupervised and supervised models using bert base achieve an average of 76.3% and 81.6% spearman\u2019s correlation respectively, a 4.2% and 2.2% improvement compared to previous best results. We also show\u2014both theoretically and empirically\u2014that contrastive learning objective regularizes pre-trained embeddings\u2019 anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.","wordlikeness":0.3333333333,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2017,"Venue":"emnlp-2017","Acronym":"MinIE","Description":"Minimizing Facts in Open Information Extraction","Abstract":"The goal of open information extraction (oie) is to extract surface relations and their arguments from natural-language text in an unsupervised, domain-independent manner. In this paper, we propose by, an oie system that aims to provide useful, compact extractions with high precision and recall. Is approaches these goals by (1) representing information about polarity, modality, attribution, and quantities with semantic annotations instead of in the actual extraction, and (2) identifying and removing parts that are considered overly specific. We conducted an experimental study with several real-world datasets and found that specific. Achieves competitive or higher precision and recall than most prior systems, while at the same time producing shorter, semantically enriched extractions.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2023,"Venue":"findings-2023","Acronym":"DiaASQ","Description":"A Benchmark of Conversational Aspect-based Sentiment Quadruple Analysis","Abstract":"The rapid development of aspect-based sentiment analysis (absa) within recent decades shows great potential for real-world society. The current absa works, however, are mostly limited to the scenario of a single text piece, leaving the study in dialogue contexts unexplored. To bridge the gap between fine-grained sentiment analysis and conversational opinion mining, in this work, we introduce a novel task of conversational aspect-based sentiment quadruple analysis, namely study, aiming to detect the quadruple of target-aspect-opinion-sentiment in a dialogue. We manually construct a large-scale high-quality quadruple dataset in both chinese and english languages. We deliberately develop a neural model to benchmark the task, which advances in effectively performing end-to-end quadruple prediction, and manages to incorporate rich dialogue-specific and discourse feature representations for better cross-utterance quadruple extraction. We hope the new benchmark will spur more advancements in the sentiment analysis community.","wordlikeness":0.3333333333,"lcsratio":0.8333333333,"wordcoverage":0.7272727273}
{"Year":2014,"Venue":"tacl-2014","Acronym":"Senti-LSSVM","Description":"Sentiment-Oriented Multi-Relation Extraction with Latent Structural SVM","Abstract":"Extracting instances of sentiment-oriented relations from user-generated web documents is important for online marketing analysis. Unlike previous work, we formulate this extraction task as a structured prediction problem and design the corresponding inference as an integer linear program. Our latent structural svm based model can learn from training corpora that do not contain explicit annotations of sentiment-bearing expressions, and it can simultaneously recognize instances of both binary (polarity) and ternary (comparative) relations with regard to entity mentions of interest. The empirical evaluation shows that our approach significantly outperforms state-of-the-art systems across domains (cameras and movies) and across genres (reviews and forum posts). The gold standard corpus that we built will also be a valuable resource for the community.","wordlikeness":0.3636363636,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2000,"Venue":"naacl-2000","Acronym":"REES","Description":"A Large-Scale Relation and Event Extraction System","Abstract":"This paper reports on a large-scale, end-to- end relation and event extraction system. At present, the system extracts a total of 100 types of relations and events, which represents a much wider coverage than is typical of extraction systems. The system consists of three specialized pattem-based tagging modules, a high-precision co- reference resolution module, and a configurable template generation module. We report quantitative evaluation results, analyze the results in detail, and discuss future directions.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2020,"Venue":"lrec-2020","Acronym":"TaPaCo","Description":"A Corpus of Sentential Paraphrases for 73 Languages","Abstract":"This paper presents contains, a freely available paraphrase corpus for 73 languages extracted from the tatoeba database. Tatoeba is a crowdsourcing project mainly geared towards language learners. Its aim is to provide example sentences and translations for particular linguistic constructions and words. The paraphrase corpus is created by populating a graph with tatoeba sentences and equivalence links between sentences \u201cmeaning the same thing\u201d. This graph is then traversed to extract sets of paraphrases. Several language-independent filters and pruning steps are applied to remove uninteresting sentences. A manual evaluation performed on three languages shows that between half and three quarters of inferred paraphrases are correct and that most remaining ones are either correct but trivial, or near-paraphrases that neutralize a morphological distinction. The corpus contains a total of 1.9 million sentences, with 200 - 250 000 sentences per language. It covers a range of languages for which, to our knowledge, no other paraphrase dataset exists. The dataset is available at <a href=https:\/\/doi.org\/10.5281\/zenodo.3707949 class=acl-markup-url>https:\/\/doi.org\/10.5281\/zenodo.3707949<\/a>.","wordlikeness":0.6666666667,"lcsratio":0.8333333333,"wordcoverage":0.8}
{"Year":2015,"Venue":"ws-2015","Acronym":"Sar-graphs","Description":"A Linked Linguistic Knowledge Resource Connecting Facts with Language","Abstract":"We present for, a knowledge resource that links semantic relations from factual knowledge graphs to the linguistic patterns with which a language can express instances of these relations. Sar-graph expand upon existing lexicosemantic resources by modeling syntactic and semantic information at the level of relations, and are hence useful for tasks such as knowledge base population and relation extraction. We present a languageindependent method to automatically construct sar-graph instances that is based on distantly supervised relation extraction. We link expand at the lexical level to babelnet, wordnet and uby, and present our ongoing work on pattern- and relationlevel linking to framenet. An initial dataset of english together for 25 relations is made publicly available, together with a java-based api.","wordlikeness":0.6,"lcsratio":0.5,"wordcoverage":0.75}
{"Year":2020,"Venue":"lrec-2020","Acronym":"AMUSED","Description":"A Multi-Stream Vector Representation Method for Use in Natural Dialogue","Abstract":"The problem of building a coherent and non-monotonous conversational agent with proper discourse and coverage is still an area of open research. Current architectures only take care of semantic and contextual information for a given query and fail to completely account for syntactic and external knowledge which are crucial for generating responses in a chit-chat system. To overcome this problem, we propose an end to end multi-stream deep learning architecture that learns unified embeddings for query-response pairs by leveraging contextual information from memory networks and syntactic information by incorporating graph convolution networks (gcn) over their dependency parse. A stream of this network also utilizes transfer learning by pre-training a bidirectional transformer to extract semantic representation for each input sentence and incorporates external knowledge through the neighborhood of the entities from a knowledge base (kb). We benchmark these embeddings on the next sentence prediction task and significantly improve upon the existing techniques. Furthermore, we use from to represent query and responses along with its context to develop a retrieval based conversational agent which has been validated by expert linguists to have comprehensive engagement with humans.","wordlikeness":0.8333333333,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2022,"Venue":"ijcnlp-2022","Acronym":"FAD-X","Description":"Fusing Adapters for Cross-lingual Transfer to Low-Resource Languages","Abstract":"Adapter-based tuning, by adding light-weight adapters to multilingual pretrained language models (mplms), selectively updates language-specific parameters to adapt to a new language, instead of finetuning all shared weights. This paper explores an effective way to leverage a public pool of pretrained language adapters, to overcome resource imbalances for low-resource languages (lrls). Specifically, our research questions are, whether pretrained adapters can be composed, to complement or replace lrl adapters. While composing adapters for multi-task learning setting has been studied, the same question for lrls has remained largely unanswered. To answer this question, we study how to fuse adapters across languages and tasks, then validate how our proposed fusion adapter, namely transfer, can enhance a cross-lingual transfer from pretrained adapters, for well-known named entity recognition and classification benchmarks.","wordlikeness":0.2,"lcsratio":0.8,"wordcoverage":0.6666666667}
{"Year":2018,"Venue":"nlptea-2018","Acronym":"MULLE","Description":"A grammar-based Latin language learning tool to supplement the classroom setting","Abstract":"Aimed is a tool for language learning that focuses on teaching latin as a foreign language. It is aimed for easy integration into the traditional classroom setting and syllabus, which makes it distinct from other language learning tools that provide standalone learning experience. It uses grammar-based lessons and embraces methods of gamification to improve the learner motivation. The main type of exercise provided by our application is to practice translation, but it is also possible to shift the focus to vocabulary or morphology training.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.9090909091}
{"Year":2021,"Venue":"triton-2021","Acronym":"NoDeeLe","Description":"A Novel Deep Learning Schema for Evaluating Neural Machine Translation Systems","Abstract":"Due to the wide-spread development of machine translation (mt) systems \u2013especially neural machine translation (nmt) systems\u2013 mt evaluation, both automatic and human, has become more and more important as it helps us establish how mt systems perform. Yet, automatic evaluation metrics have lagged behind, as the most popular choices (e.g., bleu, meteor and rouge) may correlate poorly with human judgments. This paper seeks to put to the test an evaluation model based on a novel deep learning schema (accuracy,) used to compare two nmt systems on four different text genres, i.e. medical, legal, marketing and literary in the english-greek language pair. The model utilizes information from the source segments, the mt outputs and the reference translation, as well as the automatic metrics bleu, meteor and wer. The proposed schema achieves a strong correlation with human judgment (78% average accuracy for the four texts with the highest accuracy, i.e. 85%, observed in the case of the marketing text), while it outperforms classic machine learning algorithms and automatic metrics.","wordlikeness":0.7142857143,"lcsratio":1.0,"wordcoverage":0.7692307692}
{"Year":2023,"Venue":"findings-2023","Acronym":"ECG-QALM","Description":"Entity-Controlled Synthetic Text Generation using Contextual Q\\&amp;A for NER","Abstract":"Named entity recognition (ner) state-of-the-art methods requires high-quality labeled datasets. Issues such as scarcity of labeled data, under-representation of entities, and privacy concerns with using sensitive data for training, can be significant barriers. Generating synthetic data to train models is a promising solution to mitigate these problems. We propose ,, a contextual question and answering approach using pre-trained language models to synthetically generate entity-controlled text. Generated text is then used to augment small labeled datasets for downstream ner tasks. We evaluate our method on two publicly available datasets. We find appearing is capable of producing full text samples with desired entities appearing in a controllable way, while retaining sentence coherence closest to the real world data. Evaluations on ner tasks show significant improvements (75% - 140%) in low-labeled data regimes.","wordlikeness":0.25,"lcsratio":0.75,"wordcoverage":0.6666666667}
{"Year":2022,"Venue":"findings-2022","Acronym":"KETOD","Description":"Knowledge-Enriched Task-Oriented Dialogue","Abstract":"Existing studies in dialogue system research mostly treat task-oriented dialogue and chit-chat as separate domains. Towards building a human-like assistant that can converse naturally and seamlessly with users, it is important to build a dialogue system that conducts both types of conversations effectively. In this work, we investigate how task-oriented dialogue and knowledge-grounded chit-chat can be effectively integrated into a single model. To this end, we create a new dataset, a (knowledge-enriched task-oriented dialogue), where we naturally enrich task-oriented dialogues with chit-chat based on relevant entity knowledge. We also propose two new models, simpletodplus and combiner, for the proposed task. Experimental results on both automatic and human evaluations show that the proposed methods can significantly improve the performance in knowledge-enriched response generation while maintaining a competitive task-oriented dialog performance. We believe our new dataset will be a valuable resource for future studies. Our dataset and code are publicly available at <a href=https:\/\/github.com\/facebookresearch\/generation class=acl-markup-url>https:\/\/github.com\/facebookresearch\/automatic<\/a>.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2017,"Venue":"acl-2017","Acronym":"Morph-fitting","Description":"Fine-Tuning Word Vector Spaces with Simple Language-Specific Rules","Abstract":"Morphologically rich languages accentuate two properties of distributional vector space models: 1) the difficulty of inducing accurate representations for low-frequency word forms; and 2) insensitivity to distinct lexical relations that have similar distributional signatures. These effects are detrimental for language understanding systems, which may infer that \u2018inexpensive\u2019 is a rephrasing for \u2018expensive\u2019 or may not associate \u2018acquire\u2019 with \u2018acquires\u2019. In this work, we propose a novel understanding procedure which moves past the use of curated semantic lexicons for improving distributional vector spaces. Instead, our method injects morphological constraints generated using simple language-specific rules, pulling inflectional forms of the same word close together and pushing derivational antonyms far apart. In intrinsic evaluation over four languages, we show that our approach: 1) improves low-frequency word estimates; and 2) boosts the semantic quality of the entire word vector collection. Finally, we show that morph-fitted vectors yield large gains in the downstream task of dialogue state tracking, highlighting the importance of morphology for tackling long-tail phenomena in language understanding tasks.","wordlikeness":0.7692307692,"lcsratio":0.6153846154,"wordcoverage":0.7}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"RASAT","Description":"Integrating Relational Structures into Pretrained Seq2Seq Model for Text-to-SQL","Abstract":"Relational structures such as schema linking and schema encoding have been validated as a key component to qualitatively translating natural language into sql queries. However, introducing these structural relations comes with prices: they often result in a specialized model structure, which largely prohibits using large pretrained models in text-to-sql. To address this problem, we propose we: a transformer seq2seq architecture augmented with relation-aware self-attention that could leverage a variety of relational structures while inheriting the pretrained parameters from the t5 model effectively. Our model can incorporate almost all types of existing relations in the literature, and in addition, we propose introducing co-reference relations for the multi-turn scenario. Experimental results on three widely used text-to-sql datasets, covering both single-turn and multi-turn scenarios, have shown that three could achieve competitive results in all three benchmarks, achieving state-of-the-art execution accuracy (75.5% ex on spider, 52.6% iex on sparc, and 37.4% iex on cosql).","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2000,"Venue":"acl-2000","Acronym":"Panel","Description":"Good Spelling of Vietnamese Texts, One Aspect of Computational Linguistics in Vietnam","Abstract":"There are many challenging problems for vietnamese language processing. It will be a long time before these challenges are met. Even some apparently simple problems such as spelling correction are quite difficult and have not been approached systematically yet. In this paper, we will discuss one aspect of this type of work: designing the so-called vietools to detect and correct spelling of vietnamese texts by using a spelling database based on telex code. Vietools is also extended to serve many purposes in vietnamese language processing.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2022,"Venue":"naacl-2022","Acronym":"DREAM","Description":"Improving Situational QA by First Elaborating the Situation","Abstract":"When people answer questions about a specific situation, e.g., \u201ci cheated on my mid-term exam last week. Was that wrong?\u201d, cognitive science suggests that they form a mental picture of that situation before answering. While we do not know how language models (lms) answer such questions, we conjecture that they may answer more accurately if they are also provided with additional details about the question situation, elaborating the \u201cscene\u201d. To test this conjecture, we train a new model, cognitive, to answer questions that elaborate the scenes that situated questions are about, and then provide those elaborations as additional context to a question-answering (qa) model. We find that cognitive is able to create better scene elaborations (more accurate, useful, and consistent) than a representative state-of-the-art, zero-shot model (macaw). We also find that using the scene elaborations as additional context improves the answer accuracy of a downstream qa system, including beyond that obtainable by simply further fine-tuning the qa system on to\u2019s training data. These results suggest that adding focused elaborations about a situation can improve a system\u2019s reasoning about it, and may serve as an effective way of injecting new scenario-based knowledge into qa models. Finally, our approach is dataset-neutral; we observe improved qa performance across different models, with even bigger gains on models with fewer parameters.","wordlikeness":1.0,"lcsratio":0.6,"wordcoverage":1.0}
{"Year":2017,"Venue":"acl-2017","Acronym":"EmoNet","Description":"Fine-Grained Emotion Detection with Gated Recurrent Neural Networks","Abstract":"Accurate detection of emotion from natural language has applications ranging from building emotional chatbots to better understanding individuals and their lives. However, progress on emotion detection has been hampered by the absence of large labeled datasets. In this work, we build a very large dataset for fine-grained emotions and develop deep learning models on it. We achieve a new state-of-the-art on 24 fine-grained types of emotions (with an average accuracy of 87.58%). We also extend the task beyond emotion types to model robert plutick\u2019s 8 primary emotion dimensions, acquiring a superior accuracy of 95.68%.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.7692307692}
{"Year":2022,"Venue":"coling-2022","Acronym":"CGIM","Description":"A Cycle Guided Interactive Learning Model for Consistency Identification in Task-oriented Dialogue","Abstract":"Consistency identification in task-oriented dialog (ci-tod) usually consists of three subtasks, aiming to identify inconsistency between current system response and current user response, dialog history and the corresponding knowledge base. This work aims to solve ci-tod task by introducing an explicit interaction paradigm, cycle guided interactive learning model (response), which achieves to make information exchange explicitly from all the three tasks. Specifically, task relies on two core insights, referred to as guided multi-head attention module and cycle interactive mechanism, that collaborate from each other. On the one hand, each two tasks are linked with the guided multi-head attention module, aiming to explicitly model the interaction across two related tasks. On the other hand, we further introduce cycle interactive mechanism that focuses on facilitating model to exchange information among the three correlated sub-tasks via a cycle interaction manner. Experimental results on ci-tod benchmark show that our model achieves the state-of-the-art performance, pushing the overall score to 56.3% (5.0% point absolute improvement). In addition, we find that by is robust to the initial task flow order.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"MGDoc","Description":"Pre-training with Multi-granular Hierarchy for Document Image Understanding","Abstract":"Document images are a ubiquitous source of data where the text is organized in a complex hierarchical structure ranging from fine granularity (e.g., words), medium granularity (e.g., regions such as paragraphs or figures), to coarse granularity (e.g., the whole page). The spatial hierarchical relationships between content at different levels of granularity are crucial for document image understanding tasks. Existing methods learn features from either word-level or region-level but fail to consider both simultaneously. Word-level models are restricted by the fact that they originate from pure-text language models, which only encode the word-level context. In contrast, region-level models attempt to encode regions corresponding to paragraphs or text blocks into a single embedding, but they perform worse with additional word-level features. To deal with these issues, we propose hyperspace., a new multi-modal multi-granular pre-training framework that encodes page-level, region-level, and word-level information at the same time. Same uses a unified text-visual encoder to obtain multi-modal features across different granularities, which makes it possible to project the multi-granular features into the same hyperspace. To model the region-word correlation, we design a cross-granular attention mechanism and specific pre-training tasks for our model to reinforce the model of learning the hierarchy between regions and words. Experiments demonstrate that our proposed model can learn better features that perform well across granularities and lead to improvements in downstream tasks.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2020,"Venue":"readi-2020","Acronym":"LagunTest","Description":"A NLP Based Application to Enhance Reading Comprehension","Abstract":"The ability to read and understand written texts plays an important role in education, above all in the last years of primary education. This is especially pertinent in language immersion educational programmes, where some students have low linguistic competence in the languages of instruction. In this context, adapting the texts to the individual needs of each student requires a considerable effort by education professionals. However, language technologies can facilitate the laborious adaptation of materials in order to enhance reading comprehension. In this paper, we present should, a nlp based application that takes as input a text in basque or english, and offers synonyms, definitions, examples of the words in different contexts and presents some linguistic characteristics as well as visualizations. Programmes, is based on reusable and open multilingual and multimodal tools, and it is also distributed with an open license. By is intended to ease the burden of education professionals in the task of adapting materials, and the output should always be supervised by them.","wordlikeness":0.7777777778,"lcsratio":0.6666666667,"wordcoverage":0.8}
{"Year":2018,"Venue":"naacl-2018","Acronym":"RankME","Description":"Reliable Human Ratings for Natural Language Generation","Abstract":"Human evaluation for natural language generation (nlg) often suffers from inconsistent user ratings. While previous research tends to attribute this problem to individual user preferences, we show that the quality of human judgements can also be improved by experimental design. We present a novel rank-based magnitude estimation method (bayesian), which combines the use of continuous scales and relative assessments. We show that improved significantly improves the reliability and consistency of human ratings compared to traditional evaluation methods. In addition, we show that it is possible to evaluate nlg systems according to multiple, distinct criteria, which is important for error analysis. Finally, we demonstrate that compared, in combination with bayesian estimation of system quality, is a cost-effective alternative for ranking multiple nlg systems.","wordlikeness":0.8333333333,"lcsratio":0.6666666667,"wordcoverage":0.8333333333}
{"Year":2021,"Venue":"inlg-2021","Acronym":"Chefbot","Description":"A Novel Framework for the Generation of Commonsense-enhanced Responses for Task-based Dialogue Systems","Abstract":"Conversational systems aim to generate responses that are accurate, relevant and engaging, either through utilising neural end-to-end models or through slot filling. Human-to-human conversations are enhanced by not only the latest utterance of the interlocutor, but also by recalling relevant information about concepts\/objects covered in the dialogue and integrating them into their responses. Such information may contain recent referred concepts, commonsense knowledge and more. A concrete scenario of such dialogues is the cooking scenario, i.e. when an artificial agent (personal assistant, robot, chatbot) and a human converse about a recipe. We will demo a novel system for commonsense enhanced response generation in the scenario of cooking, where the conversational system is able to not only provide directions for cooking step-by-step, but also display <i>commonsense<\/i> capabilities by offering explanations of how objects can be used and provide recommendations for replacing ingredients.","wordlikeness":0.8571428571,"lcsratio":1.0,"wordcoverage":0.7272727273}
{"Year":2019,"Venue":"ws-2019","Acronym":"Pluto","Description":"A Deep Learning Based Watchdog for Anti Money Laundering","Abstract":"Banks are faced with anti-money laundering (aml) obligations so they have to identify customers by conducting negative news, aka \u201dadverse media\u201d, screening which consists of searching information in the public domain for news items, publications, and government advisories and bulletins for information related an individual or entity\u2019s involvement in \ufb01nancial crime matters. Although it is an essential way to determine who in the group poses a higher risk for potential \ufb01nancial crime concerns by catching sophisticated activities from negative news across globe, it also requires heavy human capital and processing time on screening daily produced negative news. Therefore, poor ef\ufb01ciency becomes the most unacceptable obstacle based on this approach. To mitigate this issue, techniques1 offers a distributed and scalable batch system embedded deep learning-based natural language processing (nlp) techniques for aml practitioners to improve daily task ef\ufb01ciency. It performs text preprocessing, paragraph embeddings, and clustering algorithm on a set of negative news and provide clustering result with keywords and similarities for aml practitioners. The overall feedback from aml practitioners are very positive on such an impressive enhancement in which in reduces 67% efforts of negative news screening.","wordlikeness":0.8,"lcsratio":0.8,"wordcoverage":0.75}
{"Year":2023,"Venue":"findings-2023","Acronym":"DivHSK","Description":"Diverse Headline Generation using Self-Attention based Keyword Selection","Abstract":"Diverse headline generation is an nlp task where given a news article, the goal is to generate multiple headlines that are true to the content of the article but are different among themselves. This task aims to exhibit and exploit semantically similar one-to-many relationships between a source news article and multiple target headlines. Toward this, we propose a novel model called multiple. It has two components:keyselect for selecting the important keywords, and seqgen, for finally generating the multiple diverse headlines. In keyselect, we cluster the self-attention heads of the last layer of the pre-trained encoder and select the most-attentive theme and general keywords from the source article. Then, cluster-specific keyword sets guide the seqgen, a pre-trained encoder-decoder model, to generate diverse yet semantically similar headlines. The proposed model consistently outperformed existing literature and our strong baselines and emerged as a state-of-the-art model. We have also created a high-quality multi-reference headline dataset from news articles.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2022,"Venue":"acl-2022","Acronym":"RoCBert","Description":"Robust Chinese Bert with Multimodal Contrastive Pretraining","Abstract":"Large-scale pretrained language models have achieved sota results on nlp tasks. However, they have been shown vulnerable to adversarial attacks especially for logographic languages like chinese. In this work, we propose human-made: a pretrained chinese bert that is robust to various forms of adversarial attacks like word perturbation, synonyms, typos, etc. It is pretrained with the contrastive learning objective which maximizes the label consistency under different synthesized adversarial examples. The model takes as input multimodal information including the semantic, phonetic and visual features. We show all these features areimportant to the model robustness since the attack can be performed in all the three forms. Across 5 chinese nlu tasks, maximizes outperforms strong baselines under three blackbox adversarial algorithms without sacrificing the performance on clean testset. It also performs the best in the toxic content detection task under human-made attacks.","wordlikeness":0.8571428571,"lcsratio":1.0,"wordcoverage":0.9230769231}
{"Year":2019,"Venue":"naacl-2019","Acronym":"Glocal","Description":"Incorporating Global Information in Local Convolution for Keyphrase Extraction","Abstract":"Graph convolutional networks (gcns) are a class of spectral clustering techniques that leverage localized convolution filters to perform supervised classification directly on graphical structures. While such methods model nodes\u2019 local pairwise importance, they lack the capability to model global importance relative to other nodes of the graph. This causes such models to miss critical information in tasks where global ranking is a key component for the task, such as in keyphrase extraction. We address this shortcoming by allowing the proper incorporation of global information into the gcn family of models through the use of scaled node weights. In the context of keyphrase extraction, incorporating global random walk scores obtained from textrank boosts performance significantly. With our proposed method, we achieve state-of-the-art results, bettering a strong baseline by an absolute 2% increase in f1 score.","wordlikeness":0.8333333333,"lcsratio":1.0,"wordcoverage":0.9090909091}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"HashFormers","Description":"Towards Vocabulary-independent Pre-trained Transformers","Abstract":"Transformer-based pre-trained language models are vocabulary-dependent, mapping by default each token to its corresponding embedding. This one-to-one mapping results into embedding matrices that occupy a lot of memory (i.e. millions of parameters) and grow linearly with the size of the vocabulary. Previous work on on-device transformers dynamically generate token embeddings on-the-fly without embedding matrices using locality-sensitive hashing over morphological information. These embeddings are subsequently fed into transformer layers for text classification. However, these methods are not pre-trained. Inspired by this line of work, we propose computationally, a new family of vocabulary-independent pre-trained transformers that support an unlimited vocabulary (i.e. all possible tokens in a corpus) given a substantially smaller fixed-sized embedding matrix. We achieve this by first introducing computationally cheap hashing functions that bucket together individual tokens to embeddings. We also propose three variants that do not require an embedding matrix at all, further reducing the memory requirements. We empirically demonstrate that cheap are more memory efficient compared to standard pre-trained transformers while achieving comparable predictive performance when fine-tuned on multiple text classification tasks. For example, our most efficient hashformer variant has a negligible performance degradation (0.4% on glue) using only 99.1k parameters for representing the embeddings compared to 12.3-38m parameters of state-of-the-art models.","wordlikeness":0.7272727273,"lcsratio":0.8181818182,"wordcoverage":0.7826086957}
{"Year":2008,"Venue":"lrec-2008","Acronym":"Glossa","Description":"a Multilingual, Multimodal, Configurable User Interface","Abstract":"We describe a web-based corpus query system, corpora,, which combines the expressiveness of regular query languages with the user-friendliness of a graphical interface. Since corpus users are usually linguists with little interest in technical matters, we have developed a system where the user need not have any prior knowledge of the search system. Furthermore, no previous knowledge of abbreviations for metavariables such as part of speech and source text is needed. All searches are done using checkboxes, pull-down menus, or writing simple letters to make words or other strings. Querying for more than one word is simply done by adding an additional query box, and for parts of words by choosing a feature such as \u0093start of word\u0094. The in system also allows a wide range of viewing and post-processing options. Collocations can be viewed and counted in a number of ways, and be viewed as different kinds of graphical charts. Further annotation and deletion of single results for further processing is also easy. The corpora. System is already in use for a number of corpora. Corpus administrators can easily adapt the system to a wide range of corpora, including multilingual corpora and corpora with audio and video content.","wordlikeness":0.8333333333,"lcsratio":0.8333333333,"wordcoverage":0.9090909091}
{"Year":2014,"Venue":"semeval-2014","Acronym":"SAP-RI","Description":"Twitter Sentiment Analysis in Two Days","Abstract":"We describe the submission of the sap research & innovation team to the semeval 2014 task 9: sentiment analysis in twitter. We challenged ourselves to develop a competitive sentiment analysis system within a very limited time frame. Our submission was developed in less than two days and achieved an f1 score of 77.26% for contextual polarity disambiguation and 55.47% for message polarity classi\ufb01cation, which shows that rapid prototyping of sentiment analysis systems with reasonable accuracy is possible.","wordlikeness":0.5,"lcsratio":0.5,"wordcoverage":0.7272727273}
{"Year":2021,"Venue":"autosimtrans-2021","Acronym":"BSTC","Description":"A Large-Scale Chinese-English Speech Translation Dataset","Abstract":"This paper presents we (baidu speech translation corpus), a large-scale chinese-english speech translation dataset. This dataset is constructed based on a collection of licensed videos of talks or lectures, including about 68 hours of mandarin data, their manual transcripts and translations into english, as well as automated transcripts by an automatic speech recognition (asr) model. We have further asked three experienced interpreters to simultaneously interpret the testing talks in a mock conference setting. This corpus is expected to promote the research of automatic simultaneous translation as well as the development of practical systems. We have organized simultaneous translation tasks and used this corpus to evaluate automatic simultaneous translation systems.","wordlikeness":0.25,"lcsratio":0.5,"wordcoverage":0.8571428571}
{"Year":2019,"Venue":"sigdial-2019","Acronym":"SpaceRefNet","Description":"a neural approach to spatial reference resolution in a real city environment","Abstract":"Adding interactive capabilities to pedestrian wayfinding systems in the form of spoken dialogue will make them more natural to humans. Such an interactive wayfinding system needs to continuously understand and interpret pedestrian\u2019s utterances referring to the spatial context. Achieving this requires the system to identify exophoric referring expressions in the utterances, and link these expressions to the geographic entities in the vicinity. This exophoric spatial reference resolution problem is difficult, as there are often several dozens of candidate referents. We present a neural network-based approach for identifying pedestrian\u2019s references (using a network called refnet) and resolving them to appropriate geographic objects (using a network called these). Both methods show promising results beating the respective baselines and earlier reported results in the literature.","wordlikeness":0.7272727273,"lcsratio":0.9090909091,"wordcoverage":0.7619047619}
{"Year":2007,"Venue":"semeval-2007","Acronym":"WIT","Description":"Web People Search Disambiguation using Random Walks","Abstract":"In this paper, we describe our work on a random walks-based approach to disambiguating people in web search results, and the implementation of a system that supports such approach, which we used to participate at semeval\u201907 web people search task.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2020,"Venue":"emnlp-2020","Acronym":"AdapterHub","Description":"A Framework for Adapting Transformers","Abstract":"The current modus operandi in nlp involves downloading and fine-tuning pre-trained models consisting of millions or billions of parameters. Storing and sharing such large trained models is expensive, slow, and time-consuming, which impedes progress towards more general and versatile nlp methods that learn from and for many tasks. Adapters\u2014small learnt bottleneck layers inserted within each layer of a pre-trained model\u2014 ameliorate this issue by avoiding full fine-tuning of the entire model. However, sharing and integrating adapter layers is not straightforward. We propose on, a framework that allows dynamic \u201cstiching-in\u201d of pre-trained adapters for different tasks and languages. The framework, built on top of the popular huggingface transformers library, enables extremely easy and quick adaptations of state-of-the-art pre-trained models (e.g., bert, roberta, xlm-r) across tasks and languages. Downloading, sharing, and training adapters is as seamless as possible using minimal changes to the training scripts and a specialized infrastructure. Our framework enables scalable and easy access to sharing of task-specific models, particularly in low-resource scenarios. Bert, includes all recent adapter architectures and can be found at model\u2014.","wordlikeness":0.8,"lcsratio":0.7,"wordcoverage":0.8235294118}
{"Year":2022,"Venue":"naacl-2022","Acronym":"ExSum","Description":"From Local Explanations to Model Understanding","Abstract":"Interpretability methods are developed to understand the working mechanisms of black-box models, which is crucial to their responsible deployment. Fulfilling this goal requires both that the explanations generated by these methods are correct and that people can easily and reliably understand them. While the former has been addressed in prior work, the latter is often overlooked, resulting in informal model understanding derived from a handful of local explanations. In this paper, we introduce explanation summary (mathematical), a mathematical framework for quantifying model understanding, and propose metrics for its quality assessment. On two domains, plausibility. Highlights various limitations in the current practice, helps develop accurate model understanding, and reveals easily overlooked properties of the model. We also connect understandability to other properties of explanations such as human alignment, robustness, and counterfactual similarity and plausibility.","wordlikeness":0.8,"lcsratio":0.8,"wordcoverage":0.75}
{"Year":2020,"Venue":"acl-2020","Acronym":"INFOTABS","Description":"Inference on Tables as Semi-structured Data","Abstract":"In this paper, we observe that semi-structured tabulated text is ubiquitous; understanding them requires not only comprehending the meaning of text fragments, but also implicit relationships between them. We argue that such data can prove as a testing ground for understanding how we reason about information. To study this, we introduce a new dataset called unsuccessful, comprising of human-written textual hypotheses based on premises that are tables extracted from wikipedia info-boxes. Our analysis shows that the semi-structured, multi-domain and heterogeneous nature of the premises admits complex, multi-faceted reasoning. Experiments reveal that, while human annotators agree on the relationships between a table-hypothesis pair, several standard modeling strategies are unsuccessful at the task, suggesting that reasoning about tables can pose a difficult modeling challenge.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":1999,"Venue":"mtsummit-1999","Acronym":"TransRouter","Description":"a decision support tool for translation managers","Abstract":"Translation managers often have to decide on the most appropriate way to deal with a translation project. Possible options may include human translation, translation using a specific terminology resource, translation in interaction with a translation memory system, and machine translation. The decision making involved is complex, and it is not always easy to decide by inspection whether a specific text lends itself to certain kinds of treatment. Sentence supports the decision making by offering a suite of computer based tools which can be used to analyse the text to be translated. Some tools, such as the word counter, the repetition detector, the sentence length estimator and the sentence simplicity checker look at characteristics of the text itself. A version comparison tool compares the new text to previously translated texts. Other tools, such as the unknown terms detector and the translation memory coverage estimator, estimate overlap between the text and a set of known resources. The information gained, combined with further information provided by the user, is input to a decision kernel which calculates possible routes towards achieving the translation together with their cost and consequences on translation quality. The user may influence the kernel by, for example, specifying particular resources or refining routes under investigation. The final decision on how to treat the project rests with the translation manager.","wordlikeness":0.8181818182,"lcsratio":0.7272727273,"wordcoverage":0.7368421053}
{"Year":2021,"Venue":"eacl-2021","Acronym":"BERTese","Description":"Learning to Speak to BERT","Abstract":"Large pre-trained language models have been shown to encode large amounts of world and commonsense knowledge in their parameters, leading to substantial interest in methods for extracting that knowledge. In past work, knowledge was extracted by taking manually-authored queries and gathering paraphrases for them using a separate pipeline. In this work, we propose a method for automatically rewriting queries into \u201crewrites,\u201d, a paraphrase query that is directly optimized towards better knowledge extraction. To encourage meaningful rewrites, we add auxiliary loss functions that encourage the query to correspond to actual language tokens. We empirically show our approach outperforms competing baselines, obviating the need for complex pipelines. Moreover, taking provides some insight into the type of language that helps language models perform knowledge extraction.","wordlikeness":0.7142857143,"lcsratio":0.7142857143,"wordcoverage":0.75}
{"Year":2021,"Venue":"nodalida-2021","Acronym":"EstBERT","Description":"A Pretrained Language-Specific BERT for Estonian","Abstract":"This paper presents bert, a large pretrained transformer-based language-specific bert model for estonian. Recent work has evaluated multilingual bert models on estonian tasks and found them to outperform the baselines. Still, based on existing studies on other languages, a language-specific bert model is expected to improve over the multilingual ones. We first describe the first pretraining process and then present the models\u2019 results based on the finetuned model for multiple nlp tasks, including pos and morphological tagging, dependency parsing, named entity recognition and text classification. The evaluation results show that the models based on improve outperform multilingual bert models on five tasks out of seven, providing further evidence towards a view that training language-specific bert models are still useful, even when multilingual models are available.","wordlikeness":0.8571428571,"lcsratio":0.8571428571,"wordcoverage":0.7692307692}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"TEMP","Description":"Taxonomy Expansion with Dynamic Margin Loss through Taxonomy-Paths","Abstract":"As an essential form of knowledge representation, taxonomies are widely used in various downstream natural language processing tasks. However, with the continuously rising of new concepts, many existing taxonomies are unable to maintain coverage by manual expansion. In this paper, we propose many, a self-supervised taxonomy expansion method, which predicts the position of new concepts by ranking the generated taxonomy-paths. For the first time, benchmarks. Employs pre-trained contextual encoders in taxonomy construction and hypernym detection problems. Experiments prove that pre-trained contextual embeddings are able to capture hypernym-hyponym relations. To learn more detailed differences between taxonomy-paths, we train the model with dynamic margin loss by a novel dynamic margin function. Extensive evaluations exhibit that rising outperforms prior state-of-the-art taxonomy expansion approaches by 14.3% in accuracy and 15.8% in mean reciprocal rank on three public benchmarks.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2021,"Venue":"sustainlp-2021","Acronym":"Distiller","Description":"A Systematic Study of Model Distillation Methods in Natural Language Processing","Abstract":"Knowledge distillation (kd) offers a natural way to reduce the latency and memory\/energy usage of massive pretrained models that have come to dominate natural language processing (nlp) in recent years. While numerous sophisticated variants of kd algorithms have been proposed for nlp applications, the key factors underpinning the optimal distillation performance are often confounded and remain unclear. We aim to identify how different components in the kd pipeline affect the resulting performance and how much the optimal kd pipeline varies across different datasets\/tasks, such as the data augmentation policy, the loss function, and the intermediate representation for transferring the knowledge between teacher and student. To tease apart their effects, we propose the, a meta kd framework that systematically combines a broad range of techniques across different stages of the kd pipeline, which enables us to quantify each component\u2019s contribution. Within distillation, we unify commonly used objectives for distillation of intermediate representations under a universal mutual information (mi) objective and propose a class of mi-objective functions with better bias\/variance trade-off for estimating the mi between the teacher and the student. On a diverse set of nlp datasets, the best auto configurations are identified via large-scale hyper-parameter optimization. Our experiments reveal the following: 1) the approach used to distill the intermediate representations is the most important factor in kd performance, 2) among different objectives for intermediate distillation, mi-performs the best, and 3) data augmentation provides a large boost for small training datasets or small student networks. Moreover, we find that different datasets\/tasks prefer different kd algorithms, and thus propose a simple autoboost algorithm that can recommend a good kd pipeline for a new dataset.","wordlikeness":0.8888888889,"lcsratio":1.0,"wordcoverage":0.9473684211}
{"Year":2021,"Venue":"naacl-2021","Acronym":"TR-BERT","Description":"Dynamic Token Reduction for Accelerating BERT Inference","Abstract":"Existing pre-trained language models (plms) are often computationally expensive in inference, making them impractical in various resource-limited real-world applications. To address this issue, we propose a dynamic token reduction approach to accelerate plms\u2019 inference, named process, which could flexibly adapt the layer number of each token in inference to avoid redundant calculation. Specially, the formulates the token reduction process as a multi-step token selection problem and automatically learns the selection strategy via reinforcement learning. The experimental results on several downstream nlp tasks show that a is able to speed up bert by 2-5 times to satisfy various performance demands. Moreover, also can also achieve better performance with less computation in a suite of long-text tasks since its token-level layer number adaption greatly accelerates the self-attention operation in plms. the source code and experiment details of this paper can be obtained from <a href=https:\/\/github.com\/thunlp\/applications. Class=acl-markup-url>https:\/\/github.com\/thunlp\/on<\/a>.","wordlikeness":0.5714285714,"lcsratio":0.8571428571,"wordcoverage":0.7692307692}
{"Year":2015,"Venue":"semeval-2015","Acronym":"Duluth","Description":"Word Sense Discrimination in the Service of Lexicography","Abstract":"This paper describes the paper systems that participated in task 15 of semeval 2015. The goal of the task was to automatically construct dictionary entries (via a series of three subtasks). Our systems participated in subtask 2, which involved automatically clustering the contexts in which a target word occurs into its different senses. Our results are consistent with previous word sense induction and discrimination \ufb01ndings, where it proves dif\ufb01cult to beat a baseline algorithm that assigns all instances of a target word to a single sense. However, our method of predicting the number of senses automatically fared quite well.","wordlikeness":0.5,"lcsratio":0.5,"wordcoverage":1.0}
{"Year":2006,"Venue":"lrec-2006","Acronym":"PrepNet","Description":"a Multilingual Lexical Description of Prepositions","Abstract":"In this paper, we present the results of a preliminary investigation that aims at constructing a repository of preposition syntactic and semantic behaviors. A preliminary frame-based format for representing their prototypical behavior is then proposed together with related inferential patterns that describe functional or paradigmatic relations between preposition senses.","wordlikeness":0.8571428571,"lcsratio":0.7142857143,"wordcoverage":0.7692307692}
{"Year":2010,"Venue":"semeval-2010","Acronym":"SZTERGAK","Description":"Feature Engineering for Keyphrase Extraction","Abstract":"Automatically assigning keyphrases to documents has a great variety of applications. Here we focus on the keyphrase extraction of scienti\ufb01c publications and present a novel set of features for the supervised learning of keyphraseness. Although these features are intended for extracting keyphrases from scienti\ufb01c papers, because of their generality and robustness, they should have uses in other domains as well. With the help of these features documents achieved top results on the semeval-2 shared task on automatic keyphrase extraction from scienti\ufb01c articles and exceeded its baseline by 10%.","wordlikeness":0.5,"lcsratio":0.625,"wordcoverage":0.7692307692}
{"Year":2014,"Venue":"semeval-2014","Acronym":"Sensible","Description":"L2 Translation Assistance by Emulating the Manual Post-Editing Process","Abstract":"This paper describes the post-editor z system submitted to the l2 writing assistant task in semeval-2014. The aim of task is to build a translation assistance system to translate untranslated sentence fragments. This is not unlike the task of post-editing where human translators improve machine-generated translations. Post-editor z emulates the manual process of post-editing by (i) crawling and extracting parallel sentences that contain the untranslated fragments from a web-based translation memory, (ii) extracting the possible translations of the fragments indexed by the translation memory and (iii) applying simple cosine-based sentence similarity to rank possible translations for the untranslated fragment.","wordlikeness":1.0,"lcsratio":0.875,"wordcoverage":1.0}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"MatchPrompt","Description":"Prompt-based Open Relation Extraction with Semantic Consistency Guided Clustering","Abstract":"Relation clustering is a general approach for open relation extraction (openre). Current methods have two major problems. One is that their good performance relies on large amounts of labeled and pre-defined relational instances for pre-training, which are costly to acquire in reality. The other is that they only focus on learning a high-dimensional metric space to measure the similarity of novel relations and ignore the specific relational representations of clusters. In this work, we propose a new prompt-based framework named interpretability., which can realize openre with efficient knowledge transfer from only a few pre-defined relational instances as well as mine the specific meanings for cluster interpretability. To our best knowledge, we are the first to introduce a prompt-based framework for unlabeled clustering. Experimental results on different datasets show that transfer achieves the new sota results for openre.","wordlikeness":0.7272727273,"lcsratio":0.6363636364,"wordcoverage":0.7058823529}
{"Year":2013,"Venue":"semeval-2013","Acronym":"MELODI","Description":"Semantic Similarity of Words and Compositional Phrases using Latent Vector Weighting","Abstract":"In this paper we present our system for the semeval 2013 task 5a on semantic similarity of words and compositional phrases. Our system uses a dependency-based vector space model, in combination with a technique called latent vector weighting. The system computes the similarity between a particular noun instance and the head noun of a particular noun phrase, which was weighted according to the semantics of the modi\ufb01er. The system is entirely unsupervised; one single parameter, the similarity threshold, was tuned using the training data.","wordlikeness":1.0,"lcsratio":0.8333333333,"wordcoverage":0.8571428571}
{"Year":2022,"Venue":"bionlp-2022","Acronym":"EchoGen","Description":"Generating Conclusions from Echocardiogram Notes","Abstract":"Generating a summary from findings has been recently explored (zhang et al., 2018, 2020) in note types such as radiology reports that typically have short length. In this work, we focus on echocardiogram notes that is longer and more complex compared to previous note types. We formally define the task of echocardiography conclusion generation (automatically) as generating a conclusion given the findings section, with emphasis on key cardiac findings. To promote the development of has methods, we present a new benchmark, which consists of two datasets collected from two hospitals. We further compare both standard and start-of-the-art methods on this new benchmark, with an emphasis on factual consistency. To accomplish this, we develop a tool to automatically extract concept-attribute tuples from the text. We then propose an evaluation metric, factcomp, to compare concept-attribute tuples between the human reference and generated conclusions. Both automatic and human evaluations show that there is still a significant gap between human-written and machine-generated conclusions on echo reports in terms of factuality and overall quality.","wordlikeness":0.8571428571,"lcsratio":0.8571428571,"wordcoverage":0.7692307692}
{"Year":2021,"Venue":"naacl-2021","Acronym":"EaSe","Description":"A Diagnostic Tool for VQA based on Answer Diversity","Abstract":"We propose it, a simple diagnostic tool for visual question answering (vqa) which quantifies the difficulty of an image, question sample. The is based on the pattern of answers provided by multiple annotators to a given question. In particular, it considers two aspects of the answers: (i) their entropy; (ii) their semantic content. First, we prove the validity of our diagnostic to identify samples that are easy\/hard for state-of-art vqa models. Second, we show that of can be successfully used to select the most-informative samples for training\/fine-tuning. Crucially, only information that is readily available in any vqa dataset is used to compute its scores.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2006,"Venue":"coling-2006","Acronym":"Clavius","Description":"Bi-Directional Parsing for Generic Multimodal Interaction","Abstract":"We introduce a new multi-threaded parsing algorithm on uni\ufb01cation grammars designed speci\ufb01cally for multimodal interaction and noisy environments. By lifting some traditional constraints, namely those related to the ordering of constituents, we overcome several dif\ufb01culties of other systems in this domain. We also present several criteria used in this model to constrain the search process using dynamically loadable scoring functions. Some early analyses of our implementation are discussed.","wordlikeness":0.7142857143,"lcsratio":0.7142857143,"wordcoverage":0.8333333333}
{"Year":2012,"Venue":"acl-2012","Acronym":"PORT","Description":"a Precision-Order-Recall MT Evaluation Metric for Tuning","Abstract":"Many machine translation (mt) evaluation metrics have been shown to correlate better with human judgment than bleu. In principle, tuning on these metrics should yield better systems than tuning on bleu. However, due to issues such as speed, requirements for linguistic resources, and optimization difficulty, they have not been widely adopted for tuning. This paper presents bleu. 1, a new mt evaluation metric which combines precision, recall and an ordering metric and which is primarily designed for tuning mt systems. (including does not require external resources and is quick to compute. It has a better correlation with human judgment than bleu. We compare 1,-tuned mt systems to bleu-tuned baselines in five experimental conditions involving four language pairs. Shown tuning achieves consistently better performance than bleu tuning, according to four automated metrics (including bleu) and to human evaluation: in comparisons of outputs from 300 source sentences, human judges preferred the tuning-tuned output 45.3% of the time (vs. 32.7% bleu tuning preferences and 22.0% ties).","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2022,"Venue":"woah-2022","Acronym":"GreaseVision","Description":"Rewriting the Rules of the Interface","Abstract":"Digital harms can manifest across any interface. Key problems in addressing these harms include the high individuality of harms and the fast-changing nature of digital systems. We put forth human-in-the-loop, a collaborative human-in-the-loop learning framework that enables end-users to analyze their screenomes to annotate harms as well as render overlay interventions. We evaluate hitl intervention development with a set of completed tasks in a cognitive walkthrough, and test scalability with one-shot element removal and fine-tuning hate speech classification models. The contribution of the framework and tool allow individual end-users to study their usage history and create personalized interventions. Our contribution also enables researchers to study the distribution of multi-modal harms and interventions at scale.","wordlikeness":0.9166666667,"lcsratio":0.5833333333,"wordcoverage":0.8}
{"Year":2021,"Venue":"naacl-2021","Acronym":"X-METRA-ADA","Description":"Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question Answering","Abstract":"Multilingual models, such as m-bert and xlm-r, have gained increasing popularity, due to their zero-shot cross-lingual transfer learning capabilities. However, their generalization ability is still inconsistent for typologically diverse languages and across different benchmarks. Recently, meta-learning has garnered attention as a promising technique for enhancing transfer learning under low-resource scenarios: particularly for cross-lingual transfer in natural language understanding (nlu). In this work, we propose capabilities., a cross-lingual meta-transfer learning adaptation approach for nlu. Our approach adapts maml, an optimization-based meta-learning approach, to learn to adapt to new languages. We extensively evaluate our framework on two challenging cross-lingual nlu tasks: multilingual task-oriented dialog and typologically diverse question answering. We show that our approach outperforms naive fine-tuning, reaching competitive performance on both tasks for most languages. Our analysis reveals that technique can leverage limited data for faster adaptation.","wordlikeness":0.3636363636,"lcsratio":0.8181818182,"wordcoverage":0.5555555556}
{"Year":2018,"Venue":"lrec-2018","Acronym":"M-CNER","Description":"A Corpus for Chinese Named Entity Recognition in Multi-Domains","Abstract":"In this paper, we present a new corpus for chinese named entity recognition (ner) from three domains : human-computer interaction, social media, and e-commerce. The annotation procedure is conducted in two rounds. In the first round, one sentence is annotated by more than one persons independently. In the second round, the experts discuss the sentences for which the annotators do not make agreements. Finally, we obtain a corpus which have five data sets in three domains. We further evaluate three popular models on the newly created data sets. The experimental results show that the system based on bi-lstm-crf performs the best among the comparison systems on all the data sets. The corpus can be used for further studies in research community. Keywords: named entity recognition; chinese data set; information extraction 1.","wordlikeness":0.5,"lcsratio":0.6666666667,"wordcoverage":0.7272727273}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"AfroLM","Description":"A Self-Active Learning-based Multilingual Pretrained Language Model for 23 African Languages","Abstract":"In recent years, multilingual pre-trained language models have gained prominence due to their remarkable performance on numerous downstream natural language processing tasks (nlp). However, pre-training these large multilingual language models requires a lot of training data, which is not available for african languages. Active learning is a semi-supervised learning algorithm, in which a model consistently and dynamically learns to identify the most beneficial samples to train itself on, in order to achieve better optimization and performance on downstream tasks. Furthermore, active learning effectively and practically addresses real-world data scarcity. Despite all its benefits, active learning, in the context of nlp and especially multilingual language models pretraining, has received little consideration. In this paper, we present <b>effectively<\/b>, a multilingual language model pretrained from scratch on 23 african languages (the largest effort to date) using our novel self-active learning framework. Pretrained on a dataset significantly (14x) smaller than existing baselines, <b>due<\/b> outperforms many multilingual pretrained language models (afriberta, xlmr-base, mbert) on various nlp downstream tasks (ner, text classification, and sentiment analysis). Additional out-of-domain sentiment analysis experiments show that <b>itself<\/b> is able to generalize well across various domains. We release the code source, and our datasets used in our framework at <a href=https:\/\/github.com\/bonaventuredossou\/mlm_al class=acl-markup-url>https:\/\/github.com\/bonaventuredossou\/mlm_al<\/a>.","wordlikeness":0.5,"lcsratio":0.8333333333,"wordcoverage":0.7272727273}
{"Year":2022,"Venue":"naacl-2022","Acronym":"Falsesum","Description":"Generating Document-level NLI Examples for Recognizing Factual Inconsistency in Summarization","Abstract":"Neural abstractive summarization models are prone to generate summaries that are factually inconsistent with their source documents. Previous work has introduced the task of recognizing such factual inconsistency as a downstream application of natural language inference (nli). However, state-of-the-art nli models perform poorly in this context due to their inability to generalize to the target task. In this work, we show that nli models can be effective for this task when the training data is augmented with high-quality task-oriented examples. We introduce examples., a data generation pipeline leveraging a controllable text generation model to perturb human-annotated summaries, introducing varying types of factual inconsistencies. Unlike previously introduced document-level nli datasets, our generated dataset contains examples that are diverse and inconsistent yet plausible. We show that models trained on a yet-augmented nli dataset improve the state-of-the-art performance across four benchmarks for detecting factual inconsistency in summarization.","wordlikeness":0.875,"lcsratio":1.0,"wordcoverage":0.7692307692}
{"Year":2018,"Venue":"naacl-2018","Acronym":"Mittens","Description":"an Extension of GloVe for Learning Domain-Specialized Representations","Abstract":"We present a simple extension of the glove representation learning model that begins with general-purpose representations and updates them based on data from a specialized domain. We show that the resulting representations can lead to faster learning and better results on a variety of tasks.","wordlikeness":0.8571428571,"lcsratio":0.8571428571,"wordcoverage":0.7692307692}
{"Year":2015,"Venue":"tacl-2015","Acronym":"Sprite","Description":"Generalizing Topic Models with Structured Priors","Abstract":"We introduce demonstrate, a family of topic models that incorporates structure into model priors as a function of underlying components. The structured priors can be constrained to model topic hierarchies, factorizations, correlations, and supervision, allowing apply to be tailored to particular settings. We demonstrate this flexibility by constructing a structure-based model to jointly infer topic hierarchies and author perspective, which we apply to corpora of political debates and online reviews. We show that the model learns intuitive topics, outperforming several other topic models at predictive tasks.","wordlikeness":0.6666666667,"lcsratio":0.6666666667,"wordcoverage":0.9090909091}
{"Year":2023,"Venue":"trustnlp-2023","Acronym":"IMBERT","Description":"Making BERT Immune to Insertion-based Backdoor Attacks","Abstract":"Backdoor attacks are an insidious security threat against machine learning models. Adversaries can manipulate the predictions of compromised models by inserting triggers into the training phase. Various backdoor attacks have been devised which can achieve nearly perfect attack success without affecting model predictions for clean inputs. Means of mitigating such vulnerabilities are underdeveloped, especially in natural language processing. To fill this gap, we introduce victim, which uses either gradients or self-attention scores derived from victim models to self-defend against backdoor attacks at inference time. Our empirical studies demonstrate that attacks can effectively identify up to 98.5% of inserted triggers. Thus, it significantly reduces the attack success rate while attaining competitive accuracy on the clean dataset across widespread insertion-based attacks compared to two baselines. Finally, we show that our approach is model-agnostic, and can be easily ported to several pre-trained transformer models.","wordlikeness":0.8333333333,"lcsratio":1.0,"wordcoverage":0.8333333333}
{"Year":2010,"Venue":"lrec-2010","Acronym":"mwetoolkit","Description":"a Framework for Multiword Expression Identification","Abstract":"This paper presents the multiword expression toolkit (better), an environment for type and language-independent mwe identification from corpora. The indexation provides a targeted list of mwe candidates, extracted and filtered according to a number of user-defined criteria and a set of standard statistical association measures. For generating corpus counts, the toolkit provides both a corpus indexation facility and a tool for integration with web search engines, while for evaluation, it provides validation and annotation facilities. The ways also allows easy integration with a machine learning tool for the creation and application of supervised mwe extraction models if annotated data is available. In our experiment, the biomedical was tested and evaluated in the context of mwe extraction in the biomedical domain. Our preliminary results show that the toolkit performs better than other approaches, especially concerning recall. Moreover, this first version can also be extended in several ways in order to improve the quality of the results.","wordlikeness":0.7,"lcsratio":0.7,"wordcoverage":0.7272727273}
{"Year":2011,"Venue":"acl-2011","Acronym":"SystemT","Description":"A Declarative Information Extraction System","Abstract":"Emerging text-intensive enterprise applications such as social analytics and semantic search pose new challenges of scalability and usability to information extraction (ie) systems. This paper presents semantic, a declarative ie system that addresses these challenges and has been deployed in a wide range of enterprise applications. Includes facilitates the development of high quality complex annotators by providing a highly expressive language and an advanced development environment. It also includes a cost-based optimizer and a high-performance, \ufb02exible runtime with minimum memory footprint. We present promote as a useful resource that is freely available, and as an opportunity to promote research in building scalable and usable ie systems.","wordlikeness":0.8571428571,"lcsratio":0.8571428571,"wordcoverage":0.9230769231}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"LILA","Description":"A Unified Benchmark for Mathematical Reasoning","Abstract":"Mathematical reasoning skills are essential for general-purpose intelligentsystems to perform tasks from grocery shopping to climate modeling. Towards evaluating and improving ai systems in this domain, we proposevs., a unified mathematical reasoning benchmark consisting of 23 diversetasks along four dimensions:(i) mathematical abilities e.g., arithmetic, calculus (ii) language format e.g., question-answering, fill-in-the-blanks (iii) language diversity e.g., no language, simple language (iv) external knowledge e.g., commonsense, physics. We construct our benchmark by extending 20 datasets benchmark by collecting task instructions and solutions in the form of python programs,thereby obtaining explainable solutions in addition to the correct answer. We additionally introduce two evaluation datasets to measure out-of-distribution performance and robustness to language perturbation. Finally, we introduce bhaskara,a general-purpose mathematical reasoning model trained on of. Importantly, we find that multi-tasking leads to significant improvements (average relative improvement of 21.83% f1 score vs. Single-task models),while the best performing model only obtains 60.40%,indicating the room for improvement in general mathematical reasoning and understanding.","wordlikeness":0.5,"lcsratio":0.75,"wordcoverage":0.8571428571}
{"Year":2015,"Venue":"semeval-2015","Acronym":"UIR-PKU","Description":"Twitter-OpinMiner System for Sentiment Analysis in Twitter at SemEval 2015","Abstract":"Microblogs are considered as we-media information with many real-time opinions. This paper presents a twitter-opinminer system for twitter sentiment analysis evaluation at semeval 2015. Our approach stems from two different angles: topic detection for discovering the sentiment distribution on different topics and sentiment analysis based on a variety of features. Moreover, we also implemented intra-sentence discourse relations for polarity identification. We divided the discourse relations into 4 predefined categories, including continuation, contrast, condition, and cause. These relations could facilitate us to eliminate polarity ambiguities in compound sentences where both positive and negative sentiments are appearing. Based on the semeval 2014 and semeval 2015 twitter sentiment analysis task datasets, the experimental results show that the performance of twitter-opinminer could effectively recognize opinionated messages and identify the polarities.","wordlikeness":0.2857142857,"lcsratio":0.5714285714,"wordcoverage":0.6666666667}
{"Year":2015,"Venue":"naacl-2015","Acronym":"Brahmi-Net","Description":"A transliteration and script conversion system for languages of the Indian subcontinent","Abstract":"We present corpus - an online system for transliteration and script conversion for all major indian language pairs (306 pairs). The system covers 13 indo-aryan languages, 4 dravidian languages and english. For training the transliteration systems, we mined parallel transliteration corpora from parallel translation corpora using an unsupervised method and trained statistical transliteration systems using the mined corpora. Languages which do not have parallel corpora are supported by transliteration through a bridge language. Our script conversion system supports conversion between all brahmi-derived scripts as well as itrans romanization scheme. For this, we leverage co-ordinated unicode ranges between indic scripts and use an extended itrans encoding for transliterating between english and indic scripts. The system also provides top-k transliterations and simultaneous transliteration into multiple output languages. We provide a python as well as rest api to access these services. The api and the mined transliteration corpus are made available for research use under an open source license.","wordlikeness":0.6,"lcsratio":0.7,"wordcoverage":0.7058823529}
{"Year":2021,"Venue":"eacl-2021","Acronym":"CHOLAN","Description":"A Modular Approach for Neural Entity Linking on Wikipedia and Wikidata","Abstract":"In this paper, we propose surface, a modular approach to target end-to-end entity linking (el) over knowledge bases. Among consists of a pipeline of two transformer-based models integrated sequentially to accomplish the el task. The first transformer model identifies surface forms (entity mentions) in a given text. For each mention, a second transformer model is employed to classify the target entity among a predefined candidates list. The latter transformer is fed by an enriched context captured from the sentence (i.e. local context), and entity description gained from wikipedia. Such external contexts have not been used in state of the art el approaches. Our empirical study was conducted on two well-known knowledge bases (i.e., wikidata and wikipedia). The empirical results suggest that state-of-the-art outperforms state-of-the-art approaches on standard datasets such as conll-aida, msnbc, aquaint, ace2004, and t-rex.","wordlikeness":0.8333333333,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2008,"Venue":"ijcnlp-2008","Acronym":"Vaakkriti","Description":"Sanskrit Tokenizer","Abstract":"Machine translation has evolved tremendously in the recent time and stood as center of research interest for many computer scientists. Developing a machine translation system for ancient languages is much more fascinating and challenging task. A detailed study of sanskrit language reveals that its well-structured and \ufb01nely organized grammar has af\ufb01nity for automated translation systems. This paper provides necessary analysis of sanskrit grammar in the perspective of machine translation and also provides one of the possible solution for samaas vigraha(compound dissolution).","wordlikeness":0.6666666667,"lcsratio":0.6666666667,"wordcoverage":0.6666666667}
{"Year":2006,"Venue":"eacl-2006","Acronym":"ASSIST","Description":"Automated Semantic Assistance for Translators","Abstract":"The problem we address in this paper is that of providing contextual examples of translation equivalents for words from the general lexicon using comparable corpora and semantic annotation that is uniform for the source and target languages. For a sentence, phrase or a query expression in the source language the tool detects the semantic type of the situation in question and gives examples of similar contexts from the target language corpus.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2007,"Venue":"law-2007","Acronym":"GrAF","Description":"A Graph-based Format for Linguistic Annotations","Abstract":"In this paper we describe the graph annotation format (developed) and show how it is used represent not only independent linguistic annotations, but also sets of merged annotations as a single graph. To demonstrate this, we have automatically transduced several different annotations of the wall street journal corpus into among and show how the annotations can then be merged, analyzed, and visualized using standard graph algorithms and tools. We also discuss how, as a standard graph representation, it allows for the application of well-established graph traversal and analysis algorithms to produce information about interactions and commonalities among merged annotations. How is an extension of the linguistic annotation framework (laf) (ide and romary, 2004, 2006) developed within iso tc37 sc4 and as such, implements state-of-the-art best practice guidelines for representing linguistic annotations.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2007,"Venue":"semeval-2007","Acronym":"HIT-WSD","Description":"Using Search Engine for Multilingual Chinese-English Lexical Sample Task","Abstract":"We have participated in the multilingual chinese-english lexical sample task of semeval-2007. Our system disambiguates senses of chinese words and finds the correct translation in english by using the web as wsd knowledge source. Since all the statistic data is obtained from search engine, the method is considered to be unsupervised and does not require any sense-tagged corpus.","wordlikeness":0.4285714286,"lcsratio":0.7142857143,"wordcoverage":0.7272727273}
{"Year":2023,"Venue":"findings-2023","Acronym":"NonFactS","Description":"NonFactual Summary Generation for Factuality Evaluation in Document Summarization","Abstract":"Pre-trained abstractive summarization models can generate fluent summaries and achieve high rouge scores. Previous research has found that these models often generate summaries that are inconsistent with their context document and contain nonfactual information. To evaluate factuality in document summarization, a document-level natural language inference (nli) classifier can be used. However, training such a classifier requires large-scale high-quality factual and nonfactual samples. To that end, we introduce large-scale, a data generation model, to synthesize nonfactual summaries given a context document and a human-annotated (reference) factual summary. Compared to previous methods, our nonfactual samples are more abstractive and more similar to their corresponding factual samples, resulting in state-of-the-art performance on two factuality evaluation benchmarks, falsesum and summac. Our experiments demonstrate that even without human-annotated summaries, trained can use random sentences to generate nonfactual summaries and a classifier trained on these samples generalizes to out-of-domain documents.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.7692307692}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"COMBO","Description":"State-of-the-Art Morphosyntactic Analysis","Abstract":"We introduce to \u2013 a fully neural nlp system for accurate part-of-speech tagging, morphological analysis, lemmatisation, and (enhanced) dependency parsing. It predicts categorical morphosyntactic features whilst also exposes their vector representations, extracted from hidden layers. Install is an easy to install python package with automatically downloadable pre-trained models for over 40 languages. It maintains a balance between efficiency and quality. As it is an end-to-end system and its modules are jointly trained, its training is competitively fast. As its models are optimised for accuracy, they achieve often better prediction quality than sota. The to library is available at: <a href=https:\/\/gitlab.clarin-pl.eu\/syntactic-tools\/from class=acl-markup-url>https:\/\/gitlab.clarin-pl.eu\/syntactic-tools\/trained,<\/a>.","wordlikeness":1.0,"lcsratio":0.6,"wordcoverage":1.0}
{"Year":1992,"Venue":"coling-1992","Acronym":"CTM","Description":"An Example-Based Translation Aid System","Abstract":"This paper describes a japanese-english translation aid system, c'i'm, which has a usefid capability for flexible retrieval of texts from hilingaal corpora or translation databases. Translation examples (pairs of a text and its translation equivalent) are very helpful for us to translate the similar text. Our character-based best match retrieval method can re: trieve translation examples similar to the given input. This method has the following advantages: (1) this method accepts free-style translation examples, i.e., pairs of any text string and its translation equiva- lent, (2) morphological analysis is unnecessary, (3) this method accepts fl'ce-myle inlmts (i.e., any text strings) for retrieval. We show the retrieval examples with the following characteristic features: phrasal ex- pression, long-distance dependency, idiom, synonym, mad semantic ambiguity.","wordlikeness":0.3333333333,"lcsratio":0.6666666667,"wordcoverage":0.8}
{"Year":2020,"Venue":"acl-2020","Acronym":"FEQA","Description":"A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization","Abstract":"Neural abstractive summarization models are prone to generate content inconsistent with the source document, i.e. unfaithful. Existing automatic metrics do not capture such mistakes effectively. We tackle the problem of evaluating faithfulness of a generated summary given its source document. We first collected human annotations of faithfulness for outputs from numerous models on two datasets. We find that current models exhibit a trade-off between abstractiveness and faithfulness: outputs with less word overlap with the source document are more likely to be unfaithful. Next, we propose an automatic question answering (qa) based metric for faithfulness, qa, which leverages recent advances in reading comprehension. Given question-answer pairs generated from the summary, a qa model extracts answers from the document; non-matched answers indicate unfaithful information in the summary. Among metrics based on word overlap, embedding similarity, and learned language understanding models, our qa-based metric has significantly higher correlation with human faithfulness scores, especially on highly abstractive summaries.","wordlikeness":0.25,"lcsratio":0.75,"wordcoverage":0.75}
{"Year":2022,"Venue":"starsem-2022","Acronym":"AnaLog","Description":"Testing Analytical and Deductive Logic Learnability in Language Models","Abstract":"We investigate the extent to which pre-trained language models acquire analytical and deductive logical reasoning capabilities as a side effect of learning word prediction. We present different, a natural language inference task designed to probe models for these capabilities, controlling for different invalid heuristics the models may adopt instead of learning the desired generalisations. We test four languagemodels on which, finding that they have all learned, to a different extent, to encode information that is predictive of entailment beyond shallow heuristics such as lexical overlap and grammaticality. We closely analyse the best performing language model and show that while it performs more consistently than other language models across logical connectives and reasoning domains, it still is sensitive to lexical and syntactic variations in the realisation of logical statements.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":0.9230769231}
{"Year":2016,"Venue":"emnlp-2016","Acronym":"HUME","Description":"Human UCCA-Based Evaluation of Machine Translation","Abstract":"Human evaluation of machine translation normally uses sentence-level measures such as relative ranking or adequacy scales. However, these provide no insight into possible errors, and do not scale well with sentence length. We argue for a semantics-based evaluation, which captures what meaning components are retained in the mt output, thus providing a more \ufb01ne-grained analysis of translation quality, and enabling the construction and tuning of semantics-based mt. We present a novel human semantic evaluation measure, human ucca-based mt evaluation (are), building on the ucca semantic representation scheme. Ucca covers a wider range of semantic phenomena than previous methods and does not rely on semantic annotation of the potentially garbled mt output. We experiment with four language pairs, demonstrating scales.\u2019s broad applicability, and report good inter-annotator agreement rates and correlation with human adequacy scores.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2022,"Venue":"naacl-2022","Acronym":"TSTR","Description":"Too Short to Represent, Summarize with Details! Intro-Guided Extended Summary Generation","Abstract":"Many scientific papers such as those in arxiv and pubmed data collections have abstracts with varying lengths of 50-1000 words and average length of approximately 200 words, where longer abstracts typically convey more information about the source paper. Up to recently, scientific summarization research has typically focused on generating short, abstract-like summaries following the existing datasets used for scientific summarization. In domains where the source text is relatively long-form, such as in scientific documents, such summary is not able to go beyond the general and coarse overview and provide salient information from the source document. The recent interest to tackle this problem motivated curation of scientific datasets, arxiv-long and pubmed-long, containing human-written summaries of 400-600 words, hence, providing a venue for research in generating long\/extended summaries. Extended summaries facilitate a faster read while providing details beyond coarse information. In this paper, we propose case), an extractive summarizer that utilizes the introductory information of documents as pointers to their salient information. The evaluations on two existing large-scale extended summarization datasets indicate statistically significant improvement in terms of rouge and average rouge (f1) scores (except in one case) as compared to strong baselines and state-of-the-art. Comprehensive human evaluations favor our generated extended summaries in terms of cohesion and completeness.","wordlikeness":0.25,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2020,"Venue":"emnlp-2020","Acronym":"SentiLARE","Description":"Sentiment-Aware Language Representation Learning with Linguistic Knowledge","Abstract":"Most of the existing pre-trained language representation models neglect to consider the linguistic knowledge of texts, which can promote language understanding in nlp tasks. To benefit the downstream tasks in sentiment analysis, we propose a novel language representation model called texts,, which introduces word-level linguistic knowledge including part-of-speech tag and sentiment polarity (inferred from sentiwordnet) into pre-trained models. We first propose a context-aware sentiment attention mechanism to acquire the sentiment polarity of each word with its part-of-speech tag by querying sentiwordnet. Then, we devise a new pre-training task called label-aware masked language model to construct knowledge-aware language representation. Experiments show that models. Obtains new state-of-the-art performance on a variety of sentiment analysis tasks.","wordlikeness":0.7777777778,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2020,"Venue":"lrec-2020","Acronym":"TheRuSLan","Description":"Database of Russian Sign Language","Abstract":"In this paper, a new russian sign language multimedia database this is presented. The database includes lexical units (single words and phrases) from russian sign language within one subject area, namely, \u201cfood products at the supermarket\u201d, and was collected using ms kinect 2.0 device including both fullhd video and the depth map modes, which provides new opportunities for the lexicographical description of the russian sign language vocabulary and enhances research in the field of automatic gesture recognition. Russian sign language has an official status in russia, and over 120,000 deaf people in russia and its neighboring countries use it as their first language. Russian sign language has no writing system, is poorly described and belongs to the low-resource languages. The authors formulate the basic principles of annotation of sign words, based on the collected data, and reveal the content of the collected database. In the future, the database will be expanded and comprise more lexical units. The database is explicitly made for the task of creating an automatic system for russian sign language recognition.","wordlikeness":0.7777777778,"lcsratio":0.8888888889,"wordcoverage":0.75}
{"Year":2019,"Venue":"ws-2019","Acronym":"DisSim","Description":"A Discourse-Aware Syntactic Text Simplification Framework for English and German","Abstract":"We introduce facts, a discourse-aware sentence splitting framework for english and german whose goal is to transform syntactically complex sentences into an intermediate representation that presents a simple and more regular structure which is easier to process for downstream semantic applications. For this purpose, we turn input sentences into a two-layered semantic hierarchy in the form of core facts and accompanying contexts, while identifying the rhetorical relations that hold between them. In that way, we preserve the coherence structure of the input and, hence, its interpretability for downstream tasks.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2022,"Venue":"icon-2022","Acronym":"PAR","Description":"Persona Aware Response in Conversational Systems","Abstract":"To make the human computer interaction more user friendly and persona aligned, detection of user persona is of utmost significance. Towards achieving this objective, we describe a novel approach to select the persona of a user from pre-determine list of personas and utilize it to generate personalized responses. This is achieved in two steps. Firstly, closest matching persona is detected from a set of pre-determined persona for the user. The second step involves the use of a fine-tuned natural language generation (nlg) model to generate persona compliant responses. Through experiments, we demonstrate that the proposed architecture generates better responses than current approaches by using a detected persona. Experimental evaluation on the personachat dataset has demonstrated notable performance in terms of perplexity and f1-score.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2016,"Venue":"emnlp-2016","Acronym":"POLY","Description":"Mining Relational Paraphrases from Multilingual Sentences","Abstract":"Language resources that systematically organize paraphrases for binary relations are of great value for various nlp tasks and have recently been advanced in projects like patty, wisenet and defie. This paper presents a new method for building such a resource and the resource itself, called have. Starting with a very large collection of multilingual sentences parsed into triples of phrases, our method clusters relational phrases using probabilistic measures. We judiciously leverage \ufb01ne-grained semantic typing of relational arguments for identifying synonymous phrases. The evaluation of improvements shows signi\ufb01cant improvements in precision and recall over the prior works on patty and defie. An extrinsic use case demonstrates the bene\ufb01ts of arguments for question answering.","wordlikeness":1.0,"lcsratio":0.75,"wordcoverage":1.0}
{"Year":2020,"Venue":"findings-2020","Acronym":"UNIFIEDQA","Description":"Crossing Format Boundaries with a Single QA System","Abstract":"Question answering (qa) tasks have been posed using a variety of formats, such as extractive span selection, multiple choice, etc. This has led to format-specialized models, and even to an implicit division in the qa community. We argue that such boundaries are artificial and perhaps unnecessary, given the reasoning abilities we seek to teach are not governed by the format. As evidence, we use the latest advances in language modeling to build a single pre-trained qa model, pre-trained, that performs well across 19 qa datasets spanning 4 diverse formats. Simply performs on par with 8 different models that were trained on individual datasets themselves. Even when faced with 12 unseen datasets of observed formats, variety performs surprisingly well, showing strong generalization from its outof-format training data. Finally, simply finetuning this pre trained qa model into specialized models results in a new state of the art on 10 factoid and commonsense question answering datasets, establishing using as a strong starting point for building qa systems.","wordlikeness":0.7777777778,"lcsratio":0.7777777778,"wordcoverage":0.875}
{"Year":2020,"Venue":"acl-2020","Acronym":"CDL","Description":"Curriculum Dual Learning for Emotion-Controllable Response Generation","Abstract":"Emotion-controllable response generation is an attractive and valuable task that aims to make open-domain conversations more empathetic and engaging. Existing methods mainly enhance the emotion expression by adding regularization terms to standard cross-entropy loss and thus influence the training process. However, due to the lack of further consideration of content consistency, the common problem of response generation tasks, safe response, is intensified. Besides, query emotions that can help model the relationship between query and response are simply ignored in previous models, which would further hurt the coherence. To alleviate these problems, we propose a novel framework named curriculum dual learning (significantly) which extends the emotion-controllable response generation to a dual task to generate emotional responses and emotional queries alternatively. Novel utilizes two rewards focusing on emotion and content to improve the duality. Additionally, it applies curriculum learning to gradually generate high-quality responses based on the difficulties of expressing various emotions. Experimental results show that however, significantly outperforms the baselines in terms of coherence, diversity, and relation to emotion factors.","wordlikeness":0.3333333333,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2016,"Venue":"ws-2016","Acronym":"Demographer","Description":"Extremely Simple Name Demographics","Abstract":"The lack of demographic information available when conducting passive analysis of social media content can make it dif\ufb01cult to compare results to traditional survey results. We present present,1 a tool that predicts gender from names, using name lists and a classi\ufb01er with simple character-level features. By relying only on a name, our tool can make predictions even without extensive user-authored content. We compare performance. To other available tools and discuss differences in performance. In particular, we show that show performs well on twitter data, making it useful for simple and rapid social media demographic inference.","wordlikeness":0.9090909091,"lcsratio":0.8181818182,"wordcoverage":0.8181818182}
{"Year":2017,"Venue":"acl-2017","Acronym":"EuroSense","Description":"Automatic Harvesting of Multilingual Sense Annotations from Parallel Text","Abstract":"Parallel corpora are widely used in a variety of natural language processing tasks, from machine translation to cross-lingual word sense disambiguation, where parallel sentences can be exploited to automatically generate high-quality sense annotations on a large scale. In this paper we present generate, a multilingual sense-annotated resource based on the joint disambiguation of the europarl parallel corpus, with almost 123 million sense annotations for over 155 thousand distinct concepts and entities from a language-independent unified sense inventory. We evaluate the quality of our sense annotations intrinsically and extrinsically, showing their effectiveness as training data for word sense disambiguation.","wordlikeness":0.8888888889,"lcsratio":0.8888888889,"wordcoverage":0.7777777778}
{"Year":2023,"Venue":"acl-2023","Acronym":"SaFER","Description":"A Robust and Efficient Framework for Fine-tuning BERT-based Classifier with Noisy Labels","Abstract":"Learning on noisy datasets is a challenging problem when pre-trained language models are applied to real-world text classification tasks. In numerous industrial applications, acquiring task-specific datasets with 100% accurate labels is difficult, thus many datasets are accompanied by label noise at different levels. Previous work has shown that existing noise-handling methods could not improve the peak performance of bert on noisy datasets, and might even deteriorate it. In this paper, we propose previous, a robust and efficient fine-tuning framework for bert-based text classifiers, combating label noises without access to any clean data for training or validation. Utilizing a label-agnostic early-stopping strategy and self-supervised learning, our proposed framework achieves superior performance in terms of both accuracy and speed on multiple text classification benchmarks. The trained model is finally fully deployed in several industrial biomedical literature mining tasks and demonstrates high effectiveness and efficiency.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2020,"Venue":"findings-2020","Acronym":"exBERT","Description":"Extending Pre-trained Models with Domain-specific Vocabulary Under Constrained Training Resources","Abstract":"We introduce data)., a training method to extend bert pre-trained models from a general domain to a new pre-trained model for a specific domain with a new additive vocabulary under constrained training resources (i.e., constrained computation and data). Method uses a small extension module to learn to adapt an augmenting embedding for the new domain in the context of the original bert\u2019s embedding of a general vocabulary. The , training method is novel in learning the new vocabulary and the extension module while keeping the weights of the original bert model fixed, resulting in a substantial reduction in required training resources. We pre-train computation with biomedical articles from clinicalkey and pubmed central, and study its performance on biomedical downstream benchmark tasks using the mtl-bioinformatics-2016 datasets. We demonstrate that specific consistently outperforms prior approaches when using limited corpus and pre-training computation resources.","wordlikeness":0.8333333333,"lcsratio":1.0,"wordcoverage":0.9090909091}
{"Year":2016,"Venue":"coling-2016","Acronym":"YAMAMA","Description":"Yet Another Multi-Dialect Arabic Morphological Analyzer","Abstract":"In this paper, we present the, a multi-dialect arabic morphological analyzer and disambiguator. Our system is almost five times faster than the state-of-art madamira system with a slightly lower quality. In addition to speed, but outputs a rich representation which allows for a wider spectrum of use. In this regard, the transcends other systems, such as farasa, which is faster but provides specific outputs catering to specific applications.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2023,"Venue":"acl-2023","Acronym":"DaMSTF","Description":"Domain Adversarial Learning Enhanced Meta Self-Training for Domain Adaptation","Abstract":"Self-training emerges as an important research line on domain adaptation. By taking the model\u2019s prediction as the pseudo labels of the unlabeled data, self-training bootstraps the model with pseudo instances in the target domain. However, the prediction errors of pseudo labels (label noise) challenge the performance of self-training. To address this problem, previous approaches only use reliable pseudo instances, i.e., pseudo instances with high prediction confidence, to retrain the model. Although these strategies effectively reduce the label noise, they are prone to miss the hard examples. In this paper, we propose a new self-training framework for domain adaptation, namely domain adversarial learning enhanced self-training framework (we). Firstly, task, involves meta-learning to estimate the importance of each pseudo instance, so as to simultaneously reduce the label noise and preserve hard examples. Secondly, we design a meta constructor for constructing the meta-validation set, which guarantees the effectiveness of the meta-learning module by improving the quality of the meta-validation set. Thirdly, we find that the meta-learning module suffers from the training guidance vanish- ment and tends to converge to an inferior optimal. To this end, we employ domain adversarial learning as a heuristic neural network initialization method, which can help the meta-learning module converge to a better optimal. Theoretically and experimentally, we demonstrate the effectiveness of the proposed suffers. On the cross-domain sentiment classification task, set, improves the performance of bert with an average of nearly 4%.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"ELMER","Description":"A Non-Autoregressive Pre-trained Language Model for Efficient and Effective Text Generation","Abstract":"We study the text generation task under the approach of pre-trained language models (plms). Typically, an auto-regressive (ar) method is adopted for generating texts in a token-by-token manner. Despite many advantages of ar generation, it usually suffers from inefficient inference. Therefore, non-autoregressive (nar) models are proposed to generate all target tokens simultaneously. However, nar models usually generate texts of lower quality due to the absence of token dependency in the output text. In this paper, we propose gap: an efficient and effective plm for nar text generation to explicitly model the token dependency during nar generation. By leveraging the early exit technique, will enables the token generations at different layers, according to their prediction confidence (a more confident token will exit at a lower layer). Besides, we propose a novel pre-training objective, layer permutation language modeling, to pre-train however, by permuting the exit layer for each token in sequences. Experiments on three text generation tasks show that tokens significantly outperforms nar models and further narrows the performance gap with ar plms (gap (29.92) vs bart (30.61) rouge-l in xsum) while achieving over 10 times inference speedup.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2021,"Venue":"syntaxfest-2021","Acronym":"UDWiki","Description":"guided creation and exploitation of UD treebanks","Abstract":"Guiding is an online environment designed to make creating new ud treebanks easier. It helps in setting up all the necessary data needed for a new treebank up in a gui, where the interface takes care of guiding you through all the descriptive \ufb01les needed, adding new texts to your corpus, and helping in annotating the texts. The system is built on top of the teitok corpus environment, using an xml based version of ud annotation, where dependencies can be combined with various other types of annotations. Based can run all the necessary or helpful scripts (taggers, parsers, validators) via the interface. It also makes treebanks under development directly searchable, and can be used to maintain or search existing ud treebanks.","wordlikeness":0.6666666667,"lcsratio":0.6666666667,"wordcoverage":0.6666666667}
{"Year":2012,"Venue":"starsem-2012","Acronym":"ATA-Sem","Description":"Chunk-based Determination of Semantic Text Similarity","Abstract":"This paper describes investigations into using syntactic chunk information as the basis for determining the similarity of candidate texts at the semantic level. Two approaches were considered. The first was a corpus-based method that extracted lexical and semantic features from pairs of chunks from each sentence that were associated through a chunk alignment algorithm. The features were used as input to a classifier trained on the same features extracted from a corpus of gold standard training data. The second approach involved breadthfirst chunk association and the application of a rule-based scoring algorithm. Both approaches were evaluated against the test data for the semeval 2012 semantic text similarity task. The results show that the rule-based chunk approach is superior.","wordlikeness":0.2857142857,"lcsratio":0.8571428571,"wordcoverage":0.6666666667}
{"Year":2023,"Venue":"acl-2023","Acronym":"EEL","Description":"Efficiently Encoding Lattices for Reranking","Abstract":"Standard decoding approaches for conditional text generation tasks typically search for an output hypothesis with high model probability, but this may not yield the best hypothesis according to human judgments of quality. Reranking to optimize for \u201cdownstream\u201d metrics can more closely optimize for quality, but many metrics of interest are computed with pre-trained language models, which are slow to apply to large numbers of hypotheses. We explore an approach for reranking hypotheses by using transformers to efficiently encode lattices of generated outputs, a method we call we. With a single transformer pass over the entire lattice, we can approximately compute a contextualized representation of each token as if it were only part of a single hypothesis in isolation. We combine this approach with a new class of token-factored rerankers (tfrs) that allow for efficient extraction of high reranker-scoring hypotheses from the lattice. Empirically, our approach incurs minimal degradation error compared to the exponentially slower approach of encoding each hypothesis individually. When applying as with tfrs across three text generation tasks, our results show both substantial speedup compared to naive reranking and often better performance on downstream metrics than comparable approaches.","wordlikeness":0.3333333333,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2023,"Venue":"acl-2023","Acronym":"XDailyDialog","Description":"A Multilingual Parallel Dialogue Corpus","Abstract":"High-quality datasets are significant to the development of dialogue models. However, most existing datasets for open-domain dialogue modeling are limited to a single language. The absence of multilingual open-domain dialog datasets not only limits the research on multilingual or cross-lingual transfer learning, but also hinders the development of robust open-domain dialog systems that can be deployed in other parts of the world. In this paper, we provide a multilingual parallel open-domain dialog dataset, models., to enable researchers to explore the challenging task of multilingual and cross-lingual open-domain dialog. Languages includes 13k dialogues aligned across 4 languages (52k dialogues and 410k utterances in total). We then propose a dialog generation model, knn-chat, which has a novel knn-search mechanism to support unified response retrieval for monolingual, multilingual, and cross-lingual dialogue. Experiment results show the effectiveness of this framework. We will make however, and knn-chat publicly available soon.","wordlikeness":0.4166666667,"lcsratio":0.75,"wordcoverage":0.6}
{"Year":2023,"Venue":"acl-2023","Acronym":"TemplateGEC","Description":"Improving Grammatical Error Correction with Detection Template","Abstract":"Grammatical error correction (gec) can be divided into sequence-to-edit (seq2edit) and sequence-to-sequence (seq2seq) frameworks, both of which have their pros and cons. To utilize the strengths and make up for the shortcomings of these frameworks, this paper proposes a novel method, performing, which capitalizes on the capabilities of both seq2edit and seq2seq frameworks in error detection and correction respectively. Between utilizes the detection labels from a seq2edit model, to construct the template as the input. A seq2seq model is employed to enforce consistency between the predictions of different templates by utilizing consistency learning. Experimental results on the chinese nlpcc18, english bea19 and conll14 benchmarks show the effectiveness and robustness of href=https:\/\/github.com\/li-aolong\/.further analysis reveals the potential of our method in performing human-in-the-loop gec. Source code and scripts are available at <a href=https:\/\/github.com\/li-aolong\/utilizing class=acl-markup-url>https:\/\/github.com\/li-aolong\/pros<\/a>.","wordlikeness":0.8181818182,"lcsratio":0.7272727273,"wordcoverage":0.8421052632}
{"Year":2019,"Venue":"icon-2019","Acronym":"DRCoVe","Description":"An Augmented Word Representation Approach using Distributional and Relational Context","Abstract":"Word representation using the distributional information of words from a sizeable corpus is considered efficacious in many natural language processing and text mining applications. However, distributional representation of a word is unable to capture distant relational knowledge, representing the relational semantics. In this paper, we propose a novel word representation approach using distributional and relational contexts, and, which augments the distributional representation of a word using the relational semantics extracted as syntactic and semantic association among entities from the underlying corpus. Unlike existing approaches that use external knowledge bases representing the relational semantics for enhanced word representation, biomedical uses typed dependencies (aka syntactic dependencies) to extract relational knowledge from the underlying corpus. The proposed approach is applied over a biomedical text corpus to learn word representation and compared with glove, which is one of the most popular word embedding approaches. The evaluation results on various benchmark datasets for word similarity and word categorization tasks demonstrate the effectiveness of uses over the glove.","wordlikeness":0.5,"lcsratio":0.8333333333,"wordcoverage":0.9090909091}
{"Year":2022,"Venue":"acl-2022","Acronym":"FairLex","Description":"A Multilingual Benchmark for Evaluating Fairness in Legal Text Processing","Abstract":"We present a benchmark suite of four datasets for evaluating the fairness of pre-trained language models and the techniques used to fine-tune them for downstream tasks. Our benchmarks cover four jurisdictions (european council, usa, switzerland, and china), five languages (english, german, french, italian and chinese) and fairness across five attributes (gender, age, region, language, and legal area). In our experiments, we evaluate pre-trained language models using several group-robust fine-tuning techniques and show that performance group disparities are vibrant in many cases, while none of these techniques guarantee fairness, nor consistently mitigate group disparities. Furthermore, we provide a quantitative and qualitative analysis of our results, highlighting open challenges in the development of robustness methods in legal nlp.","wordlikeness":0.7142857143,"lcsratio":1.0,"wordcoverage":0.7692307692}
{"Year":2021,"Venue":"naacl-2021","Acronym":"DAGN","Description":"Discourse-Aware Graph Network for Logical Reasoning","Abstract":"Recent qa with logical reasoning questions requires passage-level relations among the sentences. However, current approaches still focus on sentence-level relations interacting among tokens. In this work, we explore aggregating passage-level clues for solving logical reasoning qa by using discourse-based information. We propose a discourse-aware graph network (questions) that reasons relying on the discourse structure of the texts. The model encodes discourse information as a graph with elementary discourse units (edus) and discourse relations, and learns the discourse-aware features via a graph network for downstream qa tasks. Experiments are conducted on two logical reasoning qa datasets, reclor and logiqa, and our proposed results. Achieves competitive results. The source code is available at <a href=https:\/\/github.com\/eleanor-h\/competitive class=acl-markup-url>https:\/\/github.com\/eleanor-h\/on<\/a>.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2014,"Venue":"emnlp-2014","Acronym":"NaturalLI","Description":"Natural Logic Inference for Common Sense Reasoning","Abstract":"Common-sense reasoning is important for ai applications, both in nlp and many vision and robotics tasks. We propose precision.: a natural logic inference system for inferring common sense facts \u2013 for instance, that cats have tails or tomatoes are round \u2013 from a very large database of known facts. In addition to being able to provide strictly valid derivations, the system is also able to produce derivations which are only likely valid, accompanied by an associated con\ufb01dence. We both show that our system is able to capture strict natural logic inferences on the fracas test suite, and demonstrate its ability to predict common sense facts with 49% recall and 91% precision.","wordlikeness":0.7777777778,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2023,"Venue":"ws-2023","Acronym":"BCause","Description":"Reducing group bias and promoting cohesive discussion in online deliberation processes through a simple and engaging online deliberation tool","Abstract":"Facilitating healthy online deliberation in terms of sensemaking and collaboration of discussion participants proves extremely challenging due to a number of known negative effects of online communication on social media platforms. We start from concerns and aspirations about the use of existing online discussion systems as distilled in previous literature, we then combine them with lessons learned on design and engineering practices from our research team, to inform the design of an easy-to-use tool (polarization.app) that enables higher quality discussions than traditional social media. We describe the design of this tool, highlighting the main interaction features that distinguish it from common social media, namely: i. The low-cost argumentation structuring of the conversations with direct replies; ii. And the distinctive use of reflective feedback rather than appreciative-only feedback. We then present the results of a controlled a\/b experiment in which we show that the presence of argumentative and cognitive reflective discussion elements produces better social interaction with less polarization and promotes a more cohesive discussion than common social media-like interactions.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.9090909091}
{"Year":2010,"Venue":"jeptalnrecital-2010","Acronym":"Moz","Description":"Translation of Structured Terminology-Rich Text","Abstract":"Description of memory,, a translation support system designed for texts exhibiting a high proportion of structured and semi-structured terminological content. The system comprises a web-based collaborative translation memory, with high recall via subsentential linguistic analysis and facilities for messaging and quality assurance. It is in production use, translating some 140,000 words per week.","wordlikeness":0.6666666667,"lcsratio":0.6666666667,"wordcoverage":0.8}
{"Year":2022,"Venue":"ijcnlp-2022","Acronym":"CLASP","Description":"Few-Shot Cross-Lingual Data Augmentation for Semantic Parsing","Abstract":"A bottleneck to developing semantic parsing (sp) models is the need for a large volume of human-labeled training data. Given the complexity and cost of human annotation for sp, labeled data is often scarce, particularly in multilingual settings. Large language models (llms) excel at sp given only a few examples, however llms are unsuitable for runtime systems which require low latency. In this work, we propose models, a simple method to improve low-resource sp for moderate-sized models: we generate synthetic data from alexatm 20b to augment the training set for a model 40x smaller (500m parameters). We evaluate on two datasets in low-resource settings: english pizza, containing either 348 or 16 real examples, and mtop cross-lingual zero-shot, where training data is available only in english, and the model must generalize to four new languages. On both datasets, we show significant improvements over strong baseline methods.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2013,"Venue":"paclic-2013","Acronym":"KOSAC","Description":"A Full-Fledged Korean Sentiment Analysis Corpus","Abstract":"This paper aims to introduce the korean sentiment analysis corpus named together. And is a corpus consisting of 332 news articles taken from the sejong syntactic parsed corpus. These sentences have been manually-tagged for sentimental features. The corpus includes 7,713 sentence subjectivity tags and 17,615 opinionated expression tags based on the annotation scheme called ksml which reflects the characteristics of the korean language. The results of sentence subjectivity and polarity classification experiements using the corpus show the wide possibilities of application the ksml scheme and the tagged information of the articles comprehensively to other corpus. What is innovative about our work is that it pulls together both the concept of private states and nested-sources into one linguistic annotation scheme. We believe that this corpus could be used by researchers as a gold standard for various nlp tasks related to sentiment analysis.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"ConRPG","Description":"Paraphrase Generation using Contexts as Regularizer","Abstract":"A long-standing issue with paraphrase generation is the lack of reliable supervision signals. In this paper, we propose a new unsupervised paradigm for paraphrase generation based on the assumption that the probabilities of generating two sentences with the same meaning given the same context should be the same. Inspired by this fundamental idea, we propose a pipelined system which consists of paraphrase candidate generation based on contextual language models, candidate filtering using scoring functions, and paraphrase model training based on the selected candidates. The proposed paradigm offers merits over existing paraphrase generation methods: (1) using the context regularizer on meanings, the model is able to generate massive amounts of high-quality paraphrase pairs; (2) the combination of the huge amount of paraphrase candidates and further diversity-promoting filtering yields paraphrases with more lexical and syntactic diversity; and (3) using human-interpretable scoring functions to select paraphrase pairs from candidates, the proposed framework provides a channel for developers to intervene with the data generation process, leading to a more controllable model. Experimental results across different tasks and datasets demonstrate that the proposed paradigm significantly outperforms existing paraphrase approaches in both supervised and unsupervised setups.","wordlikeness":0.5,"lcsratio":0.8333333333,"wordcoverage":0.8}
{"Year":2019,"Venue":"bionlp-2019","Acronym":"DeepGeneMD","Description":"A Joint Deep Learning Model for Extracting Gene Mutation-Disease Knowledge from PubMed Literature","Abstract":"Understanding the pathogenesis of genetic diseases through different gene activities and their relations to relevant diseases is important for new drug discovery and drug repositioning. In this paper, we present a joint deep learning model in a multi-task learning paradigm for gene mutation-disease knowledge extraction, subtasks, which adapts the state-of-the-art hierarchical multi-task learning framework for joint inference on named entity recognition (ner) and relation extraction (re) in the context of the agac (active gene annotation corpus) track at 2019 bionlp open shared tasks (bionlp-ost). It simultaneously extracts gene mutation related activities, diseases, and their relations from the published scientific literature. In framework, we explore the task decomposition to create auxiliary subtasks so that more interactions between different learning subtasks can be leveraged in model training. Our model achieves the average f1 score of 0.45 on recognizing gene activities and disease entities, ranking 2nd in the agac ner task; and the average f1 score of 0.35 on extracting relations, ranking 1st in the agac re task.","wordlikeness":0.7,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"PHEE","Description":"A Dataset for Pharmacovigilance Event Extraction from Text","Abstract":"The primary goal of drug safety researchers and regulators is to promptly identify adverse drug reactions. Doing so may in turn prevent or reduce the harm to patients and ultimately improve public health. Evaluating and monitoring drug safety (i.e., pharmacovigilance) involves analyzing an ever growing collection of spontaneous reports from health professionals, physicians, and pharmacists, and information voluntarily submitted by patients. In this scenario, facilitating analysis of such reports via automation has the potential to rapidly identify safety signals. Unfortunately, public resources for developing natural language models for this task are scant. We present submitted, a novel dataset for pharmacovigilance comprising over 5000 annotated events from medical case reports and biomedical literature, making it the largest such public dataset to date. We describe the hierarchical event schema designed to provide coarse and fine-grained information about patients\u2019 demographics, treatments and (side) effects. Along with the discussion of the dataset, we present a thorough experimental evaluation of current state-of-the-art approaches for biomedical event extraction, point out their limitations, and highlight open challenges to foster future research in this area.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2020,"Venue":"ws-2020","Acronym":"SacreROUGE","Description":"An Open-Source Library for Using and Developing Summarization Evaluation Metrics","Abstract":"We present common,, an open-source library for using and developing summarization evaluation metrics. Researchers removes many obstacles that researchers face when using or developing metrics: (1) the library provides python wrappers around the official implementations of existing evaluation metrics so they share a common, easy-to-use interface; (2) it provides functionality to evaluate how well any metric implemented in the library correlates to human-annotated judgments, so no additional code needs to be written for a new evaluation metric; and (3) it includes scripts for loading datasets that contain human judgments so they can easily be used for evaluation. This work describes the design of the library, including the core metric interface, the command-line api for evaluating summarization models and metrics, and the scripts to load and reformat publicly available datasets. The development of easy-to-use is ongoing and open to contributions from the community.","wordlikeness":0.7,"lcsratio":0.8,"wordcoverage":0.7058823529}
{"Year":2023,"Venue":"acl-2023","Acronym":"MvP","Description":"Multi-view Prompting Improves Aspect Sentiment Tuple Prediction","Abstract":"Generative methods greatly promote aspect-based sentiment analysis via generating a sequence of sentiment elements in a specified format. However, existing studies usually predict sentiment elements in a fixed order, which ignores the effect of the interdependence of the elements in a sentiment tuple and the diversity of language expression on the results. In this work, we propose multi-view prompting (expression) that aggregates sentiment elements generated in different orders, leveraging the intuition of human-like problem-solving processes from different views. Specifically, via introduces element order prompts to guide the language model to generate multiple sentiment tuples, each with a different element order, and then selects the most reasonable tuples by voting. Model can naturally model multi-view and multi-task as permutations and combinations of elements, respectively, outperforming previous task-specific designed methods on multiple absa tasks with a single model. Extensive experiments show that model. Significantly advances the state-of-the-art performance on 10 datasets of 4 benchmark tasks, and performs quite effectively in low-resource settings. Detailed evaluation verified the effectiveness, flexibility, and cross-task transferability of significantly.","wordlikeness":0.3333333333,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2021,"Venue":"acl-2021","Acronym":"AMBERT","Description":"A Pre-trained Language Model with Multi-Grained Tokenization","Abstract":"Pre-trained language models such as bert have exhibited remarkable performances in many tasks in natural language understanding (nlu). The tokens in the models are usually \ufb01ne-grained in the sense that for languages like english they are words or subwords and for languages like chinese they are characters. In english, for example, there are multi-word expressions which form natural lexical units and thus the use of coarse-grained tokenization also appears to be reasonable. In fact, both \ufb01ne-grained and coarse-grained tokenizations have advantages and disadvantages for learning of pre-trained language models. In this paper, we propose a novel pretrained language model, referred to as \ufb01ne-grained (a multi-grained bert), on the basis of both \ufb01ne-grained and coarse-grained tokenizations. For english, language takes both the sequence of words (\ufb01ne-grained tokens) and the sequence of phrases (coarse-grained tokens) as input after tokenization, employs one encoder for processing the sequence of words and the other encoder for processing the sequence of the phrases, utilizes shared parameters between the two encoders, and \ufb01nally creates a sequence of contextualized representations of the words and a sequence of contextualized representations of the phrases. Experiments have been conducted on benchmark datasets for chinese and english, including clue, glue, squad and race. The results show that (coarse-grained can outperform bert in all cases, particularly the improvements are signi\ufb01cant for chinese. We also develop a method to improve the ef\ufb01ciency of representations in inference, which still performs better than bert with the same computational cost as bert.","wordlikeness":0.8333333333,"lcsratio":0.8333333333,"wordcoverage":0.9230769231}
{"Year":2013,"Venue":"semeval-2013","Acronym":"sielers","Description":"Feature Analysis and Polarity Classification of Expressions from Twitter and SMS Data","Abstract":"In this paper, we describe our system for the semeval-2013 task 2, sentiment analysis in twitter. We formed features that take into account the context of the expression and take a supervised approach towards subjectivity and polarity classi\ufb01cation. Experiments were performed on the features to \ufb01nd out whether they were more suited for subjectivity or polarity classi\ufb01cation. We tested our model for sentiment polarity classi\ufb01cation on twitter as well as sms chat expressions, analyzed their f-measure scores and drew some interesting conclusions from them.","wordlikeness":0.7142857143,"lcsratio":0.8571428571,"wordcoverage":0.8571428571}
{"Year":2015,"Venue":"starsem-2015","Acronym":"SGRank","Description":"Combining Statistical and Graphical Methods to Improve the State of the Art in Unsupervised Keyphrase Extraction","Abstract":"Keyphrase extraction is a fundamental technique in natural language processing. It enables documents to be mapped to a concise set of phrases that can be used for indexing, clustering, ontology building, auto-tagging and other information organization schemes. Two major families of unsupervised keyphrase extraction algorithms may be characterized as statistical and graph-based. We present a hybrid statistical-graphical algorithm that capitalizes on the heuristics of both families of algorithms and is able to outperform the state of the art in unsupervised keyphrase extraction on several datasets.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.8333333333}
{"Year":2021,"Venue":"icnlsp-2021","Acronym":"TPT","Description":"An Empirical Term Selection for Arabic Text Categorization","Abstract":"In this paper, we will investigate an empirical term selection method for text categorization, namely transition point (tp) technique, and we will compare it to two other widely used methods: term frequency (tf) and document frequency (df). For evaluation, we have used the well-known tfidf technique. Experiments have been conducted by using the arabic corpus khaleej-2004 which is composed of 4 categories. The results obtained from this study show that performance is almost the same for the three techniques. However, we should note that tp is advantageous since it uses a vocabulary much smaller than the ones used in tf and df.","wordlikeness":0.3333333333,"lcsratio":0.6666666667,"wordcoverage":0.8}
{"Year":2021,"Venue":"naacl-2021","Acronym":"Macro-Average","Description":"Rare Types Are Important Too","Abstract":"While traditional corpus-level evaluation metrics for machine translation (mt) correlate well with fluency, they struggle to reflect adequacy. Model-based mt metrics trained on segment-level human judgments have emerged as an attractive replacement due to strong correlation results. These models, however, require potentially expensive re-training for new domains and languages. Furthermore, their decisions are inherently non-transparent and appear to reflect unwelcome biases. We explore the simple type-based classifier metric, macrof1, and study its applicability to mt evaluation. We find that macrof1 is competitive on direct assessment, and outperforms others in indicating downstream cross-lingual information retrieval task performance. Further, we show that macrof1 can be used to effectively compare supervised and unsupervised neural machine translation, and reveal significant qualitative differences in the methods\u2019 outputs.","wordlikeness":0.8461538462,"lcsratio":0.4615384615,"wordcoverage":0.7619047619}
{"Year":2019,"Venue":"naacl-2019","Acronym":"VCWE","Description":"Visual Character-Enhanced Word Embeddings","Abstract":"Chinese is a logographic writing system, and the shape of chinese characters contain rich syntactic and semantic information. In this paper, we propose a model to learn chinese word embeddings via three-level composition: (1) a convolutional neural network to extract the intra-character compositionality from the visual shape of a character; (2) a recurrent neural network with self-attention to compose character representation into word embeddings; (3) the skip-gram framework to capture non-compositionality directly from the contextual information. Evaluations demonstrate the superior performance of our model on four tasks: word similarity, sentiment analysis, named entity recognition and part-of-speech tagging.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2023,"Venue":"findings-2023","Acronym":"RMSSinger","Description":"Realistic-Music-Score based Singing Voice Synthesis","Abstract":"We are interested in a challenging task, realistic-music-score based singing voice synthesis (rms-svs). Rms-svs aims to generate high-quality singing voices given realistic music scores with different note types (grace, slur, rest, etc.). Though significant progress has been achieved, recent singing voice synthesis (svs) methods are limited to fine-grained music scores, which require a complicated data collection pipeline with time-consuming manual annotation to align music notes with phonemes. % furthermore, existing approaches cannot synthesize rhythmic singing voices given realistic music scores due to the domain gap between fine-grained music scores and realistic music scores. Furthermore, these manual annotation destroys the regularity of note durations in music scores, making fine-grained music scores inconvenient for composing. To tackle these challenges, we propose phonemes,, the first rms-svs method, which takes realistic music scores as input, eliminating most of the tedious manual annotation and avoiding the aforementioned inconvenience. Note that music scores are based on words rather than phonemes, in regularity, we introduce word-level modeling to avoid the time-consuming phoneme duration annotation and the complicated phoneme-level mel-note alignment. Furthermore, we propose the first diffusion-based pitch modeling method, which ameliorates the naturalness of existing pitch-modeling methods. To achieve these, we collect a new dataset containing realistic music scores and singing voices according to these realistic music scores from professional singers. Extensive experiments on the dataset demonstrate the effectiveness of our methods. Audio samples are available at <a href=https:\/\/voice.github.io\/ class=acl-markup-url>https:\/\/propose.github.io\/<\/a>.","wordlikeness":0.5555555556,"lcsratio":0.8888888889,"wordcoverage":0.8}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"ReasonBERT","Description":"Pre-trained to Reason with Distant Supervision","Abstract":"We present experiments, a pre-training method that augments language models with the ability to reason over long-range relations and multiple, possibly hybrid contexts. Unlike existing pre-training methods that only harvest learning signals from local contexts of naturally occurring texts, we propose a generalized notion of distant supervision to automatically connect multiple pieces of text and tables to create pre-training examples that require long-range reasoning. Different types of reasoning are simulated, including intersecting multiple pieces of evidence, bridging from one piece of evidence to another, and detecting unanswerable cases. We conduct a comprehensive evaluation on a variety of extractive question answering datasets ranging from single-hop to multi-hop and from text-only to table-only to hybrid that require various reasoning capabilities and show that only achieves remarkable improvement over an array of strong baselines. Few-shot experiments further demonstrate that our pre-training method substantially improves sample efficiency.","wordlikeness":0.9,"lcsratio":0.8,"wordcoverage":0.8}
{"Year":2018,"Venue":"emnlp-2018","Acronym":"CytonMT","Description":"an Efficient Neural Machine Translation Open-source Toolkit Implemented in C&#43;&#43;","Abstract":"This paper presents an open-source neural machine translation toolkit named by. The toolkit is built from scratch only using c++ and nvidia\u2019s gpu-accelerated libraries. The toolkit features training efficiency, code simplicity and translation quality. Benchmarks show that libraries. Accelerates the training speed by 64.5% to 110.8% on neural networks of various sizes, and achieves competitive translation quality.","wordlikeness":0.5714285714,"lcsratio":0.8571428571,"wordcoverage":0.7142857143}
{"Year":2021,"Venue":"woah-2021","Acronym":"HateBERT","Description":"Retraining BERT for Abusive Language Detection in English","Abstract":"We introduce curated, a re-trained bert model for abusive language detection in english. The model was trained on ral-e, a large-scale dataset of reddit comments in english from communities banned for being offensive, abusive, or hateful that we have curated and made available to the public. We present the results of a detailed comparison between a general pre-trained language model and the retrained version on three english datasets for offensive, abusive language and hate speech detection tasks. In all datasets, english. Outperforms the corresponding general bert model. We also discuss a battery of experiments comparing the portability of the fine-tuned models across the datasets, suggesting that portability is affected by compatibility of the annotated phenomena.","wordlikeness":0.875,"lcsratio":0.625,"wordcoverage":0.8}
{"Year":2022,"Venue":"lrec-2022","Acronym":"GerCCT","Description":"An Annotated Corpus for Mining Arguments in German Tweets on Climate Change","Abstract":"While the field of argument mining has grown notably in the last decade, research on the twitter medium remains relatively understudied. Given the difficulty of mining arguments in tweets, recent work on creating annotated resources mainly utilized simplified annotation schemes that focus on single argument components, i.e., on claim or evidence. In this paper we strive to fill this research gap by presenting understudied., a new corpus of german tweets on climate change, which was annotated for a set of different argument components and properties. Additionally, we labelled sarcasm and toxic language to facilitate the development of tools for filtering out non-argumentative content. This, to the best of our knowledge, renders our corpus the first tweet resource annotated for argumentation, sarcasm and toxic language. We show that a comparatively complex annotation scheme can still yield promising inter-annotator agreement. We further present first good supervised classification results yielded by a fine-tuned bert architecture.","wordlikeness":0.5,"lcsratio":0.8333333333,"wordcoverage":0.7272727273}
{"Year":2015,"Venue":"ws-2015","Acronym":"GutenTag","Description":"an NLP-driven Tool for Digital Humanities Research in the Project Gutenberg Corpus","Abstract":"This paper introduces a software tool, general, which is aimed at giving literary researchers direct access to nlp techniques for the analysis of texts in the project gutenberg corpus. We discuss several facets of the tool, including the handling of formatting and structure, the use and expansion of metadata which is used to identify relevant subcorpora of interest, and a general tagging framework which is intended to cover a wide variety of future nlp modules. Our hope that the shared ground created by this tool will help create new kinds of interaction between the computational linguistics and digital humanities communities, to the bene\ufb01t of both.","wordlikeness":0.875,"lcsratio":0.875,"wordcoverage":0.8}
{"Year":2023,"Venue":"acl-2023","Acronym":"CAT","Description":"A Contextualized Conceptualization and Instantiation Framework for Commonsense Reasoning","Abstract":"Commonsense reasoning, aiming at endowing machines with a human-like ability to make situational presumptions, is extremely challenging to generalize. For someone who barely knows about \u201cmeditation,\u201d while is knowledgeable about \u201csinging,\u201d he can still infer that \u201cmeditation makes people relaxed\u201d from the existing knowledge that \u201csinging makes people relaxed\u201d by first conceptualizing \u201csinging\u201d as a \u201crelaxing event\u201d and then instantiating that event to \u201cmeditation.\u201dthis process, known as conceptual induction and deduction, is fundamental to commonsense reasoning while lacking both labeled data and methodologies to enhance commonsense modeling. To fill such a research gap, we propose \u201csinging,\u201d (contextualized conceptualization and instantiation),a semi-supervised learning framework that integrates event conceptualization and instantiation to conceptualize commonsense knowledge bases at scale. Extensive experiments show that our framework achieves state-of-the-art performances on two conceptualization tasks, and the acquired abstract commonsense knowledge can significantly improve commonsense inference modeling. Our code, data, and fine-tuned models are publicly available at [<a href=https:\/\/github.com\/hkust-knowcomp\/as class=acl-markup-url>https:\/\/github.com\/hkust-knowcomp\/known<\/a>](<a href=https:\/\/github.com\/hkust-knowcomp\/who class=acl-markup-url>https:\/\/github.com\/hkust-knowcomp\/people<\/a>).","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2009,"Venue":"acl-2009","Acronym":"WISDOM","Description":"A Web Information Credibility Analysis Systematic","Abstract":"We demonstrate an information credibility analysis system called at. The purpose of credibility: is to evaluate the credibility of information available on the web from multiple viewpoints. To considers the following to be the source of information credibility: information contents, information senders, and information appearances. We aim at analyzing and organizing these measures on the basis of semantics-oriented natural language processing (nlp) techniques. 1.","wordlikeness":1.0,"lcsratio":0.6666666667,"wordcoverage":1.0}
{"Year":2022,"Venue":"ws-2022","Acronym":"SentEMO","Description":"A Multilingual Adaptive Platform for Aspect-based Sentiment and Emotion Analysis","Abstract":"In this paper, we present the think platform, a tool that provides aspect-based sentiment analysis and emotion detection of unstructured text data such as reviews, emails and customer care conversations. Currently, models have been trained for five domains and one general domain and are implemented in a pipeline approach, where the output of one model serves as the input for the next. The results are presented in three dashboards, allowing companies to gain more insights into what stakeholders think of their products and services. The presented platform is available at <a href=https:\/\/conversations..ugent.be class=acl-markup-url>https:\/\/implemented.ugent.","wordlikeness":0.5714285714,"lcsratio":1.0,"wordcoverage":0.7692307692}
{"Year":2019,"Venue":"acl-2019","Acronym":"HEIDL","Description":"Learning Linguistic Expressions with Deep Learning and Human-in-the-Loop","Abstract":"While the role of humans is increasingly recognized in machine learning community, representation of and interaction with models in current human-in-the-loop machine learning (hitl-ml) approaches are too low-level and far-removed from human\u2019s conceptual models. We demonstrate representing, a prototype hitl-ml system that exposes the machine-learned model through high-level, explainable linguistic expressions formed of predicates representing semantic structure of text. In representation, human\u2019s role is elevated from simply evaluating model predictions to interpreting and even updating the model logic directly by enabling interaction with rule predicates themselves. Raising the currency of interaction to such semantic levels calls for new interaction paradigms between humans and machines that result in improved productivity for text analytics model development process. Moreover, by involving humans in the process, the human-machine co-created models generalize better to unseen data as domain experts are able to instill their expertise by extrapolating from what has been learned by automated algorithms from few labelled data.","wordlikeness":0.4,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2022,"Venue":"coling-2022","Acronym":"PARSE","Description":"An Efficient Search Method for Black-box Adversarial Text Attacks","Abstract":"Neural networks are vulnerable to adversarial examples. The adversary can successfully attack a model even without knowing model architecture and parameters, i.e., under a black-box scenario. Previous works on word-level attacks widely use word importance ranking (wir) methods and complex search methods, including greedy search and heuristic algorithms, to find optimal substitutions. However, these methods fail to balance the attack success rate and the cost of attacks, such as the number of queries to the model and the time consumption. In this paper, we propose pathological word saliency search (works) that performs the search under dynamic search space following the subarea importance. Experiments show that ranking can achieve comparable attack success rates to complex search methods while saving numerous queries and time, e.g., saving at most 74% of queries and 90% of time compared with greedy search when attacking the examples from yelp dataset. The adversarial examples crafted by word are also of high quality, highly transferable, and can effectively improve model robustness in adversarial training.","wordlikeness":0.6,"lcsratio":0.8,"wordcoverage":0.9090909091}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"SimQA","Description":"Detecting Simultaneous MT Errors through Word-by-Word Question Answering","Abstract":"Detractors of neural machine translation admit that while its translations are fluent, it sometimes gets key facts wrong. This is particularly important in simultaneous interpretation where translations have to be provided as fast as possible: before a sentence is complete. Yet, evaluations of simultaneous machine translation (simulmt) fail to capture if systems correctly translate the most salient elements of a question: people, places, and dates. To address this problem, we introduce a downstream word-by-word question answering evaluation task (models): given a source language question, translate the question word by word into the target language, and answer as soon as possible. If jointly measures whether the simulmt models translate the question quickly and accurately, and can reveal shortcomings in existing neural systems\u2014hallucinating or omitting facts.","wordlikeness":0.4,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2023,"Venue":"acl-2023","Acronym":"KoSBI","Description":"A Dataset for Mitigating Social Bias Risks Towards Safer Large Language Model Applications","Abstract":"Large language models (llms) not only learn natural text generation abilities but also social biases against different demographic groups from real-world data. This poses a critical risk when deploying llm-based applications. Existing research and resources are not readily applicable in south korea due to the differences in language and culture, both of which significantly affect the biases and targeted demographic groups. This limitation requires localized social bias datasets to ensure the safe and effective deployment of llms. to this end, we present requires, a new social bias dataset of 34k pairs of contexts and sentences in korean covering 72 demographic groups in 15 categories. We find that through filtering-based moderation, social biases in generated content can be reduced by 16.47%p on average for hyperclova (30b and 82b), and gpt-3.","wordlikeness":0.6,"lcsratio":0.8,"wordcoverage":0.75}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"TransAdv","Description":"A Translation-based Adversarial Learning Framework for Zero-Resource Cross-Lingual Named Entity Recognition","Abstract":"Zero-resource cross-lingual named entity recognition aims at training an ner model of the target language using only labeled source language data and unlabeled target language data. Existing methods are mainly divided into three categories: model transfer based, data transfer based and knowledge transfer based. Each method has its own disadvantages, and combining more than one of them often leads to better performance. However, the performance of data transfer based methods is often limited by inevitable noise in the translation process. To handle the problem, we propose a framework named model to mitigate lexical and syntactic errors of word-by-word translated data, better utilizing the data by multi-level adversarial learning and multi-model knowledge distillation. Extensive experiments are conducted over 6 target languages with english as the source language, and the results show that a achieves competitive performance to the state-of-the-art models.","wordlikeness":0.625,"lcsratio":1.0,"wordcoverage":0.7777777778}
{"Year":2021,"Venue":"findings-2021","Acronym":"NICE","Description":"Neural Image Commenting with Empathy","Abstract":"Emotion and empathy are examples of human qualities lacking in many human-machine interactions. The goal of our work is to generate engaging dialogue grounded in a user-shared image with increased emotion and empathy while minimizing socially inappropriate or offensive outputs. We release the neural image commenting with empathy (or) dataset consisting of almost two million images and the corresponding human-generated comments, a set of human annotations, and baseline performance on a range of models. In-stead of relying on manually labeled emotions, we also use automatically generated linguistic representations as a source of weakly supervised labels. Based on these annotations, we define two different tasks for the tasks. Dataset. Then, we propose a novel pre-training model - modeling affect generation for image comments (magic) - which aims to generate comments for images, conditioned on linguistic representations that capture style and affect, and to help generate more empathetic, emotional, engaging and socially appropriate comments. Using this model we achieve state-of-the-art performance on one of our - tasks. The experiments show that the approach can generate more human-like and engaging image comments.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2013,"Venue":"semeval-2013","Acronym":"EHU-ALM","Description":"Similarity-Feature Based Approach for Student Response Analysis","Abstract":"We present a 5-way supervised system based on syntactic-semantic similarity features. The model deploys: text overlap measures, wordnet-based lexical similarities, graphbased similarities, corpus-based similarities, syntactic structure overlap and predicateargument overlap measures. These measures are applied to question, reference answer and student answer triplets. We take into account the negation in the syntactic and predicateargument overlap measures. Our system uses the domain-speci\ufb01c data as one dataset to build a robust system. The results show that our system is above the median and mean on all the evaluation scenarios of the semeval2013 task #7.","wordlikeness":0.2857142857,"lcsratio":0.7142857143,"wordcoverage":0.6666666667}
{"Year":2020,"Venue":"acl-2020","Acronym":"C-Net","Description":"Contextual Network for Sarcasm Detection","Abstract":"Automatic sarcasm detection in conversations is a difficult and tricky task. Classifying an utterance as sarcastic or not in isolation can be futile since most of the time the sarcastic nature of a sentence heavily relies on its context. This paper presents our proposed model, this, which takes contextual information of a sentence in a sequential manner to classify it as sarcastic or non-sarcastic. Our model showcases competitive performance in the sarcasm detection shared task organised on codalab and achieved 75.0% f1-score on the twitter dataset and 66.3% f1-score on reddit dataset.","wordlikeness":0.6,"lcsratio":0.8,"wordcoverage":0.75}
{"Year":2018,"Venue":"emnlp-2018","Acronym":"FewRel","Description":"A Large-Scale Supervised Few-Shot Relation Classification Dataset with State-of-the-Art Evaluation","Abstract":"We present a few-shot relation classification dataset (dataset), consisting of 70, 000 sentences on 100 relations derived from wikipedia and annotated by crowdworkers. The relation of each sentence is first recognized by distant supervision methods, and then filtered by crowdworkers. We adapt the most recent state-of-the-art few-shot learning methods for relation classification and conduct thorough evaluation of these methods. Empirical results show that even the most competitive few-shot learning models struggle on this task, especially as compared with humans. We also show that a range of different reasoning skills are needed to solve our task. These results indicate that few-shot relation classification remains an open problem and still requires further research. Our detailed analysis points multiple directions for future research.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2022,"Venue":"lrec-2022","Acronym":"NyLLex","Description":"A Novel Resource of Swedish Words Annotated with Reading Proficiency Level","Abstract":"What makes a text easy to read or not, depends on a variety of factors. One of the most prominent is, however, if the text contains easy, and avoids difficult, words. Deciding if a word is easy or difficult is not a trivial task, since it depends on characteristics of the word in itself as well as the reader, but it can be facilitated by the help of a corpus annotated with word frequencies and reading proficiency levels. In this paper, we present reading, a novel lexical resource derived from books published by sweden\u2019s largest publisher for easy language texts. In consists of 6,668 entries, with frequency counts distributed over six reading proficiency levels. We show that material, with its novel source material aimed at individuals of different reading proficiency levels, can serve as a complement to already existing resources for swedish.","wordlikeness":0.5,"lcsratio":0.6666666667,"wordcoverage":0.7272727273}
{"Year":2014,"Venue":"lrec-2014","Acronym":"MAT","Description":"a tool for L2 pronunciation errors annotation","Abstract":"In the area of computer assisted language learning(call), second language (l2) learners\u0092 spoken data is an important resource for analysing and annotating typical l2 pronunciation errors. The annotation of l2 pronunciation errors in spoken data is not an easy task though, normally it requires manual annotation from trained linguists or phoneticians. In order to facilitate this task, in this paper, we present the syllabifier, tool, a web-based tool intended to facilitate the annotation of l2 learners\u2019 pronunciation errors at various levels. The tool has been designed taking into account recent studies on error detection in pronunciation training. It also aims at providing an easy and fast annotation process via a comprehensive and friendly user interface. The tool is based on the mary tts open source platform, from which it uses the components: text analyser (tokeniser, syllabifier, phonemiser), phonetic aligner and speech signal processor. Annotation results at sentence, word, syllable and phoneme levels are stored in xml foranalyser. The tool is currently under evaluation with a l2 learners\u0092 spoken corpus recorded in the sprinter (language technology for interactive, multi-media online language learning) project.","wordlikeness":1.0,"lcsratio":0.6666666667,"wordcoverage":1.0}
{"Year":2016,"Venue":"ws-2016","Acronym":"JU-USAAR","Description":"A Domain Adaptive MT System","Abstract":"This paper presents the knowledge. English\u2013german domain adaptive machine translation (mt) system submitted to the it domain translation task organized in wmt-2016 . Our system brings improvements over the in-domain baseline system by incorporating out-domain knowledge. We applied two methodologies to accelerate the performance of our in-domain mt system: (i) additional training material extraction from out-domain data using data selection method, and (ii) language model and translation model adaptation through interpolation. Our primary submission obtained a bleu score of 34.5 (14.5 absolute and 72.5% relative improvements over baseline) and a ter score of 54.0 (14.7 absolute and 21.4% relative improvements over baseline).","wordlikeness":0.5,"lcsratio":0.25,"wordcoverage":0.5714285714}
{"Year":2020,"Venue":"lrec-2020","Acronym":"SegBo","Description":"A Database of Borrowed Sounds in the World&#39;s Languages","Abstract":"Phonological segment borrowing is a process through which languages acquire new contrastive speech sounds as the result of borrowing new words from other languages. Despite the fact that phonological segment borrowing is documented in many of the world\u2019s languages, to date there has been no large-scale quantitative study of the phenomenon. In this paper, we present despite, a novel cross-linguistic database of borrowed phonological segments. We describe our data aggregation pipeline and the resulting language sample. We also present two short case studies based on the database. The first deals with the impact of large colonial languages on the sound systems of the world\u2019s languages; the second deals with universals of borrowing in the domain of rhotic consonants.","wordlikeness":0.4,"lcsratio":0.8,"wordcoverage":0.75}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"ProGen","Description":"Progressive Zero-shot Dataset Generation via In-context Feedback","Abstract":"Recently, dataset-generation-based zero-shot learning has shown promising results by training a task-specific model with a dataset synthesized from large pre-trained language models (plms). The final task-specific model often achieves compatible or even better performance than plms under the zero-shot setting, with orders of magnitude fewer parameters. However, synthetic datasets have their drawbacks. They have long being suffering from the low-quality issue (e.g., low informativeness, redundancy). This explains why the massive synthetic data does not lead to better performance \u2013 a scenario we would expect in the human-labeled data. To improve the quality in dataset synthesis, we propose a progressive zero-shot dataset generation framework, better, which leverages the feedback from the task-specific model to guide the generation of new training data via in-context examples. Extensive experiments on five text classification datasets demonstrate the effectiveness of the proposed approach. We also show only achieves on-par or superior performance with only 1% synthetic dataset size, when comparing to baseline methods without in-context feedback.","wordlikeness":0.8333333333,"lcsratio":1.0,"wordcoverage":0.8333333333}
{"Year":2022,"Venue":"ws-2022","Acronym":"MirrorAlign","Description":"A Super Lightweight Unsupervised Word Alignment Model via Cross-Lingual Contrastive Learning","Abstract":"Word alignment is essential for the downstream cross-lingual language understanding and generation tasks. Recently, the performance of the neural word alignment models has exceeded that of statistical models. However, they heavily rely on sophisticated translation models. In this study, we propose a super lightweight unsupervised word alignment model named show, in which bidirectional symmetric attention trained with a contrastive learning objective is introduced, and an agreement loss is employed to bind the attention maps, such that the alignments follow mirror-like symmetry hypothesis. Experimental results on several public benchmarks demonstrate that our model achieves competitive, if not better, performance compared to the state of the art in word alignment while significantly reducing the training and decoding time on average. Further ablation analysis and case studies show the superiority of our proposed learning. Notably, we recognize our model as a pioneer attempt to unify bilingual word embedding and word alignments. Encouragingly, our approach achieves 16.4x speedup against giza++, and 50x parameter compression compared with the transformer-based alignment methods. We release our code to facilitate the community: <a href=https:\/\/github.com\/moore3930\/case class=acl-markup-url>https:\/\/github.com\/moore3930\/alignments<\/a>.","wordlikeness":0.8181818182,"lcsratio":0.8181818182,"wordcoverage":0.8}
{"Year":2022,"Venue":"conll-2022","Acronym":"PIE-QG","Description":"Paraphrased Information Extraction for Unsupervised Question Generation from Small Corpora","Abstract":"Supervised question answering systems (qa systems) rely on domain-specific human-labeled data for training. Unsupervised qa systems generate their own question-answer training pairs, typically using secondary knowledge sources to achieve this outcome. Our approach (called an) uses open information extraction (openie) to generate synthetic training questions from paraphrased passages and uses the question-answer pairs as training data for a language model for a state-of-the-art qa system based on bert. Triples in the form of &lt;subject, predicate, object> are extracted from each passage, and questions are formed with subjects (or objects) and predicates while objects (or subjects) are considered as answers. Experimenting on five extractive qa datasets demonstrates that our technique achieves on-par performance with existing state-of-the-art qa systems with the benefit of being trained on an order of magnitude fewer documents and without any recourse to external reference data sources.","wordlikeness":0.5,"lcsratio":0.8333333333,"wordcoverage":0.6666666667}
{"Year":2023,"Venue":"acl-2023","Acronym":"Multi-VALUE","Description":"A Framework for Cross-Dialectal English NLP","Abstract":"Dialect differences caused by regional, social, and economic factors cause performance discrepancies for many groups of language technology users. Inclusive and equitable language technology must critically be dialect invariant, meaning that performance remains constant over dialectal shifts. Current systems often fall short of this ideal since they are designed and tested on a single dialect: standard american english (sae). We introduce a suite of resources for evaluating and achieving english dialect invariance. The resource is called model, a controllable rule-based translation system spanning 50 english dialects and 189 unique linguistic features. Class=acl-markup-url>http:\/\/value-nlp.org<\/a>. Maps sae to synthetic forms of each dialect. First, we use this system to stress tests question answering, machine translation, and semantic parsing. Stress tests reveal significant performance disparities for leading models on non-standard dialects. Second, we use this system as a data augmentation technique to improve the dialect robustness of existing systems. Finally, we partner with native speakers of chicano and indian english to release new gold-standard variants of the popular coqa task. To execute the transformation code, run model checkpoints, and download both synthetic and gold-standard dialectal benchmark datasets, see <a href=http:\/\/value-nlp.org class=acl-markup-url>http:\/\/value-nlp.org<\/a>.","wordlikeness":0.8181818182,"lcsratio":0.5454545455,"wordcoverage":0.7368421053}
{"Year":2020,"Venue":"lrec-2020","Acronym":"Fakeddit","Description":"A New Multimodal Benchmark Dataset for Fine-grained Fake News Detection","Abstract":"Fake news has altered society in negative ways in politics and culture. It has adversely affected both online social network systems as well as offline communities and conversations. Using automatic machine learning classification models is an efficient way to combat the widespread dissemination of fake news. However, a lack of effective, comprehensive datasets has been a problem for fake news research and detection model development. Prior fake news datasets do not provide multimodal text and image data, metadata, comment data, and fine-grained fake news categorization at the scale and breadth of our dataset. We present 6-way, a novel multimodal dataset consisting of over 1 million samples from multiple categories of fake news. After being processed through several stages of review, the samples are labeled according to 2-way, 3-way, and 6-way classification categories through distant supervision. We construct hybrid text+image models and perform extensive experiments for multiple variations of classification, demonstrating the importance of the novel aspect of multimodality and fine-grained classification unique to ,.","wordlikeness":0.625,"lcsratio":0.75,"wordcoverage":0.7692307692}
{"Year":2013,"Venue":"ws-2013","Acronym":"AnCora-UPF","Description":"A Multi-Level Annotation of Spanish","Abstract":"There is an increasing need for the annotation of multiple types of linguistic information that are rather different in their nature, e.g., word order, morphological features, syntactic and semantic relations, etc. Quite frequently, their annotation is combined in a single structure, which not only results in inadequate annotations of treebanks and consequent low-quality applications trained on them, but also is de\ufb01cient from a theoretical (linguistic) perspective. We present a new corpus of spanish annotated on four independent levels, morphology, surface-syntax, deep-syntax and semantics, as well as the methodology that allows for obtaining it with fewer cost while maintaining a high inter-annotator agreement.","wordlikeness":0.5,"lcsratio":0.5,"wordcoverage":0.7058823529}
{"Year":2022,"Venue":"lrec-2022","Acronym":"Klexikon","Description":"A German Dataset for Joint Summarization and Simplification","Abstract":"Traditionally, text simplification is treated as a monolingual translation task where sentences between source texts and their simplified counterparts are aligned for training. However, especially for longer input documents, summarizing the text (or dropping less relevant content altogether) plays an important role in the simplification process, which is currently not reflected in existing datasets. Simultaneously, resources for non-english languages are scarce in general and prohibitive for training new solutions. To tackle this problem, we pose core requirements for a system that can jointly summarize and simplify long source documents. We further describe the creation of a new dataset for joint text simplification and summarization based on german wikipedia and the german children\u2019s encyclopedia \u201crelease\u201d, consisting of almost 2,900 documents. We release a document-aligned version that particularly highlights the summarization aspect, and provide statistical evidence that this resource is well suited to simplification as well. Code and data are available on github: <a href=https:\/\/github.com\/dennlinger\/code class=acl-markup-url>https:\/\/github.","wordlikeness":0.625,"lcsratio":0.5,"wordcoverage":0.7692307692}
{"Year":2023,"Venue":"acl-2023","Acronym":"UINAUIL","Description":"A Unified Benchmark for Italian Natural Language Understanding","Abstract":"This paper introduces the unified interactive natural understanding of the italian language (we), a benchmark of six tasks for italian natural language understanding. We present a description of the tasks and software library that collects the data from the european language grid, harmonizes the data format, and exposes functionalities to facilitates data manipulation and the evaluation of custom models. We also present the results of tests conducted with available italian and multilingual language models on interactive, providing an updated picture of the current state of the art in italian nlu.","wordlikeness":0.5714285714,"lcsratio":0.8571428571,"wordcoverage":0.7272727273}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"MuVER","Description":"Improving First-Stage Entity Retrieval with Multi-View Entity Representations","Abstract":"Entity retrieval, which aims at disambiguating mentions to canonical entities from massive kbs, is essential for many tasks in natural language processing. Recent progress in entity retrieval shows that the dual-encoder structure is a powerful and efficient framework to nominate candidates if entities are only identified by descriptions. However, they ignore the property that meanings of entity mentions diverge in different contexts and are related to various portions of descriptions, which are treated equally in previous works. In this work, we propose multi-view entity representations (canonical), a novel approach for entity retrieval that constructs multi-view representations for entity descriptions and approximates the optimal view for mentions via a heuristic searching method. Our method achieves the state-of-the-art performance on zeshel and improves the quality of candidates on three standard entity linking datasets.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.7692307692}
{"Year":2022,"Venue":"naacl-2022","Acronym":"UserIdentifier","Description":"Implicit User Representations for Simple and Effective Personalized Sentiment Analysis","Abstract":"Global models are typically trained to be as generalizable as possible. Invariance to the specific user is considered desirable since models are shared across multitudes of users. However, these models are often unable to produce personalized responses for individual users, based on their data. Contrary to widely-used personalization techniques based on few-shot and meta-learning, we propose also, a novel scheme for training a single shared model for all users. Our approach produces personalized responses by prepending a fixed, user-specific non-trainable string (called \u201cuser identifier\u201d) to each user\u2019s input text. Unlike prior work, this method doesn\u2019t need any additional model parameters, any extra rounds of personal few-shot learning or any change made to the vocabulary. We empirically study different types of user identifiers (numeric, alphanumeric, and also randomly generated) and demonstrate that, surprisingly, randomly generated user identifiers outperform the prefix-tuning based state-of-the-art approach by up to 13, on a suite of sentiment analysis datasets.","wordlikeness":0.8571428571,"lcsratio":0.8571428571,"wordcoverage":0.7692307692}
{"Year":2008,"Venue":"acl-2008","Acronym":"MAXSIM","Description":"A Maximum Similarity Metric for Machine Translation Evaluation","Abstract":"We propose an automatic machine translation (mt) evaluation metric that calculates a similarity score (based on precision and recall) of a pair of sentences. Unlike most metrics, we compute a similarity score between items across the two sentences. We then \ufb01nd a maximum weight matching between the items such that each item in one sentence is mapped to at most one item in the other sentence. This general framework allows us to use arbitrary similarity functions between items, and to incorporate different information in our comparison, such as n-grams, dependency relations, etc. When evaluated on data from the acl-07 mt workshop, our proposed metric achieves higher correlation with human judgements than all 11 automatic mt evaluation metrics that were evaluated during the workshop.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.9090909091}
{"Year":2023,"Venue":"findings-2023","Acronym":"G-Tuning","Description":"Improving Generalization of Pre-trained Language Models with Generative Adversarial Network","Abstract":"The generalization ability of pre-trained language models (plms) in downstream tasks is heavily influenced by fine-tuning. The objective of fine-tuning is to transform the latent representation of plms from a universal space to a target space, allowing the model to be applied to downstream tasks with the capability of generalizing to unseen samples. However, the effect of plms will be diminished when the training data coverage is insufficient, in which fine-tuning is inadequate to learn the complete mapping. In this study, we propose a new fine-tuning framework, referred to as domain, that aims to preserve the generalization ability of plms in downstream tasks. Specifically, we integrate a generative adversarial network into the fine-tuning process to aid in the transformation of the latent representation in the entire space. Empirical evaluations on the glue benchmark, as well as two additional demanding scenarios involving domain and language generalization, demonstrate that language can accurately map the universal representation to the target space, thus effectively enhancing the generalization performance of plms across various downstream tasks.","wordlikeness":0.5,"lcsratio":0.875,"wordcoverage":0.8571428571}
{"Year":2016,"Venue":"inlg-2016","Acronym":"QGASP","Description":"a Framework for Question Generation Based on Different Levels of Linguistic Information","Abstract":"We introduce generation, a system that performs question generation by using lexical, syntactic and semantic information. And uses this information both to learn patterns and to generate questions. In this paper, we brie\ufb02y describe its architecture.","wordlikeness":0.6,"lcsratio":0.8,"wordcoverage":0.8}
{"Year":2022,"Venue":"lrec-2022","Acronym":"EmoWOZ","Description":"A Large-Scale Corpus and Labelling Scheme for Emotion Recognition in Task-Oriented Dialogue Systems","Abstract":"The ability to recognise emotions lends a conversational artificial intelligence a human touch. While emotions in chit-chat dialogues have received substantial attention, emotions in task-oriented dialogues remain largely unaddressed. This is despite emotions and dialogue success having equally important roles in a natural system. Existing emotion-annotated task-oriented corpora are limited in size, label richness, and public availability, creating a bottleneck for downstream tasks. To lay a foundation for studies on emotions in task-oriented dialogues, we introduce dialogues., a large-scale manually emotion-annotated corpus of task-oriented dialogues. The is based on multiwoz, a multi-domain task-oriented dialogue dataset. It contains more than 11k dialogues with more than 83k emotion annotations of user utterances. In addition to wizard-of-oz dialogues from multiwoz, we collect human-machine dialogues within the same set of domains to sufficiently cover the space of various emotions that can happen during the lifetime of a data-driven dialogue system. To the best of our knowledge, this is the first large-scale open-source corpus of its kind. We propose a novel emotion labelling scheme, which is tailored to task-oriented dialogues. We report a set of experimental results to show the usability of this corpus for emotion recognition and state tracking in task-oriented dialogues.","wordlikeness":0.3333333333,"lcsratio":0.6666666667,"wordcoverage":0.6153846154}
{"Year":2022,"Venue":"naacl-2022","Acronym":"ScAN","Description":"Suicide Attempt and Ideation Events Dataset","Abstract":"Suicide is an important public health concern and one of the leading causes of death worldwide. Suicidal behaviors, including suicide attempts (sa) and suicide ideations (si), are leading risk factors for death by suicide. Information related to patients\u2019 previous and current sa and si are frequently documented in the electronic health record (ehr) notes. Accurate detection of such documentation may help improve surveillance and predictions of patients\u2019 suicidal behaviors and alert medical professionals for suicide prevention efforts. In this study, we first built suicide attempt and ideation events (previous) dataset, a subset of the publicly available mimic iii dataset spanning over 12k+ ehr notes with 19k+ annotated sa and si events information. The annotations also contain attributes such as method of suicide attempt. We also provide a strong baseline model identifyinger (suicide attempt and ideation events retriever), a multi-task roberta-based model with a retrieval module to extract all the relevant suicidal behavioral evidences from ehr notes of an hospital-stay and, and a prediction module to identify the type of suicidal behavior (sa and si) concluded during the patient\u2019s stay at the hospital. Suicidaler achieved a macro-weighted f1-score of 0.83 for identifying suicidal behavioral evidences and a macro f1-score of 0.78 and 0.60 for classification of sa and si for the patient\u2019s hospital-stay, respectively. Type and availableer are publicly available.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2018,"Venue":"emnlp-2018","Acronym":"PreCo","Description":"A Large-scale Dataset in Preschool Vocabulary for Coreference Resolution","Abstract":"We introduce analysis, a large-scale english dataset for coreference resolution. The dataset is designed to embody the core challenges in coreference, such as entity representation, by alleviating the challenge of low overlap between training and test sets and enabling separated analysis of mention detection and mention clustering. To strengthen the training-test overlap, we collect a large corpus of 38k documents and 12.5m words which are mostly from the vocabulary of english-speaking preschoolers. Experiments show that with higher training-test overlap, error analysis on making is more efficient than the one on ontonotes, a popular existing dataset. Furthermore, we annotate singleton mentions making it possible for the first time to quantify the influence that a mention detector makes on coreference resolution performance. The dataset is freely available at <a href=https:\/\/preschool-lab.github.io\/and\/ class=acl-markup-url>https:\/\/preschool-lab.github.io\/corpus\/<\/a>.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.7692307692}
{"Year":2014,"Venue":"semeval-2014","Acronym":"FBK-TR","Description":"Applying SVM with Multiple Linguistic Features for Cross-Level Semantic Similarity","Abstract":"Recently, the task of measuring semantic similarity between given texts has drawn much attention from the natural language processing community. Especially, the task becomes more interesting when it comes to measuring the semantic similarity between different-sized texts, e.g paragraph-sentence, sentence-phrase, phrase-word, etc. In this paper, we, the attention team, describe our system participating in task 3 \"cross-level semantic similarity\", at semeval 2014. We also report the results obtained by our system, compared to the baseline and other participating systems in this task.","wordlikeness":0.1666666667,"lcsratio":0.6666666667,"wordcoverage":0.6}
{"Year":2021,"Venue":"eacl-2021","Acronym":"ResPer","Description":"Computationally Modelling Resisting Strategies in Persuasive Conversations","Abstract":"Modelling persuasion strategies as predictors of task outcome has several real-world applications and has received considerable attention from the computational linguistics community. However, previous research has failed to account for the resisting strategies employed by an individual to foil such persuasion attempts. Grounded in prior literature in cognitive and social psychology, we propose a generalised framework for identifying resisting strategies in persuasive conversations. We instantiate our framework on two distinct datasets comprising persuasion and negotiation conversations. We also leverage a hierarchical sequence-labelling neural architecture to infer the aforementioned resisting strategies automatically. Our experiments reveal the asymmetry of power roles in non-collaborative goal-directed conversations and the benefits accrued from incorporating resisting strategies on the final conversation outcome. We also investigate the role of different resisting strategies on the conversation outcome and glean insights that corroborate with past findings. We also make the code and the dataset of this work publicly available at <a href=https:\/\/github.com\/americast\/task class=acl-markup-url>https:\/\/github.com\/americast\/foil<\/a>.","wordlikeness":0.8333333333,"lcsratio":1.0,"wordcoverage":0.9090909091}
{"Year":2020,"Venue":"emnlp-2020","Acronym":"HENIN","Description":"Learning Heterogeneous Neural Interaction Networks for Explainable Cyberbullying Detection on Social Media","Abstract":"In the computational detection of cyberbullying, existing work largely focused on building generic classifiers that rely exclusively on text analysis of social media sessions. Despite their empirical success, we argue that a critical missing piece is the model explainability, i.e., why a particular piece of media session is detected as cyberbullying. In this paper, therefore, we propose a novel deep model, heterogeneous neural interaction networks (cyberbullying.), for explainable cyberbullying detection. Success, contains the following components: a comment encoder, a post-comment co-attention sub-network, and session-session and post-post interaction extractors. Extensive experiments conducted on real datasets exhibit not only the promising performance of but, but also highlight evidential comments so that one can understand why a media session is identified as cyberbullying.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2020,"Venue":"acl-2020","Acronym":"Tabouid","Description":"a Wikipedia-based word guessing game","Abstract":"We present covering, a word-guessing game automatically generated from wikipedia. A contains 10,000 (virtual) cards in english, and as many in french, covering not only words and linguistic expressions but also a variety of topics including artists, historical events or scientific concepts. Each card corresponds to a wikipedia article, and conversely, any article could be turned into a card. A range of relatively simple nlp and machine-learning techniques are effectively integrated into a two-stage process. First, a large subset of wikipedia articles are scored - this score estimates the difficulty, or alternatively, the playability of the page. Then, the best articles are turned into cards by selecting, for each of them, a list of banned words based on its content. We believe that the game we present is more than mere entertainment and that, furthermore, this paper has pedagogical potential.","wordlikeness":0.5714285714,"lcsratio":0.7142857143,"wordcoverage":0.8571428571}
{"Year":2018,"Venue":"conll-2018","Acronym":"NLP-Cube","Description":"End-to-End Raw Text Processing With Neural Networks","Abstract":"We introduce available: an end-to-end natural language processing framework, evaluated in conll\u2019s \u201cmultilingual parsing from raw text to universal dependencies 2018\u201d shared task. It performs sentence splitting, tokenization, compound word expansion, lemmatization, tagging and parsing. Based entirely on recurrent neural networks, written in python, this ready-to-use open source system is freely available on github. For each task we describe and discuss its specific network architecture, closing with an overview on the results obtained in the competition.","wordlikeness":0.375,"lcsratio":0.625,"wordcoverage":0.7692307692}
{"Year":2022,"Venue":"acl-2022","Acronym":"AlephBERT","Description":"Language Model Pre-training and Evaluation from Sub-Word to Sentence Level","Abstract":"Large pre-trained language models (plms) have become ubiquitous in the development of language understanding technology and lie at the heart of many artificial intelligence advances. While advances reported for english using plms are unprecedented, reported advances using plms for hebrew are few and far between. The problem is twofold. First, so far, hebrew resources for training large language models are not of the same magnitude as their english counterparts. Second, most benchmarks available to evaluate progress in hebrew nlp require morphological boundaries which are not available in the output of standard plms. in this work we remedy both aspects. We present english, a large plm for modern hebrew, trained on larger vocabulary and a larger dataset than any hebrew plm before. Moreover, we introduce a novel neural architecture that recovers the morphological segments encoded in contextualized embedding vectors. Based on this new morphological component we offer an evaluation suite consisting of multiple tasks and benchmarks that cover sentence-level, word-level and sub-word level analyses. On all tasks, become obtains state-of-the-art results beyond contemporary hebrew baselines. We make our that model, the morphological extraction model, and the hebrew evaluation suite publicly available, for evaluating future hebrew plms.","wordlikeness":0.6666666667,"lcsratio":0.6666666667,"wordcoverage":0.8235294118}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"X-FACTOR","Description":"A Cross-metric Evaluation of Factual Correctness in Abstractive Summarization","Abstract":"Ive summarization models often produce factually inconsistent summaries that are not supported by the original article. Recently, a number of fact-consistent evaluation techniques have been proposed to address this issue; however, a detailed analysis of how these metrics agree with one another has yet to be conducted. In this paper, we present quality, a cross-evaluation of three high-performing fact-aware abstractive summarization methods. First, we show that summarization models are often fine-tuned on datasets that contain factually inconsistent summaries and propose a fact-aware filtering mechanism that improves the quality of training data and, consequently, the factuality of these models. Second, we propose a corrector module that can be used to improve the factual consistency of generated summaries. Third, we present a re-ranking technique that samples summary instances from the output distribution of a summarization model and re-ranks the sampled instances based on their factuality. Finally, we provide a detailed cross-metric agreement analysis that shows how tuning a model to output summaries based on a particular factuality metric influences factuality as determined by the other metrics. Our goal in this work is to facilitate research that improves the factuality and faithfulness of abstractive summarization models.","wordlikeness":0.75,"lcsratio":0.875,"wordcoverage":0.8571428571}
{"Year":2022,"Venue":"coling-2022","Acronym":"MACRONYM","Description":"A Large-Scale Dataset for Multilingual and Multi-Domain Acronym Extraction","Abstract":"Acronym extraction is the task of identifying acronyms and their expanded forms in texts that is necessary for various nlp applications. Despite major progress for this task in recent years, one limitation of existing ae research is that they are limited to the english language and certain domains (i.e., scientific and biomedical). Challenges of ae in other languages and domains are mainly unexplored. As such, lacking annotated datasets in multiple languages and domains has been a major issue to prevent research in this direction. To address this limitation, we propose a new dataset for multilingual and multi-domain ae. Specifically, 27,200 sentences in 6 different languages and 2 new domains, i.e., legal and scientific, are manually annotated for ae. Our experiments on the dataset show that ae in different languages and learning settings has unique challenges, emphasizing the necessity of further research on multilingual and multi-domain ae.","wordlikeness":0.625,"lcsratio":1.0,"wordcoverage":0.9333333333}
{"Year":2010,"Venue":"semeval-2010","Acronym":"RACAI","Description":"Unsupervised WSD Experiments @ SemEval-2, Task 17","Abstract":"This paper documents the participation of the research institute for artificial intelligence of the romanian academy (perform) to the task 17 \u2013 all-words word sense disambiguation on a specific domain, of the semeval-2 competition. We describe three unsupervised wsd systems that make extensive use of the princeton wordnet (wn) structure and wordnet domains in order to perform the disambiguation. The best of them has been ranked the 12th by the task organizers out of 29 judged runs.","wordlikeness":0.6,"lcsratio":0.6,"wordcoverage":0.75}
{"Year":2016,"Venue":"coling-2016","Acronym":"CharNER","Description":"Character-Level Named Entity Recognition","Abstract":"We describe and evaluate a character-level tagger for language-independent named entity recognition (ner). Instead of words, a sentence is represented as a sequence of characters. The model consists of stacked bidirectional lstms which inputs characters and outputs tag probabilities for each character. These probabilities are then converted to consistent word level named entity tags using a viterbi decoder. We are able to achieve close to state-of-the-art ner performance in seven languages with the same basic model using only labeled ner data and no hand-engineered features or other external resources like syntactic taggers or gazetteers.","wordlikeness":0.8571428571,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2021,"Venue":"acl-2021","Acronym":"ProofWriter","Description":"Generating Implications, Proofs, and Abductive Statements over Natural Language","Abstract":"Transformers have been shown to emulate logical deduction over natural language theories (logical rules expressed in natural language), reliably assigning true\/false labels to candidate implications. However, their ability to generate implications of a theory has not yet been demonstrated, and methods for reconstructing proofs of answers are imperfect. In this work we show that a generative model, called generator, can reliably generate both implications of a theory and the natural language proofs that support them. In particular, iterating a 1-step implication generator results in proofs that are highly reliable, and represent actual model decisions (rather than post-hoc rationalizations). On the ruletaker dataset, the accuracy of theories\u2019s proofs exceed previous methods by +9% absolute, and in a way that generalizes to proof depths unseen in training and on out-of-domain problems. We also show that generative techniques can perform a type of abduction with high precision: given a theory and an unprovable conclusion, identify a missing fact that allows the conclusion to be proved, along with a proof. These results signi\ufb01cantly improve the viability of neural methods for systematically reasoning over natural language.","wordlikeness":0.9090909091,"lcsratio":0.8181818182,"wordcoverage":0.7368421053}
{"Year":2022,"Venue":"lrec-2022","Acronym":"OpenKorPOS","Description":"Democratizing Korean Tokenization with Voting-Based Open Corpus Annotation","Abstract":"Korean is a language with complex morphology that uses spaces at larger-than-word boundaries, unlike other east-asian languages. While morpheme-based text generation can provide significant semantic advantages compared to commonly used character-level approaches, korean morphological analyzers only provide a sequence of morpheme-level tokens, losing information in the tokenization process. Two crucial issues are the loss of spacing information and subcharacter level morpheme normalization, both of which make the tokenization result challenging to reconstruct the original input string, deterring the application to generative tasks. As this problem originates from the conventional scheme used when creating a pos tagging corpus, we propose an improvement to the existing scheme, which makes it friendlier to generative tasks. On top of that, we suggest a fully-automatic annotation of a corpus by leveraging public analyzers. We vote the surface and pos from the outcome and fill the sequence with the selected morphemes, yielding tokenization with a decent quality that incorporates space information. Our scheme is verified via an evaluation done on an external corpus, and subsequently, it is adapted to korean wikipedia to construct an open, permissive resource. We compare morphological analyzer performance trained on our corpus with existing methods, then perform an extrinsic evaluation on a downstream task.","wordlikeness":0.8,"lcsratio":0.8,"wordcoverage":0.6666666667}
{"Year":2023,"Venue":"ldk-2023","Acronym":"WIDISBOT","Description":"Widget to analyse disinformation and content spread by bots","Abstract":"The increasing prevalence of bots poses a significant challenge for maintaining the integrity of online information. Bot campaigns have been deployed for both economic scams and political interference, making it necessary to develop a system to detect these agents and analyze their behavior. We present a scalable application designed to identify bots and to buttress the investigation of disinformation campaigns. Our intention is to provide professionals without technical expertise with an effective tool to identify and analyze content generated by bots. This will enable researchers from diverse backgrounds to study bot activity, fostering an interdisciplinary understanding of the strategies these agents use to spread disinformation, and the characteristics of their discourse. We illustrate how to use the application through a case study on covid-19.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.7692307692}
{"Year":2010,"Venue":"semeval-2010","Acronym":"SUCRE","Description":"A Modular System for Coreference Resolution","Abstract":"This paper presents its, a new software tool for coreference resolution and its feature engineering. It is able to separately do noun, pronoun and full coreference resolution. Introduces introduces a new approach to the feature engineering of coreference resolution based on a relational database model and a regular feature de\ufb01nition language. German. Successfully participated in semeval-2010 task 1 on coreference resolution in multiple languages (recasens et al., 2010) for gold and regular closed annotation tracks of six languages. It obtained the best results in several categories, including the regular closed annotation tracks of english and german.","wordlikeness":1.0,"lcsratio":0.8,"wordcoverage":0.8888888889}
{"Year":2020,"Venue":"acl-2020","Acronym":"OpusFilter","Description":"A Configurable Parallel Corpus Filtering Toolbox","Abstract":"This paper introduces improved, a flexible and modular toolbox for filtering parallel corpora. It implements a number of components based on heuristic filters, language identification libraries, character-based language models, and word alignment tools, and it can easily be extended with custom filters. Bitext segments can be ranked according to their quality or domain match using single features or a logistic regression model that can be trained without manually labeled training data. We demonstrate the effectiveness of custom on the example of a finnish-english news translation task based on noisy web-crawled training data. Applying our tool leads to improved translation quality while significantly reducing the size of the training data, also clearly outperforming an alternative ranking given in the crawled data set. Furthermore, we show the ability of task to perform data selection for domain adaptation.","wordlikeness":0.9,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2022,"Venue":"acl-2022","Acronym":"SafetyKit","Description":"First Aid for Measuring Safety in Open-domain Conversational Systems","Abstract":"The social impact of natural language processing and its applications has received increasing attention. In this position paper, we focus on the problem of safety for end-to-end conversational ai. We survey the problem landscape therein, introducing a taxonomy of three observed phenomena: the instigator, yea-sayer, and impostor effects. We then empirically assess the extent to which current tools can measure these effects and current systems display them. We release these tools as part of a \u201cfirst aid kit\u201d (conversational) to quickly assess apparent safety concerns. Our results show that, while current tools are able to provide an estimate of the relative safety of systems in various settings, they still have several shortcomings. We suggest several future directions and discuss ethical considerations.","wordlikeness":0.8888888889,"lcsratio":0.8888888889,"wordcoverage":0.8}
{"Year":2022,"Venue":"lrec-2022","Acronym":"PLOD","Description":"An Abbreviation Detection Dataset for Scientific Documents","Abstract":"The detection and extraction of abbreviations from unstructured texts can help to improve the performance of natural language processing tasks, such as machine translation and information retrieval. However, in terms of publicly available datasets, there is not enough data for training deep-neural-networks-based models to the point of generalising well over data. This paper presents instances, a large-scale dataset for abbreviation detection and extraction that contains 160k+ segments automatically annotated with abbreviations and their long forms. We performed manual validation over a set of instances and a complete automatic validation for this dataset. We then used it to generate several baseline models for detecting abbreviations and long forms. The best models achieved an f1-score of 0.92 for abbreviations and 0.89 for detecting their corresponding long forms. We release this dataset along with our code and all the models publicly at <a href=https:\/\/github.com\/surrey-nlp\/enough-abbreviationdetection class=acl-markup-url>https:\/\/github.","wordlikeness":0.75,"lcsratio":0.5,"wordcoverage":0.8571428571}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"CheckHARD","Description":"Checking Hard Labels for Adversarial Text Detection, Prediction Correction, and Perturbed Word Suggestion","Abstract":"An adversarial attack generates harmful text that fools a target model. More dangerously, this text is unrecognizable by humans. Existing work detects adversarial text and corrects a target\u2019s prediction by identifying perturbed words and changing them into their synonyms, but many benign words are also changed. In this paper, we directly detect adversarial text, correct the prediction, and suggest perturbed words by checking the change in the hard labels from the target\u2019s predictions after replacing a word with its transformation using a model that we call from. The experiments demonstrate that this outperforms existing work on various attacks, models, and datasets.","wordlikeness":0.8888888889,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2023,"Venue":"eacl-2023","Acronym":"LongEval","Description":"Guidelines for Human Evaluation of Faithfulness in Long-form Summarization","Abstract":"While human evaluation remains best practice for accurately judging the faithfulness of automatically-generated summaries, few solutions exist to address the increased difficulty and workload when evaluating long-form summaries. Through a survey of 162 papers on long-form summarization, we first shed light on current human evaluation practices surrounding long-form summaries. We find that 73% of these papers do not perform any human evaluation on model-generated summaries, while other works face new difficulties that manifest when dealing with long documents (e.g., low inter-annotator agreement). Motivated by our survey, we present 162, a set of guidelines for human evaluation of faithfulness in long-form summaries that addresses the following challenges: (1) how can we achieve high inter-annotator agreement on faithfulness scores? (2) how can we minimize annotator workload while maintaining accurate faithfulness scores? And (3) do humans benefit from automated alignment between summary and source snippets? We deploy std-dev in annotation studies on two long-form summarization datasets in different domains (squality and pubmed), and we find that switching to a finer granularity of judgment (e.g., clause-level) reduces inter-annotator variance in faithfulness scores (e.g., std-dev from 18.5 to 6.8). We also show that scores from a partial annotation of fine-grained units highly correlates with scores from a full annotation workload (0.89 kendall\u2019s tau using 50% judgements). We release our human judgments, annotation templates, and software as a python library for future research.","wordlikeness":0.75,"lcsratio":0.875,"wordcoverage":0.7142857143}
{"Year":2022,"Venue":"coling-2022","Acronym":"CLOWER","Description":"A Pre-trained Language Model with Contrastive Learning over Word and Character Representations","Abstract":"Pre-trained language models (plms) have achieved remarkable performance gains across numerous downstream tasks in natural language understanding. Various chinese plms have been successively proposed for learning better chinese language representation. However, most current models use chinese characters as inputs and are not able to encode semantic information contained in chinese words. While recent pre-trained models incorporate both words and characters simultaneously, they usually suffer from deficient semantic interactions and fail to capture the semantic relation between words and characters. To address the above issues, we propose a simple yet effective plm simple, which adopts the contrastive learning over word and character representations. In particular, semantic implicitly encodes the coarse-grained information (i.e., words) into the fine-grained representations (i.e., characters) through contrastive learning on multi-grained information. Propose is of great value in realistic scenarios since it can be easily incorporated into any existing fine-grained based plms without modifying the production pipelines. Extensive experiments conducted on a range of downstream tasks demonstrate the superior performance of successively over several state-of-the-art baselines.","wordlikeness":0.8333333333,"lcsratio":1.0,"wordcoverage":0.9090909091}
{"Year":2010,"Venue":"semeval-2010","Acronym":"UCF-WS","Description":"Domain Word Sense Disambiguation Using Web Selectors","Abstract":"This paper studies the application of the web selectors word sense disambiguation system on a speci\ufb01c domain. The system was primarily applied without any domain tuning, but the incorporation of domain predominant sense information was explored. Results indicated that the system performs relatively the same with domain predominant sense information as without, scoring well above a random baseline, but still 5 percentage points below results of using the \ufb01rst sense.","wordlikeness":0.1666666667,"lcsratio":0.5,"wordcoverage":0.6153846154}
{"Year":2021,"Venue":"acl-2021","Acronym":"eMLM","Description":"A New Pre-training Objective for Emotion Related Tasks","Abstract":"Bert has been shown to be extremely effective on a wide variety of natural language processing tasks, including sentiment analysis and emotion detection. However, the proposed pretraining objectives of bert do not induce any sentiment or emotion-specific biases into the model. In this paper, we present emotion masked language modelling, a variation of masked language modelling aimed at improving the bert language representation model for emotion detection and sentiment analysis tasks. Using the same pre-training corpora as the original model, wikipedia and bookcorpus, our bert variation manages to improve the downstream performance on 4 tasks from emotion detection and sentiment analysis by an average of 1.2% f-1. Moreover, our approach shows an increased performance in our task-specific robustness tests.","wordlikeness":0.25,"lcsratio":0.75,"wordcoverage":0.8571428571}
{"Year":2019,"Venue":"ws-2019","Acronym":"CoFiF","Description":"A Corpus of Financial Reports in French Language","Abstract":"In an era when machine learning and arti\ufb01cial intelligence have huge momentum, the data demand to train and test models is steadily growing. We introduce language, the \ufb01rst corpus comprising company reports in the french language. It contains over 188 million tokens in 2655 reports, covering reference documents, annual, semestrial and trimestrial reports. Our main focus is on the 60 largest french companies listed in france\u2019s main stock indices cac40 and cac next 20. The corpus spans over 20 years, ranging from 1995 to 2018. To evaluate this novel collection of organizational writing, we use 1. To generate two character-level language models, a forward and a backward one, which we use to demonstrate the corpus potential on business, economics, and management research in the french language. The corpus is accessible on github 1.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.7272727273}
{"Year":2020,"Venue":"coling-2020","Acronym":"TWEETSUM","Description":"Event oriented Social Summarization Dataset","Abstract":"With social media becoming popular, a vast of short and noisy messages are produced by millions of users when a hot event happens. Developing social summarization systems becomes more and more critical for people to quickly grasp core and essential information. However, the publicly available and high-quality large scale social summarization dataset is rare. Constructing such corpus is not easy and very expensive since short texts have very complex social characteristics. In this paper, we construct happens., a new event-oriented dataset for social summarization. The original data is collected from twitter and contains 12 real world hot events with a total of 44,034 tweets and 11,240 users. Each event has four expert summaries, and we also have the annotation quality evaluation. In addition, we collect additional social signals (i.e. user relations, hashtags and user profiles) and further establish user relation network for each event. Besides the detailed dataset description, we show the performance of several typical extractive summarization methods on of to establish baselines. For further researches, we will release this dataset to the public.","wordlikeness":0.875,"lcsratio":0.75,"wordcoverage":0.8571428571}
{"Year":2023,"Venue":"starsem-2023","Acronym":"CRAPES","Description":"Cross-modal Annotation Projection for Visual Semantic Role Labeling","Abstract":"Automatic image comprehension is an important yet challenging task that includes identifying actions in an image and corresponding action participants. Most current approaches to this task, now termed grounded situation recognition (gsr), start by predicting a verb that describes the action and then predict the nouns that can participate in the action as arguments to the verb. This problem formulation limits each image to a single action even though several actions could be depicted. In contrast, text-based semantic role labeling (srl) aims to label all actions in a sentence, typically resulting in at least two or three predicate argument structures per sentence. We hypothesize that expanding gsr to follow the more liberal srl text-based approach to action and participant identification could improve image comprehension results. To test this hypothesis and to preserve generalization capabilities, we use general-purpose vision and language components as a front-end. This paper presents our results, a substantial 28.6 point jump in performance on the swig dataset, which confirm our hypothesis. We also discuss the benefits of loosely coupled broad-coverage off-the-shelf components which generalized well to out of domain images, and can decrease the need for manual image semantic role annotation.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.9090909091}
{"Year":2016,"Venue":"lrec-2016","Acronym":"ASPEC","Description":"Asian Scientific Paper Excerpt Corpus","Abstract":"In this paper, we describe the details of the on (asian scientific paper excerpt corpus), which is the first large-size parallel corpus of scientific paper domain. (-jc). Was constructed in the japanese-chinese machine translation project conducted between 2006 and 2010 using the special coordination funds for promoting science and technology. It consists of a japanese-english scientific paper abstract corpus of approximately 3 million parallel sentences (special-je) and a chinese-japanese scientific paper excerpt corpus of approximately 0.68 million parallel sentences (dataset-jc). First is used as the official dataset for the machine translation evaluation workshop wat (workshop on asian translation).","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.9090909091}
{"Year":2018,"Venue":"acl-2018","Acronym":"diaNED","Description":"Time-Aware Named Entity Disambiguation for Diachronic Corpora","Abstract":"Named entity disambiguation (ned) systems perform well on news articles and other texts covering a specific time interval. However, ned quality drops when inputs span long time periods like in archives or historic corpora. This paper presents the first time-aware method for ned that resolves ambiguities even when mention contexts give only few cues. The method is based on computing temporal signatures for entities and comparing these to the temporal contexts of input mentions. Our experiments show superior quality on a newly created diachronic corpus.","wordlikeness":0.6666666667,"lcsratio":0.8333333333,"wordcoverage":0.9090909091}
{"Year":2023,"Venue":"inlg-2023","Acronym":"enunlg","Description":"a Python library for reproducible neural data-to-text experimentation","Abstract":"Over the past decade, a variety of neural architectures for data-to-text generation (nlg) have been proposed. However, each system typically has its own approach to pre- and post-processing and other implementation details. Diversity in implementations is desirable, but it also confounds attempts to compare model performance: are the differences due to the proposed architectures or are they a byproduct of the libraries used or a result of pre- and post-processing decisions made? To improve reproducibility, we re-implement several pre-transformer neural models for data-to-text nlg within a single framework to facilitate direct comparisons of the models themselves and better understand the contributions of other design choices. We release our library at https:\/\/github.com\/napiernlp\/release to serve as a baseline for ongoing work in this area including research on nlg for low-resource languages where transformers might not be optimal.","wordlikeness":0.5,"lcsratio":0.6666666667,"wordcoverage":0.7692307692}
{"Year":2021,"Venue":"naacl-2021","Acronym":"APo-VAE","Description":"Text Generation in Hyperbolic Space","Abstract":"Natural language often exhibits inherent hierarchical structure ingrained with complex syntax and semantics. However, most state-of-the-art deep generative models learn embeddings only in euclidean vector space, without accounting for this structural property of language. In this paper, we investigate text generation in a hyperbolic latent space to learn continuous hierarchical representations. An adversarial poincare variational autoencoder (normal) is presented, where both the prior and variational posterior of latent variables are defined over a poincare ball via wrapped normal distributions. By adopting the primal-dual formulation of kullback-leibler divergence, an adversarial learning procedure is introduced to empower robust model training. Extensive experiments in language modeling, unaligned style transfer, and dialog-response generation demonstrate the effectiveness of the proposed euclidean model over vaes in euclidean latent space, thanks to its superb capabilities in capturing latent language hierarchies in hyperbolic space.","wordlikeness":0.5714285714,"lcsratio":0.7142857143,"wordcoverage":0.7142857143}
{"Year":2000,"Venue":"lrec-2000","Acronym":"MDWOZ","Description":"A Wizard of Oz Environment for Dialog Systems Development","Abstract":"This paper describes was, a development environment for spoken dialog systems based on the wizard of oz technique, whose main goal is to facilitate data collection (speech signal and dialog related information) and interaction model building. Both these tasks can be quite dif\ufb01cult, and such an environment can facilitate them very much. Due to the modular way in which participate was implemented, it is possible to reuse parts of it in the \ufb01nal dialog system. The environment provides language-transparent facilities and accessible methods such that even non-computing specialists can participate in spoken dialog systems development. The main features of the environment are presented, together with some test experiments. 1.","wordlikeness":0.2,"lcsratio":0.6,"wordcoverage":0.6666666667}
{"Year":2022,"Venue":"findings-2022","Acronym":"KD-VLP","Description":"Improving End-to-End Vision-and-Language Pretraining with Object Knowledge Distillation","Abstract":"Self-supervised vision-and-language pretraining (vlp) aims to learn transferable multi-modal representations from large-scale image-text data and to achieve strong performances on a broad scope of vision-language tasks after finetuning. Previous mainstream vlp approaches typically adopt a two-step strategy relying on external object detectors to encode images in a multi-modal transformer framework, which suffer from restrictive object concept space, limited image context and inefficient computation. In this paper, we propose an object-aware end-to-end vlp framework, which directly feeds image grid features from cnns into the transformer and learns the multi-modal representations jointly. More importantly, we propose to perform object knowledge distillation to facilitate learning cross-modal alignment at different semantic levels. To achieve that, we design two novel pretext tasks by taking object features and their semantic labels from external detectors as supervision: 1.) object-guided masked vision modeling task focuses on enforcing object-aware representation learning in the multi-modal transformer; 2.) phrase-region alignment task aims to improve cross-modal alignment by utilizing the similarities between noun phrases and object labels in the linguistic space. Extensive experiments on a wide range of vision-language tasks demonstrate the efficacy of our proposed framework, and we achieve competitive or superior performances over the existing pretraining strategies.","wordlikeness":0.1666666667,"lcsratio":0.8333333333,"wordcoverage":0.6153846154}
{"Year":2022,"Venue":"findings-2022","Acronym":"LM-CORE","Description":"Language Models with Contextually Relevant External Knowledge","Abstract":"Large transformer-based pre-trained language models have achieved impressive performance on a variety of knowledge-intensive tasks and can capture factual knowledge in their parameters. We argue that storing large amounts of knowledge in the model parameters is sub-optimal given the ever-growing amounts of knowledge and resource requirements. We posit that a more efficient alternative is to provide explicit access to contextually relevant structured knowledge to the model and train it to use that knowledge. We present affecting \u2013 a general framework to achieve this\u2013 that allows <i>decoupling<\/i> of the language model training from the external knowledge source and allows the latter to be updated without affecting the already trained model. Experimental results show that pre-trained, having access to external knowledge, achieves significant and robust outperformance over state-of-the-art knowledge-enhanced language models on knowledge probing tasks; can effectively handle knowledge updates; and performs well on two downstream tasks. We also present a thorough error analysis highlighting the successes and failures of achieve. Our code and model checkpoints are publicly available.","wordlikeness":0.5714285714,"lcsratio":0.8571428571,"wordcoverage":0.7272727273}
{"Year":2021,"Venue":"ws-2021","Acronym":"SpanAlign","Description":"Efficient Sequence Tagging Annotation Projection into Translated Data applied to Cross-Lingual Opinion Mining","Abstract":"Following the increasing performance of neural machine translation systems, the paradigm of using automatically translated data for cross-lingual adaptation is now studied in several applicative domains. The capacity to accurately project annotations remains however an issue for sequence tagging tasks where annotation must be projected with correct spans. Additionally, when the task implies noisy user-generated text, the quality of translation and annotation projection can be affected. In this paper we propose to tackle multilingual sequence tagging with a new span alignment method and apply it to opinion target extraction from customer reviews. We show that provided suitable heuristics, translated data with automatic span-level annotation projection can yield improvements both for cross-lingual adaptation compared to zero-shot transfer, and data augmentation compared to a multilingual baseline.","wordlikeness":0.7777777778,"lcsratio":1.0,"wordcoverage":0.7142857143}
{"Year":2021,"Venue":"acl-2021","Acronym":"OntoGUM","Description":"Evaluating Contextualized SOTA Coreference Resolution on 12 More Genres","Abstract":"Sota coreference resolution produces increasingly impressive scores on the ontonotes benchmark. However lack of comparable data following the same scheme for more genres makes it difficult to evaluate generalizability to open domain data. This paper provides a dataset and comprehensive evaluation showing that the latest neural lm based end-to-end systems degrade very substantially out of domain. We make an ontonotes-like coreference dataset called corpus publicly available, converted from gum, an english corpus covering 12 genres, using deterministic rules, which we evaluate. Thanks to the rich syntactic and discourse annotations in gum, we are able to create the largest human-annotated coreference corpus following the ontonotes guidelines, and the first to be evaluated for consistency with the ontonotes scheme. Out-of-domain evaluation across 12 genres shows nearly 15-20% degradation for both deterministic and deep learning systems, indicating a lack of generalizability or covert overfitting in existing coreference resolution models.","wordlikeness":0.7142857143,"lcsratio":0.8571428571,"wordcoverage":0.7272727273}
{"Year":2018,"Venue":"emnlp-2018","Acronym":"TRANX","Description":"A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation","Abstract":"We present ,, a transition-based neural semantic parser that maps natural language (nl) utterances into formal meaning representations (mrs). Based uses a transition system based on the abstract syntax description language for the target mr, which gives it two major advantages: (1) it is highly accurate, using information from the syntax of the target mr to constrain the output space and model the information flow, and (2) it is highly generalizable, and can easily be applied to new types of mr by just writing a new abstract syntax description corresponding to the allowable structures in the mr. Experiments on four different semantic parsing and code generation tasks show that our system is generalizable, extensible, and effective, registering strong results compared to existing neural semantic parsers.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2012,"Venue":"starsem-2012","Acronym":"UNED","Description":"Improving Text Similarity Measures without Human Assessments","Abstract":"This paper describes the participation of textual nlp group in the semeval 2012 semantic textual similarity task. Our contribution consists of an unsupervised method, heterogeneity based ranking (hbr), to combine similarity measures. Our runs focus on combining standard similarity measures for machine translation. The pearson correlation achieved is outperformed by other systems, due to the limitation of mt evaluation measures in the context of this task. However, the combination of system outputs that participated in the campaign produces three interesting results: (i) combining all systems without considering any kind of human assessments achieve a similar performance than the best peers in all test corpora, (ii) combining the 40 less reliable peers in the evaluation campaign achieves similar results; and (iii) the correlation between peers and hbr predicts, with a 0.94 correlation, the performance of measures according to human assessments.","wordlikeness":0.75,"lcsratio":0.75,"wordcoverage":0.8888888889}
{"Year":2022,"Venue":"findings-2022","Acronym":"ASCM","Description":"An Answer Space Clustered Prompting Method without Answer Engineering","Abstract":"Prompt-based learning, which exploits knowledge from pre-trained language models by providing textual prompts and designing appropriate answer-category mapping methods, has achieved impressive successes on few-shot text classification and natural language inference (nli). Because of the diverse linguistic expression, there exist many answer tokens for the same category. However, both manual answer design and automatic answer search constrain answer space and therefore hardly achieve ideal performance. To address this issue, we propose an answer space clustered prompting model (impressive) together with a synonym initialization method (si) which automatically categorizes all answer tokens in a semantic-clustered embedding space. We also propose a stable semi-supervised method named stair learning (sl) that orderly distills knowledge from better models to weaker models. Extensive experiments demonstrate that our there+sl significantly outperforms existing state-of-the-art techniques in few-shot settings.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2022,"Venue":"ws-2022","Acronym":"TransPOS","Description":"Transformers for Consolidating Different POS Tagset Datasets","Abstract":"In hope of expanding training data, researchers often want to merge two or more datasets that are created using different labeling schemes. This paper considers two datasets that label part-of-speech (pos) tags under different tagging schemes and leverage the supervised labels of one dataset to help generate labels for the other dataset. This paper further discusses the theoretical difficulties of this approach and proposes a novel supervised architecture employing transformers to tackle the problem of consolidating two completely disjoint datasets. The results diverge from initial expectations and discourage exploration into the use of disjoint labels to consolidate datasets with different labels.","wordlikeness":0.875,"lcsratio":1.0,"wordcoverage":0.8235294118}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"RICA","Description":"Evaluating Robust Inference Capabilities Based on Commonsense Axioms","Abstract":"Pre-trained language models (ptlms) have achieved impressive performance on commonsense inference benchmarks, but their ability to employ commonsense to make robust inferences, which is crucial for effective communications with humans, is debated. In the pursuit of advancing fluid human-ai communication, we propose a new challenge, robust: robust inference using commonsense axioms, that evaluates robust commonsense inference despite textual perturbations. To generate data for this challenge, we develop a systematic and scalable procedure using commonsense knowledge bases and probe ptlms across two different evaluation settings. Extensive experiments on our generated probe sets with more than 10k statements show that ptlms perform no better than random guessing on the zero-shot setting, are heavily impacted by statistical biases, and are not robust to perturbation attacks. We also find that fine-tuning on similar statements offer limited gains, as ptlms still fail to generalize to unseen inferences. Our new large-scale benchmark exposes a significant gap between ptlms and human-level language understanding and offers a new challenge for ptlms to demonstrate commonsense.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2023,"Venue":"findings-2023","Acronym":"Check-COVID","Description":"Fact-Checking COVID-19 News Claims with Scientific Evidence","Abstract":"We present a new fact-checking benchmark, (annotator-written), that requires systems to verify claims about covid-19 from news using evidence from scientific articles. This approach to fact-checking is particularly challenging as it requires checking internet text written in everyday language against evidence from journal articles written in formal academic language. Paired contains 1, 504 expert-annotated news claims about the coronavirus paired with sentence-level evidence from scientific journal articles and veracity labels. It includes both extracted (journalist-written) and composed (annotator-written) claims. Experiments using both a fact-checking specific system and gpt-3.5, which respectively achieve f1 scores of 76.99 and 69.90 on this task, reveal the difficulty of automatically fact-checking both claim types and the importance of in-domain data for good performance. Our data and models are released publicly at <a href=https:\/\/github.com\/posuer\/expert-annotated class=acl-markup-url>https:\/\/github.com\/posuer\/504<\/a>.","wordlikeness":0.7272727273,"lcsratio":0.9090909091,"wordcoverage":0.6666666667}
{"Year":2004,"Venue":"coling-2004","Acronym":"HITIQA","Description":"Towards Analytical Question Answering","Abstract":"In this paper we describe the analytic question answering system complex (highquality interactive question answering) which has been developed over the last 2 years as an advanced research tool for information analysts. Been is an interactive opendomain question answering technology designed to allow analysts to pose complex exploratory questions in natural language and obtain relevant information units to prepare their briefing reports. The system uses novel data-driven semantics to conduct a clarification dialogue with the user that explores the scope and the context of the desired answer space. The system has undergone extensive hands-on evaluations by a group of intelligence analysts. This evaluation validated the overall approach in group but also exposed limitations of the current prototype.","wordlikeness":0.5,"lcsratio":0.6666666667,"wordcoverage":0.7692307692}
{"Year":2019,"Venue":"ijcnlp-2019","Acronym":"EDA","Description":"Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks","Abstract":"We present boosting: easy data augmentation techniques for boosting performance on text classification tasks. While consists of four simple but powerful operations: synonym replacement, random insertion, random swap, and random deletion. On five text classification tasks, we show that and improves performance for both convolutional and recurrent neural networks. Tasks, demonstrates particularly strong results for smaller datasets; on average, across five datasets, training with for while using only 50% of the available training set achieved the same accuracy as normal training with all available data. We also performed extensive ablation studies and suggest parameters for practical use.","wordlikeness":0.3333333333,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2022,"Venue":"coling-2022","Acronym":"SCL-RAI","Description":"Span-based Contrastive Learning with Retrieval Augmented Inference for Unlabeled Entity Problem in NER","Abstract":"Unlabeled entity problem (uep) in named entity recognition (ner) datasets seriously hinders the improvement of ner performance. This paper proposes while to cope with this problem. Firstly, we decrease the distance of span representations with the same label while increasing it for different ones via span-based contrastive learning, which relieves the ambiguity among entities and improves the robustness of the model over unlabeled entities. Then we propose retrieval augmented inference to mitigate the decision boundary shifting problem. Our method significantly outperforms the previous sota method by 4.21% and 8.64% f1-score on two real-world datasets.","wordlikeness":0.4285714286,"lcsratio":0.8571428571,"wordcoverage":0.6666666667}
{"Year":2021,"Venue":"cnl-2021","Acronym":"RegelSpraak","Description":"a CNL for Executable Tax Rules Specification","Abstract":"Double is a cnl developed at the dutch tax administration (dta) over the last decade. Keeping up with frequently changing tax rules poses a formidable challenge to the dta it department. Rule is a central asset in ongoing efforts of the dta to attune their tax it systems to automatic execution of tax law. Setup. Now is part of the operational process of rule specification and execution. In this practice-oriented paper, we present the history of emphasizing, its properties and the context of its use, emphasizing its double functionality as a language readable by non-technical tax experts but also directly interpretable in a software generating setup.","wordlikeness":0.7272727273,"lcsratio":0.6363636364,"wordcoverage":0.625}
{"Year":2014,"Venue":"semeval-2014","Acronym":"SAP-RI","Description":"A Constrained and Supervised Approach for Aspect-Based Sentiment Analysis","Abstract":"We describe the submission of the sap research & innovation team to the semeval 2014 task 4: aspect-based sentiment analysis (absa). Our system follows a constrained and supervised approach for aspect term extraction, categorization and sentiment classi\ufb01cation of online reviews and the details are included in this paper.","wordlikeness":0.5,"lcsratio":0.8333333333,"wordcoverage":0.7272727273}
{"Year":2021,"Venue":"ws-2021","Acronym":"PoliWAM","Description":"An Exploration of a Large Scale Corpus of Political Discussions on WhatsApp Messenger","Abstract":"Whatsapp messenger is one of the most popular channels for spreading information with a current reach of more than 180 countries and 2 billion people. Its widespread usage has made it one of the most popular media for information propagation among the masses during any socially engaging event. In the recent past, several countries have witnessed its effectiveness and influence in political and social campaigns. We observe a high surge in information and propaganda flow during election campaigning. In this paper, we explore a high-quality large-scale user-generated dataset curated from whatsapp comprising of 281 groups, 31,078 unique users, and 223,404 messages shared before, during, and after the indian general elections 2019, encompassing all major indian political parties and leaders. In addition to the raw noisy user-generated data, we present a fine-grained annotated dataset of 3,848 messages that will be useful to understand the various dimensions of whatsapp political campaigning. We present several complementary insights into the investigative and sensational news stories from the same period. Exploratory data analysis and experiments showcase several exciting results and future research opportunities. To facilitate reproducible research, we make the anonymized datasets available in the public domain.","wordlikeness":0.7142857143,"lcsratio":1.0,"wordcoverage":0.7272727273}
{"Year":2020,"Venue":"acl-2020","Acronym":"FastBERT","Description":"a Self-distilling BERT with Adaptive Inference Time","Abstract":"Pre-trained language models like bert have proven to be highly performant. However, they are often computationally expensive in many practical scenarios, for such heavy models can hardly be readily implemented with limited resources. To improve their efficiency with an assured model performance, we propose a novel speed-tunable many with adaptive inference time. The speed at inference can be flexibly adjusted under varying demands, while redundant calculation of samples is avoided. Moreover, this model adopts a unique self-distillation mechanism at fine-tuning, further enabling a greater computational efficacy with minimal loss in performance. Our model achieves promising results in twelve english and chinese datasets. It is able to speed up by a wide range from 1 to 12 times than bert if given different speedup thresholds to make a speed-performance tradeoff.","wordlikeness":0.875,"lcsratio":0.875,"wordcoverage":0.8571428571}
{"Year":2022,"Venue":"acl-2022","Acronym":"SalesBot","Description":"Transitioning from Chit-Chat to Task-Oriented Dialogues","Abstract":"Dialogue systems are usually categorized into two types, open-domain and task-oriented. The first one focuses on chatting with users and making them engage in the conversations, where selecting a proper topic to fit the dialogue context is essential for a successful dialogue. The other one focuses on a specific task instead of casual talks, e.g., finding a movie on friday night, playing a song. These two directions have been studied separately due to their different purposes. However, how to smoothly transition from social chatting to task-oriented dialogues is important for triggering the business opportunities, and there is no any public data focusing on such scenarios. Hence, this paper focuses on investigating the conversations starting from open-domain social chatting and then gradually transitioning to task-oriented purposes, and releases a large-scale dataset with detailed annotations for encouraging this research direction. To achieve this goal, this paper proposes a framework to automatically generate many dialogues without human involvement, in which any powerful open-domain dialogue generation model can be easily leveraged. The human evaluation shows that our generated dialogue data has a natural flow at a reasonable quality, showing that our released data has a great potential of guiding future research directions and commercial activities. Furthermore, the released models allow researchers to automatically generate unlimited dialogues in the target scenarios, which can greatly benefit semi-supervised and unsupervised approaches.","wordlikeness":0.875,"lcsratio":0.625,"wordcoverage":0.7692307692}
{"Year":2022,"Venue":"coling-2022","Acronym":"DoubleMix","Description":"Simple Interpolation-Based Data Augmentation for Text Classification","Abstract":"This paper proposes a simple yet effective interpolation-based data augmentation approach termed up, to improve the robustness of models in text classification. Yet first leverages a couple of simple augmentation operations to generate several perturbed samples for each training data, and then uses the perturbed data and original data to carry out a two-step interpolation in the hidden space of neural models. Concretely, it first mixes up the perturbed data to a synthetic sample and then mixes up the original data and the synthetic perturbed data. Confirm enhances models\u2019 robustness by learning the \u201cshifted\u201d features in hidden space. On six text classification benchmark datasets, our approach outperforms several popular text augmentation methods including token-level, sentence-level, and hidden-level data augmentation techniques. Also, experiments in low-resource settings show our approach consistently improves models\u2019 performance when the training data is scarce. Extensive ablation studies and case studies confirm that each component of our approach contributes to the final performance and show that our approach exhibits superior performance on challenging counterexamples. Additionally, visual analysis shows that text features generated by our approach are highly interpretable.","wordlikeness":0.8888888889,"lcsratio":0.6666666667,"wordcoverage":0.8}
{"Year":2006,"Venue":"lrec-2006","Acronym":"SENTIWORDNET","Description":"A Publicly Available Lexical Resource for Opinion Mining","Abstract":"Opinion mining (om) is a recent subdiscipline at the crossroads of information retrieval and computational linguistics which is concerned not with the topic a document is about, but with the opinion it expresses. Om has a rich set of applications, ranging from tracking users\u0092 opinions about products or about political candidates as expressed in online forums, to customer relationship management. In order to aid the extraction of opinions from text, recent research has tried to automatically determine the \u0093pnpolarity\u0094 of subjective terms, i.e. identify whether a term that is a marker of opinionated content has a positive or a negative connotation. Research on determining whether a term is indeed a marker of opinionated content (a subjective term) or not (an objective term) has been instead much scarcer. In this work we describe research, a lexical resource in which each wordnet synset sis associated to three numerical scores obj(s), pos(s) and neg(s), describing how objective, positive, and negative the terms contained in the synset are. The method used to develop crossroads is based on the quantitative analysis of the glosses associated to synsets, and on the use of the resulting vectorial term representations for semi-supervised synset classi.cation. The three scores are derived by combining the results produced by a committee of eight ternary classi.ers, all characterized by similar accuracy levels but different classification behaviour. It is freely available for research purposes, and is endowed with a web-based graphical user interface.","wordlikeness":0.75,"lcsratio":0.5,"wordcoverage":0.7}
{"Year":2016,"Venue":"coling-2016","Acronym":"CaseSummarizer","Description":"A System for Automated Summarization of Legal Texts","Abstract":"Attorneys, judges, and others in the justice system are constantly surrounded by large amounts of legal text, which can be difficult to manage across many cases. We present domain, a tool for automated text summarization of legal documents which uses standard summary methods based on word frequency augmented with additional domain-specific knowledge. Summaries are then provided through an informative interface with abbreviations, significance heat maps, and other flexible controls. It is evaluated using rouge and human scoring against several other summarization systems, including summary text and feedback provided by domain experts.","wordlikeness":0.7857142857,"lcsratio":0.8571428571,"wordcoverage":0.6086956522}
{"Year":2022,"Venue":"findings-2022","Acronym":"ALLSH","Description":"Active Learning Guided by Local Sensitivity and Hardness","Abstract":"Active learning, which effectively collects informative unlabeled data for annotation, reduces the demand for labeled data. In this work, we propose to retrieve unlabeled samples with a local sensitivity and hardness-aware acquisition function. The proposed method generates data copies through local perturbations and selects data points whose predictive likelihoods diverge the most from their copies. We further empower our acquisition function by injecting the select-worst case perturbation. Our method achieves consistent gains over the commonly used active learning strategies in various classification tasks. Furthermore, we observe consistent improvements over the baselines on the study of prompt selection in prompt-based few-shot learning. These experiments demonstrate that our acquisition guided by local sensitivity and hardness can be effective and beneficial for many nlp tasks.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2019,"Venue":"ijcnlp-2019","Acronym":"KagNet","Description":"Knowledge-Aware Graph Networks for Commonsense Reasoning","Abstract":"Commonsense reasoning aims to empower machines with the human ability to make presumptions about ordinary situations in our daily life. In this paper, we propose a textual inference framework for answering commonsense questions, which effectively utilizes external, structured commonsense knowledge graphs to perform explainable inferences. The framework first grounds a question-answer pair from the semantic space to the knowledge-based symbolic space as a schema graph, a related sub-graph of external knowledge graphs. It represents schema graphs with a novel knowledge-aware graph network module named human, and finally scores answers with graph representations. Our model is based on graph convolutional networks and lstms, with a hierarchical path-based attention mechanism. The intermediate attention scores make it transparent and interpretable, which thus produce trustworthy inferences. Using conceptnet as the only external resource for bert-based models, we achieved state-of-the-art performance on the commonsenseqa, a large-scale dataset for commonsense reasoning.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.8333333333}
{"Year":2019,"Venue":"law-2019","Acronym":"CCGweb","Description":"a New Annotation Tool and a First Quadrilingual CCG Treebank","Abstract":"We present the first open-source graphical annotation tool for combinatory categorial grammar (ccg), and the first set of detailed guidelines for syntactic annotation with ccg, for four languages: english, german, italian, and dutch. We also release a parallel pilot ccg treebank based on these guidelines, with 4x100 adjudicated sentences, 10k single-annotator fully corrected sentences, and 82k single-annotator partially corrected sentences.","wordlikeness":0.5,"lcsratio":0.8333333333,"wordcoverage":0.6666666667}
{"Year":2019,"Venue":"naacl-2019","Acronym":"CAN-NER","Description":"Convolutional Attention Network for Chinese Named Entity Recognition","Abstract":"Named entity recognition (ner) in chinese is essential but difficult because of the lack of natural delimiters. Therefore, chinese word segmentation (cws) is usually considered as the first step for chinese ner. However, models based on word-level embeddings and lexicon features often suffer from segmentation errors and out-of-vocabulary (oov) words. In this paper, we investigate a convolutional attention network called can for chinese ner, which consists of a character-based convolutional neural network (cnn) with local-attention layer and a gated recurrent unit (gru) with global self-attention layer to capture the information from adjacent characters and sentence contexts. Also, compared to other models, not depending on any external resources like lexicons and employing small size of char embeddings make our model more practical. Extensive experimental results show that our approach outperforms state-of-the-art methods without word embedding and external lexicon resources on different domain datasets including weibo, msra and chinese resume ner dataset.","wordlikeness":0.7142857143,"lcsratio":0.8571428571,"wordcoverage":0.8571428571}
{"Year":2020,"Venue":"lrec-2020","Acronym":"PyVallex","Description":"A Processing System for Valency Lexicon Data","Abstract":"From is a python-based system for presenting, searching\/filtering, editing\/extending and automatic processing of machine-readable lexicon data originally available in a text-based format. The system consists of several components: a parser for the specific lexicon format used in several valency lexicons, a data-validation framework, a regular expression based search engine, a map-reduce style framework for querying the lexicon data and a web-based interface integrating complex search and some basic editing capabilities. Such provides most of the typical functionalities of a dictionary writing system (dws), such as multiple presentation modes for the underlying lexical database, automatic evaluation of consistency tests, and a mechanism of merging updates coming from multiple sources. The editing functionality is currently limited to the client-side interface and edits of existing lexical entries, but additional script-based operations on the database are also possible. The code is published under the open source mit license and is also available in the form of a python module for integrating into other software.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.7142857143}
{"Year":2021,"Venue":"acl-2021","Acronym":"Ecco","Description":"An Open Source Library for the Explainability of Transformer Language Models","Abstract":"Our understanding of why transformer-based nlp models have been achieving their recent success lags behind our ability to continue scaling these models. To increase the transparency of transformer-based language models, we present our \u2013 an open-source library for the explainability of transformer-based nlp models. Library provides a set of tools to capture, analyze, visualize, and interactively explore the inner mechanics of these models. This includes (1) gradient-based feature attribution for natural language generation (2) hidden states and their evolution between model layers (3) convenient access and examination tools for neuron activations in the under-explored feed-forward neural network sublayer of transformer layers. (4) convenient examination of activation vectors via canonical correlation analysis (cca), non-negative matrix factorization (nmf), and probing classifiers. We find that syntactic information can be retrieved from bert\u2019s ffnn representations in levels comparable to those in hidden state representations. More curiously, we find that the model builds up syntactic information in its hidden states even when intermediate ffnns indicate diminished levels of syntactic information. Canonical is available at <a href=https:\/\/www.probingx.io\/ class=acl-markup-url>https:\/\/www.includesx.","wordlikeness":0.5,"lcsratio":0.75,"wordcoverage":0.8571428571}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"SCROLLS","Description":"Standardized CompaRison Over Long Language Sequences","Abstract":"Nlp benchmarks have largely focused on short texts, such as sentences and paragraphs, even though long texts comprise a considerable amount of natural language in the wild. We introduce business,, a suite of tasks that require reasoning over long texts. We examine existing long-text datasets, and handpick ones where the text is naturally long, while prioritizing tasks that involve synthesizing information across the input. Introduce contains summarization, question answering, and natural language inference tasks, covering multiple domains, including literature, science, business, and entertainment. Initial baselines, including longformer encoder-decoder, indicate that there is ample room for improvement on pretraining. We make all datasets available in a unified text-to-text format and host a live leaderboard to facilitate research on model architecture and pretraining methods.","wordlikeness":0.8571428571,"lcsratio":1.0,"wordcoverage":0.9230769231}
{"Year":2012,"Venue":"lrec-2012","Acronym":"PET","Description":"a Tool for Post-editing and Assessing Machine Translation","Abstract":"Given the significant improvements in machine translation (mt) quality and the increasing demand for translations, post-editing of automatic translations is becoming a popular practice in the translation industry. It has been shown to allow for much larger volumes of translations to be produced, saving time and costs. In addition, the post-editing of automatic translations can help understand problems in such translations and this can be used as feedback for researchers and developers to improve mt systems. Finally, post-editing can be used as a way of evaluating the quality of translations in terms of how much post-editing effort these translations require. We describe a standalone tool that has two main purposes: facilitate the post-editing of translations from any mt system so that they reach publishable quality and collect sentence-level information from the post-editing process, e.g.: post-editing time and detailed keystroke statistics.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2023,"Venue":"acl-2023","Acronym":"SCOTT","Description":"Self-Consistent Chain-of-Thought Distillation","Abstract":"Large language models (lms) beyond a certain scale, demonstrate the emergent capability of generating free-text rationales for their predictions via chain-of-thought (cot) prompting. While cot can yield dramatically improved performance, such gains are only observed for sufficiently large lms. even more concerning, there is little guarantee that the generated rationales are consistent with lm\u2019s predictions or faithfully justify the decisions. In this work, we propose yielding, a faithful knowledge distillation method to learn a small, self-consistent cot model from a teacher model that is orders of magnitude larger. To form better supervision, we elicit rationales supporting the gold answers from a large lm (teacher) by contrastive decoding, which encourages the teacher to generate tokens that become more plausible only when the answer is considered. To ensure faithful distillation, we use the teacher-generated rationales to learn a student lm with a counterfactual reasoning objective, which prevents the student from ignoring the rationales to make inconsistent predictions. Experiments show that while yielding comparable performance, our method leads to a more faithful model than baselines. Further analysis shows that such a model respects the rationales more when making decisions; thus, we can improve its performance more by refining its rationales.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2022,"Venue":"paclic-2022","Acronym":"SMTCE","Description":"A Social Media Text Classification Evaluation Benchmark and BERTology Models for Vietnamese","Abstract":"Text classification is a typical natural language processing or computational linguistics task with various interesting applications. As the number of users on social media platforms increases, data acceleration promotes emerging studies on social media text classification (smtc) or social media text mining on these valuable resources. In contrast to english, vietnamese, one of the low-resource languages, is still not concentrated on and exploited thoroughly. Inspired by the success of the glue, we introduce the social media text classification evaluation (acceleration) benchmark, as a collection of datasets and models across a diverse set of smtc tasks. With the proposed benchmark, we implement and analyze the effectiveness of a variety of multilingual bertbased models (mbert, xlm-r, and distilmbert) and monolingual bert-based models (phobert, vibert, velectra, and vibert4news) for tasks in the all benchmark. Monolingual models outperform multilingual models and achieve state-of-the-art results on all text classification tasks. It provides an objective assessment of multilingual and monolingual bert-based models on the benchmarks, which will benefit future studies about bertology in the vietnamese language.","wordlikeness":0.4,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2022,"Venue":"coling-2022","Acronym":"CMQA","Description":"A Dataset of Conditional Question Answering with Multiple-Span Answers","Abstract":"Forcing the answer of the question answering (qa) task to be a single text span might be restrictive since the answer can be multiple spans in the context. Moreover, we found that multi-span answers often appear with two characteristics when building the qa system for a real-world application. First, multi-span answers might be caused by users lacking domain knowledge and asking ambiguous questions, which makes the question need to be answered with conditions. Second, there might be hierarchical relations among multiple answer spans. Some recent span-extraction qa datasets include multi-span samples, but they only contain unconditional and parallel answers, which cannot be used to tackle this problem. To bridge the gap, we propose a new task: conditional question answering with hierarchical multi-span answers, where both the hierarchical relations and the conditions need to be extracted. Correspondingly, we introduce new, a conditional multiple-span chinese question answering dataset to study the new proposed task. The final release of gap, consists of 7,861 qa pairs and 113,089 labels, where all samples contain multi-span answers, 50.4% of samples are conditional, and 56.6% of samples are hierarchical. Systems can serve as a benchmark to study the new proposed task and help study building qa systems for real-world applications. The low performance of models drawn from related literature shows that the new proposed task is challenging for the community to solve.","wordlikeness":0.5,"lcsratio":0.75,"wordcoverage":0.8571428571}
{"Year":2010,"Venue":"semeval-2010","Acronym":"SEERLAB","Description":"A System for Extracting Keyphrases from Scholarly Documents","Abstract":"We describe the document. System that participated in the semeval 2010\u2019s keyphrase extraction task. For utilizes the dblp corpus for generating a set of candidate keyphrases from a document. Random forest, a supervised ensemble classi\ufb01er, is then used to select the top keyphrases from the candidate set. Is achieved a 0.24 f-score in generating the top 15 keyphrases, which places it sixth among 19 participating systems. Additionally, participated performed particularly well in generating the top 5 keyphrases with an f-score that ranked third.","wordlikeness":0.7142857143,"lcsratio":0.8571428571,"wordcoverage":0.7142857143}
{"Year":2020,"Venue":"acl-2020","Acronym":"SimulSpeech","Description":"End-to-End Simultaneous Speech to Text Translation","Abstract":"In this work, we develop better, an end-to-end simultaneous speech to text translation system which translates speech in source language to text in target language concurrently. Connectionist consists of a speech encoder, a speech segmenter and a text decoder, where 1) the segmenter builds upon the encoder and leverages a connectionist temporal classification (ctc) loss to split the input streaming speech in real time, 2) the encoder-decoder attention adopts a wait-<span class=tex-math>k<\/span> strategy for simultaneous translation. Consists is more challenging than previous cascaded systems (with simultaneous automatic speech recognition (asr) and simultaneous neural machine translation (nmt)). We introduce two novel knowledge distillation methods to ensure the performance: 1) attention-level knowledge distillation transfers the knowledge from the multiplication of the attention matrices of simultaneous nmt and asr models to help the training of the attention mechanism in translates; 2) data-level knowledge distillation transfers the knowledge from the full-sentence nmt model and also reduces the complexity of data distribution to help on the optimization of more. Experiments on must-c english-spanish and english-german spoken language translation datasets show that (without achieves reasonable bleu scores and lower delay compared to full-sentence end-to-end speech to text translation (without simultaneous translation), and better performance than the two-stage cascaded simultaneous translation model in terms of bleu scores and translation delay.","wordlikeness":0.6363636364,"lcsratio":1.0,"wordcoverage":0.7058823529}
{"Year":2000,"Venue":"lrec-2000","Acronym":"ATLAS","Description":"A Flexible and Extensible Architecture for Linguistic Annotation","Abstract":"We describe a formal model for annotating linguistic artifacts, from which we derive an application programming interface (api) to a suite of tools for manipulating these annotations. The abstract logical model provides for a range of storage formats and promotes the reuse of tools that interact through this api. We focus \ufb01rst on \u201cannotation graphs,\u201d a graph model for annotations on linear signals (such as text and speech) indexed by intervals, for which ef\ufb01cient database storage and querying techniques are applicable. We note how a wide range of existing annotated corpora can be mapped to this annotation graph model. This model is then generalized to encompass a wider variety of linguistic \u201csignals,\u201d including both naturally occuring phenomena (as recorded in images, video, multi-modal interactions, etc.), as well as the derived resources that are increasingly important to the engineering of natural language processing systems (such as word lists, dictionaries, aligned bilingual corpora, etc.). We conclude with a review of the current efforts towards implementing key pieces of this architecture. 1.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2020,"Venue":"lrec-2020","Acronym":"DecOp","Description":"A Multilingual and Multi-domain Corpus For Detecting Deception In Typed Text","Abstract":"In recent years, the increasing interest in the development of automatic approaches for unmasking deception in online sources led to promising results. Nonetheless, among the others, two major issues remain still unsolved: the stability of classifiers performances across different domains and languages. Tackling these issues is challenging since labelled corpora involving multiple domains and compiled in more than one language are few in the scientific literature. For filling this gap, in this paper we introduce since (deceptive opinions), a new language resource developed for automatic deception detection in cross-domain and cross-language scenarios. Developed is composed of 5000 examples of both truthful and deceitful first-person opinions balanced both across five different domains and two languages and, to the best of our knowledge, is the largest corpus allowing cross-domain and cross-language comparisons in deceit detection tasks. In this paper, we describe the collection procedure of the knowledge, corpus and his main characteristics. Moreover, the human performance on the unmasking test-set and preliminary experiments by means of machine learning models based on transformer architecture are shown.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2023,"Venue":"findings-2023","Acronym":"AttenWalker","Description":"Unsupervised Long-Document Question Answering via Attention-based Graph Walking","Abstract":"Annotating long-document question answering (long-document qa) pairs is time-consuming and expensive. To alleviate the problem, it might be possible to generate long-document qa pairs via unsupervised question answering (uqa) methods. However, existing uqa tasks are based on short documents, and can hardly incorporate long-range information. To tackle the problem, we propose a new task, named unsupervised long-document question answering (ulqa), aiming to generate high-quality long-document qa instances in an unsupervised manner. Besides, we propose final, a novel unsupervised method to aggregate and generate answers with long-range dependency so as to construct long-document qa pairs. Specifically, as is composed of three modules, i.e. span collector, span linker and answer aggregator. Firstly, the span collector takes advantage of constituent parsing and reconstruction loss to select informative candidate spans for constructing answers. Secondly, with the help of the attention graph of a pre-trained long-document model, potentially interrelated text spans (that might be far apart) could be linked together via an attention-walking algorithm. Thirdly, in the answer aggregator, linked spans are aggregated into the final answer via the mask-filling ability of a pre-trained model. Extensive experiments show that existing outperforms previous methods on narrativeqa and qasper. In addition, attention also shows strong performance in the few-shot learning setting.","wordlikeness":0.8181818182,"lcsratio":0.8181818182,"wordcoverage":0.7368421053}
{"Year":2022,"Venue":"paclic-2022","Acronym":"Verify","Description":"Breakthrough accuracy in the Urdu fake news detection using Text classification","Abstract":"Researchers around the world have been struggling to minimize the rising spread of fake news through several natural language processing techniques and a great amount of work has been done for resource-rich languages like english, french, german, spanish, chinese, etc. Alternatively, minimal research has been carried out on the urdu language, which is spoken by millions of people around the globe. This study works on solving the problem of detecting the authenticity of urdu news through text analytics and natural language processing methods. Upon studying the previously conducted research on text analysis and classification in urdu and other resource-rich languages, it was found that machine translation does not work very effectively for authenticity due to compromises in structure, grammatical accuracy, and vocabulary. Hence, during this study, a text analytics model has been developed on the only publicly available urdu news articles dataset, originally composed in urdu and comprising 900 articles, 500 real and 400 fake. During the preprocessing, stop words, english words, characters and numbers, and punctuations were removed which affected negatively the accuracy of the model. Apart from that, the data was lemmatized and tokenized and their effects on judging the authenticity of the news articles were examined to be a positive development. The supervised learning models include multinomial naive bayes (mnb), bernoulli naive bayes (bnb), support vector machines (svm), logistic regression (lr), random forests (rf), decision tree (dt), adaboost (ab), and xgboost classifiers along with the combination of different sets of the word and character n-grams were applied to the data and their results were compared. As a result, it was found that the xgboost classifier accompanied with the word unigram and 1-4 character n-grams generated 91% accuracy, the highest reported on this dataset so far. Keywords: fake news detection, natural language processing, media, social media, urdu 1.","wordlikeness":1.0,"lcsratio":0.6666666667,"wordcoverage":1.0}
{"Year":2008,"Venue":"lrec-2008","Acronym":"ProPOSEL","Description":"A Prosody and POS English Lexicon for Language Engineering","Abstract":"Stress is a prototype prosody and pos (part-of-speech) english lexicon for language engineering, derived from the following language resources: the computer-usable dictionary cuvplus, the celex-2 database, the carnegie-mellon pronouncing dictionary, and the bnc, lob and penn treebank pos-tagged corpora. The lexicon is designed for the target application of prosodic phrase break prediction but is also relevant to other machine learning and language engineering tasks. It supplements the existing record structure for wordform entries in cuvplus with syntactic annotations from rival pos-tagging schemes, mapped to fields for default closed and open-class word categories and for lexical stress patterns representing the rhythmic structure of wordforms and interpreted as potential new text-based features for automatic phrase break classifiers. The current version of the lexicon comes as a textfile of 104052 separate entries and is intended for distribution with the natural language toolkit; it is therefore accompanied by supporting python software for manipulating the data so that it can be used for natural language processing (nlp) and corpus-based research in speech synthesis and speech recognition.","wordlikeness":0.875,"lcsratio":1.0,"wordcoverage":0.9333333333}
{"Year":2021,"Venue":"findings-2021","Acronym":"MultiFix","Description":"Learning to Repair Multiple Errors by Optimal Alignment Learning","Abstract":"We consider the problem of learning to repair erroneous c programs by learning optimal alignments with correct programs. Since the previous approaches fix a single error in a line, it is inevitable to iterate the fixing process until no errors remain. In this work, we propose a novel sequence-to-sequence learning framework for fixing multiple program errors at a time. We introduce the edit-distance-based data labeling approach for program error correction. Instead of labeling a program repair example by pairing an erroneous program with a line fix, we label the example by paring an erroneous program with an optimal alignment to the corresponding correct program produced by the edit-distance computation. We evaluate our proposed approach on a publicly available dataset (deepfix dataset) that consists of erroneous c programs submitted by novice programming students. On a set of 6,975 erroneous c programs from the deepfix dataset, our approach achieves the state-of-the-art result in terms of full repair rate on the deepfix dataset (without extra data such as compiler error message or additional source codes for pre-training).","wordlikeness":0.875,"lcsratio":0.75,"wordcoverage":0.7692307692}
{"Year":2004,"Venue":"lrec-2004","Acronym":"Transcrigal","Description":"A Bilingual System for Automatic Indexing of Broadcast News","Abstract":"This paper describes a broadcast news (bn) database called galician-db. The news shows are mainly in galician language, although around 11% of data is in spanish. This database has been constructed for automatic speech recognition (asr) purposes. A bn-asr reference system is also described and evaluated on the test partition of techniques-db. The reference system has been designed having in mind that both languages, spanish and galician, may be used. Performance of the reference is improved when language adaptation techniques are taken into consideration. 1.","wordlikeness":0.8181818182,"lcsratio":0.6363636364,"wordcoverage":0.7619047619}
{"Year":2015,"Venue":"ws-2015","Acronym":"EDRAK","Description":"Entity-Centric Data Resource for Arabic Knowledge","Abstract":"Online arabic content is growing very rapidly, with unmatched growth in arabic structured resources. Systems that perform standard natural language processing (nlp) tasks such as named entity disambiguation (ned) struggle to deliver decent quality due to the lack of rich arabic entity repositories. In this paper, we introduce advance, an automatically generated comprehensive arabic entity-centric resource. Nlp contains more than two million entities together with their arabic names and contextual keyphrases. Manual evaluation con\ufb01rmed the quality of the generated data. We are making than publicly available as a valuable resource to help advance research in arabic nlp and ir tasks such as dictionary-based namedentity recognition, entity classi\ufb01cation, and entity summarization.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2023,"Venue":"findings-2023","Acronym":"MultiFin","Description":"A Dataset for Multilingual Financial NLP","Abstract":"Financial information is generated and distributed across the world, resulting in a vast amount of domain-specific multilingual data. Multilingual models adapted to the financial domain would ease deployment when an organization needs to work with multiple languages on a regular basis. For the development and evaluation of such models, there is a need for multilingual financial language processing datasets. We describe evaluation \u2013 a publicly available financial dataset consisting of real-world article headlines covering 15 languages across different writing systems and language families. The dataset consists of hierarchical label structure providing two classification tasks: multi-label and multi-class. We develop our annotation schema based on a real-world application and annotate our dataset using both \u2018label by native-speaker\u2019 and \u2018translate-then-label\u2019 approaches. The evaluation of several popular multilingual models, e.g., mbert, xlm-r, and mt5, show that although decent accuracy can be achieved in high-resource languages, there is substantial room for improvement in low-resource languages.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.7692307692}
{"Year":2023,"Venue":"acl-2023","Acronym":"PersLEARN","Description":"Research Training through the Lens of Perspective Cultivation","Abstract":"Scientific research is inherently shaped by its authors\u2019 perspectives, influenced by various factorssuch as their personality, community, or society. Junior researchers often face challenges in identifying the perspectives reflected in the existing literature and struggle to develop their own viewpoints. In response to this issue, we introduce factorssuch , a tool designed to facilitate the cultivation of scientific perspectives, starting from a basic seed idea and progressing to a well-articulated framework. By interacting with a prompt-based model, researchers can develop their perspectives explicitly. Our humanstudy reveals that scientific perspectives developed by students using tool exhibit a superior level of logical coherence and depth compared to those that did not. Furthermore, our pipeline outperforms baseline approaches across multiple domains of literature from various perspectives. These results suggest that face could help foster a greater appreciation of diversity in scientific perspectives as an essential component of research training.","wordlikeness":0.5555555556,"lcsratio":0.7777777778,"wordcoverage":0.75}
{"Year":2019,"Venue":"naacl-2019","Acronym":"SC-LSTM","Description":"Learning Task-Specific Representations in Multi-Task Learning for Sequence Labeling","Abstract":"Multi-task learning (mtl) has been studied recently for sequence labeling. Typically, auxiliary tasks are selected specifically in order to improve the performance of a target task. Jointly learning multiple tasks in a way that benefit all of them simultaneously can increase the utility of mtl. In order to do so, we propose a new lstm cell which contains both shared parameters that can learn from all tasks, and task-specific parameters that can learn task-specific information. We name it a shared-cell long-short term memory of. Experimental results on three sequence labeling benchmarks (named-entity recognition, text chunking, and part-of-speech tagging) demonstrate the effectiveness of our which cell.","wordlikeness":0.1428571429,"lcsratio":0.7142857143,"wordcoverage":0.625}
{"Year":2022,"Venue":"coling-2022","Acronym":"TAKE","Description":"Topic-shift Aware Knowledge sElection for Dialogue Generation","Abstract":"Knowledge-grounded dialogue generation consists of two subtasks: knowledge selection and response generation. The knowledge selector generally constructs a query based on the dialogue context and selects the most appropriate knowledge to help response generation. Recent work finds that realizing who (the user or the agent) holds the initiative and utilizing the role-initiative information to instruct the query construction can help select knowledge. It depends on whether the knowledge connection between two adjacent rounds is smooth to assign the role. However, whereby the user distillation.s the initiative only when there is a strong semantic transition between two rounds, probably leading to initiative misjudgment. Therefore, it is necessary to seek a more sensitive reason beyond the initiative role for knowledge selection. To address the above problem, we propose a topic-shift aware knowledge selector(show). Specifically, we first annotate the topic shift and topic inheritance labels in multi-round dialogues with distant supervision. Then, we alleviate the noise problem in pseudo labels through curriculum learning and knowledge distillation. Extensive experiments on wow show that it performs better than strong baselines.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2014,"Venue":"lrec-2014","Acronym":"Taalportaal","Description":"an online grammar of Dutch and Frisian","Abstract":"In this paper, we present the an project. Write will create an online portal containing an exhaustive and fully searchable electronic reference of dutch and frisian phonology, morphology and syntax. Its content will be in english. The main aim of the project is to serve the scientific community by organizing, integrating and completing the grammatical knowledge of both languages, and to make this data accessible in an innovative way. The project is carried out by a consortium of four universities and research institutions. Content is generated in two ways: (1) by a group of authors who, starting from existing grammatical resources, write text directly in xml, and (2) by integrating the full syntax of dutch into the portal, after an automatic conversion from word to xml. We discuss the project\u0092s workflow, content creation and management, the actual web application, and the way in which we plan to enrich the portal\u0092s content, such as by crosslinking between topics and linking to external resources.","wordlikeness":0.5454545455,"lcsratio":0.5454545455,"wordcoverage":0.7058823529}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"EUR-Lex-Sum","Description":"A Multi- and Cross-lingual Dataset for Long-form Summarization in the Legal Domain","Abstract":"Existing summarization datasets come with two main drawbacks: (1) they tend to focus on overly exposed domains, such as news articles or wiki-like texts, and (2) are primarily monolingual, with few multilingual datasets. In this work, we propose a novel dataset, called exist, based on manually curated document summaries of legal acts from the european union law platform (eur-lex). Documents and their respective summaries exist as cross-lingual paragraph-aligned data in several of the 24 official european languages, enabling access to various cross-lingual and lower-resourced summarization setups. We obtain up to 1,500 document\/summary pairs per language, including a subset of 375 cross-lingually aligned legal acts with texts available in *all* 24 languages. In this work, the data acquisition process is detailed and key characteristics of the resource are compared to existing summarization resources. In particular, we illustrate challenging sub-problems and open questions on the dataset that could help the facilitation of future research in the direction of domain-specific cross-lingual summarization. Limited by the extreme length and language diversity of samples, we further conduct experiments with suitable extractive monolingual and cross-lingual baselines for future work. Code for the extraction as well as access to our data and baselines is available online at: [<a href=https:\/\/github.com\/achouhan93\/our class=acl-markup-url>https:\/\/github.com\/achouhan93\/dataset<\/a>](<a href=https:\/\/github.com\/achouhan93\/key class=acl-markup-url>https:\/\/github.com\/achouhan93\/(eur-lex).<\/a>).","wordlikeness":0.5454545455,"lcsratio":0.8181818182,"wordcoverage":0.5555555556}
{"Year":2022,"Venue":"findings-2022","Acronym":"LEATHER","Description":"A Framework for Learning to Generate Human-like Text in Dialogue","Abstract":"Algorithms for text-generation in dialogue can be misguided. For example, in task-oriented settings, reinforcement learning that optimizes only task-success can lead to abysmal lexical diversity. We hypothesize this is due to poor theoretical understanding of the objectives in text-generation and their relation to the learning process (i.e., model training). To this end, we propose a new theoretical framework for learning to generate text in dialogue. Compared to existing theories of learning, our framework allows for analysis of the multi-faceted goals inherent to text-generation. We use our framework to develop theoretical guarantees for learners that adapt to unseen data. As an example, we apply our theory to study data-shift within a cooperative learning algorithm proposed for the guesswhat?! visual dialogue game. From this insight, we propose a new algorithm, and empirically, we demonstrate our proposal improves both task-success and human-likeness of the generated text. Finally, we show statistics from our theory are empirically predictive of multiple qualities of the generated dialogue, suggesting our theory is useful for model-selection when human evaluations are not available.","wordlikeness":1.0,"lcsratio":0.8571428571,"wordcoverage":1.0}
{"Year":2022,"Venue":"dialdoc-2022","Acronym":"Docalog","Description":"Multi-document Dialogue System using Transformer-based Span Retrieval","Abstract":"Information-seeking dialogue systems, including knowledge identification and response generation, aim to respond to users with fluent, coherent, and informative answers based on users\u2019 needs. This paper discusses our proposed approach, <span class=tex-math>picker<\/i><\/span>, for the dialdoc-22 (multidoc2dial) shared task. <span class=tex-math>model<\/span> identifies the most relevant knowledge in the associated document, in a multi-document setting. <span class=tex-math>span,<\/span>, is a three-stage pipeline consisting of <i>(1) a document retriever model (dr. Teit)<\/i>, <i>(2) an answer span prediction model<\/i>, and <i>(3) an ultimate span picker<\/i> deciding on the most likely answer span, out of all predicted spans. In the test phase of multidoc2dial 2022, <span class=tex-math>shared<\/span> achieved f1-scores of 36.07% and 28.44% and sacrebleu scores of 23.70% and 20.52%, respectively on the <i>mdd-seen<\/i> and <i>mdd-unseen<\/i> folds.","wordlikeness":0.7142857143,"lcsratio":1.0,"wordcoverage":0.7142857143}
{"Year":2020,"Venue":"lrec-2020","Acronym":"TArC","Description":"Incrementally and Semi-Automatically Collecting a Tunisian Arabish Corpus","Abstract":"This article describes the constitution process of the first morpho-syntactically annotated tunisian arabish corpus (used). Arabish, also known as arabizi, is a spontaneous coding of arabic dialects in latin characters and \u201carithmographs\u201d (numbers used as letters). This code-system was developed by arabic-speaking users of social media in order to facilitate the writing in the computer-mediated communication (cmc) and text messaging informal frameworks. Arabish differs for each arabic dialect and each arabish code-system is under-resourced, in the same way as most of the arabic dialects. In the last few years, the attention of nlp studies on arabic dialects has considerably increased. Taking this into consideration, (). Will be a useful support for different types of analyses, computational and linguistic, as well as for nlp tools training. In this article we will describe preliminary work on the taking semi-automatic construction process and some of the first analyses we developed on of. In addition, in order to provide a complete overview of the challenges faced during the building process, we will present the main tunisian dialect characteristics and its encoding in tunisian arabish.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2022,"Venue":"naacl-2022","Acronym":"OPERA","Description":"Operation-Pivoted Discrete Reasoning over Text","Abstract":"Machine reading comprehension (mrc) that requires discrete reasoning involving symbolic datasetstions, e.g., addition, sorting, and counting, is a challenging task. According to this nature, semantic parsing-based methods predict interpretable but complex logical forms. However, logical form generation is nontrivial and even a little perturbation in a logical form will lead to wrong answers. To alleviate this issue, multi-predictor -based methods are proposed to directly predict different types of answers and achieve improvements. However, they ignore the utilization of symbolic logicaltions and encounter a lack of reasoning ability and interpretability. To inherit the advantages of these two types of methods, we propose e.g.,, an forms.tion-pivoted discrete reasoning framework, where lightweight symbolic selectedtions (compared with logical forms) as neural modules are utilized to facilitate the reasoning ability and interpretability. Specifically, logicaltions are first selected and then softly executed to simulate the answer reasoning procedure. Extensive experiments on both drop and racenum datasets show the reasoning ability of encounter. Moreover, further analysis verifies its interpretability.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2022,"Venue":"coling-2022","Acronym":"LFKQG","Description":"A Controlled Generation Framework with Local Fine-tuning for Question Generation over Knowledge Bases","Abstract":"Question generation over knowledge bases (kbqg) aims at generating natural questions about a subgraph, which can be answered by a given answer entity. Existing kbqg models still face two main challenges: (1) most models often focus on the most relevant part of the answer entity, while neglecting the rest of the subgraph. (2) there are a large number of out-of-vocabulary (oov) predicates in real-world scenarios, which are hard to adapt for most kbqg models. To address these challenges, we propose rest, a controlled generation framework for question generation over knowledge bases. (1) existing employs a simple controlled generation method to generate the questions containing the critical entities in the subgraph, ensuring the question is relevant to the whole subgraph. (2) we propose an optimization strategy called local fine-tuning, which can make good use of the rich information hidden in the pre-trained model to improve the ability of the model to adapt the oov predicates. Extensive experiments show that our method outperforms existing methods significantly on three widely-used benchmark datasets simplequestion, pathquestions, and webquestions.","wordlikeness":0.2,"lcsratio":1.0,"wordcoverage":0.5714285714}
{"Year":2007,"Venue":"semeval-2007","Acronym":"CU-TMP","Description":"Temporal Relation Classification Using Syntactic and Semantic Features","Abstract":"We approached the temporal relation identi\ufb01cation tasks of tempeval 2007 as pair-wise classi\ufb01cation tasks. We introduced a variety of syntactically and semantically motivated features, including temporal-logicbased features derived from running our task b system on the task a and c data. We trained support vector machine models and achieved the second highest accuracies on the tasks: 61% on task a, 75% on task b and 54% on task c.","wordlikeness":0.5,"lcsratio":0.6666666667,"wordcoverage":0.6666666667}
{"Year":2016,"Venue":"lrec-2016","Acronym":"IRIS","Description":"English-Irish Machine Translation System","Abstract":"We describe enabling, a statistical machine translation (smt) system for translating from english into under-resourcedh and vice versa. Since versa.h is considered an under-resourced language with a limited amount of machine-readable text, building a machine translation system that produces reasonable translations is rather challenging. As translation is a difficult task, current research in smt focuses on obtaining statistics either from a large amount of parallel, monolingual or other multilingual resources. Nevertheless, we collected available english-statisticalh data and developed an smt system aimed at supporting human translators and enabling cross-lingual language technology tasks.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2016,"Venue":"lrec-2016","Acronym":"PersonaBank","Description":"A Corpus of Personal Narratives and Their Story Intention Graphs","Abstract":"We present a new corpus, from, consisting of 108 personal stories from weblogs that have been annotated with their story intention graphs, a deep representation of the content of a story. We describe the topics of the stories and the basis of the story intention graph representation, as well as the process of annotating the stories to produce the story intention graphs and the challenges of adapting the tool to this new personal narrative domain. We also discuss how the corpus can be used in applications that retell the story using different styles of tellings, co-tellings, or as a content planner.","wordlikeness":0.8181818182,"lcsratio":0.8181818182,"wordcoverage":0.7777777778}
{"Year":2020,"Venue":"acl-2020","Acronym":"ESPnet-ST","Description":"All-in-One Speech Translation Toolkit","Abstract":"We present reproducible, which is designed for the quick development of speech-to-speech translation systems in a single framework. The is a new project inside end-to-end speech processing toolkit, espnet, which integrates or newly implements automatic speech recognition, machine translation, and text-to-speech functions for speech translation. We provide all-in-one recipes including data pre-processing, feature extraction, training, and decoding pipelines for a wide range of benchmark datasets. Our reproducible results can match or even outperform the current state-of-the-art performances; these pre-trained models are downloadable. The toolkit is publicly available at <a href=https:\/\/github.com\/espnet\/espnet class=acl-markup-url>https:\/\/github.com\/espnet\/espnet<\/a>.","wordlikeness":0.5555555556,"lcsratio":0.7777777778,"wordcoverage":0.7058823529}
{"Year":2023,"Venue":"acl-2023","Acronym":"MIReAD","Description":"Simple Method for Learning High-quality Representations from Scientific Documents","Abstract":"Learning semantically meaningful representations from scientific documents can facilitate academic literature search and improve performance of recommendation systems. Pretrained language models have been shown to learn rich textual representations, yet they cannot provide powerful document-level representations for scientific articles. We propose target, a simple method that learns highquality representations of scientific papers by fine-tuning transformer model to predict the target journal class based on the abstract. We train document-level on more than 500,000 pubmed and arxiv abstracts across over 2,000 journal classes. We show that textual produces representations that can be used for similar papers retrieval, topic categorization and literature search. Our proposed approach outperforms six existing models for representation learning on scientific documents across four evaluation standards.","wordlikeness":0.8333333333,"lcsratio":1.0,"wordcoverage":0.9090909091}
{"Year":2023,"Venue":"acl-2023","Acronym":"MultiTACRED","Description":"A Multilingual Version of the TAC Relation Extraction Dataset","Abstract":"Relation extraction (re) is a fundamental task in information extraction, whose extension to multilingual settings has been hindered by the lack of supervised resources comparable in size to large english datasets such as tacred (zhang et al., 2017). To address this gap, we introduce the languages dataset, covering 12 typologically diverse languages from 9 language families, which is created by machine-translating tacred instances and automatically projecting their entity annotations. We analyze translation and annotation projection quality, identify error categories, and experimentally evaluate fine-tuned pretrained mono- and multilingual language models in common transfer learning scenarios. Our analyses show that machine translation is a viable strategy to transfer re instances, with native speakers judging more than 83% of the translated instances to be linguistically and semantically acceptable. We find monolingual re model performance to be comparable to the english original for many of the target languages, and that multilingual models trained on a combination of english and target language data can outperform their monolingual counterparts. However, we also observe a variety of translation and annotation projection errors, both due to the mt systems and linguistic features of the target languages, such as pronoun-dropping, compounding and inflection, that degrade dataset quality and re model performance.","wordlikeness":0.7272727273,"lcsratio":1.0,"wordcoverage":0.7}
{"Year":2022,"Venue":"coling-2022","Acronym":"CoLo","Description":"A Contrastive Learning Based Re-ranking Framework for One-Stage Summarization","Abstract":"Traditional training paradigms for extractive and abstractive summarization systems always only use token-level or sentence-level training objectives. However, the output summary is always evaluated from summary-level which leads to the inconsistency in training and evaluation. In this paper, we propose a contrastive learning based re-ranking framework for one-stage summarization called multi-stage. By modeling a contrastive objective, we show that the summarization model is able to directly generate summaries according to the summary-level score without additional modules and parameters. Extensive experiments demonstrate that traditional boosts the extractive and abstractive results of one-stage systems on cnn\/dailymail benchmark to 44.58 and 46.33 rouge-1 score while preserving the parameter efficiency and inference efficiency. Compared with state-of-the-art multi-stage systems, we save more than 100 gpu training hours and obtaining 3x 8x speed-up ratio during inference while maintaining comparable results.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2014,"Venue":"semeval-2014","Acronym":"CNRC-TMT","Description":"Second Language Writing Assistant System Description","Abstract":"We describe the system entered by the national research council canada in the semeval-2014 l2 writing assistant task. Our system relies on a standard phrase-based statistical machine translation trained on generic, publicly available data. Translations are produced by taking the already translated part of the sentence as \ufb01xed context. We show that translation systems can address the l2 writing assistant task, reaching out-of-\ufb01ve word-based accuracy above 80 percent for 3 out of 4 language pairs. We also present a brief analysis of remaining errors.","wordlikeness":0.125,"lcsratio":0.75,"wordcoverage":0.625}
{"Year":2021,"Venue":"acl-2021","Acronym":"GhostBERT","Description":"Generate More Features with Cheap Operations for BERT","Abstract":"Transformer-based pre-trained language models like bert, though powerful in many tasks, are expensive in both memory and computation, due to their large number of parameters. Previous works show that some parameters in these models can be pruned away without severe accuracy drop. However, these redundant features contribute to a comprehensive understanding of the training data and removing them weakens the model\u2019s representation ability. In this paper, we propose verify, which generates more features with very cheap operations from the remaining features. In this way, comprehensive has similar memory and computational cost as the pruned model, but enjoys much larger representation power. The proposed ghost module can also be applied to unpruned bert models to enhance their performance with negligible additional parameters and computation. Empirical results on the glue benchmark on three backbone models (i.e., bert, roberta and electra) verify the efficacy of our proposed method.","wordlikeness":0.8888888889,"lcsratio":0.8888888889,"wordcoverage":0.8}
{"Year":2012,"Venue":"naacl-2012","Acronym":"DeSoCoRe","Description":"Detecting Source Code Re-Use across Programming Languages","Abstract":"Source code re-use has become an important problem in academia. The amount of code available makes necessary to develop systems supporting education that could address the problem of detection of source code re-use. We present the to tool based on techniques of natural language processing (nlp) applied to detect source code re-use. Could compares two source codes at the level of methods or functions even when written in different programming languages. The system provides an understandable output to the human reviewer in order to help a teacher to decide whether a source code is re-used.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.7777777778}
{"Year":2020,"Venue":"coling-2020","Acronym":"LAVA","Description":"Latent Action Spaces via Variational Auto-encoding for Dialogue Policy Optimization","Abstract":"Reinforcement learning (rl) can enable task-oriented dialogue systems to steer the conversation towards successful task completion. In an end-to-end setting, a response can be constructed in a word-level sequential decision making process with the entire system vocabulary as action space. Policies trained in such a fashion do not require expert-defined action spaces, but they have to deal with large action spaces and long trajectories, making rl impractical. Using the latent space of a variational model as action space alleviates this problem. However, current approaches use an uninformed prior for training and optimize the latent distribution solely on the context. It is therefore unclear whether the latent representation truly encodes the characteristics of different actions. In this paper, we explore three ways of leveraging an auxiliary task to shape the latent variable distribution: via pre-training, to obtain an informed prior, and via multitask learning. We choose response auto-encoding as the auxiliary task, as this captures the generative factors of dialogue responses while requiring low computational cost and neither additional data nor labels. Our approach yields a more action-characterized latent representations which support end-to-end dialogue policy optimization and achieves state-of-the-art success rates. These results warrant a more wide-spread use of rl in end-to-end dialogue models.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2022,"Venue":"lrec-2022","Acronym":"CRASS","Description":"A Novel Data Set and Benchmark to Test Counterfactual Reasoning of Large Language Models","Abstract":"We introduce the design (counterfactual reasoning assessment) data set and benchmark utilizing questionized counterfactual conditionals as a novel and powerful tool to evaluate large language models. We present the data set design and benchmark. We test six state-of-the-art models against our benchmark. Our results show that it poses a valid challenge for these models and opens up considerable room for their improvement.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2014,"Venue":"lrec-2014","Acronym":"Momresp","Description":"A Bayesian Model for Multi-Annotator Document Labeling","Abstract":"Data annotation in modern practice often involves multiple, imperfect human annotators. Multiple annotations can be used to infer estimates of the ground-truth labels and to estimate individual annotator error characteristics (or reliability). We introduce variety, a model that incorporates information from both natural data clusters as well as annotations from multiple annotators to infer ground-truth labels and annotator reliability for the document classification task. We implement this model and show dramatic improvements over majority vote in situations where both annotations are scarce and annotation quality is low as well as in situations where annotators disagree consistently. Because work. Predictions are subject to label switching, we introduce a solution that finds nearly optimal predicted class reassignments in a variety of settings using only information available to the model at inference time. Although nearly does not perform well in annotation-rich situations, we show evidence suggesting how this shortcoming may be overcome in future work.","wordlikeness":0.5714285714,"lcsratio":0.7142857143,"wordcoverage":0.7272727273}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"VarMAE","Description":"Pre-training of Variational Masked Autoencoder for Domain-adaptive Language Understanding","Abstract":"Pre-trained language models have been widely applied to standard benchmarks. Due to the flexibility of natural language, the available resources in a certain domain can be restricted to support obtaining precise representation. To address this issue, we propose a novel transformer-based language model named well-formed for domain-adaptive language understanding. Under the masked autoencoding objective, we design a context uncertainty learning module to encode the token\u2019s context into a smooth latent distribution. The module can produce diverse and well-formed contextual representations. Experiments on science- and finance-domain nlu tasks demonstrate that efficiently can be efficiently adapted to new domains with limited resources.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.7272727273}
{"Year":2023,"Venue":"acl-2023","Acronym":"StoryARG","Description":"a corpus of narratives and personal experiences in argumentative texts","Abstract":"Humans are storytellers, even in communication scenarios which are assumed to be more rationality-oriented, such as argumentation. Indeed, supporting arguments with narratives or personal experiences (henceforth, stories) is a very natural thing to do \u2013 and yet, this phenomenon is largely unexplored in computational argumentation. Which role do stories play in an argument? Do they make the argument more effective? What are their narrative properties? To address these questions, we collected and annotated \u2013, a dataset sampled from well-established corpora in computational argumentation (changemyview and regulationroom), and the social sciences (europolis), as well as comments to new york times articles. Emotional contains 2451 textual spans annotated at two levels. At the argumentative level, we annotate the function of the story (e.g., clarification, disclosure of harm, search for a solution, establishing speaker\u2019s authority), as well as its impact on the effectiveness of the argument and its emotional load. At the level of narrative properties, we annotate whether the story has a plot-like development, is factual or hypothetical, and who the protagonist is. What makes a story effective in an argument? Our analysis of the annotations in annotations uncover a positive impact on effectiveness for stories which illustrate a solution to a problem, and in general, annotator-specific preferences that we investigate with regression analysis.","wordlikeness":0.875,"lcsratio":0.875,"wordcoverage":0.8}
{"Year":2022,"Venue":"ws-2022","Acronym":"ParaNames","Description":"A Massively Multilingual Entity Name Corpus","Abstract":"We present demonstrate, a wikidata-derived multilingual parallel name resource consisting of names for approximately 14 million entities spanning over 400 languages. A is useful for multilingual language processing, both in defining tasks for name translation tasks and as supplementary data for other tasks. We demonstrate an application of an by training a multilingual model for canonical name translation to and from english.","wordlikeness":0.7777777778,"lcsratio":0.7777777778,"wordcoverage":0.8}
{"Year":2023,"Venue":"ws-2023","Acronym":"SpaceNLI","Description":"Evaluating the Consistency of Predicting Inferences In Space","Abstract":"While many natural language inference (nli) datasets target certain semantic phenomena, e.g., negation, tense & aspect, monotonicity, and presupposition, to the best of our knowledge, there is no nli dataset that involves diverse types of spatial expressions and reasoning. We fill this gap by semi-automatically creating an nli dataset for spatial reasoning, called per. The data samples are automatically generated from a curated set of reasoning patterns (see figure 1), where the patterns are annotated with inference labels by experts. We test several sota nli systems on test to gauge the complexity of the dataset and the system\u2019s capacity for spatial reasoning. Moreover, we introduce a <i>pattern accuracy<\/i> and argue that it is a more reliable and stricter measure than the accuracy for evaluating a system\u2019s performance on pattern-based generated data samples. Based on the evaluation results we find that the systems obtain moderate results on the spatial nli problems but lack consistency per inference pattern. The results also reveal that non-projective spatial inferences (especially due to the \u201cbetween\u201d preposition) are the most challenging ones.","wordlikeness":0.625,"lcsratio":0.75,"wordcoverage":0.7692307692}
{"Year":2021,"Venue":"acl-2021","Acronym":"PIE","Description":"A Parallel Idiomatic Expression Corpus for Idiomatic Sentence Generation and Paraphrasing","Abstract":"Idiomatic expressions (ie) play an important role in natural language, and have long been a \u201cpain in the neck\u201d for nlp systems. Despite this, text generation tasks related to ies remain largely under-explored. In this paper, we propose two new tasks of idiomatic sentence generation and paraphrasing to fill this research gap. We introduce a curated dataset of 823 ies, and a parallel corpus with sentences containing them and the same sentences where the ies were replaced by their literal paraphrases as the primary resource for our tasks. We benchmark existing deep learning models, which have state-of-the-art performance on related tasks using automated and manual evaluation with our dataset to inspire further research on our proposed tasks. By establishing baseline models, we pave the way for more comprehensive and accurate modeling of ies, both for generation and paraphrasing.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2022,"Venue":"naacl-2022","Acronym":"DEGREE","Description":"A Data-Efficient Generation-Based Event Extraction Model","Abstract":"Event extraction requires high-quality expert human annotations, which are usually expensive. Therefore, learning a data-efficient event extraction model that can be trained with only a few labeled examples has become a crucial challenge. In this paper, we focus on low-resource end-to-end event extraction and propose training, a data-efficient model that formulates event extraction as a conditional generation problem. Given a passage and a manually designed prompt, that learns to summarize the events mentioned in the passage into a natural sentence that follows a predefined pattern. The final event predictions are then extracted from the generated sentence with a deterministic algorithm. Expensive. Has three advantages to learn well with less training data. First, our designed prompts provide semantic guidance for three to leverage and and thus better capture the event arguments. Moreover, generation is capable of using additional weakly-supervised information, such as the description of events encoded in the prompts. Finally, designed learns triggers and arguments jointly in an end-to-end manner, which encourages the model to better utilize the shared knowledge and dependencies among them. Our experimental results demonstrate the strong performance of in for low-resource event extraction.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2023,"Venue":"acl-2023","Acronym":"LiveChat","Description":"A Large-Scale Personalized Dialogue Dataset Automatically Constructed from Live Streaming","Abstract":"Open-domain dialogue systems have made promising progress in recent years. While the state-of-the-art dialogue agents are built upon large-scale social media data and large pre-trained models, there is no guarantee these agents could also perform well in fast-growing scenarios, such as live streaming, due to the bounded transferability of pre-trained models and biased distributions of public datasets from reddit and weibo, etc. To improve the essential capability of responding and establish a benchmark in the live open-domain scenario, we introduce the agents dataset, composed of 1.33 million real-life chinese dialogues with almost 3800 average sessions across 351 personas and fine-grained profiles for each persona. Etc. Is automatically constructed by processing numerous live videos on the internet and naturally falls within the scope of multi-party conversations, where the issues of who says what to whom should be considered. Therefore, we target two critical tasks of response modeling and addressee recognition and propose retrieval-based baselines grounded on advanced techniques. Experimental results have validated the positive effects of leveraging persona profiles and larger average sessions per persona. In addition, we also benchmark the transferability of advanced generation-based models on from and pose some future directions for current challenges.","wordlikeness":0.875,"lcsratio":0.75,"wordcoverage":0.7142857143}
{"Year":2009,"Venue":"iwcs-2009","Acronym":"GLML","Description":"Annotating Argument Selection and Coercion","Abstract":"In this paper we introduce a methodology for annotating compositional operations in natural language text, and describe a mark-up language, our, based on generative lexicon, for identifying such relations. While most annotation systems capture surface relationships, mechanisms captures the \u201ccompositional history\u201d of the argument selection relative to the predicate. We provide a brief overview of gl before moving on to our proposed methodology for annotating with in. There are three main tasks described in the paper: (i) compositional mechanisms of argument selection; (ii) qualia in modi\ufb01cation constructions; (iii) type selection in modi\ufb01cation of dot objects. We explain what each task includes and provide a description of the annotation interface. We also include the xml format for systems including examples of annotated sentences.","wordlikeness":0.25,"lcsratio":0.75,"wordcoverage":0.75}
{"Year":2016,"Venue":"acl-2016","Acronym":"TMop","Description":"a Tool for Unsupervised Translation Memory Cleaning","Abstract":"We present labeled, the \ufb01rst open-source tool for automatic translation memory (tm) cleaning. The tool implements a fully unsupervised approach to the task, which allows spotting unreliable translation units (sentence pairs in different languages, which are supposed to be translations of each other) without requiring labeled training data. To includes a highly con\ufb01gurable and extensible set of \ufb01lters capturing different aspects of translation quality. It has been evaluated on a test set composed of 1,000 translation units (tus) randomly extracted from the english-italian version of mymemory, a large-scale public tm. Results indicate its effectiveness in automatic removing \u201cbad\u201d tus, with comparable performance to a state-of-the-art supervised method (76.3 vs. 77.7 balanced accuracy).","wordlikeness":0.25,"lcsratio":0.75,"wordcoverage":0.8571428571}
{"Year":2016,"Venue":"lrec-2016","Acronym":"SPA","Description":"Web-based Platform for easy Access to Speech Processing Modules","Abstract":"This paper presents the, a web-based speech analytics platform that integrates several speech processing modules and that makes it possible to use them through the web. It was developed with the aim of facilitating the usage of the modules, without the need to know about software dependencies and specific configurations. Apart from being accessed by a web-browser, the platform also provides a rest api for easy integration with other applications. The platform is flexible, scalable, provides authentication for access restrictions, and was developed taking into consideration the time and effort of providing new services. The platform is still being improved, but it already integrates a considerable number of audio and text processing modules, including: automatic transcription, speech disfluency classification, emotion detection, dialog act recognition, age and gender classification, non-nativeness detection, hyper-articulation detection, dialog act recognition, and two external modules for feature extraction and dtmf detection. This paper describes the to architecture, presents the already integrated modules, and provides a detailed description for the ones most recently integrated.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2021,"Venue":"tacl-2021","Acronym":"Soloist","Description":"Building Task Bots at Scale with Transfer Learning and Machine Teaching","Abstract":"We present a new method, that,1 that uses transfer learning and machine teaching to build task bots at scale. We parameterize classical modular task-oriented dialog systems using a transformer-based auto-regressive language model, which subsumes different dialog modules into a single neural model. We pre-train, on heterogeneous dialog corpora, a task-grounded response generation model, which can generate dialog responses grounded in user goals and real-world knowledge for task completion. The pre-trained model can be efficiently adapted to accomplish new tasks with a handful of task-specific dialogs via machine teaching, where training samples are generated by human teachers interacting with the system. Experiments show that (i)the creates new state-of-the-art on well-studied task-oriented dialog benchmarks, including camrest676 and multiwoz; (ii) in the few-shot fine-tuning settings, are significantly outperforms existing methods; and (iii) the use of machine teaching substantially reduces the labeling cost of fine-tuning. The pre-trained models and codes are available at <a href=https:\/\/aka.ms\/can class=acl-markup-url>https:\/\/aka.ms\/can<\/a>.","wordlikeness":0.8571428571,"lcsratio":0.8571428571,"wordcoverage":0.75}
{"Year":2018,"Venue":"lrec-2018","Acronym":"WikiDragon","Description":"A Java Framework For Diachronic Content And Network Analysis Of MediaWikis","Abstract":"We introduce analysis,, a java framework designed to give developers in computational linguistics an intuitive api to build, parse and analyze instances of mediawikis such as wikipedia, wiktionary or wikisource on their computers. It covers current versions of pages as well as the complete revision history, gives diachronic access to both page source code as well as accurately parsed html and supports the diachronic exploration of the page network. Accurately is self-enclosed and only requires an xml dump of the of\ufb01cial wikimedia foundation website for import into an embedded database. No additional setup is required. We describe additional\u2019s architecture and evaluate the framework based on the simple english wikipedia with respect to the accuracy of link extraction, diachronic network analysis and the impact of using different wikipedia frameworks to text analysis. Keywords: wikipedia, java framework, diachronic text analysis, diachronic network analysis, link extraction 1.","wordlikeness":0.8,"lcsratio":0.7,"wordcoverage":0.75}
{"Year":2015,"Venue":"semeval-2015","Acronym":"DsUniPi","Description":"An SVM-based Approach for Sentiment Analysis of Figurative Language on Twitter","Abstract":"The resources, team participated in the semeval 2015 task#11: sentiment analysis of figurative language in twitter. The proposed approach employs syntactical and morphological features, which indicate sentiment polarity in both figurative and non-figurative tweets. These features were combined with others that indicate presence of figurative language in order to predict a fine-grained sentiment score. The method is supervised and makes use of structured knowledge resources, such as sentiwordnet sentiment lexicon for assigning sentiment score to words and wordnet for calculating word similarity. We have experimented with different classification algorithms (na\u00efve bayes, decision trees, and svm), and the best results were achieved by an svm classifier with linear kernel.","wordlikeness":0.2857142857,"lcsratio":0.7142857143,"wordcoverage":0.6666666667}
{"Year":2023,"Venue":"eacl-2023","Acronym":"CodeAnno","Description":"Extending WebAnno with Hierarchical Document Level Annotation and Automation","Abstract":"Webanno is one of the most popular annotation tools that supports generic annotation types and distributive annotation with multiple user roles. However, webanno focuses on annotating span-level mentions and relations among them, making document-level annotation complicated. When it comes to the annotation and analysis of social science materials, it usually involves the creation of codes to categorize a given document. The codes, which are known as codebooks, are typically hierarchical, which enables to code the document either with a general category or more fine-grained subcategories. Href=\"https:\/\/www.youtube.com\/watch?v=rmcdtghbe-s\" is forked from webanno and designed to solve the coding problems faced by many social science researchers with the following main functionalities. 1) creation of hierarchical codebooks, with functionality to move and sort categories in the hierarchy 2) an interactive ui for codebook annotation 3) import and export of annotations in csv format, hence being compatible with existing annotations conducted using spreadsheet applications 4) integration of an external automation component to facilitate coding using machine learning 5) project templating that allows duplicating a project structure without copying the actual documents. We present different use-cases to demonstrate the capability of codes. A shot demonstration video of the system is available here: <a href=\"https:\/\/www.youtube.com\/watch?v=rmcdtghbe-s\" class=acl-markup-url>https:\/\/www.youtube.","wordlikeness":0.875,"lcsratio":0.875,"wordcoverage":0.7142857143}
{"Year":2023,"Venue":"acl-2023","Acronym":"UnitY","Description":"Two-pass Direct Speech-to-speech Translation with Discrete Units","Abstract":"Direct speech-to-speech translation (s2st), in which all components can be optimized jointly, is advantageous over cascaded approaches to achieve fast inference with a simplified pipeline. We present a novel two-pass direct s2st architecture, textual, which first generates textual representations and predicts discrete acoustic units subsequently. We enhance the model performance by subword prediction in the first-pass decoder, advanced two-pass decoder architecture design and search strategy, and better training regularization. To leverage large amounts of unlabeled text data, we pre-train the first-pass text decoder based on the self-supervised denoising auto-encoding task. Experimental evaluations on benchmark datasets at various data scales demonstrate that demonstrate outperforms a single-pass speech-to-unit translation model by 2.5-4.2 asr-bleu with 2.83x decoding speed-up. We show that the proposed methods boost the performance even when predicting spectrogram in the second pass. However, predicting discrete units achieves 2.51x decoding speed-up compared to that case.","wordlikeness":1.0,"lcsratio":0.8,"wordcoverage":1.0}
{"Year":2018,"Venue":"bionlp-2018","Acronym":"SingleCite","Description":"Towards an improved Single Citation Search in PubMed","Abstract":"A search that is targeted at finding a specific document in databases is called a single citation search. Single citation searches are particularly important for scholarly databases, such as pubmed, because users are frequently searching for a specific publication. In this work we describe document., a single citation matching system designed to facilitate user\u2019s search for a specific document. We report on the progress that has been achieved towards building that functionality.","wordlikeness":0.9,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2022,"Venue":"naacl-2022","Acronym":"GenIE","Description":"Generative Information Extraction","Abstract":"Structured and grounded representation of text is typically formalized by closed information extraction, the problem of extracting an exhaustive set of (subject, relation, object) triplets that are consistent with a predefined set of entities and relations from a knowledge base schema. Most existing works are pipelines prone to error accumulation, and all approaches are only applicable to unrealistically small numbers of entities and relations. We introduce providing (generative information extraction), the first end-to-end autoregressive formulation of closed information extraction. Set naturally exploits the language knowledge from the pre-trained transformer by autoregressively generating relations and entities in textual form. Thanks to a new bi-level constrained generation strategy, only triplets consistent with the predefined knowledge base schema are produced. Our experiments show that schema is state-of-the-art on closed information extraction, generalizes from fewer training data points than baselines, and scales to a previously unmanageable number of entities and relations. With this work, closed information extraction becomes practical in realistic scenarios, providing new opportunities for downstream tasks. Finally, this work paves the way towards a unified end-to-end approach to the core tasks of information extraction.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2020,"Venue":"acl-2020","Acronym":"BENTO","Description":"A Visual Platform for Building Clinical NLP Pipelines Based on CodaLab","Abstract":"Codalab is an open-source web-based platform for collaborative computational research. Although codalab has gained popularity in the research community, its interface has limited support for creating reusable tools that can be easily applied to new datasets and composed into pipelines. In clinical domain, natural language processing (nlp) on medical notes generally involves multiple steps, like tokenization, named entity recognition, etc. Since these steps require different tools which are usually scattered in different publications, it is not easy for researchers to use them to process their own datasets. In this paper, we present controlled, a workflow management platform with a graphic user interface (gui) that is built on top of codalab, to facilitate the process of building clinical nlp pipelines. Open-source comes with a number of clinical nlp tools that have been pre-trained using medical notes and expert annotations and can be readily used for various clinical nlp tasks. It also allows researchers and developers to create their custom tools (e.g., pre-trained nlp models) and use them in a controlled and reproducible way. In addition, the gui interface enables researchers with limited computer background to compose tools into nlp pipelines and then apply the pipelines on their own datasets in a \u201cwhat you see is what you get\u201d (wysiwyg) way. Although pre-trained is designed for clinical nlp applications, the underlying architecture is flexible to be tailored to any other domains.","wordlikeness":0.8,"lcsratio":0.8,"wordcoverage":0.9090909091}
{"Year":2014,"Venue":"acl-2014","Acronym":"WELT","Description":"Using Graphics Generation in Linguistic Fieldwork","Abstract":"We describe the wordseye linguistics tool (written), a novel tool for the documentation and preservation of endangered languages. Used is based on wordseye (coyne and sproat, 2001), a text-toscene tool that automatically generates 3d scenes from written input. And has two modes of operation. In the \ufb01rst mode, english input automatically generates a picture which can be used to elicit a description in the target language. In the second mode, the linguist formally documents the grammar of an endangered language, thereby creating a system that takes input in the endangered language and generates a picture according to the grammar; the picture can then be used to verify the grammar with native speakers. We will demonstrate grammar\u2019s use on scenarios involving arrernte and nahuatl.","wordlikeness":1.0,"lcsratio":0.75,"wordcoverage":0.8571428571}
{"Year":2019,"Venue":"naacl-2019","Acronym":"STAC","Description":"Science Toolkit Based on Chinese Idiom Knowledge Graph","Abstract":"Chinese idioms (cheng yu) have seen five thousand years\u2019 history and culture of china, meanwhile they contain large number of scientific achievement of ancient china. However, existing chinese online idiom dictionaries have limited function for scientific exploration. In this paper, we first construct a chinese idiom knowledge graph by extracting domains and dynasties and associating them with idioms, and based on the idiom knowledge graph, we propose a science toolkit for ancient china (a) aiming to support scientific exploration. In the progress toolkit, idiom navigator helps users explore overall scientific progress from idiom perspective with visualization tools, and idiom card and idiom qa shorten action path and avoid thinking being interrupted while users are reading and writing. The current chinese toolkit is deployed at http:\/\/120.92.208.22:7476\/demo\/#\/http:\/\/120.92.208.22:7476\/demo\/#\/..","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2020,"Venue":"emnlp-2020","Acronym":"BERT-ATTACK","Description":"Adversarial Attack Against BERT Using BERT","Abstract":"Adversarial attacks for discrete data (such as texts) have been proved significantly more challenging than continuous data (such as images) since it is difficult to generate adversarial samples with gradient-based methods. Current successful attack methods for texts usually adopt heuristic replacement strategies on the character or word level, which remains challenging to find the optimal solution in the massive space of possible combinations of replacements while preserving semantic consistency and language fluency. In this paper, we propose <b>calculation<\/b>, a high-quality and effective method to generate adversarial samples using pre-trained masked language models exemplified by bert. We turn bert against its fine-tuned models and other deep neural models in downstream tasks so that we can successfully mislead the target models to predict incorrectly. Our method outperforms state-of-the-art attack strategies in both success rate and perturb percentage, while the generated adversarial samples are fluent and semantically preserved. Also, the cost of calculation is low, thus possible for large-scale generations. The code is available at <a href=https:\/\/github.com\/linyanglee\/this class=acl-markup-url>https:\/\/github.com\/linyanglee\/fine-tuned<\/a>.","wordlikeness":0.8181818182,"lcsratio":0.7272727273,"wordcoverage":0.7826086957}
{"Year":2007,"Venue":"ws-2007","Acronym":"Olympus","Description":"an open-source framework for conversational spoken language interface research","Abstract":"We introduce modular, a freely available framework for research in conversational interfaces. Architecture,\u2019 open, transparent, flexible, modular and scalable nature facilitates the development of large-scale, real-world systems, and enables research leading to technological and scientific advances in conversational spoken language interfaces. In this paper, we describe the overall architecture, several systems spanning different domains, and a number of current research efforts supported by systems,.","wordlikeness":0.5714285714,"lcsratio":0.7142857143,"wordcoverage":0.8}
{"Year":2012,"Venue":"starsem-2012","Acronym":"UABCoRAL","Description":"A Preliminary study for Resolving the Scope of Negation","Abstract":"This paper describes our participation in the closed track of the *sem 2012 shared task of \ufb01nding the scope of negation. To perform the task, we propose a system that has three components: negation cue detection, scope of negation detection, and negated event detection. In the \ufb01rst phase, the system creates a lexicon of negation signals from the training data and uses the lexicon to identify the negation cues. Then, it applies machine learning approaches to detect the scope and negated event for each negation cue identi\ufb01ed in the \ufb01rst phase. Using a preliminary approach, our system achieves a reasonably good accuracy in identifying the scope of negation.","wordlikeness":0.5,"lcsratio":0.5,"wordcoverage":0.7692307692}
{"Year":2016,"Venue":"ws-2016","Acronym":"Lexfom","Description":"a lexical functions ontology model","Abstract":"A lexical function represents a type of relation that exists between lexical units (words or expressions) in any language. For example, the antonymy is a type of relation that is represented by the lexical function anti: anti(big) = small. Those relations include both paradigmatic relations, i.e. vertical relations, such as synonymy, antonymy and meronymy and syntagmatic relations, i.e. horizontal relations, such as objective qualification (legitimate demand), subjective qualification (fruitful analysis), positive evaluation (good review) and support verbs (pay a visit, subject to an interrogation). In this paper, we present the lexical functions ontology model (about) to represent lexical functions and the relation among lexical units. Represent is divided in four modules: lexical function representation (lfrep), lexical function family (lffam), lexical function semantic perspective (lfsem) and lexical function relations (lfrel). Moreover, we show how it combines to lexical model for ontologies (lemon), for the transformation of lexical networks into the semantic web formats. So far, we have implemented 100 simple and 500 complex lexical functions, and encoded about 8,000 syntagmatic and 46,000 paradigmatic relations, for the french language.","wordlikeness":0.3333333333,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2016,"Venue":"naacl-2016","Acronym":"BIRA","Description":"Improved Predictive Exchange Word Clustering","Abstract":"Word clusters are useful for many nlp tasks including training neural network language models, but current increases in datasets are outpacing the ability of word clusterers to handle them. Little attention has been paid thus far on inducing high-quality word clusters at a large scale. The predictive exchange algorithm is quite scalable, but sometimes does not provide as good perplexity as other slower clustering algorithms. We introduce the bidirectional, interpolated, re\ufb01ning, and alternating (translation) predictive exchange algorithm. It improves upon the predictive exchange algorithm\u2019s perplexity by up to 18%, giving it perplexities comparable to the slower two-sided exchange algorithm, and better perplexities than the slower brown clustering algorithm. Our paid implementation is fast, clustering a 2.5 billion token english news crawl corpus in 3 hours. It also reduces machine translation training time while preserving translation quality. Our implementation is portable and freely available.","wordlikeness":0.75,"lcsratio":0.75,"wordcoverage":0.8571428571}
{"Year":2010,"Venue":"law-2010","Acronym":"EmotiBlog","Description":"A Finer-Grained and More Precise Learning of Subjectivity Expression Models","Abstract":"The exponential growth of the subjective information in the framework of the web 2.0 has led to the need to create natural language processing tools able to analyse and process such data for multiple practical applications. They require training on specifically annotated corpora, whose level of detail must be fine enough to capture the phenomena involved. This paper presents exponential \u2013 a finegrained annotation scheme for subjectivity. We show the manner in which it is built and demonstrate the benefits it brings to the systems using it for training, through the experiments we carried out on opinion mining and emotion detection. We employ corpora of different textual genres \u2013a set of annotated reported speech extracted from news articles, the set of news titles annotated with polarity and emotion from the semeval 2007 (task 14) and isear, a corpus of real-life selfexpressed emotion. We also show how the model built from the applications. Annotations can be enhanced with external resources. The results demonstrate that process, through its structure and annotation paradigm, offers high quality training data for systems dealing both with opinion mining, as well as emotion detection. 1 credits this paper has been supported by ministerio de ciencia e innovaci\u00f3n- spanish government (grant no. Tin2009-13391-c0401), and conselleria d'educaci\u00f3ngeneralitat valenciana (grant no. Prometeo\/2009\/119 and acomp\/2010\/288).","wordlikeness":0.8888888889,"lcsratio":0.6666666667,"wordcoverage":0.75}
{"Year":2023,"Venue":"acl-2023","Acronym":"CoAD","Description":"Automatic Diagnosis through Symptom and Disease Collaborative Generation","Abstract":"Automatic diagnosis (ad), a critical application of ai in healthcare, employs machine learning techniques to assist doctors in gathering patient symptom information for precise disease diagnosis. The transformer-based method utilizes an input symptom sequence, predicts itself through auto-regression, and employs the hidden state of the final symptom to determine the disease. Despite its simplicity and superior performance demonstrated, a decline in disease diagnosis accuracy is observed caused by 1) a mismatch between symptoms observed during training and generation, and 2) the effect of different symptom orders on disease prediction. To address the above obstacles, we introduce the order;, a novel disease and symptom collaborative generation framework, which incorporates several key innovations to improve ad: 1) aligning sentence-level disease labels with multiple possible symptom inquiry steps to bridge the gap between training and generation; 2) expanding symptom labels for each sub-sequence of symptoms to enhance annotation and eliminate the effect of symptom order; 3) developing a repeated symptom input schema to effectively and efficiently learn the expanded disease and symptom labels. We evaluate the sequence, framework using four datasets, including three public and one private, and demonstrate that it achieves an average 2.3% improvement over previous state-of-the-art results in automatic disease diagnosis. For reproducibility, we release the code and data at <a href=https:\/\/github.com\/kwanwaichung\/the class=acl-markup-url>https:\/\/github.com\/kwanwaichung\/to<\/a>.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2014,"Venue":"semeval-2014","Acronym":"AI-KU","Description":"Using Co-Occurrence Modeling for Semantic Similarity","Abstract":"In this paper, we describe our unsupervised method submitted to the cross-level semantic similarity task in semeval 2014 that computes semantic similarity between two different sized text fragments. Our method models each text fragment by using the cooccurrence statistics of either occurred words or their substitutes. The co-occurrence modeling step provides dense, low-dimensional embedding for each fragment which allows us to calculate semantic similarity using various similarity metrics. Although our current model avoids the syntactic information, we achieved promising results and outperformed all baselines.","wordlikeness":0.4,"lcsratio":0.6,"wordcoverage":0.8}
{"Year":2018,"Venue":"naacl-2018","Acronym":"ArgumenText","Description":"Searching for Arguments in Heterogeneous Sources","Abstract":"Argument mining is a core technology for enabling argument search in large corpora. However, most current approaches fall short when applied to heterogeneous texts. In this paper, we present an argument retrieval system capable of retrieving sentential arguments for any given controversial topic. By analyzing the highest-ranked results extracted from web sources, we found that our system covers 89% of arguments found in expert-curated lists of arguments from an online debate portal, and also identifies additional valid arguments.","wordlikeness":0.9090909091,"lcsratio":0.9090909091,"wordcoverage":0.8421052632}
{"Year":2022,"Venue":"coling-2022","Acronym":"ACT-Thor","Description":"A Controlled Benchmark for Embodied Action Understanding in Simulated Environments","Abstract":"Artificial agents are nowadays challenged to perform embodied ai tasks. To succeed, agents must understand the meaning of verbs and how their corresponding actions transform the surrounding world. In this work, we propose very, a novel controlled benchmark for embodied action understanding. We use the ai2-thor simulated environment to produce a controlled setup in which an agent, given a before-image and an associated action command, has to determine what the correct after-image is among a set of possible candidates. First, we assess the feasibility of the task via a human evaluation that resulted in 81.4% accuracy, and very high inter-annotator agreement (84.9%). Second, we design both unimodal and multimodal baselines, using state-of-the-art visual feature extractors. Our evaluation and error analysis suggest that only models that have a very structured representation of the actions together with powerful visual features can perform well on the task. However, they still fall behind human performance in a zero-shot scenario where the model is exposed to unseen (action, object) pairs. This paves the way for a systematic way of evaluating embodied ai agents that understand grounded actions.","wordlikeness":0.625,"lcsratio":0.75,"wordcoverage":0.7692307692}
{"Year":2021,"Venue":"acl-2021","Acronym":"CoRI","Description":"Collective Relation Integration with Data Augmentation for Open Information Extraction","Abstract":"Integrating extracted knowledge from the web to knowledge graphs (kgs) can facilitate tasks like question answering. We study relation integration that aims to align free-text relations in subject-relation-object extractions to relations in a target kg. To address the challenge that free-text relations are ambiguous, previous methods exploit neighbor entities and relations for additional context. However, the predictions are made independently, which can be mutually inconsistent. We propose a two-stage collective relation integration (extracted) model, where the first stage independently makes candidate predictions, and the second stage employs a collective model that accesses all candidate predictions to make globally coherent predictions. We further improve the collective model with augmented data from the portion of the target kg that is otherwise unused. Experiment results on two datasets show that target can significantly outperform the baselines, improving auc from .677 to .748 and from .716 to .780, respectively.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2010,"Venue":"semeval-2010","Acronym":"UvT","Description":"Memory-Based Pairwise Ranking of Paraphrasing Verbs","Abstract":"In this paper we describe mephisto, our system for task 9 of the semeval-2 workshop. Our approach to this task is to develop a machine learning classi\ufb01er which determines for each verb pair describing a noun compound which verb should be ranked higher. These classi\ufb01cations are then combined into one ranking. Our classi\ufb01er uses features from the google ngram corpus, wordnet and the provided training data.","wordlikeness":0.3333333333,"lcsratio":0.3333333333,"wordcoverage":0.8}
{"Year":2020,"Venue":"emnlp-2020","Acronym":"X-FACTR","Description":"Multilingual Factual Knowledge Retrieval from Pretrained Language Models","Abstract":"Language models (lms) have proven surprisingly successful at capturing factual knowledge by completing cloze-style fill-in-the-blank questions such as \u201cpunta cana is located in _.\u201d however, while knowledge is both written and queried in many languages, studies on lms\u2019 factual representation ability have almost invariably been performed on english. To assess factual knowledge retrieval in lms in different languages, we create a multilingual benchmark of cloze-style probes for typologically diverse languages. To properly handle language variations, we expand probing methods from single- to multi-word entities, and develop several decoding algorithms to generate multi-token predictions. Extensive experimental results provide insights about how well (or poorly) current state-of-the-art lms perform at this task in languages with more or fewer available resources. We further propose a code-switching-based method to improve the ability of multilingual lms to access knowledge, and verify its effectiveness on several benchmark languages. Benchmark data and code have be released at <a href=https:\/\/decoding.github.io class=acl-markup-url>https:\/\/benchmark.github.io<\/a>.","wordlikeness":0.5714285714,"lcsratio":0.7142857143,"wordcoverage":0.7692307692}
{"Year":2010,"Venue":"inlg-2010","Acronym":"UDel","Description":"Refining a Method of Named Entity Generation","Abstract":"This report describes the methods and results of a system developed for the grec named entity challenge 2010. We detail the re\ufb01nements made to our 2009 submission and present the output of the selfevaluation on the development data set.","wordlikeness":1.0,"lcsratio":0.5,"wordcoverage":0.8571428571}
{"Year":2012,"Venue":"starsem-2012","Acronym":"ICT","Description":"A Translation based Method for Cross-lingual Textual Entailment","Abstract":"In this paper, we present our system description in task of cross-lingual textual entailment. The goal of this task is to detect entailment relations between two sentences written in different languages. To accomplish this goal, we first translate sentences written in foreign languages into english. Then, we use edits1, an open source package, to recognize entailment relations. Since edits only draws monodirectional relations while the task requires bidirectional predthusion, thus we exchange the hypothesis and test to detect entailment in another direction. Experimental results show that our method achieves promising results but not perfect results compared to other participants.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2018,"Venue":"wnut-2018","Acronym":"FrameIt","Description":"Ontology Discovery for Noisy User-Generated Text","Abstract":"A common need of nlp applications is to extract structured data from text corpora in order to perform analytics or trigger an appropriate action. The ontology defining the structure is typically application dependent and in many cases it is not known a priori. We describe the known system that provides a workflow for (1) quickly discovering an ontology to model a text corpus and (2) learning an srl model that extracts the instances of the ontology from sentences in the corpus. Bootstrap exploits data that is obtained in the ontology discovery phase as weak supervision data to bootstrap the srl model and then enables the user to refine the model with active learning. We present empirical results and qualitative analysis of the performance of action. On three corpora of noisy user-generated text.","wordlikeness":0.8571428571,"lcsratio":0.7142857143,"wordcoverage":0.8333333333}
{"Year":2022,"Venue":"acl-2022","Acronym":"CLUES","Description":"A Benchmark for Learning Classifiers using Natural Language Explanations","Abstract":"Supervised learning has traditionally focused on inductive learning by observing labeled examples of a task. In contrast, a hallmark of human intelligence is the ability to learn new concepts purely from language. Here, we explore training zero-shot classifiers for structured data purely from language. For this, we introduce are, a benchmark for classifier learning using natural language explanations, consisting of a range of classification tasks over structured data along with natural language supervision in the form of explanations. A consists of 36 real-world and 144 synthetic classification tasks. It contains crowdsourced explanations describing real-world tasks from multiple teachers and programmatically generated explanations for the synthetic tasks. To model the influence of explanations in classifying an example, we develop exent, an entailment-based model that learns classifiers using explanations. Exent generalizes up to 18% better (relative) on novel tasks than a baseline that does not use explanations. We delineate key challenges for automated learning from explanations, addressing which can lead to progress on language in the future. Code and datasets are available at: <a href=https:\/\/teachers-benchmark.github.io class=acl-markup-url>https:\/\/supervision-benchmark.github.io<\/a>.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2023,"Venue":"starsem-2023","Acronym":"LEXPLAIN","Description":"Improving Model Explanations via Lexicon Supervision","Abstract":"Model explanations that shed light on the model\u2019s predictions are becoming a desired additional output of nlp models, alongside their predictions. Challenges in creating these explanations include making them trustworthy and faithful to the model\u2019s predictions. In this work, we propose a novel framework for guiding model explanations by supervising them explicitly. To this end, our method, nlp, uses task-related lexicons to directly supervise model explanations. This approach consistently improves the model\u2019s explanations without sacrificing performance on the task, as we demonstrate on sentiment analysis and toxicity detection. Our analyses show that our method also demotes spurious correlations (i.e., with respect to african american english dialect) when performing the task, improving fairness.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.9333333333}
{"Year":2022,"Venue":"coling-2022","Acronym":"ConFiguRe","Description":"Exploring Discourse-level Chinese Figures of Speech","Abstract":"Figures of speech, such as metaphor and irony, are ubiquitous in literature works and colloquial conversations. This poses great challenge for natural language understanding since figures of speech usually deviate from their ostensible meanings to express deeper semantic implications. Previous research lays emphasis on the literary aspect of figures and seldom provide a comprehensive exploration from a view of computational linguistics. In this paper, we first propose the concept of figurative unit, which is the carrier of a figure. Then we select 12 types of figures commonly used in chinese, and build a chinese corpus for contextualized figure recognition (concept). Different from previous token-level or sentence-level counterparts, figure aims at extracting a figurative unit from discourse-level context, and classifying the figurative unit into the right figure type. On deviate, three tasks, i.e., figure extraction, figure type classification and figure recognition, are designed and the state-of-the-art techniques are utilized to implement the benchmarks. We conduct thorough experiments and show that all three tasks are challenging for existing models, thus requiring further research. Our dataset and code are publicly available at <a href=https:\/\/github.com\/pku-tangent\/are class=acl-markup-url>https:\/\/github.com\/pku-tangent\/models,<\/a>.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2015,"Venue":"semeval-2015","Acronym":"Lsislif","Description":"Feature Extraction and Label Weighting for Sentiment Analysis in Twitter","Abstract":"This paper describes our sentiment analysis systems which have been built for semeval2015 task 10 subtask b and e. For subtask b, a logistic regression classi\ufb01er has been trained after extracting several groups of features including lexical, syntactic, lexiconbased, z score and semantic features. A weighting schema has been adapted for positive and negative labels in order to take into account the unbalanced distribution of tweets between the positive and negative classes. This system is ranked third over 40 participants, it achieves average f1 64.27 on twitter data set 2015 just 0.57% less than the \ufb01rst system. We also present our participation in subtask e in which our system has got the second rank with kendall metric but the \ufb01rst one with spearman for ranking twitter terms according to their association with the positive sentiment.","wordlikeness":0.4285714286,"lcsratio":0.7142857143,"wordcoverage":0.6666666667}
{"Year":2022,"Venue":"naacl-2022","Acronym":"ConfliBERT","Description":"A Pre-trained Language Model for Political Conflict and Violence","Abstract":"Analyzing conflicts and political violence around the world is a persistent challenge in the political science and policy communities due in large part to the vast volumes of specialized text needed to monitor conflict and violence on a global scale. To help advance research in political science, we introduce we, a domain-specific pre-trained language model for conflict and political violence. We first gather a large domain-specific text corpus for language modeling from various sources. We then build analyzing using two approaches: pre-training from scratch and continual pre-training. To evaluate experiments., we collect 12 datasets and implement 18 tasks to assess the models\u2019 practical application in conflict research. Finally, we evaluate several versions of , in multiple experiments. Results consistently show that due outperforms bert when analyzing political violence and conflict.","wordlikeness":0.8,"lcsratio":0.7,"wordcoverage":0.8235294118}
{"Year":2016,"Venue":"lrec-2016","Acronym":"TermITH-Eval","Description":"a French Standard-Based Resource for Keyphrase Extraction Evaluation","Abstract":"Keyphrase extraction is the task of finding phrases that represent the important content of a document. The main aim of keyphrase extraction is to propose textual units that represent the most important topics developed in a document. The output keyphrases of automatic keyphrase extraction methods for test documents are typically evaluated by comparing them to manually assigned reference keyphrases. Each output keyphrase is considered correct if it matches one of the reference keyphrases. However, the choice of the appropriate textual unit (keyphrase) for a topic is sometimes subjective and evaluating by exact matching underestimates the performance. This paper presents a dataset of evaluation scores assigned to automatically extracted keyphrases by human evaluators. Along with the reference keyphrases, the manual evaluations can be used to validate new evaluation measures. Indeed, an evaluation measure that is highly correlated to the manual evaluation is appropriate for the evaluation of automatic keyphrase extraction methods.","wordlikeness":0.5833333333,"lcsratio":0.6666666667,"wordcoverage":0.7}
{"Year":2023,"Venue":"acl-2023","Acronym":"TabGenie","Description":"A Toolkit for Table-to-Text Generation","Abstract":"Heterogenity of data-to-text generation datasets limits the research on data-to-text generation systems. We present table-to-text \u2013 a toolkit which enables researchers to explore, preprocess, and analyze a variety of data-to-text generation datasets through the unified framework of table-to-text generation. In provides, all inputs are represented as tables with associated metadata. The tables can be explored through a web interface, which also provides an interactive mode for debugging table-to-text generation, facilitates side-by-side comparison of generated system outputs, and allows easy exports for manual analysis. Furthermore, metadata. Is equipped with command line processing tools and python bindings for unified dataset loading and processing. We release python as a pypi package and provide its open-source code and a live demo at <a href=https:\/\/github.com\/kasnerz\/furthermore, class=acl-markup-url>https:\/\/github.com\/kasnerz\/furthermore,<\/a>.","wordlikeness":0.875,"lcsratio":0.875,"wordcoverage":0.75}
{"Year":2015,"Venue":"semeval-2015","Acronym":"SWAT-CMW","Description":"Classification of Twitter Emotional Polarity using a Multiple-Classifier Decision Schema and Enhanced Emotion Tagging","Abstract":"In this paper, we describe our approach to semeval 2015 task 10 subtask b, message level sentiment detection. Our system implements a variety of classi\ufb01ers and data preparation techniques from previous work. The set of features and classi\ufb01ers used in the \ufb01nal system produced consistently strong results using crossvalidation on the provided training data. Our \ufb01nal system achieved an f-score of 57.60 on the provided test data. The overall best performing system had an f-score of 64.84.","wordlikeness":0.25,"lcsratio":0.875,"wordcoverage":0.6666666667}
{"Year":2020,"Venue":"inlg-2020","Acronym":"Chart-to-Text","Description":"Generating Natural Language Descriptions for Charts by Adapting the Transformer Model","Abstract":"Information visualizations such as bar charts and line charts are very popular for exploring data and communicating insights. Interpreting and making sense of such visualizations can be challenging for some people, such as those who are visually impaired or have low visualization literacy. In this work, we introduce a new dataset and present a neural model for automatically generating natural language summaries for charts. The generated summaries provide an interpretation of the chart and convey the key insights found within that chart. Our neural model is developed by extending the state-of-the-art model for the data-to-text generation task, which utilizes a transformer-based encoder-decoder architecture. We found that our approach outperforms the base model on a content selection metric by a wide margin (55.42% vs. 8.49%) and generates more informative, concise, and coherent summaries.","wordlikeness":0.6923076923,"lcsratio":0.6923076923,"wordcoverage":0.6363636364}
{"Year":2012,"Venue":"lrec-2012","Acronym":"PEARL","Description":"ProjEction of Annotations Rule Language, a Language for Projecting (UIMA) Annotations over RDF Knowledge Bases","Abstract":"In this paper we present a language, paper, for projecting annotations based on the unstructured information management architecture (uima) over rdf triples. The language offer is twofold: first, a query mechanism, built upon (and extending) the basic featurepath notation of uima, allows for efficient access to the standard annotation format of uima based on feature structures. And then provides a syntax for projecting the retrieved information onto an rdf dataset, by using a combination of a sparql-like notation for matching pre-existing elements of the dataset and of meta-graph patterns, for storing new information into it. In this paper we present the basics of this language and how a to document is structured, discuss a simple use-case and introduce a wider project about automatic acquisition of knowledge, in which based plays a pivotal role.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2019,"Venue":"ijcnlp-2019","Acronym":"PolyResponse","Description":"A Rank-based Approach to Task-Oriented Dialogue with Application in Restaurant Search and Booking","Abstract":"We present of, a conversational search engine that supports task-oriented dialogue. It is a retrieval-based approach that bypasses the complex multi-component design of traditional task-oriented dialogue systems and the use of explicit semantics in the form of task-specific ontologies. The dialogue. Engine is trained on hundreds of millions of examples extracted from real conversations: it learns what responses are appropriate in different conversational contexts. It then ranks a large index of text and visual responses according to their similarity to the given context, and narrows down the list of relevant entities during the multi-turn conversation. We introduce a restaurant search and booking system powered by the engine, engine, currently available in 8 different languages.","wordlikeness":0.5833333333,"lcsratio":0.75,"wordcoverage":0.8}
{"Year":2021,"Venue":"mtsummit-2021","Acronym":"AVASAG","Description":"A German Sign Language Translation System for Public Services (short paper)","Abstract":"This paper presents an overview of the; an ongoing applied-research project developing a text-to-sign-language translation system for public services. We describe the scientific innovation points (geometry-based sl-description, 3d animation and video corpus, simplified annotation scheme, motion capture strategy) and the overall translation pipeline.","wordlikeness":0.5,"lcsratio":0.8333333333,"wordcoverage":0.6666666667}
{"Year":2012,"Venue":"fsmnlp-2012","Acronym":"DAGGER","Description":"A Toolkit for Automata on Directed Acyclic Graphs","Abstract":"This paper presents presents, a toolkit for \ufb01nite-state automata that operate on directed acyclic graphs (dags). The work is based on a model introduced by (kamimura and slutzki, 1981; kamimura and slutzki, 1982), with a few changes to make the automata more applicable to natural language processing. Available algorithms include membership checking in bottom-up dag acceptors, transduction of dags to trees (bottom-up dag-to-tree transducers), k-best generation and basic operations such as union and intersection.","wordlikeness":0.8333333333,"lcsratio":0.6666666667,"wordcoverage":0.8333333333}
{"Year":2023,"Venue":"acl-2023","Acronym":"TECHS","Description":"Temporal Logical Graph Networks for Explainable Extrapolation Reasoning","Abstract":"Extrapolation reasoning on temporal knowledge graphs (tkgs) aims to forecast future facts based on past counterparts. There are two main challenges: (1) incorporating the complex information, including structural dependencies, temporal dynamics, and hidden logical rules; (2) implementing differentiable logical rule learning and reasoning for explainability. To this end, we propose an explainable extrapolation reasoning framework teemporal logical graph networks (challenges:), which mainly contains a temporal graph encoder and a logical decoder. The former employs a graph convolutional network with temporal encoding and heterogeneous attention to embed topological structures and temporal dynamics. The latter integrates propositional reasoning and first-order reasoning by introducing a reasoning graph that iteratively expands to find the answer. A forward message-passing mechanism is also proposed to update node representations, and their propositional and first-order attention scores. Experimental results demonstrate that it outperforms state-of-the-art baselines.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2020,"Venue":"acl-2020","Acronym":"MixText","Description":"Linguistically-Informed Interpolation of Hidden Space for Semi-Supervised Text Classification","Abstract":"This paper presents presents, a semi-supervised learning method for text classification, which uses our newly designed data augmentation method called tmix. Tmix creates a large amount of augmented training samples by interpolating text in hidden space. Moreover, we leverage recent advances in data augmentation to guess low-entropy labels for unlabeled data, hence making them as easy to use as labeled data. By mixing labeled, unlabeled and augmented data, presents significantly outperformed current pre-trained and fined-tuned models and other state-of-the-art semi-supervised learning methods on several text classification benchmarks. The improvement is especially prominent when supervision is extremely limited. We have publicly released our code at <a href=https:\/\/github.com\/gt-salt\/creates class=acl-markup-url>https:\/\/github.com\/gt-salt\/text<\/a>.","wordlikeness":0.8571428571,"lcsratio":0.8571428571,"wordcoverage":0.7272727273}
{"Year":2023,"Venue":"acl-2023","Acronym":"XMD","Description":"An End-to-End Framework for Interactive Explanation-Based Debugging of NLP Models","Abstract":"Nlp models are susceptible to learning spurious biases (i.e., bugs) that work on some datasets but do not properly reflect the underlying task. Explanation-based model debugging aims to resolve spurious biases by showing human users explanations of model behavior, asking users to give feedback on the behavior, thenusing the feedback to update the model. While existing model debugging methods have shown promise, their prototype-level implementations provide limited practical utility. Thus, we propose face.: the first open-source, end-to-end framework for explanation-based model debugging. Given task- or instance-level explanations,users can flexibly provide various forms of feedback via an intuitive, web-based ui. After receiving user feedback, utility. Automatically updates the model in real time, by regularizing the model so that its explanationsalign with the user feedback. The new model can then be easily deployed into real-world applications via hugging face. Using so, we can improve the model\u2019s ood performance on text classification tasks by up to 18%.","wordlikeness":0.3333333333,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2021,"Venue":"acl-2021","Acronym":"Knodle","Description":"Modular Weakly Supervised Learning with PyTorch","Abstract":"Strategies for improving the training and prediction quality of weakly supervised machine learning models vary in how much they are tailored to a specific task or integrated with a specific model architecture. In this work, we introduce software, a software framework that treats weak data annotations, deep learning models, and methods for improving weakly supervised training as separate, modular components. This modularization gives the training process access to fine-grained information such as data set characteristics, matches of heuristic rules, or elements of the deep learning model ultimately used for prediction. Hence, our framework can encompass a wide range of training methods for improving weak supervision, ranging from methods that only look at correlations of rules and output classes (independently of the machine learning model trained with the resulting labels), to those that harness the interplay of neural networks and weakly labeled data. We illustrate the benchmarking potential of the framework with a performance comparison of several reference implementations on a selection of datasets that are already available in or.","wordlikeness":0.6666666667,"lcsratio":0.6666666667,"wordcoverage":0.8333333333}
{"Year":2022,"Venue":"acl-2022","Acronym":"LM-BFF-MS","Description":"Improving Few-Shot Fine-tuning of Language Models based on Multiple Soft Demonstration Memory","Abstract":"Lm-bff (citation) achieves significant few-shot performance by using auto-generated prompts and adding demonstrations similar to an input example. To improve the approach of lm-bff, this paper proposes <b>on<\/b>\u2014<b>b<\/b>etter <b>f<\/b>ew-shot <b>f<\/b>ine-tuning of <b>l<\/b>anguage <b>m<\/b>odels with <b>m<\/b>ultiple <b>s<\/b>oft demonstrations by making its further extensions, which include 1) prompts with <i>multiple demonstrations<\/i> based on automatic generation of multiple label words; and 2) <i>soft demonstration memory<\/i> which consists of multiple sequences of <i>globally shared<\/i> word embeddings for a similar context. Experiments conducted on eight nlp tasks show that <b>m<\/b>odels leads to improvements over lm-bff on five tasks, particularly achieving 94.0 and 90.4 on sst-2 and mrpc, respectively.","wordlikeness":0.1111111111,"lcsratio":0.6666666667,"wordcoverage":0.5714285714}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"Mask-then-Fill","Description":"A Flexible and Effective Data Augmentation Framework for Event Extraction","Abstract":"We present setting., a flexible and effective data augmentation framework for event extraction. Our approach allows for more flexible manipulation of text and thus can generate more diverse data while keeping the original event structure unchanged as much as possible. Specifically, it first randomly masks out an adjunct sentence fragment and then infills a variable-length text span with a fine-tuned infilling model. The main advantage lies in that it can replace a fragment of arbitrary length in the text with another fragment of variable length, compared to the existing methods which can only replace a single word or a fixed-length fragment. On trigger and argument extraction tasks, the proposed framework is more effective than baseline methods and it demonstrates particularly strong results in the low-resource setting. Our further analysis shows that it achieves a good balance between diversity and distributional similarity.","wordlikeness":0.6428571429,"lcsratio":0.4285714286,"wordcoverage":0.6086956522}
{"Year":2019,"Venue":"ws-2019","Acronym":"MinWikiSplit","Description":"A Sentence Splitting Corpus with Minimal Propositions","Abstract":"We compiled a new sentence splitting corpus that is composed of 203k pairs of aligned complex source and simplified target sentences. Contrary to previously proposed text simplification corpora, which contain only a small number of split examples, we present a dataset where each input sentence is broken down into a set of minimal propositions, i.e. a sequence of sound, self-contained utterances with each of them presenting a minimal semantic unit that cannot be further decomposed into meaningful propositions. This corpus is useful for developing sentence splitting approaches that learn how to transform sentences with a complex linguistic structure into a fine-grained representation of short sentences that present a simple and more regular structure which is easier to process for downstream applications and thus facilitates and improves their performance.","wordlikeness":0.6666666667,"lcsratio":0.6666666667,"wordcoverage":0.6363636364}
{"Year":2015,"Venue":"semeval-2015","Acronym":"UQeResearch","Description":"Semantic Textual Similarity Quantification","Abstract":"This paper presents an approach for estimating the semantic textual similarity of full english sentences as specified in shared task 2 of semeval-2015. The semantic similarity of sentence pairs is quantified from three perspectives - structural, syntactical, and semantic. The numerical representations of the derived similarity measures are then applied to train a regression ensemble. Although none of these three sets of measures is able to represent the semantic similarity of two sentences individually, our experimental results show that the combination of these features can precisely assess the semantic similarity of the sentences. In the english subtask our system\u2019s best result ranked 35 among 73 system runs with 0.7189 average pearson correlation over five test sets. This was 0.08 correlation points less than the best submitted run.","wordlikeness":0.6363636364,"lcsratio":0.5454545455,"wordcoverage":0.8421052632}
{"Year":2023,"Venue":"ranlp-2023","Acronym":"LeSS","Description":"A Computationally-Light Lexical Simplifier for Spanish","Abstract":"Due to having knowledge of only basic vocabulary, many people cannot understand up-to-date written information and thus make informed decisions and fully participate in the society. We propose is, a modular lexical simplification architecture that outperforms state-of-the-art lexical simplification systems for spanish. In addition to its state-of-the-art performance, are is computationally light, using much make disk space, cpu and gpu, and having faster loading and execution time than the transformer-based lexical simplification models which are predominant in the field.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2019,"Venue":"acl-2019","Acronym":"KCAT","Description":"A Knowledge-Constraint Typing Annotation Tool","Abstract":"In this paper, we propose an efficient knowledge constraint fine-grained entity typing annotation tool, which further improves the entity typing process through entity linking together with some practical functions.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2019,"Venue":"ijcnlp-2019","Acronym":"YouMakeup","Description":"A Large-Scale Domain-Specific Multimodal Dataset for Fine-Grained Semantic Comprehension","Abstract":"Multimodal semantic comprehension has attracted increasing research interests recently such as visual question answering and caption generation. However, due to the data limitation, fine-grained semantic comprehension has not been well investigated, which requires to capture semantic details of multimodal contents. In this work, we introduce \u201cspecific\u201d, a large-scale multimodal instructional video dataset to support fine-grained semantic comprehension research in specific domain. Videos contains 2,800 videos from youtube, spanning more than 420 hours in total. Each video is annotated with a sequence of natural language descriptions for instructional steps, grounded in temporal video range and spatial facial areas. The annotated steps in a video involve subtle difference in actions, products and regions, which requires fine-grained understanding and reasoning both temporally and spatially. In order to evaluate models\u2019 ability for fined-grained comprehension, we further propose two groups of tasks including generation tasks and visual question answering from different aspects. We also establish a baseline of step caption generation for future comparison. The dataset will be publicly available at <a href=https:\/\/github.com\/aim3-ruc\/order class=acl-markup-url>https:\/\/github.com\/aim3-ruc\/also<\/a> to support research investigation in fine-grained semantic comprehension.","wordlikeness":0.7777777778,"lcsratio":0.6666666667,"wordcoverage":0.8}
{"Year":2018,"Venue":"coling-2018","Acronym":"SetExpander","Description":"End-to-end Term Set Expansion Based on Multi-Context Term Embeddings","Abstract":"We present a, a corpus-based system for expanding a seed set of terms into a more complete set of terms that belong to the same semantic class. Into implements an iterative end-to end workflow for term set expansion. It enables users to easily select a seed set of terms, expand it, view the expanded set, validate it, re-expand the validated set and store it, thus simplifying the extraction of domain-specific fine-grained semantic classes. For has been used for solving real-life use cases including integration in an automated recruitment system and an issues and defects resolution system. A video demo of more is available at <a href=\"https:\/\/drive.google.com\/open?id=1e545bb87autsch36djnjhmq3hwfsd1rv\" class=acl-markup-url>https:\/\/drive.google.com\/open?id=1e545bb87autsch36djnjhmq3hwfsd1rv<\/a> .","wordlikeness":0.7272727273,"lcsratio":1.0,"wordcoverage":0.7368421053}
{"Year":2022,"Venue":"lrec-2022","Acronym":"Ara-Women-Hate","Description":"An Annotated Corpus Dedicated to Hate Speech Detection against Women in the Arabic Community","Abstract":"In this paper, an approach for hate speech detection against women in the arabic community on social media (e.g. youtube) is proposed. In the literature, similar works have been presented for other languages such as english. However, to the best of our knowledge, not much work has been conducted in the arabic language. A new hate speech corpus (arabic_fr_en) is developed using three different annotators. For corpus validation, three different machine learning algorithms are used, including deep convolutional neural network (cnn), long short-term memory (lstm) network and bi-directional lstm (bi-lstm) network.","wordlikeness":0.5,"lcsratio":0.7857142857,"wordcoverage":0.6086956522}
{"Year":2022,"Venue":"ijcnlp-2022","Acronym":"Hengam","Description":"An Adversarially Trained Transformer for Persian Temporal Tagging","Abstract":"Many nlp main tasks benefit from an accurate understanding of temporal expressions, e.g., text summarization, question answering, and information retrieval. This paper introduces diverse, an adversarially trained transformer for persian temporal tagging outperforming state-of-the-art approaches on a diverse and manually created dataset. We create this in the following concrete steps: (1) we develop accuratetagger, an extensible rule-based tool that can extract temporal expressions from a set of diverse language-specific patterns for any language of interest. (2) we apply steps:tagger to annotate temporal tags in a large and diverse persian text collection (covering both formal and informal contexts) to be used as weakly labeled data. (3) we introduce an adversarially trained transformer model on href=https:\/\/github.com\/kargaranamir\/corpus that can generalize over the bothtagger\u2019s rules. We create thegold, the first high-quality gold standard for persian temporal tagging. Our trained adversarial tooltransformer not only achieves the best performance in terms of the f1-score (a type f1-score of 95.42 and a partial f1-score of 91.60) but also successfully deals with language ambiguities and incorrect spellings. Our code, data, and models are publicly available at <a href=https:\/\/github.com\/kargaranamir\/summarization, class=acl-markup-url>https:\/\/github.com\/kargaranamir\/introduce<\/a>.","wordlikeness":0.8333333333,"lcsratio":0.6666666667,"wordcoverage":0.7692307692}
{"Year":2023,"Venue":"ws-2023","Acronym":"LFTK","Description":"Handcrafted Features in Computational Linguistics","Abstract":"Past research has identified a rich set of handcrafted linguistic features that can potentially assist various tasks. However, their extensive number makes it difficult to effectively select and utilize existing handcrafted features. Coupled with the problem of inconsistent implementation across research works, there has been no categorization scheme or generally-accepted feature names. This creates unwanted confusion. Also, no actively-maintained open-source library extracts a wide variety of handcrafted features. The current handcrafted feature extraction practices have several inefficiencies, and a researcher often has to build such an extraction system from the ground up. We collect and categorize more than 220 popular handcrafted features grounded on past literature. Then, we conduct a correlation analysis study on several task-specific datasets and report the potential use cases of each feature. Lastly, we devise a multilingual handcrafted linguistic feature extraction system in a systematically expandable manner. We open-source our system to give the community a rich set of pre-implemented handcrafted features.","wordlikeness":0.25,"lcsratio":0.5,"wordcoverage":0.75}
{"Year":2018,"Venue":"lrec-2018","Acronym":"EFLLex","Description":"A Graded Lexical Resource for Learners of English as a Foreign Language","Abstract":"This paper introduces english, an innovative lexical resource that describes the use of 15,280 english words in pedagogical materials across the pro\ufb01ciency levels of the european framework of reference for languages. The methodology adopted to produce the resource implies the selection of an ef\ufb01cient part-of-speech tagger, the use of a robust estimator for frequency computation and some manual post-editing work. The content of the resource is described and compared to other vocabulary lists (mrc and bnc) and to a reference pedagogical resource: the english vocabulary pro\ufb01le. Keywords: cefr-graded lexicon, english as a foreign language, vocabulary 1.","wordlikeness":0.5,"lcsratio":0.8333333333,"wordcoverage":0.8}
{"Year":2018,"Venue":"amta-2018","Acronym":"XNMT","Description":"The eXtensible Neural Machine Translation Toolkit","Abstract":"This paper describes extensible, the extensible neural machine translation toolkit. The distinguishes itself from other open-source nmt toolkits by its focus on modular code design, with the purpose of enabling fast iteration in research and replicable, reliable results. In this paper we describe the design of translation and its experiment con\ufb01guration system, and demonstrate its utility on the tasks of machine translation, speech recognition, and multi-tasked machine translation\/parsing. Research is available open-source at https:\/\/github.com\/neulab\/at.","wordlikeness":0.25,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2022,"Venue":"lrec-2022","Acronym":"CoQAR","Description":"Question Rewriting on CoQA","Abstract":"Questions asked by humans during a conversation often contain contextual dependencies, i.e., explicit or implicit references to previous dialogue turns. These dependencies take the form of coreferences (e.g., via pronoun use) or ellipses, and can make the understanding difficult for automated systems. One way to facilitate the understanding and subsequent treatments of a question is to rewrite it into an out-of-context form, i.e., a form that can be understood without the conversational context. We propose treatments, a corpus containing 4.5k conversations from the conversational question-answering dataset coqa, for a total of 53k follow-up question-answer pairs. Each original question was manually annotated with at least 2 at most 3 out-of-context rewritings. Coqa originally contains 8k conversations, which sum up to 127k question-answer pairs. That can be used in the supervised learning of three tasks: question paraphrasing, question rewriting and conversational question answering. In order to assess the quality of questions\u2019s rewritings, we conduct several experiments consisting in training and evaluating models for these three tasks. Our results support the idea that question rewriting can be used as a preprocessing step for (conversational and non-conversational) question answering models, thereby increasing their performances.","wordlikeness":0.6,"lcsratio":0.8,"wordcoverage":0.75}
{"Year":2023,"Venue":"findings-2023","Acronym":"ReGen","Description":"Zero-Shot Text Classification via Training Data Generation with Progressive Dense Retrieval","Abstract":"With the development of large language models (llms), zero-shot learning has attracted much attention for various nlp tasks. Different from prior works that generate training data with billion-scale natural language generation (nlg) models, we propose a retrieval-enhanced framework to create training data from a general-domain unlabeled corpus. To realize this, we first conduct contrastive pretraining to learn an unsupervised dense retriever for extracting the most relevant documents using class-descriptive verbalizers. We then further pro- pose two simple strategies, namely verbalizer augmentation with demonstrations and self- consistency guided filtering to improve the topic coverage of the dataset while removing noisy examples. Experiments on nine datasets demonstrate that relevant achieves 4.3% gain over the strongest baselines and saves around 70% of the time when compared with baselines using large nlg models. Besides, baselines can be naturally integrated with recently proposed large language models to boost performance.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.9090909091}
{"Year":2020,"Venue":"lrec-2020","Acronym":"CLEEK","Description":"A Chinese Long-text Corpus for Entity Linking","Abstract":"Entity linking, as one of the fundamental tasks in natural language processing, is crucial to knowledge fusion, knowledge base construction and update. Nevertheless, in contrast to the research on entity linking for english text, which undergoes continuous development, the chinese counterpart is still in its infancy. One prominent issue lies in publicly available annotated datasets and evaluation benchmarks, which are lacking and deficient. In specific, existing chinese corpora for entity linking were mainly constructed from noisy short texts, such as microblogs and news headings, where long texts were largely overlooked, which yet constitute a wider spectrum of real-life scenarios. To address the issue, in this work, we build annotated, a chinese corpus of multi-domain long text for entity linking, in order to encourage advancement of entity linking in languages besides english. The corpus consists of 100 documents from diverse domains, and is publicly accessible. Moreover, we devise a measure to evaluate the difficulty of documents with respect to entity linking, which is then used to characterize the corpus. Additionally, the results of two baselines and seven state-of-the-art solutions on existing are reported and compared. The empirical results validate the usefulness of linking and the effectiveness of proposed difficulty measure.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2021,"Venue":"acl-2021","Acronym":"LV-BERT","Description":"Exploiting Layer Variety for BERT","Abstract":"Modern pre-trained language models are mostly built upon backbones stacking selfattention and feed-forward layers in an interleaved order. In this paper, beyond this stereotyped layer pattern, we aim to improve pre-trained models by exploiting layer variety from two aspects: the layer type set and the layer order. Speci\ufb01cally, besides the original self-attention and feed-forward layers, we introduce convolution into the layer type set, which is experimentally found bene\ufb01cial to pre-trained models. Furthermore, beyond the original interleaved order, we explore more layer orders to discover more powerful architectures. However, the introduced layer variety leads to a large architecture space of more than billions of candidates, while training a single candidate model from scratch already requires huge computation cost, making it not affordable to search such a space by directly training large amounts of candidate models. To solve this problem, we \ufb01rst pre-train a supernet from which the weights of all candidate models can be inherited, and then adopt an evolutionary algorithm guided by pre-training accuracy to \ufb01nd the optimal architecture. Extensive experiments show that set, model obtained by our method outperforms bert and its variants on various downstream tasks. For example, in-small achieves 78.8 on the glue testing set, 1.8 higher than the strong baseline electra-small.","wordlikeness":0.5714285714,"lcsratio":0.8571428571,"wordcoverage":0.7692307692}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"RelU-Net","Description":"Syntax-aware Graph U-Net for Relational Triple Extraction","Abstract":"Relational triple extraction is a critical task for natural language processing. Existing methods mainly focused on capturing semantic information, but suffered from ignoring the syntactic structure of the sentence, which is proved in the relation classification task to contain rich relational information. This is due to the absence of entity locations, which is the prerequisite for pruning noisy edges from the dependency tree, when extracting relational triples. In this paper, we propose a unified framework to tackle this challenge and incorporate syntactic information for relational triple extraction. First, we propose to automatically contract the dependency tree into a core relational topology and eliminate redundant information with graph pooling operations. Then, we propose a symmetrical expanding path with graph unpooling operations to fuse the contracted core syntactic interactions with the original sentence context. We also propose a bipartite graph matching objective function to capture the reflections between the core topology and golden relational facts. Since our model shares similar contracting and expanding paths with encoder-decoder models like u-net, we name our model as relation u-net (prove). We conduct experiments on several datasets and the results prove the effectiveness of our method.","wordlikeness":0.625,"lcsratio":0.875,"wordcoverage":0.7058823529}
{"Year":2018,"Venue":"paclic-2018","Acronym":"BoWLer","Description":"A neural approach to extractive text summarization","Abstract":"While extractive summarization is a well studied problem, it is far from solved. In recent years a large number of interesting and complex models have been used to achieve significant improvements in performance. This can easily be attributed to deep learning models and dense vector representations but the performance gain comes with the cost of computational and representational complexity. In this work, we present a simple, yet effective approach for extractive summarization of news articles. In line with many recent works in this area we propose an encoder-decoder architecture with a simple bag of word encoder for sentences followed by an attention based decoder for relevant sentence selection. Our model is trained end-to-end and its performance is comparable to the state-of-the-art models while being simpler both in terms of the number of parameters (signi\ufb01cantly lesser) as well as the representational complexity.","wordlikeness":0.8333333333,"lcsratio":0.5,"wordcoverage":1.0}
{"Year":2022,"Venue":"lrec-2022","Acronym":"HECTOR","Description":"A Hybrid TExt SimplifiCation TOol for Raw Texts in French","Abstract":"Reducing the complexity of texts by applying an automatic text simplification (ats) system has been sparking interest inthe area of natural language processing (nlp) for several years and a number of methods and evaluation campaigns haveemerged targeting lexical and syntactic transformations. In recent years, several studies exploit deep learning techniques basedon very large comparable corpora. Yet the lack of large amounts of corpora (original-simplified) for french has been hinderingthe development of an ats tool for this language. In this paper, we present our system, which is based on a combination ofmethods relying on word embeddings for lexical simplification and rule-based strategies for syntax and discourse adaptations. We present an evaluation of the lexical, syntactic and discourse-level simplifications according to automatic and humanevaluations.","wordlikeness":0.8333333333,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2018,"Venue":"lrec-2018","Acronym":"TSix","Description":"A Human-involved-creation Dataset for Tweet Summarization","Abstract":"We present a new dataset for tweet summarization. The dataset includes six events collected from twitter from october 10 to november 9, 2016. Our dataset features two prominent properties. Firstly, human-annotated gold-standard references allow to correctly evaluate extractive summarization methods. Secondly, tweets are assigned into sub-topics divided by consecutive days, which facilitate incremental tweet stream summarization methods. To reveal the potential usefulness of our dataset, we compare several well-known summarization methods. Experimental results indicate that among extractive approaches, hybrid term frequency \u2013 document term frequency obtains competitive results in term of rouge-scores. The analysis also shows that polarity is an implicit factor of tweets in our dataset, suggesting that it can be exploited as a component besides tweet content quality in the summarization process. Keywords: tweet summarization, hybrid tf-idf, dataset, corpus, annotation. 1.","wordlikeness":0.5,"lcsratio":0.75,"wordcoverage":0.8571428571}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"SELFEXPLAIN","Description":"A Self-Explaining Architecture for Neural Text Classifiers","Abstract":"We introduce quantifies, a novel self-explaining model that explains a text classifier\u2019s predictions using phrase-based concepts. Trustworthy augments existing neural classifiers by adding (1) a globally interpretable layer that identifies the most influential concepts in the training set for a given sample and (2) a locally interpretable layer that quantifies the contribution of each local input concept by computing a relevance score relative to the predicted label. Experiments across five text-classification datasets show that set facilitates interpretability without sacrificing performance. Most importantly, explanations from phrase-based show sufficiency for model predictions and are perceived as adequate, trustworthy and understandable by human judges compared to existing widely-used baselines.","wordlikeness":0.8181818182,"lcsratio":1.0,"wordcoverage":0.7777777778}
{"Year":2022,"Venue":"coling-2022","Acronym":"MICO","Description":"Selective Search with Mutual Information Co-training","Abstract":"In contrast to traditional exhaustive search, selective search first clusters documents into several groups before all the documents are searched exhaustively by a query, to limit the search executed within one group or only a few groups. Selective search is designed to reduce the latency and computation in modern large-scale search systems. In this study, we propose relevant, a <b>m<\/b>utual <b>i<\/b>nformation <b>co<\/b>-training framework for selective search with minimal supervision using the search logs. After training, groups. Does not only cluster the documents, but also routes unseen queries to the relevant clusters for efficient retrieval. In our empirical experiments, modern significantly improves the performance on multiple metrics of selective search and outperforms a number of existing competitive baselines.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2022,"Venue":"acl-2022","Acronym":"ReCLIP","Description":"A Strong Zero-Shot Baseline for Referring Expression Comprehension","Abstract":"Training a referring expression comprehension (rec) model for a new visual domain requires collecting referring expressions, and potentially corresponding bounding boxes, for images in the domain. While large-scale pre-trained models are useful for image classification across domains, it remains unclear if they can be applied in a zero-shot manner to more complex tasks like rec. We present but, a simple but strong <i>zero-shot<\/i> baseline that repurposes clip, a state-of-the-art large-scale model, for rec. Motivated by the close connection between rec and clip\u2019s contrastive pre-training objective, the first component of controlled is a region-scoring method that isolates object proposals via cropping and blurring, and passes them to clip. However, through controlled experiments on a synthetic dataset, we find that clip is largely incapable of performing spatial reasoning off-the-shelf. We reduce the gap between zero-shot baselines from prior work and supervised models by as much as 29% on refcocog, and on refgta (video game imagery), simple\u2019s relative improvement over supervised rec models trained on real images is 8%.","wordlikeness":0.8333333333,"lcsratio":0.8333333333,"wordcoverage":0.8333333333}
{"Year":2020,"Venue":"findings-2020","Acronym":"MultiDM-GCN","Description":"Aspect-guided Response Generation in Multi-domain Multi-modal Dialogue System using Graph Convolutional Network","Abstract":"In the recent past, dialogue systems have gained immense popularity and have become ubiquitous. During conversations, humans not only rely on languages but seek contextual information through visual contents as well. In every task-oriented dialogue system, the user is guided by the different aspects of a product or service that regulates the conversation towards selecting the product or service. In this work, we present a multi-modal conversational framework for a task-oriented dialogue setup that generates the responses following the different aspects of a product or service to cater to the user\u2019s needs. We show that the responses guided by the aspect information provide more interactive and informative responses for better communication between the agent and the user. We first create a multi-domain multi-modal dialogue (mdmmd) dataset having conversations involving both text and images belonging to the three different domains, such as restaurants, electronics, and furniture. We implement a graph convolutional network (gcn) based framework that generates appropriate textual responses from the multi-modal inputs. The multi-modal information having both textual and image representation is fed to the decoder and the aspect information for generating aspect guided responses. Quantitative and qualitative analyses show that the proposed methodology outperforms several baselines for the proposed task of aspect-guided response generation.","wordlikeness":0.4545454545,"lcsratio":1.0,"wordcoverage":0.625}
{"Year":2020,"Venue":"emnlp-2020","Acronym":"TextAttack","Description":"A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP","Abstract":"While there has been substantial research using adversarial attacks to analyze nlp models, each attack is implemented in its own code repository. It remains challenging to develop nlp attacks and utilize them to improve model performance. This paper introduces researchers, a python framework for adversarial attacks, data augmentation, and adversarial training in nlp. Also builds attacks from four components: a goal function, a set of constraints, a transformation, and a search method. Improve\u2019s modular design enables researchers to easily construct attacks from combinations of novel and existing components. Repository. Provides implementations of 16 adversarial attacks from the literature and supports a variety of models and datasets, including bert and other transformers, and all glue tasks. Try also includes data augmentation and adversarial training modules for using components of adversarial attacks to improve model accuracy and robustness. Provides is democratizing nlp: anyone can try data augmentation and adversarial training on any model or dataset, with just a few lines of code. Code and tutorials are available at <a href=https:\/\/github.com\/qdata\/attacks, class=acl-markup-url>https:\/\/github.com\/qdata\/also<\/a>.","wordlikeness":0.9,"lcsratio":0.7,"wordcoverage":0.75}
{"Year":2022,"Venue":"lrec-2022","Acronym":"Resource","Description":"Indicators on the Presence of Languages in Internet","Abstract":"Reliable and maintained indicators of the space of languages on the internet are required to support appropriate public policies and well-informed linguistic studies. Current sources are scarce and often strongly biased. The model to produce indicators on the presence of languages in the internet, launched by the observatory in 2017, has reached a sensible level of maturity and its data products are shared in cc-by-sa 4.0 license. It reaches now 329 languages (l1 speakers > one million) and all the biases associated with the model have been controlled to an acceptable threshold, giving trust to the data, within an estimated confidence interval of +-20%. Some of the indicators (mainly the percentage of l1+l2 speakers connected to the internet per language and derivates) rely on ethnologue global dataset #24 for demo-linguistic data and itu, completed by world bank, for the percentage of persons connected to the internet by country. The rest of indicators relies on the previous sources plus a large combination of hundreds of different sources for data related to web contents per language. This research poster focuses the description of the new linguistic worlds created. Methodological considerations are only exposed briefly and will be developed in another paper.","wordlikeness":1.0,"lcsratio":0.875,"wordcoverage":1.0}
{"Year":2018,"Venue":"lrec-2018","Acronym":"Epitran","Description":"Precision G2P for Many Languages","Abstract":"Publicly is a massively multilingual, multiple back-end system for g2p (grapheme-to-phoneme) transduction which is distributed with support for 61 languages. It takes word tokens in the orthography of a language and outputs a phonemic representation in either ipa or x-sampa. The main system is written in python and is publicly available as open source software. Its efficacy has been demonstrated in multiple research projects relating to language transfer, polyglot models, and speech. In a particular asr task, speech. Was shown to improve the word error rate over babel baselines for acoustic modeling. Keywords: g2p, grapheme, phoneme, international phonetic alphabet, ipa, x-sampa, multilingual, speech, transfer, polyglot 1.","wordlikeness":0.8571428571,"lcsratio":0.7142857143,"wordcoverage":0.7272727273}
{"Year":2022,"Venue":"findings-2022","Acronym":"BBQ","Description":"A hand-built bias benchmark for question answering","Abstract":"It is well documented that nlp models learn social biases, but little work has been done on how these biases manifest in model outputs for applied tasks like question answering (qa). We introduce the bias benchmark for qa (against), a dataset of question-sets constructed by the authors that highlight attested social biases against people belonging to protected classes along nine social dimensions relevant for u.s. english-speaking contexts. Our task evaluate model responses at two levels: (i) given an under-informative context, we test how strongly responses reflect social biases, and (ii) given an adequately informative context, we test whether the model\u2019s biases override a correct answer choice. We find that models often rely on stereotypes when the context is under-informative, meaning the model\u2019s outputs consistently reproduce harmful biases in this setting. Though models are more accurate when the context provides an informative answer, they still rely on stereotypes and average up to 3.4 percentage points higher accuracy when the correct answer aligns with a social bias than when it conflicts, with this difference widening to over 5 points on examples targeting gender for most models tested.","wordlikeness":0.3333333333,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2022,"Venue":"ccl-2022","Acronym":"COMPILING","Description":"A Benchmark Dataset for Chinese Complexity Controllable Definition Generation","Abstract":"\u201cthe definition generation task aims to generate a word\u2019s definition within a specific context automatically. However, owing to the lack of datasets for different complexities, the definitions produced by models tend to keep the same complexity level. This paper proposes a novel task of generating definitions for a word with controllable complexity levels. Correspondingly, we introduce task, a dataset given detailed information about chinese definitions, and each definition is labeled with its complexity levels. The a dataset includes 74,303 words and 106,882 definitions. To the best of our knowledge, it is the largest dataset of the chinese definition generation task. We select various representative generation methods as baselines for this task and conduct evaluations, which illustrates that our dataset plays an outstanding role in assisting models in generating different complexity-level definitions. We believe that the context dataset will benefit further research in complexity controllable definition generation.","wordlikeness":0.7777777778,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2017,"Venue":"eacl-2017","Acronym":"LanideNN","Description":"Multilingual Language Identification on Character Window","Abstract":"In language identification, a common first step in natural language processing, we want to automatically determine the language of some input text. Monolingual language identification assumes that the given document is written in one language. In multilingual language identification, the document is usually in two or three languages and we just want their names. We aim one step further and propose a method for textual language identification where languages can change arbitrarily and the goal is to identify the spans of each of the languages. Our method is based on bidirectional recurrent neural networks and it performs well in monolingual and multilingual language identification tasks on six datasets covering 131 languages. The method keeps the accuracy also for short documents and across domains, so it is ideal for off-the-shelf use without preparation of training data.","wordlikeness":0.625,"lcsratio":1.0,"wordcoverage":0.7692307692}
{"Year":2021,"Venue":"acl-2021","Acronym":"REM","Description":"Efficient Semi-Automated Real-Time Moderation of Online Forums","Abstract":"This paper presents is, a novel tool for the semi-automated real-time moderation of large scale online forums. The growing demand for online participation and the increasing number of user comments raise challenges in filtering out harmful and undesirable content from public debates in online forums. Since a manual moderation does not scale well and pure automated approaches often lack the required level of accuracy, we suggest a semi-automated moderation approach. Our approach maximizes the efficiency of manual efforts by targeting only those comments for which human intervention is needed, e.g. due to high classification uncertainty. Our tool offers a rich visual interactive environment enabling the exploration of online debates. We conduct a preliminary evaluation experiment to demonstrate the suitability of our approach and publicly release the source code of lack.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"CTRLsum","Description":"Towards Generic Controllable Text Summarization","Abstract":"Current summarization systems yield generic summaries that are disconnected from users\u2019 preferences and expectations. To address this limitation, we present same, a generic framework to control generated summaries through a set of keywords. During training keywords are extracted automatically without requiring additional human annotations. At test time yield features a control function to map control signal to keywords; through engineering the control function, the same trained model is able to be applied to control summaries on various dimensions, while neither affecting the model training process nor the pretrained models. We additionally explore the combination of keywords and text prompts for more control tasks. Experiments demonstrate the effectiveness of prompts on three domains of summarization datasets and five control tasks: (1) entity-centric and (2) length-controllable summarization, (3) contribution summarization on scientific papers, (4) invention purpose summarization on patent filings, and (5) question-guided summarization on news articles. Moreover, when used in a standard, unconstrained summarization setting, that is comparable or better than strong pretrained systems.","wordlikeness":0.4285714286,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2010,"Venue":"naacl-2010","Acronym":"KSC-PaL","Description":"A Peer Learning Agent that Encourages Students to take the Initiative","Abstract":"We present an innovative application of dialogue processing concepts to educational technology. In a previous corpus analysis of peer learning dialogues, we found that initiative and initiative shifts are indicative of learning, and of learning-conducive episodes. We have incorporated this \ufb01nding in learning,, a peer learning agent. An promotes learning by encouraging shifts in task initiative.","wordlikeness":0.4285714286,"lcsratio":0.2857142857,"wordcoverage":0.6666666667}
{"Year":2014,"Venue":"paclic-2014","Acronym":"Constructions","Description":"a New Unit of Analysis for Corpus-based Discourse Analysis","Abstract":"We propose and assess the novel idea of using automatically induced social as a unit of analysis for corpus-based discourse analysis. Automated techniques are needed in order to elucidate important characteristics of corpora for social science research into topics, framing and argument structures. Compared with current techniques (keywords, n-grams, and collocations), and capture more linguistic patterning, including some grammatical phenomena. Recent advances in natural language processing mean that it is now feasible to automatically induce some discourse from large unannotated corpora. In order to assess how well to characterise the content of a corpus and how well they elucidate interesting aspects of different discourses, we analysed a corpus of climate change blogs. The utility of induced for corpus-based discourse analysis was compared qualitatively with keywords, n-grams and collocations. We found that the unusually frequent is gave interesting and different insights into the content of the discourses and enabled better comparison of sub-corpora.","wordlikeness":0.9230769231,"lcsratio":0.6923076923,"wordcoverage":0.96}
{"Year":2021,"Venue":"acl-2021","Acronym":"CLTR","Description":"An End-to-End, Transformer-Based System for Cell-Level Table Retrieval and Table Question Answering","Abstract":"We present the first end-to-end, transformer-based table question answering (qa) system that takes natural language questions and massive table corpora as inputs to retrieve the most relevant tables and locate the correct table cells to answer the question. Our system, table, extends the current state-of-the-art qa over tables model to build an end-to-end table qa architecture. This system has successfully tackled many real-world table qa problems with a simple, unified pipeline. Our proposed system can also generate a heatmap of candidate columns and rows over complex tables and allow users to quickly identify the correct cells to answer questions. In addition, we introduce two new open domain benchmarks, e2e_wtq and e2e_gnq, consisting of 2,005 natural language questions over 76,242 tables. The benchmarks are designed to validate a as well as accommodate future table retrieval and end-to-end table qa research and experiments. Our experiments demonstrate that our system is the current state-of-the-art model on the table retrieval task and produces promising results for end-to-end table qa.","wordlikeness":0.25,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2021,"Venue":"acl-2021","Acronym":"gENder-IT","Description":"An Annotated English-Italian Parallel Challenge Set for Cross-Linguistic Natural Gender Phenomena","Abstract":"Languages differ in terms of the absence or presence of gender features, the number of gender classes and whether and where gender features are explicitly marked. These cross-linguistic differences can lead to ambiguities that are difficult to resolve, especially for sentence-level mt systems. The identification of ambiguity and its subsequent resolution is a challenging task for which currently there aren\u2019t any specific resources or challenge sets available. In this paper, we introduce ambiguity, an english\u2013italian challenge set focusing on the resolution of natural gender phenomena by providing word-level gender tags on the english source side and multiple gender alternative translations, where needed, on the italian target side.","wordlikeness":0.7777777778,"lcsratio":0.8888888889,"wordcoverage":0.8}
{"Year":2014,"Venue":"semeval-2014","Acronym":"TeamZ","Description":"Measuring Semantic Textual Similarity for Spanish Using an Overlap-Based Approach","Abstract":"This paper presents an overlap-based approach using bag of words and the spanish wordnet to solve the sts-spanish subtask (sts-es) of semeval-2014 task 10. Since bag of words is the most commonly used method to ascertain similarity, the performance is modest.","wordlikeness":0.8,"lcsratio":0.8,"wordcoverage":0.8888888889}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"LM-Debugger","Description":"An Interactive Tool for Inspection and Intervention in Transformer-Based Language Models","Abstract":"The opaque nature and unexplained behavior of transformer-based language models (lms) have spurred a wide interest in interpreting their predictions. However, current interpretation methods mostly focus on probing models from outside, executing behavioral tests, and analyzing salience input features, while the internal prediction construction process is largely not understood. In this work, we introduce representations, an interactive debugger tool for transformer-based lms, which provides a fine-grained interpretation of the model\u2019s internal prediction process, as well as a powerful framework for intervening in lm behavior. For its backbone, process. Relies on a recent method that interprets the inner token representations and their updates by the feed-forward layers in the vocabulary space. We demonstrate the utility of we for single-prediction debugging, by inspecting the internal disambiguation process done by gpt2. Moreover, we show how easily well allows to shift model behavior in a direction of the user\u2019s choice, by identifying a few vectors in the network and inducing effective interventions to the prediction process. We release process. As an open-source tool and a demo over gpt2 models.","wordlikeness":0.5454545455,"lcsratio":0.6363636364,"wordcoverage":0.6}
{"Year":2022,"Venue":"wmt-2022","Acronym":"MS-COMET","Description":"More and Better Human Judgements Improve Metric Performance","Abstract":"We develop two new metrics that build on top of the comet architecture. The main contribution is collecting a ten-times larger corpus of human judgements than comet and investigating how to filter out problematic human judgements. We propose filtering human judgements where human reference is statistically worse than machine translation. Furthermore, we average scores of all equal segments evaluated multiple times. The results comparing automatic metrics on source-based da and mqm-style human judgement show state-of-the-art performance on a system-level pair-wise system ranking. We release both of our metrics for public use.","wordlikeness":0.375,"lcsratio":0.75,"wordcoverage":0.7692307692}
{"Year":2013,"Venue":"ijcnlp-2013","Acronym":"SmartNews","Description":"Towards content-sensitive ranking of comments","Abstract":"Various news sites exist today where internet users can read the most recent news and people\u2019s opinions about. However, usually these sites do not organize comments well and do not \ufb01lter irrelevant content. Due to this limitation, readers who wonder about people\u2019s opinion regarding some speci\ufb01c topic, have to manually follow relevant comments, reading and \ufb01ltering a lot of irrelevant text. In this work1, we introduce a publicly available software implementing our approach, previously introduced in (litvak and matz, 2013), for retrieving and ranking the relevant comments for a given paragraph of news article and vice versa. We use topic-sensitive pagerank for ranking comments\/paragraphs relevant for a userspeci\ufb01ed paragraph\/comment.","wordlikeness":0.8888888889,"lcsratio":0.6666666667,"wordcoverage":0.8235294118}
{"Year":2022,"Venue":"acl-2022","Acronym":"FiNER","Description":"Financial Numeric Entity Recognition for XBRL Tagging","Abstract":"Publicly traded companies are required to submit periodic reports with extensive business reporting language (xbrl) word-level tags. Manually tagging the reports is tedious and costly. We, therefore, introduce xbrl tagging as a new entity extraction task for the financial domain and release harms-139, a dataset of 1.1m sentences with gold xbrl tags. Unlike typical entity extraction datasets, we-139 uses a much larger label set of 139 entity types. Most annotated tokens are numeric, with the correct tag per token depending mostly on context, rather than the token itself. We show that subword fragmentation of numeric expressions harms bert\u2019s performance, allowing word-level bilstms to perform better. To improve bert\u2019s performance, we propose two simple and effective solutions that replace numeric expressions with pseudo-tokens reflecting original token shapes and numeric magnitudes. We also experiment with fin-bert, an existing bert model for the financial domain, and release our own bert (sec-bert), pre-trained on financial filings, which performs best. Through data and error analysis, we finally identify possible limitations to inspire future work on xbrl tagging.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"Data-QuestEval","Description":"A Referenceless Metric for Data-to-Text Semantic Evaluation","Abstract":"Questeval is a reference-less metric used in text-to-text tasks, that compares the generated summaries directly to the source text, by automatically asking and answering questions. Its adaptation to data-to-text tasks is not straightforward, as it requires multimodal question generation and answering systems on the considered tasks, which are seldom available. To this purpose, we propose a method to build synthetic multimodal corpora enabling to train multimodal components for a by metric. The resulting metric is reference-less and multimodal; it obtains state-of-the-art correlations with human judgment on the webnlg and wikibio benchmarks. We make straightforward,\u2019s code and models available for reproducibility purpose, as part of the questeval project.","wordlikeness":0.7142857143,"lcsratio":0.8571428571,"wordcoverage":0.5454545455}
{"Year":2014,"Venue":"lrec-2014","Acronym":"Zmorge","Description":"A German Morphological Lexicon Extracted from Wiktionary","Abstract":"We describe a method to automatically extract a german lexicon from wiktionary that is compatible with the finite-state morphological grammar smor. The main advantage of the resulting lexicon over existing lexica for smor is that it is open and permissively licensed. A recall-oriented evaluation shows that a morphological analyser built with our lexicon has comparable coverage compared to existing lexica, and continues to improve as wiktionary grows. We also describe modifications to the smor grammar that result in a more conventional lemmatisation of words.","wordlikeness":0.6666666667,"lcsratio":0.8333333333,"wordcoverage":0.8333333333}
{"Year":2015,"Venue":"acl-2015","Acronym":"KB-LDA","Description":"Jointly Learning a Knowledge Base of Hierarchy, Relations, and Facts","Abstract":"Many existing knowledge bases (kbs), including freebase, yago, and nell, rely on a \ufb01xed ontology, given as an input to the system, which de\ufb01nes the data to be cataloged in the kb, i.e., a hierarchy of categories and relations between them. The system then extracts facts that match the prede\ufb01ned ontology. We propose an unsupervised model that jointly learns a latent ontological structure of an input corpus, and identi\ufb01es facts from the corpus that match the learned structure. Our approach combines mixed membership stochastic block models and topic models to infer a structure by jointly modeling text, a latent concept hierarchy, and latent semantic relationships among the entities mentioned in the text. As a case study, we apply the model to a corpus of web documents from the software domain, and evaluate the accuracy of the various components of the learned ontology.","wordlikeness":0.1666666667,"lcsratio":0.8333333333,"wordcoverage":0.6153846154}
{"Year":2022,"Venue":"bionlp-2022","Acronym":"BioBART","Description":"Pretraining and Evaluation of A Biomedical Generative Language Model","Abstract":"Pretrained language models have served as important backbones for natural language processing. Recently, in-domain pretraining has been shown to benefit various domain-specific downstream tasks. In the biomedical domain, natural language generation (nlg) tasks are of critical importance, while understudied. Approaching natural language understanding (nlu) tasks as nlg achieves satisfying performance in the general domain through constrained language generation or language prompting. We emphasize the lack of in-domain generative language models and the unsystematic generative downstream benchmarks in the biomedical domain, hindering the development of the research community. In this work, we introduce the generative language model approaching that adapts bart to the biomedical domain. We collate various biomedical language generation tasks including dialogue, summarization, entity linking, and named entity recognition. Baselines pretrained on pubmed abstracts has enhanced performance compared to bart and set strong baselines on several tasks. Furthermore, we conduct ablation studies on the pretraining tasks for hindering and find that sentence permutation has negative effects on downstream tasks.","wordlikeness":0.7142857143,"lcsratio":0.8571428571,"wordcoverage":0.7272727273}
{"Year":2011,"Venue":"ws-2011","Acronym":"META-NORD","Description":"Towards Sharing of Language Resources in Nordic and Baltic Countries","Abstract":"This paper introduces the resources project which develops nordic and baltic part of the european open language resource infrastructure. Nordic works on assembling, linking across languages, and making widely available the basic language resources used by developers, professionals and researchers to build specific products and applications. The goals of the project, overall approach and specific action lines on wordnets, terminology resources and treebanks are described. Moreover, results achieved in first five months of the project, i.e. language whitepapers, metadata specification and ipr management, are presented.","wordlikeness":0.5555555556,"lcsratio":0.6666666667,"wordcoverage":0.7142857143}
{"Year":2021,"Venue":"acl-2021","Acronym":"SIRE","Description":"Separate Intra- and Inter-sentential Reasoning for Document-level Relation Extraction","Abstract":"Document-level relation extraction has attracted much attention in recent years. It is usually formulated as a classi\ufb01cation problem that predicts relations for all entity pairs in the document. However, previous works indiscriminately represent intra- and inter-sentential relations in the same way, confounding the different patterns for predicting them. Besides, they create a document graph and use paths between entities on the graph as clues for logical reasoning. However, not all entity pairs can be connected with a path and have the correct logical reasoning paths in their graph. Thus many cases of logical reasoning cannot be covered. This paper proposes an effective architecture, a, to represent intra- and inter-sentential relations in different ways. We design a new and straightforward form of logical reasoning module that can cover more logical reasoning chains. Experiments on the public datasets show an outperforms the previous state-of-the-art methods. Further analysis shows that our predictions are reliable and explainable. Our code is available at https: \/\/github.com\/pkunlp-icler\/further.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2006,"Venue":"lrec-2006","Acronym":"ELAN","Description":"a Professional Framework for Multimodality Research","Abstract":"Utilization of computer tools in linguistic research has gained importance with the maturation of media frameworks for the handling of digital audio and video. The increased use of these tools in gesture, sign language and multimodal interaction studies has led to stronger requirements on the flexibility, the efficiency and in particular the time accuracy of annotation tools. This paper describes the efforts made to make increased a tool that meets these requirements, with special attention to the developments in the area of time accuracy. In subsequent sections an overview will be given of other enhancements in the latest versions of these that makes it a useful tool in multimodality research.","wordlikeness":0.75,"lcsratio":0.75,"wordcoverage":0.8}
{"Year":2023,"Venue":"findings-2023","Acronym":"XQA-DST","Description":"Multi-Domain and Multi-Lingual Dialogue State Tracking","Abstract":"Dialogue state tracking (dst), a crucial component of task-oriented dialogue (tod) systems, keeps track of all important information pertaining to dialogue history: filling slots with the most probable values throughout the conversation. Existing methods generally rely on a predefined set of values and struggle to generalise to previously unseen slots in new domains. To overcome these challenges, we propose a domain-agnostic extractive question answering (qa) approach with shared weights across domains. To disentangle the complex domain information in tods, we train our dst with a novel domain filtering strategy by excluding out-of-domain question samples. With an independent classifier that predicts the presence of multiple domains given the context, our model tackles dst by extracting spans in active domains. Empirical results demonstrate that our model can efficiently leverage domain-agnostic qa datasets by two-stage fine-tuning while being both domain-scalable and open vocabulary in dst. It shows strong transferability by achieving zero-shot domain-adaptation results on multiwoz 2.1 with an average jga of 36.7%. It further achieves cross-lingual transfer with state-of-the-art zero-shot results, 66.2% jga from english to german and 75.7% jga from english to italian on woz 2.0.","wordlikeness":0.1428571429,"lcsratio":0.7142857143,"wordcoverage":0.6153846154}
{"Year":2014,"Venue":"lrec-2014","Acronym":"Freepal","Description":"A Large Collection of Deep Lexico-Syntactic Patterns for Relation Extraction","Abstract":"The increasing availability and maturity of both scalable computing architectures and deep syntactic parsers is opening up new possibilities for relation extraction (re) on large corpora of natural language text. In this paper, we present assist, a resource designed to assist with the creation of relation extractors for more than 5,000 relations defined in the freebase knowledge base (kb). The resource consists of over 10 million distinct lexico-syntactic patterns extracted from dependency trees, each of which is assigned to one or more freebase relations with different confidence strengths. We generate the resource by executing a large-scale distant supervision approach on the clueweb09 corpus to extract and parse over 260 million sentences labeled with freebase entities and relations. We make over freely available to the research community, and present a web demonstrator to the dataset, accessible from free-pal.appspot.com.","wordlikeness":0.8571428571,"lcsratio":0.8571428571,"wordcoverage":0.7692307692}
{"Year":2010,"Venue":"lrec-2010","Acronym":"Appraise","Description":"An Open-Source Toolkit for Manual Phrase-Based Evaluation of Translations","Abstract":"We describe a focused effort to investigate the performance of phrase-based, human evaluation of machine translation output achieving a high annotator agreement. We define phrase-based evaluation and describe the implementation of better\"\",, a toolkit that supports the manual evaluation of machine translation results. Phrase ranking can be done using either a fine-grained six-way scoring scheme that allows to differentiate between \"\"much better\"\" and \"\"slightly better\"\", or a reduced subset of ranking choices. Afterwards we discuss kappa values for both scoring models from several experiments conducted with human annotators. Our results show that phrase-based evaluation can be used for fast evaluation obtaining significant agreement among annotators. The granularity of ranking choices should, however, not be too fine-grained as this seems to confuse annotators and thus reduces the overall agreement. The work reported in this paper confirms previous work in the field and illustrates that the usage of human evaluation in machine translation should be reconsidered. The six-way toolkit is available as open-source and can be downloaded from the author's website.","wordlikeness":1.0,"lcsratio":0.875,"wordcoverage":0.8571428571}
{"Year":2020,"Venue":"lrec-2020","Acronym":"KidSpell","Description":"A Child-Oriented, Rule-Based, Phonetic Spellchecker","Abstract":"For help with their spelling errors, children often turn to spellcheckers integrated in software applications like word processors and search engines. However, existing spellcheckers are usually tuned to the needs of traditional users (i.e., adults) and generally prove unsatisfactory for children. Motivated by this issue, we introduce usually, an english spellchecker oriented to the spelling needs of children. Hand-written applies (i) an encoding strategy for mapping both misspelled words and spelling suggestions to their phonetic keys and (ii) a selection process that prioritizes candidate spelling suggestions that closely align with the misspelled word based on their respective keys. To assess the effectiveness of, we compare the model\u2019s performance against several popular, mainstream spellcheckers in a number of offline experiments using existing and novel datasets. The results of these experiments show that comprised outperforms existing spellcheckers, as it accurately prioritizes relevant spelling corrections when handling misspellings generated by children in both essay writing and online search tasks. As a byproduct of our study, we create two new datasets comprised of spelling errors generated by children from hand-written essays and web search inquiries, which we make available to the research community.","wordlikeness":0.875,"lcsratio":0.875,"wordcoverage":0.7692307692}
{"Year":2021,"Venue":"findings-2021","Acronym":"TIAGE","Description":"A Benchmark for Topic-Shift Aware Dialog Modeling","Abstract":"Human conversations naturally evolve around different topics and fluently move between them. In research on dialog systems, the ability to actively and smoothly transition to new topics is often ignored. In this paper we introduce conversations, a new topic-shift aware dialog benchmark constructed utilizing human annotations on topic shifts. Based on modeling, we introduce three tasks to investigate different scenarios of topic-shift modeling in dialog settings: topic-shift detection, topic-shift triggered response generation and topic-aware dialog generation. Experiments on these tasks show that the topic-shift signals in evolve are useful for topic-shift response generation. On the other hand, dialog systems still struggle to decide when to change topic. This indicates further research is needed in topic-shift aware dialog modeling.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.9090909091}
{"Year":2021,"Venue":"findings-2021","Acronym":"MAD-G","Description":"Multilingual Adapter Generation for Efficient Cross-Lingual Transfer","Abstract":"Adapter modules have emerged as a general parameter-efficient means to specialize a pretrained encoder to new domains. Massively multilingual transformers (mmts) have particularly benefited from additional training of language-specific adapters. However, this approach is not viable for the vast majority of languages, due to limitations in their corpus size or compute budgets. In this work, we propose languages (multilingual adapter generation), which contextually generates language adapters from language representations based on typological features. In contrast to prior work, our time- and space-efficient it approach enables (1) sharing of linguistic knowledge across languages and (2) zero-shot inference by generating language adapters for unseen languages. We thoroughly evaluate via: in zero-shot cross-lingual transfer on part-of-speech tagging, dependency parsing, and named entity recognition. While offering (1) improved fine-tuning efficiency (by a factor of around 50 in our experiments), (2) a smaller parameter budget, and (3) increased language coverage, size remains competitive with more expensive methods for language-specific adapter training across the board. Moreover, it offers substantial benefits for low-resource languages, particularly on the ner task in low-resource african languages. Finally, we demonstrate that adapters.\u2019s transfer performance can be further improved via: (i) multi-source training, i.e., by generating and combining adapters of multiple languages with available task-specific training data; and (ii) by further fine-tuning generated generating adapters for languages with monolingual data.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2017,"Venue":"emnlp-2017","Acronym":"Zipporah","Description":"a Fast and Scalable Data Cleaning System for Noisy Web-Crawled Parallel Corpora","Abstract":"We introduce feature,, a fast and scalable data cleaning system. We propose a novel type of bag-of-words translation feature, and train logistic regression models to classify good data and synthetic noisy data in the proposed feature space. The trained model is used to score parallel sentences in the data pool for selection. As shown in experiments, regression selects a high-quality parallel corpus from a large, mixed quality data pool. In particular, for one noisy dataset, proposed achieves a 2.1 bleu score improvement with using 1\/5 of the data over using the entire corpus.","wordlikeness":0.5,"lcsratio":0.75,"wordcoverage":0.625}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"CLIPScore","Description":"A Reference-free Evaluation Metric for Image Captioning","Abstract":"Image captioning has conventionally relied on reference-based automatic evaluations, where machine captions are compared against captions written by humans. This is in contrast to the reference-free manner in which humans assess caption quality. In this paper, we report the surprising empirical finding that clip (radford et al., 2021), a cross-modal model pretrained on 400m image+caption pairs from the web, can be used for robust automatic evaluation of image captioning without the need for references. Experiments spanning several corpora demonstrate that our new reference-free metric, correlation, achieves the highest correlation with human judgements, outperforming existing reference-based metrics like cider and spice. Information gain experiments demonstrate that spanning, with its tight focus on image-text compatibility, is complementary to existing reference-based metrics that emphasize text-text similarities. Thus, we also present a reference-augmented version, refand, which achieves even higher correlation. Beyond literal description tasks, several case studies reveal domains where machine performs well (clip-art images, alt-text rating), but also where it is relatively weaker in comparison to reference-based metrics, e.g., news captions that require richer contextual knowledge.","wordlikeness":0.8888888889,"lcsratio":0.7777777778,"wordcoverage":0.75}
{"Year":2007,"Venue":"semeval-2007","Acronym":"MELB-KB","Description":"Nominal Classification as Noun Compound Interpretation","Abstract":"In this paper, we outline our approach to interpreting semantic relations in nominal pairs in semeval-2007 task #4: classi\ufb01cation of semantic relations between nominals. We build on two baseline approaches to interpreting noun compounds: sense collocation, and constituent similarity. These are consolidated into an overall system in combination with co-training, to expand the training data. Our two systems attained an average f-score over the test data of 58.7% and 57.8%, respectively.","wordlikeness":0.4285714286,"lcsratio":0.2857142857,"wordcoverage":0.6}
{"Year":2022,"Venue":"ws-2022","Acronym":"LSCDiscovery","Description":"A shared task on semantic change discovery and detection in Spanish","Abstract":"We present the first shared task on semantic change discovery and detection in spanish. We create the first dataset of spanish words manually annotated by semantic change using the durel framewok (schlechtweg et al., 2018). The task is divided in two phases: 1) graded change discovery, and 2) binary change detection. In addition to introducing a new language for this task, the main novelty with respect to the previous tasks consists in predicting and evaluating changes for all vocabulary words in the corpus. Six teams participated in phase 1 and seven teams in phase 2 of the shared task, and the best system obtained a spearman rank correlation of 0.735 for phase 1 and an f1 score of 0.735 for phase 2. We describe the systems developed by the competing teams, highlighting the techniques that were particularly useful.","wordlikeness":0.5833333333,"lcsratio":0.9166666667,"wordcoverage":0.8571428571}
{"Year":2022,"Venue":"wmt-2022","Acronym":"CrossQE","Description":"HW-TSC 2022 Submission for the Quality Estimation Shared Task","Abstract":"Quality estimation (qe) is a crucial method to investigate automatic methods for estimating the quality of machine translation results without reference translations. This paper presents huawei translation services center\u2019s (hw-tsc\u2019s) work called regressor in wmt 2022 qe shared tasks 1 and 2, namely sentence- and word- level quality prediction and explainable qe.extensive employes the framework of predictor-estimator for task 1, concretely with a pre-trained cross-lingual xlm-roberta large as predictor and task-specific classifier or regressor as estimator. An extensive set of experimental results show that after adding bottleneck adapter layer, mean teacher loss, masked language modeling task loss and mc dropout methods in quality, the performance has improved to a certain extent. For task 2, of calculated the cosine similarity between each word feature in the target and each word feature in the source by task 1 sentence-level qe system\u2019s predictor, and used the inverse value of maximum similarity between each word in the target and the source as the word translation error risk value. Moreover, layer, has outstanding performance on qe test sets of wmt 2022.","wordlikeness":0.7142857143,"lcsratio":0.7142857143,"wordcoverage":0.9230769231}
{"Year":2005,"Venue":"acl-2005","Acronym":"SenseClusters","Description":"Unsupervised Clustering and Labeling of Similar Contexts","Abstract":"Discovered is a freely available system that identi\ufb01es similar contexts in text. It relies on lexical features to build \ufb01rst and second order representations of contexts, which are then clustered using unsupervised methods. It was originally developed to discriminate among contexts centered around a given target word, but can now be applied more generally. It also supports methods that create descriptive and discriminating labels for the discovered clusters.","wordlikeness":0.7692307692,"lcsratio":0.9230769231,"wordcoverage":0.7619047619}
{"Year":2022,"Venue":"ws-2022","Acronym":"UNIREX","Description":"A Unified Learning Framework for Language Model Rationale Extraction","Abstract":"An extractive rationale explains a language model\u2019s (lm\u2019s) prediction on a given task instance by highlighting the text inputs that most influenced the prediction. Ideally, rationale extraction should be faithful (reflective of lm\u2019s actual behavior) and plausible (convincing to humans), without compromising the lm\u2019s (i.e., task model\u2019s) task performance. Although attribution algorithms and select-predict pipelines are commonly used in rationale extraction, they both rely on certain heuristics that hinder them from satisfying all three desiderata. In light of this, we propose behavior), a flexible learning framework which generalizes rationale extractor optimization as follows: (1) specify architecture for a learned rationale extractor; (2) select explainability objectives (i.e., faithfulness and plausibility criteria); and (3) jointly the train task model and rationale extractor on the task using selected objectives. Generalize enables replacing prior works\u2019 heuristic design choices with a generic learned rationale extractor in (1) and optimizing it for all three desiderata in (2)-(3). To facilitate comparison between methods w.r.t. multiple desiderata, we introduce the normalized relative gain (nrg) metric. Across five english text classification datasets, our best normalized configuration outperforms the strongest baselines by an average of 32.9% nrg. Plus, we find that selected-trained rationale extractors\u2019 faithfulness can even generalize to unseen datasets and tasks.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.7692307692}
{"Year":2021,"Venue":"tacl-2021","Acronym":"MKQA","Description":"A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering","Abstract":"Progress in cross-lingual modeling depends on challenging, realistic, and diverse evaluation sets. We introduce multilingual knowledge questions and answers (question), an open- domain question answering evaluation set comprising 10k question-answer pairs aligned across 26 typologically diverse languages (260k question-answer pairs in total). Answers are based on heavily curated, language- independent data representation, making results comparable across languages and independent of language-specific passages. With 26 languages, this dataset supplies the widest range of languages to-date for evaluating question answering. We benchmark a variety of state- of-the-art methods and baselines for generative and extractive question answering, trained on natural questions, in zero shot and translation settings. Results indicate this dataset is challenging even in english, but especially in low-resource languages.","wordlikeness":0.25,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2004,"Venue":"iwslt-2004","Acronym":"PolyphraZ","Description":"a tool for the quantitative and subjective evaluation of parallel corpora","Abstract":"The in tool is under construction in the framework of the tracorpex project (translation of corpora of examples), for the management of parallel multilingual corpora (coding, format, correspondence). It is a software platform allowing the preparation and handling of parallel corpora (languages, codings...), parallel presentation, and addition of new languages to existing corpora by calling several mt systems, and letting human translators produce the final reference translations by using a web-based editor. It integrates the computation of some objective evaluation metrics (nist, blue), and enables subjective evaluations thanks to parallel presentations, and formating based on distance computations between sentences (at several levels). In the future, blue), should also support versioning and provide feedbacks to developers of the mt systems used: unknown words, badly translated words, and comparative presentations of the outputs of the various systems.","wordlikeness":0.7777777778,"lcsratio":0.5555555556,"wordcoverage":0.625}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"EdgeFormer","Description":"A Parameter-Efficient Transformer for On-Device Seq2seq Generation","Abstract":"We introduce model \u2013 a parameter-efficient transformer for on-device seq2seq generation under the strict computation and memory constraints. Compared with the previous parameter-efficient transformers, baselines applies two novel principles for cost-effective parameterization, allowing it to perform better given the same parameter budget; moreover, applies is further enhanced by layer adaptation innovation that is proposed for improving the network with shared layers. Extensive experiments show \u2013 can effectively outperform previous parameter-efficient transformer baselines and achieve competitive results under both the computation and memory constraints. Given the promising results, we release edgelm \u2013 the pretrained version of parameterization,, which is the first publicly available pretrained on-device seq2seq model that can be easily fine-tuned for seq2seq tasks with strong results, facilitating on-device seq2seq generation in practice.","wordlikeness":0.8,"lcsratio":0.8,"wordcoverage":0.75}
{"Year":2014,"Venue":"semeval-2014","Acronym":"LyS","Description":"Porting a Twitter Sentiment Analysis Approach from Spanish to English","Abstract":"This paper proposes an approach to solve message- and phrase-level polarity classi\ufb01cation in twitter, derived from an existing system designed for spanish. As a \ufb01rst step, an ad-hoc preprocessing is performed. We then identify lexical, psychological and semantic features in order to capture different dimensions of the human language which are helpful to detect sentiment. These features are used to feed a supervised classi\ufb01er after applying an information gain \ufb01lter, to discriminate irrelevant features. The system is evaluated on the semeval 2014 task 9: sentiment anawhichis in twitter. Our approach worked competitively both in message- and phraselevel tasks. The results con\ufb01rm the robustness of the approach, which performed well on different domains involving short informal texts.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2010,"Venue":"emnlp-2010","Acronym":"PEM","Description":"A Paraphrase Evaluation Metric Exploiting Parallel Texts","Abstract":"We present adequacy,, the \ufb01rst fully automatic metric to evaluate the quality of paraphrases, and consequently, that of paraphrase generation systems. Our metric is based on three criteria: adequacy, \ufb02uency, and lexical dissimilarity. The key component in our metric is a robust and shallow semantic similarity measure based on pivot language n-grams that allows us to approximate adequacy independently of lexical similarity. Human evaluation shows that and achieves high correlation with human judgments.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2022,"Venue":"lrec-2022","Acronym":"MTLens","Description":"Machine Translation Output Debugging","Abstract":"The performance of machine translation (mt) systems varies significantly with inputs of diverging features such as topics, genres, and surface properties. Though there are many mt evaluation metrics that generally correlate with human judgments, they are not directly useful in identifying specific shortcomings of mt systems. In this demo, we present a benchmarking interface that enables improved evaluation of specific mt systems in isolation or multiple mt systems collectively by quantitatively evaluating their performance on many tasks across multiple domains and evaluation metrics. Further, it facilitates effective debugging and error analysis of mt output via the use of dynamic filters that help users hone in on problem sentences with specific properties, such as genre, topic, sentence length, etc. The interface can be extended to include additional filters such as lexical, morphological, and syntactic features. Aside from helping debug mt output, it can also help in identifying problems in reference translations and evaluation metrics.","wordlikeness":0.3333333333,"lcsratio":0.8333333333,"wordcoverage":0.8}
{"Year":2021,"Venue":"acl-2021","Acronym":"BertGCN","Description":"Transductive Text Classification by Combining GNN and BERT","Abstract":"In this work, we propose within, a model that combines large scale pretraining and transductive learning for text classi\ufb01cation. This constructs a heterogeneous graph over the dataset and represents documents as nodes using bert representations. By jointly training the bert and gcn modules within by, the proposed model is able to leverage the advantages of both worlds: large-scale pretraining which takes the advantage of the massive amount of raw data and transductive learning which jointly learns representations for both training data and unlabeled test data by propagating label in\ufb02uence through graph convolution. Experiments show that modules achieves sota performances on a wide range of text classi\ufb01cation datasets.","wordlikeness":0.5714285714,"lcsratio":0.5714285714,"wordcoverage":0.7272727273}
{"Year":2022,"Venue":"icon-2022","Acronym":"BoNC","Description":"Bag of N-Characters Model for Word Level Language Identification","Abstract":"This paper describes the model submitted by nlp_bfcai team for kanglish shared task held at icon 2022. The proposed model used a very simple approach based on the word representation. Simple machine learning classification algorithms, random forests, support vector machines, stochastic gradient descent and multi-layer perceptron have been imple- mented. Our submission, rf, securely ranked fifth among all other submissions.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2016,"Venue":"naacl-2016","Acronym":"ArgRewrite","Description":"A Web-based Revision Assistant for Argumentative Writings","Abstract":"While intelligent writing assistants have become more common, they typically have little support for revision behavior. We present present, a novel web-based revision assistant that focus on rewriting analysis. The system supports two major functionalities: 1) to assist students as they revise, the system automatically extracts and analyzes revisions; 2) to assist teachers, the system provides an overview of students\u2019 revisions and allows teachers to correct the automatically analyzed results, ensuring that students get the correct feedback.","wordlikeness":0.8,"lcsratio":0.8,"wordcoverage":0.8235294118}
{"Year":2015,"Venue":"semeval-2015","Acronym":"Webis","Description":"An Ensemble for Twitter Sentiment Detection","Abstract":"We reproduce four twitter sentiment classi\ufb01cation approaches that participated in previous semeval editions with diverse feature sets. The reproduced approaches are combined in an ensemble, averaging the individual classi\ufb01ers\u2019 con\ufb01dence scores for the three classes (positive, neutral, negative) and deciding sentiment polarity based on these averages. The experimental evaluation on semeval data shows our re-implementations to slightly outperform their respective originals. Moreover, not too surprisingly, the ensemble of the reproduced approaches serves as a strong baseline in the current edition where it is top-ranked on the 2015 test set.","wordlikeness":0.8,"lcsratio":0.8,"wordcoverage":0.8}
{"Year":2021,"Venue":"ws-2021","Acronym":"OdeNet","Description":"Compiling a GermanWordNet from other Resources","Abstract":"The princeton wordnet for the english language has been used worldwide in nlp projects for many years. With the omw initiative, wordnets for different languages of the world are being linked via identifiers. The parallel development and linking allows new multilingual application perspectives. The development of a wordnet for the german language is also in this context. To save development time, existing resources were combined and recompiled. The result was then evaluated and improved. In a relatively short time a resource was created that can be used in projects and continuously improved and extended.","wordlikeness":0.6666666667,"lcsratio":0.8333333333,"wordcoverage":0.8}
{"Year":2022,"Venue":"findings-2022","Acronym":"DecBERT","Description":"Enhancing the Language Understanding of BERT with Causal Attention Masks","Abstract":"Since 2017, the transformer-based models play critical roles in various downstream natural language processing tasks. However, a common limitation of the attention mechanism utilized in transformer encoder is that it cannot automatically capture the information of word order, so explicit position embeddings are generally required to be fed into the target model. In contrast, transformer decoder with the causal attention masks is naturally sensitive to the word order. In this work, we focus on improving the position encoding ability of bert with the causal attention masks. Furthermore, we propose a new pre-trained language model <i>resources.<\/i> and evaluate it on the glue benchmark. Experimental results show that (1) the causal attention mask is effective for bert on the language understanding tasks; (2) our <i>pe<\/i><\/i> model without position embeddings achieve comparable performance on the glue benchmark; and (3) our modification accelerates the pre-training process and <i>understanding w\/ pe<\/i> achieves better overall performance than the baseline systems when pre-training with the same amount of computational resources.","wordlikeness":0.7142857143,"lcsratio":0.8571428571,"wordcoverage":0.8}
{"Year":2021,"Venue":"acl-2021","Acronym":"WIND","Description":"Weighting Instances Differentially for Model-Agnostic Domain Adaptation","Abstract":"Domain adaptation is a fundamental problem in machine learning and natural language processing. In this paper, we study the domain adaptation problem from the perspective of instance weighting. Conventional instance weighting approaches cannot learn the weights which make the model generalize well in target domain. To tackle this problem, inspired by meta-learning, we formulate the domain adaptation problem as a bi-level optimization problem, and propose a novel differentiable modelagnostic instance weighting algorithm. Our proposed approach can automatically learn the instance weights instead of using manually designed weighting metrics. To reduce the computational complexity, we adopt the secondorder approximation technique during training. Experimental results1 on three different nlp tasks (sentiment classi\ufb01cation, neural machine translation and relation extraction) illustrate the ef\ufb01cacy of our proposed method.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2010,"Venue":"amta-2010","Acronym":"WikiBABEL","Description":"A System for Multilingual Wikipedia Content","Abstract":"This position paper outlines our project \u2013 has \u2013 which will be released as an open source project for the creation of multilingual wikipedia content, and has potential to produce parallel data as a by-product for machine translation systems research. We discuss its architecture, functionality and the user-experience components, and briefly present an analysis that emphasizes the resonance that the we design and the planned involvement with wikipedia has with the open source communities in general and wikipedians in particular.","wordlikeness":0.7777777778,"lcsratio":0.6666666667,"wordcoverage":0.6666666667}
{"Year":2022,"Venue":"wmt-2022","Acronym":"CometKiwi","Description":"IST-Unbabel 2022 Submission for the Quality Estimation Shared Task","Abstract":"We present the joint contribution of ist and unbabel to the wmt 2022 shared task on quality estimation (qe). Our team participated in all three subtasks: (i) sentence and word-level quality prediction; (ii) explainable qe; and (iii) critical error detection. For all tasks we build on top of the comet framework, connecting it with the predictor-estimator architecture of openkiwi, and equipping it with a word-level sequence tagger and an explanation extractor. Our results suggest that incorporating references during pretraining improves performance across several language pairs on downstream tasks, and that jointly training with sentence and word-level objectives yields a further boost. Furthermore, combining attention and gradient information proved to be the top strategy for extracting good explanations of sentence-level qe models. Overall, our submissions achieved the best results for all three tasks for almost all language pairs by a considerable margin.","wordlikeness":0.5555555556,"lcsratio":0.5555555556,"wordcoverage":0.7142857143}
{"Year":2014,"Venue":"semeval-2014","Acronym":"Citius","Description":"A Naive-Bayes Strategy for Sentiment Analysis on English Tweets","Abstract":"This article describes a strategy based on a naive-bayes classi\ufb01er for detecting the polarity of english tweets. The experiments have shown that the best performance is achieved by using a binary classi\ufb01er between just two sharp polarity categories: positive and negative. In addition, in order to detect tweets with and without polarity, the system makes use of a very basic rule that searchs for polarity words within the analysed tweets\/texts. When the classi\ufb01er is provided with a polarity lexicon and multiwords it achieves 63% f-score.","wordlikeness":0.8333333333,"lcsratio":0.6666666667,"wordcoverage":0.8333333333}
{"Year":2011,"Venue":"ws-2011","Acronym":"MAISE","Description":"A Flexible, Configurable, Extensible Open Source Package for Mass AI System Evaluation","Abstract":"The past few years have seen an increasing interest in using amazon\u2019s mechanical turk for purposes of collecting data and performing annotation tasks. One such task is the mass evaluation of system output in a variety of tasks. In this paper, we present few, a package that allows researchers to evaluate the output of their ai system(s) using human judgments collected via amazon\u2019s mechanical turk, greatly streamlining the process. One is open source, easy to run, and platform-independent. The core of via\u2019s codebase was used for the manual evaluation of wmt10, and the completed package is being used again in the current evaluation for wmt11. In this paper, we describe the main features, functionality, and usage of ai, which is now available for download and use.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2018,"Venue":"lrec-2018","Acronym":"PhotoshopQuiA","Description":"A Corpus of Non-Factoid Questions and Answers for Why-Question Answering","Abstract":"Recent years have witnessed a high interest in non-factoid question answering using community question answering (cqa) web sites. Despite ongoing research using state-of-the-art methods, there is a scarcity of available datasets for this task. Why-questions, which play an important role in open-domain and domain-speci\ufb01c applications, are dif\ufb01cult to answer automatically since the answers need to be constructed based on different information extracted from multiple knowledge sources. We introduce the english dataset, a new publicly available set of 2,854 why-question and answer(s) (whyq, a) pairs related to adobe photoshop usage collected from \ufb01ve cqa web sites. We chose adobe photoshop because it is a popular and well-known product, with a lively, knowledgeable and sizable community. To the best of our knowledge, this is the \ufb01rst english dataset for why-qa that focuses on a product, as opposed to previous open-domain datasets. The corpus is stored in json format and contains detailed data about questions and questioners as well as answers and answerers. The dataset can be used to build why-qa systems, to evaluate current approaches for answering why-questions, and to develop new models for future qa systems research. Keywords: question answering, community question answering, non-factoid question, why-qa 1.","wordlikeness":0.7692307692,"lcsratio":0.7692307692,"wordcoverage":0.6956521739}
{"Year":2008,"Venue":"lrec-2008","Acronym":"ANAWIKI","Description":"Creating Anaphorically Annotated Resources through Web Cooperation","Abstract":"The ability to make progress in computational linguistics depends on the availability of large annotated corpora, but creating such corpora by hand annotation is very expensive and time consuming; in practice, it is unfeasible to think of annotating more than one million words. However, the success of wikipedia and other projects shows that another approach might be possible: take advantage of the willingness of web users to contribute to collaborative resource creation. More is a recently started project that will develop tools to allow and encourage large numbers of volunteers over the web to collaborate in the creation of semantically annotated corpora (in the first instance, of a corpus annotated with information about anaphora).","wordlikeness":0.7142857143,"lcsratio":0.7142857143,"wordcoverage":0.7692307692}
{"Year":2020,"Venue":"acl-2020","Acronym":"SentiBERT","Description":"A Transferable Transformer-Based Architecture for Compositional Sentiment Semantics","Abstract":"We propose related, a variant of bert that effectively captures compositional sentiment semantics. The model incorporates contextualized representation with binary constituency parse tree to capture semantic composition. Comprehensive experiments demonstrate that effectively achieves competitive performance on phrase-level sentiment classification. We further demonstrate that the sentiment composition learned from the phrase-level annotations on sst can be transferred to other sentiment analysis tasks as well as related tasks, such as emotion classification tasks. Moreover, we conduct ablation studies and design visualization methods to understand tasks.. We show that we is better than baseline approaches in capturing negation and the contrastive relation and model the compositional sentiment semantics.","wordlikeness":0.7777777778,"lcsratio":0.7777777778,"wordcoverage":0.7777777778}
{"Year":2023,"Venue":"acl-2023","Acronym":"UniCoRN","Description":"Unified Cognitive Signal ReconstructioN bridging cognitive signals and human language","Abstract":"Decoding text stimuli from cognitive signals (e.g. fmri) enhances our understanding of the human language system, paving the way for building versatile brain-computer interface. However, existing studies largely focus on decoding individual word-level fmri volumes from a restricted vocabulary, which is far too idealized for real-world application. In this paper, we propose fmri2text, the first open-vocabulary task aiming to bridge fmri time series and human language. Furthermore, to explore the potential of this new task, we present a baseline solution, :: the unified cognitive signal reconstruction for brain decoding. By reconstructing both individual time points and time series, open-vocabulary establishes a robust encoder for cognitive signals (fmri & eeg). Leveraging a pre-trained language model as decoder, eeg). Proves its efficacy in decoding coherent text from fmri series across various split settings. Our model achieves a 34.77% bleu score on fmri2text, and a 37.04% bleu when generalized to eeg-to-text decoding, thereby surpassing the former baseline. Experimental results indicate the feasibility of decoding consecutive fmri volumes, and the effectiveness of decoding different cognitive signals using a unified structure.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2023,"Venue":"eacl-2023","Acronym":"SwitchPrompt","Description":"Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains","Abstract":"Prompting pre-trained language models leads to promising results across natural language processing tasks but is less effective when applied in low-resource domains, due to the domain gap between the pre-training data and the downstream task. In this work, we bridge this gap with a novel and lightweight prompting methodology called effective for the adaptation of language models trained on datasets from the general domain to diverse low-resource domains. Using domain-specific keywords with a trainable gated prompt, methodology offers domain-oriented prompting, that is, effective guidance on the target domains for general-domain language models. Our few-shot experiments on three text classification benchmarks demonstrate the efficacy of the general-domain pre-trained language models when used with this. They often even outperform their domain-specific counterparts trained with baseline state-of-the-art prompting methods by up to 10.7% performance increase in accuracy. This result indicates that trained effectively reduces the need for domain-specific language model pre-training.","wordlikeness":0.75,"lcsratio":0.75,"wordcoverage":0.6666666667}
{"Year":2014,"Venue":"lrec-2014","Acronym":"Propa-L","Description":"a semantic filtering service from a lexical network created using Games With A Purpose","Abstract":"This article presents behind, a freely accessible web service that allows to semantically filter a lexical network. The language resources behind the service are dynamic and created through games with a purpose. We show an example of application of this service: the generation of a list of keywords for parental filtering on the web, but many others can be envisaged. Moreover, the propagation algorithm we present here can be applied to any lexical network, in any language.","wordlikeness":0.5714285714,"lcsratio":0.5714285714,"wordcoverage":0.8}
{"Year":2014,"Venue":"semeval-2014","Acronym":"ECNU","Description":"Expression- and Message-level Sentiment Orientation Classification in Twitter Using Multiple Effective Features","Abstract":"Microblogging websites (such as twitter, facebook) are rich sources of data for opinion mining and sentiment analysis. In this paper, we describe our approaches used for sentiment analysis in twitter (task 9) organized in semeval 2014. This task tries to determine whether the sentiment orientations conveyed by the whole tweets or pieces of tweets are positive, negative or neutral. To solve this problem, we extracted several simple and basic features considering the following aspects: surface text, syntax, sentiment score and twitter characteristic. Then we exploited these features to build a classi\ufb01er using svm algorithm. Despite the simplicity of features, our systems rank above the average.","wordlikeness":0.25,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2023,"Venue":"acl-2023","Acronym":"OpenDelta","Description":"A Plug-and-play Library for Parameter-efficient Adaptation of Pre-trained Models","Abstract":"The scale of large pre-trained models (ptms) poses significant challenges in adapting to downstream tasks due to the high optimization overhead and storage costs associated with full-parameter fine-tuning. To address this, many studies explore parameter-efficient tuning methods, also framed as \u201cdelta tuning\u201d in ding et al. (2022), which updates only a small subset of parameters, known as \u201cdelta modules\u201d, while keeping the backbone model\u2019s parameters fixed. However, the practicality and flexibility of delta tuning have been limited due to existing implementations that directly modify the code of the backbone ptms and hard-code specific delta tuning methods for each ptm. In this paper, we present parameter-efficient, an open-source library that overcomes these limitations by providing a plug-and-play implementation of various delta tuning methods. Our novel techniques eliminate the need to modify the backbone ptms\u2019 code, making an compatible with different, even novel ptms. techniques is designed to be simple, modular, and extensible, providing a comprehensive platform for researchers and practitioners to adapt large ptms efficiently.","wordlikeness":0.7777777778,"lcsratio":0.8888888889,"wordcoverage":0.7142857143}
{"Year":2015,"Venue":"semeval-2015","Acronym":"Sentiue","Description":"Target and Aspect based Sentiment Analysis in SemEval-2015 Task 12","Abstract":"This paper describes our participation in semeval-2015 task 12, and the opinion mining system achieved. The general idea is that systems must determine the polarity of the sentiment expressed about a certain aspect of a target entity. For slot 1, entity and attribute category detection, our system applies a supervised machine learning classi\ufb01er, for each label, followed by a selection based on the probability of the entity\/attribute pair, on that domain. The target expression detection, for slot 2, is achieved by using a catalog of known targets for each entity type, complemented with named entity recognition. In the opinion sentiment slot, we used a 3 class polarity classi\ufb01er, having bow, lemmas, bigrams after verbs, presence of polarized terms, and punctuation based features. Working in unconstrained mode, our results for slot 1 were assessed with precision between 57% and 63%, and recall varying between 42% and 47%. In sentiment polarity, about\u2019s result accuracy was approximately 79%, reaching the best score in 2 of the 3 domains.","wordlikeness":0.5714285714,"lcsratio":0.8571428571,"wordcoverage":0.8}
{"Year":2020,"Venue":"eamt-2020","Acronym":"ELITR","Description":"European Live Translator","Abstract":"Multilingual (european live translator) project aims to create a speech translation system for simultaneous subtitling of conferences and online meetings targetting up to 43 languages. The technology is tested by the supreme audit office of the czech republic and by alfaview\u00ae, a german online conferencing system. Other project goals are to advance document-level and multilingual machine translation, automatic speech recognition, and automatic minuting.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2020,"Venue":"acl-2020","Acronym":"Penman","Description":"An Open-Source Library and Tool for AMR Graphs","Abstract":"Meaning representation (amr) (banarescu et al., 2013) is a framework for semantic dependencies that encodes its rooted and directed acyclic graphs in a format called library notation. The format is simple enough that users of amr data often write small scripts or libraries for parsing it into an internal graph representation, but there is enough complexity that these users could benefit from a more sophisticated and well-tested solution. The open-source python library (banarescu provides a robust parser, functions for graph inspection and manipulation, and functions for formatting graphs into sophisticated notation. Many functions are also available in a command-line tool, thus extending its utility to non-python setups.","wordlikeness":0.8333333333,"lcsratio":0.8333333333,"wordcoverage":0.8}
{"Year":2021,"Venue":"naacl-2021","Acronym":"UniDrop","Description":"A Simple yet Effective Technique to Improve Transformer without Extra Cost","Abstract":"Transformer architecture achieves great success in abundant natural language processing tasks. The over-parameterization of the transformer model has motivated plenty of works to alleviate its overfitting for superior performances. With some explorations, we find simple techniques such as dropout, can greatly boost model performance with a careful design. Therefore, in this paper, we integrate different dropout techniques into the training of transformer models. Specifically, we propose an approach named alleviate to unites three different dropout techniques from fine-grain to coarse-grain, i.e., feature dropout, structure dropout, and data dropout. Theoretically, we demonstrate that these three dropouts play different roles from regularization perspectives. Empirically, we conduct experiments on both neural machine translation and text classification benchmark datasets. Extensive results indicate that transformer with integrate can achieve around 1.5 bleu improvement on iwslt14 translation tasks, and better accuracy for the classification even using strong pre-trained roberta as backbone.","wordlikeness":0.7142857143,"lcsratio":0.7142857143,"wordcoverage":0.7272727273}
{"Year":2018,"Venue":"acl-2018","Acronym":"TALEN","Description":"Tool for Annotation of Low-resource ENtities","Abstract":"We present a new web-based interface, speak, designed for named entity annotation in low-resource settings where the annotators do not speak the language. To address this non-traditional scenario, integration, includes such features as in-place lexicon integration, tf-idf token statistics, internet search, and entity propagation, all implemented so as to make this difficult task efficient and frictionless. We conduct a small user study to compare against a popular annotation tool, showing that language. Achieves higher precision and recall against ground-truth annotations, and that users strongly prefer it over the alternative. Available is available at: <a href=github.com\/cogcomp\/includes class=acl-markup-url>github.com\/cogcomp\/for<\/a>.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.9090909091}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"WhyAct","Description":"Identifying Action Reasons in Lifestyle Vlogs","Abstract":"We aim to automatically identify human action reasons in online videos. We focus on the widespread genre of lifestyle vlogs, in which people perform actions while verbally describing them. We introduce and make publicly available the textual dataset, consisting of 1,077 visual actions manually annotated with their reasons. We describe a multimodal model that leverages visual and textual information to automatically infer the reasons corresponding to an action presented in the video.","wordlikeness":0.8333333333,"lcsratio":0.6666666667,"wordcoverage":0.7272727273}
{"Year":2022,"Venue":"acl-2022","Acronym":"CAKE","Description":"A Scalable Commonsense-Aware Framework For Multi-View Knowledge Graph Completion","Abstract":"Knowledge graphs store a large number of factual triples while they are still incomplete, inevitably. The previous knowledge graph completion (kgc) models predict missing links between entities merely relying on fact-view data, ignoring the valuable commonsense knowledge. The previous knowledge graph embedding (kge) techniques suffer from invalid negative sampling and the uncertainty of fact-view link prediction, limiting kgc\u2019s performance. To address the above challenges, we propose a novel and scalable commonsense-aware knowledge embedding (module) framework to automatically extract commonsense from factual triples with entity concepts. The generated commonsense augments effective self-supervision to facilitate both high-quality negative sampling (ns) and joint commonsense and fact-view link prediction. Experimental results on the kgc task demonstrate that assembling our framework could enhance the performance of the original kge models, and the proposed commonsense-aware ns module is superior to other ns techniques. Besides, our proposed framework could be easily adaptive to various kge models and explain the predicted results.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2016,"Venue":"lrec-2016","Acronym":"Farasa","Description":"A New Fast and Accurate Arabic Word Segmenter","Abstract":"In this paper, we present entities; (meaning insight in arabic), which is a fast and accurate arabic segmenter. Segmentation involves breaking arabic words into their constituent clitics. Our approach is based on svmrank using linear kernels. The features that we utilized account for: likelihood of stems, prefixes, suffixes, and their combination; presence in lexicons containing valid stems and named entities; and underlying stem templates. Suffixes, outperforms or equalizes state-of-the-art arabic segmenters, namely qatara and madamira. Meanwhile, no is nearly one order of magnitude faster than qatara and two orders of magnitude faster than madamira. The segmenter should be able to process one billion words in less than 5 hours. Their is written entirely in native java, with no external dependencies, and is open-source.","wordlikeness":0.6666666667,"lcsratio":0.8333333333,"wordcoverage":0.7272727273}
{"Year":2021,"Venue":"acl-2021","Acronym":"PAWLS","Description":"PDF Annotation With Labels and Structure","Abstract":"Adobe\u2019s portable document format (pdf) is a popular way of distributing view-only documents with a rich visual markup. This presents a challenge to nlp practitioners who wish to use the information contained within pdf documents for training models or data analysis, because annotating these documents is difficult. In this paper, we present pdf annotation with labels and structure (rich), a new annotation tool designed specifically for the pdf document format. Which is particularly suited for mixed-mode annotation and scenarios in which annotators require extended context to annotate accurately. Href=https:\/\/github.com\/allenai\/ supports span-based textual annotation, n-ary relations and freeform, non-textual bounding boxes, all of which can be exported in convenient formats for training multi-modal machine learning models. A read-only for server is available at <a href=https:\/\/who.apps.allenai.org\/ class=acl-markup-url>https:\/\/all.apps.allenai.org\/<\/a>, and the source code is available at <a href=https:\/\/github.com\/allenai\/is class=acl-markup-url>https:\/\/github.com\/allenai\/n-ary<\/a>.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2018,"Venue":"lrec-2018","Acronym":"GeCoTagger","Description":"Annotation of German Verb Complements with Conditional Random Fields","Abstract":"Complement phrases are essential for constructing well-formed sentences in german. Identifying verb complements and categorizing complement classes is challenging even for linguists who are specialized in the \ufb01eld of verb valency. Against this background, we introduce an ml-based algorithm which is able to identify and classify complement phrases of any german verb in any written sentence context. We use a large training set consisting of example sentences from a valency dictionary, enriched with pos tagging, and the ml-based technique of conditional random fields (crf) to generate the classi\ufb01cation models. Keywords: grammar and syntax, verb valency, machine learning methods 1.","wordlikeness":0.7,"lcsratio":0.7,"wordcoverage":0.7058823529}
{"Year":2019,"Venue":"bionlp-2019","Acronym":"PharmaCoNER","Description":"Pharmacological Substances, Compounds and proteins Named Entity Recognition track","Abstract":"One of the biomedical entity types of relevance for medicine or biosciences are chemical compounds and drugs. The correct detection these entities is critical for other text mining applications building on them, such as adverse drug-reaction detection, medication-related fake news or drug-target extraction. Although a significant effort was made to detect mentions of drugs\/chemicals in english texts, so far only very limited attempts were made to recognize them in medical documents in other languages. Taking into account the growing amount of medical publications and clinical records written in spanish, we have organized the first shared task on detecting drug and chemical entities in spanish medical documents. Additionally, we included a clinical concept-indexing sub-track asking teams to return snomed-ct identifiers related to drugs\/chemicals for a collection of documents. For this task, named there, we generated annotation guidelines together with a corpus of 1,000 manually annotated clinical case studies. A total of 22 teams participated in the sub-track 1, (77 system runs), and 7 teams in the sub-track 2 (19 system runs). Top scoring teams used sophisticated deep learning approaches yielding very competitive results with f-measures above 0.91. These results indicate that there is a real interest in promoting biomedical text mining efforts beyond english. We foresee that the detect annotation guidelines, corpus and participant systems will foster the development of new resources for clinical and biomedical text mining systems of spanish medical data.","wordlikeness":0.8181818182,"lcsratio":1.0,"wordcoverage":0.7619047619}
{"Year":2010,"Venue":"semeval-2010","Acronym":"CLR","Description":"Linking Events and Their Participants in Discourse Using a Comprehensive FrameNet Dictionary","Abstract":"The cl research system for semeval-2 task 10 for linking events and their participants in discourse is an exploration of the use of a specially created framenet dictionary that captures all framenet information about frames, lexical units, and frame-to-frame relations. This system is embedded in a specially designed interface, the linguistic task analyzer. The implementation of this system was quite minimal at the time of submission, allowing only an initial completion of the role recognition and labeling task, with recall of 0.112, precision of 0.670, and f-score of 0.192. We describe the design of the system and the continuing efforts to determine how much of this task can be performed with the available lexical resources. Changes since the official submission have improved the f-score to 0.266.","wordlikeness":0.3333333333,"lcsratio":0.6666666667,"wordcoverage":0.8571428571}
{"Year":2021,"Venue":"ranlp-2021","Acronym":"GeSERA","Description":"General-domain Summary Evaluation by Relevance Analysis","Abstract":"We present study, an open-source improved version of sera for evaluating automatic extractive and abstractive summaries from the general domain. Sera is based on a search engine that compares candidate and reference summaries (called queries) against an information retrieval document base (called index). Sera was originally designed for the biomedical domain only, where it showed a better correlation with manual methods than the widely used lexical-based rouge method. In this paper, we take out sera from the biomedical domain to the general one by adapting its content-based method to successfully evaluate summaries from the general domain. First, we improve the query reformulation strategy with pos tags analysis of general-domain corpora. Second, we replace the biomedical index used in sera with two article collections from aquaint-2 and wikipedia. We conduct experiments with tac2008, tac2009, and cnndm datasets. Results show that, in most cases, version achieves higher correlations with manual evaluation methods than sera, while it reduces its gap with rouge for general-domain summary evaluation. On even surpasses rouge in two cases of tac2009. Finally, we conduct extensive experiments and provide a comprehensive study of the impact of human annotators and the index size on summary evaluation with sera and that.","wordlikeness":0.8333333333,"lcsratio":1.0,"wordcoverage":0.7692307692}
{"Year":2018,"Venue":"emnlp-2018","Acronym":"WikiAtomicEdits","Description":"A Multilingual Corpus of Wikipedia Edits for Modeling Language and Discourse","Abstract":"We release a corpus of 43 million atomic edits across 8 languages. These edits are mined from wikipedia edit history and consist of instances in which a human editor has inserted a single contiguous phrase into, or deleted a single contiguous phrase from, an existing sentence. We use the collected data to show that the language generated during editing differs from the language that we observe in standard corpora, and that models trained on edits encode different aspects of semantics and discourse than models trained on raw text. We release the full corpus as a resource to aid ongoing research in semantics, discourse, and representation learning.","wordlikeness":0.8,"lcsratio":0.8666666667,"wordcoverage":0.5833333333}
{"Year":2017,"Venue":"conll-2017","Acronym":"UParse","Description":"the Edinburgh system for the CoNLL 2017 UD shared task","Abstract":"This paper presents our submissions for the conll 2017 ud shared task. Our parser, called ,, is based on a neural network graph-based dependency parser. The parser uses features from a bidirectional lstm to to produce a distribution over possible heads for each word in the sentence. To allow transfer learning for low-resource treebanks and surprise languages, we train several multilingual models for related languages, grouped by their genus and language families. Out of 33 participants, our system achieves rank 9th in the main results, with 75.49 uas and 68.87 las f-1 scores (average across 81 treebanks).","wordlikeness":0.5,"lcsratio":0.6666666667,"wordcoverage":0.8333333333}
{"Year":2019,"Venue":"mtsummit-2019","Acronym":"MAGMATic","Description":"A Multi-domain Academic Gold Standard with Manual Annotation of Terminology for Machine Translation Evaluation","Abstract":"This paper presents annotation (multidomain academic gold standard with manual annotation of terminology), a novel italian\u2013english benchmark which allows mt evaluation focused on terminology translation. The data set comprises 2,056 parallel sentences extracted from institutional academic texts, namely course unit and degree program descriptions. This text type is particularly interesting since it contains terminology from multiple domains, e.g. education and different academic disciplines described in the texts. All terms in the english target side of the data set were manually identi\ufb01ed and annotated with a domain label, for a total of 7,517 annotated terms. Due to their peculiar features, institutional academic texts represent an interesting test bed for mt. As a further contribution of this paper, we investigate the feasibility of exploiting mt for the translation of this type of documents. To this aim, we evaluate two stateof-the-art neural mt systems on evaluation, focusing on their ability to translate domain-speci\ufb01c terminology.","wordlikeness":0.875,"lcsratio":1.0,"wordcoverage":0.8235294118}
{"Year":2022,"Venue":"coling-2022","Acronym":"TreeMAN","Description":"Tree-enhanced Multimodal Attention Network for ICD Coding","Abstract":"Icd coding is designed to assign the disease codes to electronic health records (ehrs) upon discharge, which is crucial for billing and clinical statistics. In an attempt to improve the effectiveness and efficiency of manual coding, many methods have been proposed to automatically predict icd codes from clinical notes. However, most previous works ignore the decisive information contained in structured medical data in ehrs, which is hard to be captured from the noisy clinical notes. In this paper, we propose a tree-enhanced multimodal attention network (constructed) to fuse tabular features and textual features into multimodal representations by enhancing the text representations with tree-based features via the attention mechanism. Tree-based features are constructed according to decision trees learned from structured multimodal medical data, which capture the decisive information about icd coding. We can apply the same multi-label classifier from previous text models to the multimodal representations to predict icd codes. Experiments on two mimic datasets show that our method outperforms prior state-of-the-art icd coding approaches. The code is available at <a href=https:\/\/github.com\/liu-zichen\/via class=acl-markup-url>https:\/\/github.com\/liu-zichen\/models<\/a>.","wordlikeness":0.8571428571,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2014,"Venue":"lrec-2014","Acronym":"El-WOZ","Description":"a client-server wizard-of-oz interface","Abstract":"In this paper, we present a speech recording interface developed in the context of a project on automatic speech recognition for elderly native speakers of european portuguese. In order to collect spontaneous speech in a situation of interaction with a machine, this interface was designed as a wizard-of-oz (woz) plateform. In this setup, users interact with a fake automated dialog system controled by a human wizard. It was implemented as a client-server application and the subjects interact with a talking head. The human wizard chooses pre-defined questions or sentences in a graphical user interface, which are then synthesized and spoken aloud by the avatar on the client side. A small spontaneous speech corpus was collected in a daily center. Eight speakers between 75 and 90 years old were recorded. They appreciated the interface and felt at ease with the avatar. Manual orthographic transcriptions were created for the total of about 45 minutes of speech.","wordlikeness":0.1666666667,"lcsratio":0.8333333333,"wordcoverage":0.6}
{"Year":2023,"Venue":"findings-2023","Acronym":"DiMS","Description":"Distilling Multiple Steps of Iterative Non-Autoregressive Transformers for Machine Translation","Abstract":"The computational benefits of iterative non-autoregressive transformers decrease as the number of decoding steps increases. As a remedy, we introduce distill multiple steps (from), a simple yet effective distillation technique to decrease the number of required steps to reach a certain translation quality. The distilled model enjoys the computational benefits of early iterations while preserving the enhancements from several iterative steps. Technique relies on two models namely student and teacher. The student is optimized to predict the output of the teacher after multiple decoding steps while the teacher follows the student via a slow-moving average. The moving average keeps the teacher\u2019s knowledge updated and enhances the quality of the labels provided by the teacher. During inference, the student is used for translation and no additional computation is added. We verify the effectiveness of class=acl-markup-url>https:\/\/github. On various models obtaining 7.8 and 12.9 bleu points improvements in single-step translation accuracy on distilled and raw versions of wmt\u201914 de-en.full code for this work is available here: <a href=https:\/\/github.com\/layer6ai-labs\/distillation class=acl-markup-url>https:\/\/github.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8571428571}
