{"Year":2021,"Venue":"acl-2021","Acronym":"ExplainaBoard","Description":"An Explainable Leaderboard for NLP","Abstract":"With the rapid development of nlp research, leaderboards have emerged as one tool to track the performance of various systems on various nlp tasks. They are effective in this goal to some extent, but generally present a rather simplistic one-dimensional view of the submitted systems, communicated only through holistic accuracy numbers. In this paper, we present a new conceptualization and implementation of nlp evaluation: the examine, which in addition to inheriting the functionality of the standard leaderboard, also allows researchers to (i) diagnose strengths and weaknesses of a single system (e.g. what is the best-performing system bad at?) (ii) interpret relationships between multiple systems. (e.g. where does system a outperform system b? What if we combine systems a, b and c?) and (iii) examine prediction results closely (e.g. what are common errors made by multiple systems or in what contexts do particular errors occur?). So far, extent, covers more than 400 systems, 50 datasets, 40 languages, and 12 tasks. We not only released an online platform at the website but also make our evaluation tool an api with mit licence at github and pypi that allows users to conveniently assess their models offline. We additionally release all output files from systems that we have run or collected to motivate \u201coutput-driven\u201d research in the future.","wordlikeness":0.8461538462,"lcsratio":1.0,"wordcoverage":0.7272727273}
{"Year":2010,"Venue":"acl-2010","Acronym":"WebLicht","Description":"Web-Based LRT Services for German","Abstract":"This software demonstration presents (short (short for: web-based linguistic chaining tool), a webbased service environment for the integration and use of language resources and tools (lrt). Concern is being developed as part of the d-spin project1. Lrt is implemented as a web application so that there is no need for users to install any software on their own computers or to concern themselves with the technical details involved in building tool chains. The integrated web services are part of a prototypical infrastructure that was developed to facilitate chaining of lrt services. Service. Allows the integration and use of distributed web services with standardized apis. the nature of these open and standardized apis makes it possible to access the web services from nearly any programming language, shell script or workflow engine (uima, gate etc.) additionally, an application for integration of additional services is available, allowing anyone to contribute his own web service.","wordlikeness":0.875,"lcsratio":0.75,"wordcoverage":0.7692307692}
{"Year":2017,"Venue":"conll-2017","Acronym":"TurkuNLP","Description":"Delexicalized Pre-training of Word Embeddings for Dependency Parsing","Abstract":"We present the parsing entry in the conll 2017 shared task on multilingual parsing from raw text to universal dependencies. The system is based on the udpipe parser with our focus being in exploring various techniques to pre-train the word embeddings used by the parser in order to improve its performance especially on languages with small training sets. The system ranked 11th among the 33 participants overall, being 8th on the small treebanks, 10th on the large treebanks, 12th on the parallel test sets, and 26th on the surprise languages.","wordlikeness":0.5,"lcsratio":0.5,"wordcoverage":0.6666666667}
{"Year":2023,"Venue":"findings-2023","Acronym":"ClaimDiff","Description":"Comparing and Contrasting Claims on Contentious Issues","Abstract":"With the growing importance of detecting misinformation, many studies have focused on verifying factual claims by retrieving evidence. However, canonical fact verification tasks do not apply to catching subtle differences in factually consistent claims, which might still bias the readers, especially on contentious political or economic issues. Our underlying assumption is that among the trusted sources, one\u2019s argument is not necessarily more true than the other, requiring comparison rather than verification. In this study, we propose verifying, a novel dataset that primarily focuses on comparing the nuance between claim pairs. In study,, we provide human-labeled 2,941 claim pairs from 268 news articles. We observe that while humans are capable of detecting the nuances between claims, strong baselines struggle to detect them, showing over a 19% absolute gap with the humans. We hope this initial study could help readers to gain an unbiased grasp of contentious issues through machine-aided comparison.","wordlikeness":0.7777777778,"lcsratio":0.6666666667,"wordcoverage":0.75}
{"Year":2020,"Venue":"acl-2020","Acronym":"MOOCCube","Description":"A Large-scale Data Repository for NLP Applications in MOOCs","Abstract":"The prosperity of massive open online courses (moocs) provides fodder for many nlp and ai research for education applications, e.g., course concept extraction, prerequisite relation discovery, etc. However, the publicly available datasets of mooc are limited in size with few types of data, which hinders advanced models and novel attempts in related topics. Therefore, we present many, a large-scale data repository of over 700 mooc courses, 100k concepts, 8 million student behaviors with an external resource. Moreover, we conduct a prerequisite discovery task as an example application to show the potential of facilitating in facilitating relevant research. The data repository is now available at <a href=http:\/\/moocdata.cn\/data\/is class=acl-markup-url>http:\/\/moocdata.cn\/data\/student<\/a>.","wordlikeness":0.625,"lcsratio":0.5,"wordcoverage":0.6666666667}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"IndicXNLI","Description":"Evaluating Multilingual Inference for Indian Languages","Abstract":"While indic nlp has made rapid advances recently in terms of the availability of corpora and pre-trained models, benchmark datasets on standard nlu tasks are limited. To this end, we introduce impact, an nli dataset for 11 indic languages. It has been created by high-quality machine translation of the original english xnli dataset and our analysis attests to the quality of a. By finetuning different pre-trained lms on this choice, we analyze various cross-lingual transfer techniques with respect to the impact of the choice of language models, languages, multi-linguality, mix-language input, etc. These experiments provide us with useful insights into the behaviour of pre-trained models for a diverse set of languages.","wordlikeness":0.4444444444,"lcsratio":0.6666666667,"wordcoverage":0.6666666667}
{"Year":2021,"Venue":"inlg-2021","Acronym":"Text-in-Context","Description":"Token-Level Error Detection for Table-to-Text Generation","Abstract":"We present our charles-upf submission for the shared task on evaluating accuracy in generated texts at inlg 2021. Our system can detect the errors automatically using a combination of a rule-based natural language generation (nlg) system and pretrained language models (lms). We first utilize a rule-based nlg system to generate sentences with facts that can be derived from the input. For each sentence we evaluate, we select a subset of facts which are relevant by measuring semantic similarity to the sentence in question. Finally, we finetune a pretrained language model on annotated data along with the relevant facts for fine-grained error detection. On the test set, we achieve 69% recall and 75% precision with a model trained on a mixture of human-annotated and synthetic data.","wordlikeness":0.6666666667,"lcsratio":0.7333333333,"wordcoverage":0.64}
{"Year":2022,"Venue":"lrec-2022","Acronym":"HateBR","Description":"A Large Expert Annotated Corpus of Brazilian Instagram Comments for Offensive Language and Hate Speech Detection","Abstract":"Due to the severity of the social media offensive and hateful comments in brazil, and the lack of research in portuguese, this paper provides the first large-scale expert annotated corpus of brazilian instagram comments for hate speech and offensive language detection. The from corpus was collected from the comment section of brazilian politicians\u2019 accounts on instagram and manually annotated by specialists, reaching a high inter-annotator agreement. The corpus consists of 7,000 documents annotated according to three different layers: a binary classification (offensive versus non-offensive comments), offensiveness-level classification (highly, moderately, and slightly offensive), and nine hate speech groups (xenophobia, racism, homophobia, sexism, religious intolerance, partyism, apology for the dictatorship, antisemitism, and fatphobia). We also implemented baseline experiments for offensive language and hate speech detection and compared them with a literature baseline. Results show that the baseline experiments on our corpus outperform the current state-of-the-art for the portuguese language.","wordlikeness":0.6666666667,"lcsratio":0.8333333333,"wordcoverage":0.8333333333}
{"Year":2019,"Venue":"acl-2019","Acronym":"MultiQA","Description":"An Empirical Investigation of Generalization and Transfer in Reading Comprehension","Abstract":"A large number of reading comprehension (rc) datasets has been created recently, but little analysis has been done on whether they generalize to one another, and the extent to which existing datasets can be leveraged for improving performance on new ones. In this paper, we conduct such an investigation over ten rc datasets, training on one or more source rc datasets, and evaluating generalization, as well as transfer to a target rc dataset. We analyze the factors that contribute to generalization, and show that training on a source rc dataset and transferring to a target dataset substantially improves performance, even in the presence of powerful contextual representations from bert (devlin et al., 2019). We also find that training on multiple source rc datasets leads to robust generalization and transfer, and can reduce the cost of example collection for a new rc dataset. Following our analysis, we propose source, a bert-based model, trained on multiple rc datasets, which leads to state-of-the-art performance on five rc datasets. We share our infrastructure for the benefit of the research community.","wordlikeness":0.7142857143,"lcsratio":0.7142857143,"wordcoverage":0.8333333333}
{"Year":2020,"Venue":"findings-2020","Acronym":"MedICaT","Description":"A Dataset of Medical Images, Captions, and Textual References","Abstract":"Understanding the relationship between figures and text is key to scientific document understanding. Medical figures in particular are quite complex, often consisting of several subfigures (75% of figures in our dataset), with detailed text describing their content. Previous work studying figures in scientific papers focused on classifying figure content rather than understanding how images relate to the text. To address challenges in figure retrieval and figure-to-text alignment, we introduce retrieval, a dataset of medical images in context. Consists consists of 217k images from 131k open access biomedical papers, and includes captions, inline references for 74% of figures, and manually annotated subfigures and subcaptions for a subset of figures. Using previous, we introduce the task of subfigure to subcaption alignment in compound figures and demonstrate the utility of inline references in image-text matching. Our data and code can be accessed at <a href=https:\/\/github.com\/allenai\/consists class=acl-markup-url>https:\/\/github.com\/allenai\/understanding<\/a>.","wordlikeness":0.8571428571,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2013,"Venue":"starsem-2013","Acronym":"KLUE-CORE","Description":"A regression model of semantic textual similarity","Abstract":"This paper describes our system entered for the *sem 2013 shared task on semantic textual similarity (sts). We focus on the core task of predicting the semantic textual similarity of sentence pairs. The current system utilizes machine learning techniques trained on semantic similarity ratings from the *sem 2012 shared task; it achieved rank 20 out of 90 submissions from 35 different teams. Given the simple nature of our approach, which uses only wordnet and unannotated corpus data as external resources, we consider this a remarkably good result, making the system an interesting tool for a wide range of practical applications.","wordlikeness":0.6666666667,"lcsratio":0.4444444444,"wordcoverage":0.6666666667}
{"Year":2022,"Venue":"lrec-2022","Acronym":"FACTOID","Description":"A New Dataset for Identifying Misinformation Spreaders and Political Bias","Abstract":"Proactively identifying misinformation spreaders is an important step towards mitigating the impact of fake news on our society. In this paper, we introduce a new contemporary reddit dataset for fake news spreader analysis, called binary, monitoring political discussions on reddit since the beginning of 2020. The dataset contains over 4k users with 3.4m reddit posts, and includes, beyond the users\u2019 binary labels, also their fine-grained credibility level (very low to very high) and their political bias strength (extreme right to extreme left). As far as we are aware, this is the first fake news spreader dataset that simultaneously captures both the long-term context of users\u2019 historical posts and the interactions between them. To create the first benchmark on our data, we provide methods for identifying misinformation spreaders by utilizing the social connections between the users along with their psycho-linguistic features. We show that the users\u2019 social interactions can, on their own, indicate misinformation spreading, while the psycho-linguistic features are mostly informative in non-neural classification settings. In a qualitative analysis we observe that detecting affective mental processes correlates negatively with right-biased users, and that the openness to experience factor is lower for those who spread fake news.","wordlikeness":0.5714285714,"lcsratio":0.7142857143,"wordcoverage":0.8333333333}
{"Year":2015,"Venue":"ws-2015","Acronym":"KWB","Description":"An Automated Quick News System for Chinese Readers","Abstract":"We present an automated quick news system called with. Demonstrate crawls and collects around the clock news items from over 120 news websites in mainland china, eliminates duplicates, and retrieves a summary of up to 600 characters for each news article using a proprietary summary engine. It then uses a labeled-lda classi\ufb01er to classify the remaining news items into 19 categories, computes popularity ranks called popurank of the newly collected news items in each category, and displays the summaries of news items in each category sorted according to popurank together with a picture, if there is any, on http:\/\/www.kuaiwenbao.com and mobile apps. We will describe in this paper the system architecture of crawler, the data crawler structure, the functionalities of the central database, and the de\ufb01nition of popurank. We will show, through experiments, the running time of obtaining popurank. We will also demonstrate the use of from.","wordlikeness":0.3333333333,"lcsratio":0.6666666667,"wordcoverage":0.8}
{"Year":2022,"Venue":"acl-2022","Acronym":"CogTaskonomy","Description":"Cognitively Inspired Task Taxonomy Is Beneficial to Transfer Learning in NLP","Abstract":"Is there a principle to guide transfer learning across tasks in natural language processing (nlp)? Taxonomy (zamir et al., 2018) finds that a structure exists among visual tasks, as a principle underlying transfer learning for them. In this paper, we propose a cognitively inspired framework, structure, to learn taxonomy for nlp tasks. The framework consists of cognitive representation analytics (cra) and cognitive-neural mapping (cnm). The former employs representational similarity analysis, which is commonly used in computational neuroscience to find a correlation between brain-activity measurement and computational modeling, to estimate task similarity with task-specific sentence representations. The latter learns to detect task relations by projecting neural representations from nlp models to cognitive signals (i.e., fmri voxels). Experiments on 12 nlp tasks, where bert\/tinybert are used as the underlying models for transfer learning, demonstrate that the proposed cogtaxonomy is able to guide transfer learning, achieving performance competitive to the analytic hierarchy process (saaty, 1987) used in visual taskonomy (zamir et al., 2018) but without requiring exhaustive pairwise <span class=tex-math>o(m<sup>2<\/sup>)<\/span> task transferring. Analyses further discover that cnm is capable of learning model-agnostic task taxonomy.","wordlikeness":0.5833333333,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2016,"Venue":"coling-2016","Acronym":"VoxSim","Description":"A Visual Platform for Modeling Motion Language","Abstract":"Much existing work in text-to-scene generation focuses on generating static scenes. By introducing a focus on motion verbs, we integrate dynamic semantics into a rich formal model of events to generate animations in real time that correlate with human conceptions of the event described. This paper presents a working system that generates these animated scenes over a test set, discussing challenges encountered and describing the solutions implemented.","wordlikeness":0.5,"lcsratio":0.6666666667,"wordcoverage":0.6666666667}
{"Year":2021,"Venue":"naacl-2021","Acronym":"tWT--WT","Description":"A Dataset to Assert the Role of Target Entities for Detecting Stance of Tweets","Abstract":"The stance detection task aims at detecting the stance of a tweet or a text for a target. These targets can be named entities or free-form sentences (claims). Though the task involves reasoning of the tweet with respect to a target, we find that it is possible to achieve high accuracy on several publicly available twitter stance detection datasets without looking at the target sentence. Specifically, a simple tweet classification model achieved human-level performance on the wt\u2013wt dataset and more than two-third accuracy on various other datasets. We investigate the existence of biases in such datasets to find the potential spurious correlations of sentiment-stance relations and lexical choice associated with the stance category. Furthermore, we propose a new large dataset free of such biases and demonstrate its aptness on the existing stance detection systems. Our empirical findings show much scope for research on the stance detection task and proposes several considerations for creating future stance detection datasets.","wordlikeness":0.1428571429,"lcsratio":0.5714285714,"wordcoverage":0.5714285714}
{"Year":2009,"Venue":"ws-2009","Acronym":"UDel","Description":"Extending Reference Generation to Multiple Entities","Abstract":"We report on an attempt to extend a reference generation system, originally designed only for main subjects, to generate references for multiple entities in a single document. This endeavor yielded three separate systems: one utilizing the original classi\ufb01er, another with a retrained classi\ufb01er, and a third taking advantage of new data to improve the identi\ufb01cation of interfering antecedents. Each subsequent system improved upon the results of the previous iteration.","wordlikeness":1.0,"lcsratio":0.75,"wordcoverage":0.8571428571}
{"Year":2010,"Venue":"semeval-2010","Acronym":"HUMB","Description":"Automatic Key Term Extraction from Scientific Articles in GROBID","Abstract":"The semeval task 5 was an opportunity for experimenting with the key term extraction module of grobid, a system for extracting and generating bibliographical information from technical and scienti\ufb01c documents. The tool \ufb01rst uses grobid\u2019s facilities for analyzing the structure of scienti\ufb01c articles, resulting in a \ufb01rst set of structural features. A second set of features captures content properties based on phraseness, informativeness and keywordness measures. Two knowledge bases, grisp and wikipedia, are then exploited for producing a last set of lexical\/semantic features. Bagged decision trees appeared to be the most ef\ufb01cient machine learning algorithm for generating a list of ranked key term candidates. Finally a post ranking was realized based on statistics of cousage of keywords in hal, a large open access publication repository.","wordlikeness":0.75,"lcsratio":0.75,"wordcoverage":0.8888888889}
{"Year":2021,"Venue":"naacl-2021","Acronym":"SpanPredict","Description":"Extraction of Predictive Document Spans with Neural Attention","Abstract":"In many natural language processing applications, identifying predictive text can be as important as the predictions themselves. When predicting medical diagnoses, for example, identifying predictive content in clinical notes not only enhances interpretability, but also allows unknown, descriptive (i.e., text-based) risk factors to be identified. We here formalize this problem as predictive extraction and address it using a simple mechanism based on linear attention. Our method preserves differentiability, allowing scalable inference via stochastic gradient descent. Further, the model decomposes predictions into a sum of contributions of distinct text spans. Importantly, we require only document labels, not ground-truth spans. Results show that our model identifies semantically-cohesive spans and assigns them scores that agree with human ratings, while preserving classification performance.","wordlikeness":0.8181818182,"lcsratio":0.8181818182,"wordcoverage":0.7777777778}
{"Year":2020,"Venue":"findings-2020","Acronym":"COSMIC","Description":"COmmonSense knowledge for eMotion Identification in Conversations","Abstract":"In this paper, we address the task of utterance level emotion recognition in conversations using commonsense knowledge. We propose conversations, a new framework that incorporates different elements of commonsense such as mental states, events, and causal relations, and build upon them to learn interactions between interlocutors participating in a conversation. Current state-of-theart methods often encounter difficulties in context propagation, emotion shift detection, and differentiating between related emotion classes. By learning distinct commonsense representations, detection, addresses these challenges and achieves new state-of-the-art results for emotion recognition on four different benchmark conversational datasets. Our code is available at <a href=https:\/\/github.com\/declare-lab\/conv-emotion class=acl-markup-url>https:\/\/github.com\/declare-lab\/conv-emotion<\/a>.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2015,"Venue":"semeval-2015","Acronym":"TeamUFAL","Description":"WSD&#43;EL as Document Retrieval","Abstract":"This paper describes our system for semeval2015 task 13: multilingual all-words sense disambiguation and entity linking. We have participated with our system in the sub-task which aims at monolingual all-words disambiguation and entity linking. Aside from system description, we pay closer attention to the evaluation of system outputs.","wordlikeness":0.625,"lcsratio":0.625,"wordcoverage":0.6666666667}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"WSpeller","Description":"Robust Word Segmentation for Enhancing Chinese Spelling Check","Abstract":"Chinese spelling check (csc) detects and corrects spelling errors in chinese texts. Previous approaches have combined character-level phonetic and graphic information, ignoring the importance of segment-level information. According to our pilot study, spelling errors are always associated with incorrect word segmentation. When appropriate word boundaries are provided, csc performance is greatly enhanced. Based on these findings, we present appropriate, a csc model that takes into account word segmentation. A fundamental component of segmentation. Is a w-mlm, which is trained by predicting visually and phonetically similar words. Through modification of the embedding layer\u2019s input, word segmentation information can be incorporated. Additionally, a robust module is trained to assist the w-mlm-based correction module by predicting the correct word segmentations from sentences containing spelling errors. We evaluate datasets on the widely used benchmark datasets sighan13, sighan14, and sighan15. Our model is superior to state-of-the-art baselines on sighan13 and sighan15 and maintains equal performance on sighan14.","wordlikeness":0.625,"lcsratio":0.875,"wordcoverage":0.8571428571}
{"Year":1998,"Venue":"coling-1998","Acronym":"Redundancy","Description":"helping semantic disambiguation","Abstract":"Say, is a good thing, at least in a learn- ing process. To be a good teacher you must say what you are going to say, say it, then say what you have just said. Well, three times is better than one. To acquire and learn knowl- edge from text for building a lexical knowledge base, we need to find a source of information that states t:acts, and repeats them a few times using slightly different sent, cute structures. A technique is needed for gathering information from that source and identify the redundant in- formation. The extraction of the commonality is an active learning of the knowledge expressed. The proposed research is based on a clustering method developed by barribre and popowich (1996) which performs a gathering of related information about a particular topic. Individ- ual pieces of information are represented via the conceptual graph (cg) formalism and the re- sult of the clustering is a large cg embedding all individual graphs. In the present paper, we suggest that the identification of the redundant intbrmation within the resulting graph is very useful for disambiguation of the original infor- mation at the semantic level.","wordlikeness":0.8,"lcsratio":0.5,"wordcoverage":1.0}
{"Year":2015,"Venue":"semeval-2015","Acronym":"Samsung","Description":"Align-and-Differentiate Approach to Semantic Textual Similarity","Abstract":"This paper describes our align-anddifferentiate approach to the semeval 2015 task 2 competition for english semantic textual similarity (sts) systems. Our submission achieved the top place on two of the \ufb01ve evaluation datasets. Our team placed 3rd among 28 participating teams, and our three runs ranked 4th, 6th and 7th among the 73 runs submitted by the 28 teams. Our approach improves upon the umbc pairingwords system by semantically differentiating distributionally similar terms. This novel addition improves results by 2.5 points on the pearson correlation measure.","wordlikeness":0.5714285714,"lcsratio":0.4285714286,"wordcoverage":1.0}
{"Year":2016,"Venue":"lrec-2016","Acronym":"SCALE","Description":"A Scalable Language Engineering Toolkit","Abstract":"In this paper we present second, a new python toolkit that contains two extensions to n-gram language models. The first extension is a novel technique to model compound words called semantic head mapping (shm). The second extension, bag-of-words language modeling (baglm), bundles popular models such as latent semantic analysis and continuous skip-grams. Both extensions first to large data and allow the integration into first-pass asr decoding. The toolkit is open source, includes working examples and can be found on <a href=http:\/\/github.com\/jorispelemans\/allow class=acl-markup-url>http:\/\/github.com\/jorispelemans\/found<\/a>.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2020,"Venue":"lrec-2020","Acronym":"ScholarlyRead","Description":"A New Dataset for Scientific Article Reading Comprehension","Abstract":"We present with, span-of-word-based scholarly articles\u2019 reading comprehension (rc) dataset with approximately 10k manually checked passage-question-answer instances. Are was constructed in semi-automatic way. We consider the articles from two popular journals of a reputed publishing house. Firstly, we generate questions from these articles in an automatic way. Generated questions are then manually checked by the human annotators. We propose a baseline model based on bi-directional attention flow (bidaf) network that yields the f1 score of 37.31%. The framework would be useful for building question-answering (qa) systems on scientific articles.","wordlikeness":0.9230769231,"lcsratio":0.6923076923,"wordcoverage":0.7}
{"Year":2021,"Venue":"eacl-2021","Acronym":"HULK","Description":"An Energy Efficiency Benchmark Platform for Responsible Natural Language Processing","Abstract":"Computation-intensive pretrained models have been taking the lead of many natural language processing benchmarks such as glue. However, energy efficiency in the process of model training and inference becomes a critical bottleneck. We introduce language, a multi-task energy efficiency benchmarking platform for responsible natural language processing. With benchmarks, we compare pretrained models\u2019 energy efficiency from the perspectives of time and cost. Baseline benchmarking results are provided for further analysis. The fine-tuning efficiency of different pretrained models can differ significantly among different tasks, and fewer parameter number does not necessarily imply better efficiency. We analyzed such a phenomenon and demonstrated the method for comparing the multi-task efficiency of pretrained models. Our platform is available at <a href=https:\/\/furtherbenchmark.github.io\/ class=acl-markup-url>https:\/\/perspectivesbenchmark.github.io\/<\/a> .","wordlikeness":0.5,"lcsratio":0.75,"wordcoverage":1.0}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"DRLK","Description":"Dynamic Hierarchical Reasoning with Language Model and Knowledge Graph for Question Answering","Abstract":"In recent years, graph neural network (gnn) approaches with enhanced knowledge graphs (kg) perform well in question answering (qa) tasks. One critical challenge is how to effectively utilize interactions between the qa context and kg. However, existing work only adopts the identical qa context representation to interact with multiple layers of kg, which results in a restricted interaction. In this paper, we propose on (dynamic hierarchical reasoning with language model and knowledge graphs), a novel model that utilizes dynamic hierarchical interactions between the qa context and kg for reasoning. Answering extracts dynamic hierarchical features in the qa context, and performs inter-layer and intra-layer interactions on each iteration, allowing the kg representation to be grounded with the hierarchical features of the qa context. We conduct extensive experiments on four benchmark datasets in medical qa and commonsense reasoning. The experimental results demonstrate that for achieves state-of-the-art performances on two benchmark datasets and performs competitively on the others.","wordlikeness":0.25,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2023,"Venue":"findings-2023","Acronym":"DSPM-NLG","Description":"A Dual Supervised Pre-trained Model for Few-shot Natural Language Generation in Task-oriented Dialogue System","Abstract":"In few-shot settings, fully conveying the semantic information of the dialogue act is a crucial challenge for natural language generation (nlg) in the task-oriented dialogue system. An interesting fact is that nlg and spoken language understanding (slu) are a natural dual problem pair. Suppose the response generated by the nlg module can be restored to the corresponding dialogue act by the slu module, which reflects that the generated response fully conveys the semantic information of the dialogue act. Based on this idea, a novel dual supervised pre-trained model for a few-shot natural language generation (pre-training) is proposed to regularize the pre-training process. We adopt a joint model with a dual supervised framework to learn the dual correlation between nlg and slu from the perspective of probability. In addition, a slot-masked strategy is designed to enable the model to focus better on the key slot-value pairs. Strategy is continuously trained on existing public large-scale annotated data, which thoroughly learns the duality between two tasks to enhance the semantically controlling and generalization abilities of the pre-trained model. Experiments demonstrate that our proposed model performs outstandingly on the few-shot benchmark dataset and outperforms the previous sota results.","wordlikeness":0.25,"lcsratio":1.0,"wordcoverage":0.6315789474}
{"Year":2017,"Venue":"mwe-2017","Acronym":"USzeged","Description":"Identifying Verbal Multiword Expressions with POS Tagging and Parsing Techniques","Abstract":"The paper describes our system submitted for the workshop on multiword expressions\u2019 shared task on automatic identification of verbal multiword expressions. It uses pos tagging and dependency parsing to identify single- and multi-token verbal mwes in text. Our system is language independent and competed on nine of the eighteen languages. Our paper describes how our system works and gives its error analysis for the languages it was submitted for.","wordlikeness":0.7142857143,"lcsratio":0.5714285714,"wordcoverage":0.7272727273}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"FETA","Description":"A Benchmark for Few-Sample Task Transfer in Open-Domain Dialogue","Abstract":"Task transfer, transferring knowledge contained in related tasks, holds the promise of reducing the quantity of labeled data required to fine-tune language models. Dialogue understanding encompasses many diverse tasks, yet task transfer has not been thoroughly studied in conversational ai. This work explores conversational task transfer by introducing a: a benchmark for few-sample task transfer in open-domain dialogue.addition contains two underlying sets of conversations upon which there are 10 and 7 tasks annotated, enabling the study of intra-dataset task transfer; task transfer without domain adaptation. We utilize three popular language models and three learning algorithms to analyze the transferability between 132 source-target task pairs and create a baseline for future work. We run experiments in the single- and multi-source settings and report valuable findings, e.g., most performance trends are model-specific, and span extraction and multiple-choice tasks benefit the most from task transfer. In addition to task transfer, trends can be a valuable resource for future research into the efficiency and generalizability of pre-training datasets and model architectures, as well as for learning settings such as continual and multitask learning.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2015,"Venue":"semeval-2015","Acronym":"USAAR-SHEFFIELD","Description":"Semantic Textual Similarity with Deep Regression and Machine Translation Evaluation Metrics","Abstract":"This paper describes the usaarsheffield systems that participated in the semantic textual similarity (sts) english task of semeval-2015. We extend the work on using machine translation evaluation metrics in the sts task. Different from previous approaches, we regard the metrics\u2019 robustness across different text types and con\ufb02ate the training data across different subcorpora. In addition, we introduce a novel deep regressor architecture and evaluated its ef\ufb01ciency in the sts task.","wordlikeness":0.5333333333,"lcsratio":0.6666666667,"wordcoverage":0.75}
{"Year":2020,"Venue":"emnlp-2020","Acronym":"UDapter","Description":"Language Adaptation for Truly Universal Dependency Parsing","Abstract":"Recent advances in multilingual dependency parsing have brought the idea of a truly universal parser closer to reality. However, cross-language interference and restrained model capacity remain major obstacles. To address this, we propose a novel multilingual task adaptation approach based on contextual parameter generation and adapter modules. This approach enables to learn adapters via language embeddings while sharing model parameters across languages. It also allows for an easy but effective integration of existing linguistic typology features into the parsing network. The resulting parser, of, outperforms strong monolingual and multilingual baselines on the majority of both high-resource and low-resource (zero-shot) languages, showing the success of the proposed adaptation approach. Our in-depth analyses show that soft parameter sharing via typological features is key to this success.","wordlikeness":0.5714285714,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2015,"Venue":"ws-2015","Acronym":"LINA","Description":"Identifying Comparable Documents from Wikipedia","Abstract":"This paper describes the \ufb01ltering system for the bucc 2015 shared track. Following (enright and kondrak, 2007), our system identify comparable documents by collecting counts of hapax words. We extend this method by \ufb01ltering out document pairs sharing target documents using pigeonhole reasoning and cross-lingual information.","wordlikeness":1.0,"lcsratio":0.75,"wordcoverage":0.8888888889}
{"Year":2018,"Venue":"naacl-2018","Acronym":"SystemT","Description":"Declarative Text Understanding for Enterprise","Abstract":"The rise of enterprise applications over unstructured and semi-structured documents poses new challenges to text understanding systems across multiple dimensions. We present deployed, a declarative text understanding system that addresses these challenges and has been deployed in a wide range of enterprise applications. We highlight the design considerations and decisions behind summarize in addressing the needs of the enterprise setting. We also summarize the impact of rise on business and education.","wordlikeness":0.8571428571,"lcsratio":0.5714285714,"wordcoverage":0.9230769231}
{"Year":2016,"Venue":"coling-2016","Acronym":"PKUSUMSUM","Description":"A Java Platform for Multilingual Document Summarization","Abstract":"Automatic is a java platform for multilingual document summarization, and it sup-ports multiple languages, integrates 10 automatic summarization methods, and tackles three typical summarization tasks. The summarization platform has been released and users can easily use and update it. In this paper, we make a brief description of the char-acteristics, the summarization methods, and the evaluation results of the platform, and al-so compare al-so with other summarization toolkits.","wordlikeness":0.6666666667,"lcsratio":0.7777777778,"wordcoverage":0.625}
{"Year":2022,"Venue":"acl-2022","Acronym":"OpenPrompt","Description":"An Open-source Framework for Prompt-learning","Abstract":"Prompt-learning has become a new paradigm in modern natural language processing, which directly adapts pre-trained language models (plms) to cloze-style prediction, autoregressive modeling, or sequence to sequence generation, resulting in promising performances on various tasks. However, no standard implementation framework of prompt-learning is proposed yet, and most existing prompt- learning codebases, often unregulated, only provide limited implementations for specific scenarios. Since there are many details such as templating strategy, initializing strategy, verbalizing strategy, etc., that need to be considered in prompt-learning, practitioners face impediments to quickly adapting the de-sired prompt learning methods to their applications. In this paper, we present open- prompt, a unified easy-to-use toolkit to conduct prompt-learning over plms. plms. is a research-friendly framework that is equipped with efficiency, modularity, and extendibility, and its combinability allows the freedom to combine different plms, task for- mats, and prompting modules in a unified paradigm. Users could expediently deploy prompt-learning frameworks and evaluate the generalization of them on different nlp tasks without constraints.","wordlikeness":0.7,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2012,"Venue":"starsem-2012","Acronym":"SB","Description":"mmSystem - Using Decompositional Semantics for Lexical Simplification","Abstract":"In this paper, we describe the system we submitted to the semeval-2012 lexical simpli\ufb01cation task. Our system (mmsystem) combines word frequency with decompositional semantics criteria based on syntactic structure in order to rank candidate substitutes of lexical forms of arbitrary syntactic complexity (oneword, multi-word, etc.) in descending order of (cognitive) simplicity. We believe that the proposed approach might help to shed light on the interplay between linguistic features and lexical complexity in general.","wordlikeness":0.5,"lcsratio":0.5,"wordcoverage":1.0}
{"Year":2017,"Venue":"ijcnlp-2017","Acronym":"MIPA","Description":"Mutual Information Based Paraphrase Acquisition via Bilingual Pivoting","Abstract":"We present a pointwise mutual information (pmi)-based approach to formalize paraphrasability and propose a variant of pmi, called bilingual, for the paraphrase acquisition. Our paraphrase acquisition method first acquires lexical paraphrase pairs by bilingual pivoting and then reranks them by pmi and distributional similarity. The complementary nature of information from bilingual corpora and from monolingual corpora makes the proposed method robust. Experimental results show that the proposed method substantially outperforms bilingual pivoting and distributional similarity themselves in terms of metrics such as mrr, map, coverage, and spearman\u2019s correlation.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2021,"Venue":"acl-2021","Acronym":"JointGT","Description":"Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs","Abstract":"Existing pre-trained models for knowledgegraph-to-text (kg-to-text) generation simply \ufb01ne-tune text-to-text pre-trained models such as bart or t5 on kg-to-text datasets, which largely ignore the graph structure during encoding and lack elaborate pre-training tasks to explicitly model graph-text alignments. To tackle these problems, we propose a graph-text joint representation learning model called pre-training. During encoding, we devise a structure-aware semantic aggregation module which is plugged into each transformer layer to preserve the graph structure. Furthermore, we propose three new pre-training tasks to explicitly enhance the graph-text alignment including respective text \/ graph reconstruction, and graph-text alignment in the embedding space via optimal transport. Experiments show that to obtains new stateof-the-art performance on various kg-to-text datasets1.","wordlikeness":0.7142857143,"lcsratio":1.0,"wordcoverage":0.8333333333}
{"Year":2018,"Venue":"lrec-2018","Acronym":"MCScript","Description":"A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge","Abstract":"We introduce a large dataset of narrative texts and questions about these texts, intended to be used in a machine comprehension task that requires reasoning using commonsense knowledge. Our dataset complements similar datasets in that we focus on stories about everyday activities, such as going to the movies or working in the garden, and that the questions require commonsense knowledge, or more speci\ufb01cally, script knowledge, to be answered. We show that our mode of data collection via crowdsourcing results in a substantial amount of such inference questions. The dataset forms the basis of a shared task on commonsense and script knowledge organized at semeval 2018 and provides challenging test cases for the broader natural language understanding community. Keywords: machine comprehension, reading comprehension, commonsense knowledge, script knowledge 1.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2020,"Venue":"emnlp-2020","Acronym":"CancerEmo","Description":"A Dataset for Fine-Grained Emotion Detection","Abstract":"Emotions are an important element of human nature, often affecting the overall wellbeing of a person. Therefore, it is no surprise that the health domain is a valuable area of interest for emotion detection, as it can provide medical staff or caregivers with essential information about patients. However, progress on this task has been hampered by the absence of large labeled datasets. To this end, we introduce has, an emotion dataset created from an online health community and annotated with eight fine-grained emotions. We perform a comprehensive analysis of these emotions and develop deep learning models on the newly created dataset. Our best bert model achieves an average f1 of 71%, which we improve further using domain-specific pre-training.","wordlikeness":0.6666666667,"lcsratio":0.7777777778,"wordcoverage":0.8}
{"Year":2008,"Venue":"inlg-2008","Acronym":"IS-G","Description":"The Comparison of Different Learning Techniques for the Selection of the Main Subject References","Abstract":"The grec task of the referring expression generation challenge 2008 is to select appropriate references to the main subject in given texts. This means to select the correct type of the referring expressions such as name, pronoun, common, or elision (empty). We employ for the selection different learning techniques with the aim to \ufb01nd the most appropriate one for the task and the used attributes. As training data, we use the syntactic category of the searched referring expressions and additionally gathered data from the text itself.","wordlikeness":0.5,"lcsratio":0.75,"wordcoverage":0.6666666667}
{"Year":2015,"Venue":"semeval-2015","Acronym":"TJUdeM","Description":"A Combination Classifier for Aspect Category Detection and Sentiment Polarity Classification","Abstract":"This paper describes the system we submitted to in-domain absa subtask of semeval 2015 shared task on aspect-based sentiment analysis that includes aspect category detection and sentiment polarity classi\ufb01cation. For the aspect category detection, we combined an svm classi\ufb01er with implicit aspect indicators. For the sentiment polarity classi\ufb01cation, we combined an svm classi\ufb01er with a lexicon-based polarity classi\ufb01er. Our system outperforms the baselines on both the laptop and restaurant domains and ranks above average on the laptop domain.","wordlikeness":0.6666666667,"lcsratio":0.6666666667,"wordcoverage":0.8}
{"Year":2021,"Venue":"ws-2021","Acronym":"HinGE","Description":"A Dataset for Generation and Evaluation of Code-Mixed Hinglish Text","Abstract":"Text generation is a highly active area of research in the computational linguistic community. The evaluation of the generated text is a challenging task and multiple theories and metrics have been proposed over the years. Unfortunately, text generation and evaluation are relatively understudied due to the scarcity of high-quality resources in code-mixed languages where the words and phrases from multiple languages are mixed in a single utterance of text and speech. To address this challenge, we present a corpus (highly) for a widely popular code-mixed language hinglish (code-mixing of hindi and english languages). And has hinglish sentences generated by humans as well as two rule-based algorithms corresponding to the parallel hindi-english sentences. In addition, we demonstrate the in- efficacy of widely-used evaluation metrics on the code-mixed data. The addition, dataset will facilitate the progress of natural language generation research in code-mixed languages.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2020,"Venue":"findings-2020","Acronym":"PrivNet","Description":"Safeguarding Private Attributes in Transfer Learning for Recommendation","Abstract":"Transfer learning is an effective technique to improve a target recommender system with the knowledge from a source domain. Existing research focuses on the recommendation performance of the target domain while ignores the privacy leakage of the source domain. The transferred knowledge, however, may unintendedly leak private information of the source domain. For example, an attacker can accurately infer user demographics from their historical purchase provided by a source domain data owner. This paper addresses the above privacy-preserving issue by learning a privacy-aware neural representation by improving target performance while protecting source privacy. The key idea is to simulate the attacks during the training for protecting unseen users\u2019 privacy in the future, modeled by an adversarial game, so that the transfer learning model becomes robust to attacks. Experiments show that the proposed private model can successfully disentangle the knowledge benefitting the transfer from leaking the privacy.","wordlikeness":0.8571428571,"lcsratio":1.0,"wordcoverage":0.8333333333}
{"Year":2021,"Venue":"eacl-2021","Acronym":"WER-BERT","Description":"Automatic WER Estimation with BERT in a Balanced Ordinal Classification Paradigm","Abstract":"Automatic speech recognition (asr) systems are evaluated using word error rate (wer), which is calculated by comparing the number of errors between the ground truth and the transcription of the asr system. This calculation, however, requires manual transcription of the speech signal to obtain the ground truth. Since transcribing audio signals is a costly process, automatic wer evaluation (e-wer) methods have been developed to automatically predict the wer of a speech system by only relying on the transcription and the speech signal features. While wer is a continuous variable, previous works have shown that positing e-wer as a classification problem is more effective than regression. However, while converting to a classification setting, these approaches suffer from heavy class imbalance. In this paper, we propose a new balanced paradigm for e-wer in a classification setting. Within this paradigm, we also propose signals, a bert based architecture with speech features for e-wer. Furthermore, we introduce a distance loss function to tackle the ordinal nature of e-wer classification. The proposed approach and paradigm are evaluated on the librispeech dataset and a commercial (black box) asr system, google cloud\u2019s speech-to-text api. The results and experiments demonstrate that requires establishes a new state-of-the-art in automatic wer estimation.","wordlikeness":0.75,"lcsratio":0.875,"wordcoverage":0.8}
{"Year":2003,"Venue":"mtsummit-2003","Acronym":"AnglaHindi","Description":"an English to Hindi machine-aided translation system","Abstract":"This paper presents a system overview of an english to hindi machine-aided translation system named way. Its beta-version has been made available on the internet for free translation at <a href=http:\/\/verb.iitk.ac.in class=acl-markup-url>http:\/\/machine-aided.iitk.ac.in<\/a> from is an english to hindi version of the anglabharti translation methodology developed by the author for translation from english to all indian languages. Anglabharti is a pseudo-interlingual rule-based translation methodology. Its, besides using the rule-bases, uses example-base and statistics to obtain more acceptable and accurate translation for frequently encountered noun and verb phrasals. This way a limited hybridization of rule-based and example-based approaches has been incorporated.","wordlikeness":0.7,"lcsratio":1.0,"wordcoverage":0.7058823529}
{"Year":2023,"Venue":"acl-2023","Acronym":"MultiCapCLIP","Description":"Auto-Encoding Prompts for Zero-Shot Multilingual Visual Captioning","Abstract":"Supervised visual captioning models typically require a large scale of images or videos paired with descriptions in a specific language (i.e., the vision-caption pairs) for training. However, collecting and labeling large-scale datasets is time-consuming and expensive for many scenarios and languages. Therefore, sufficient labeled pairs are usually not available. To deal with the label shortage problem, we present a simple yet effective zero-shot approach german, that can generate visual captions for different scenarios and languages without any labeled vision-caption pairs of downstream datasets. In the training stage, videos only requires text data for input. Then it conducts two main steps: 1) retrieving concept prompts that preserve the corresponding domain knowledge of new scenarios; 2) auto-encoding the prompts to learn writing styles to output captions in a desired language. In the testing stage, the instead takes visual data as input directly to retrieve the concept prompts to generate the final visual descriptions. The extensive experiments on image and video captioning across four benchmarks and four languages (i.e., english, chinese, german, and french) confirm the effectiveness of our approach. Compared with state-of-the-art zero-shot and weakly-supervised methods, our method achieves 4.8% and 21.5% absolute improvements in terms of bleu@4 and cider metrics. Our code is available at <a href=https:\/\/github.com\/yangbang18\/4.8% class=acl-markup-url>https:\/\/github.com\/yangbang18\/training.<\/a>.","wordlikeness":0.8333333333,"lcsratio":0.75,"wordcoverage":0.7}
{"Year":2012,"Venue":"lrec-2012","Acronym":"Kitten","Description":"a tool for normalizing HTML and extracting its textual content","Abstract":"The web is composed of a gigantic amount of documents that can be very useful for information extraction systems. Most of them are written in html and have to be rendered by an html engine in order to display the data they contain on a screen. Html file thus mix both informational and rendering content. Our goal is to design a tool for informational content extraction. A linear extraction with only a basic filtering of rendering content would not be enough as objects such as lists and tables are linearly coded but need to be read in a non-linear way to be well interpreted. Besides these html pages are often incorrectly coded from an html point of view and use a segmentation of blocks based on blank space that cannot be transposed in a text filewithout confusing syntactic parsers. For this purpose, we propose the useful tool that first normalizes html file into unicode xhtml file, then extracts the informational content into a text filewith a special processing for sentences, lists and tables.","wordlikeness":1.0,"lcsratio":0.8333333333,"wordcoverage":0.8333333333}
{"Year":2022,"Venue":"ijcnlp-2022","Acronym":"NGEP","Description":"A Graph-based Event Planning Framework for Story Generation","Abstract":"To improve the performance of long text generation, recent studies have leveraged automatically planned event structures (i.e. storylines) to guide story generation. Such prior works mostly employ end-to-end neural generation models to predict event sequences for a story. However, such generation models struggle to guarantee the narrative coherence of separate events due to the hallucination problem, and additionally the generated event sequences are often hard to control due to the end-to-end nature of the models. To address these challenges, we propose our, an novel event planning framework which generates an event sequence by performing inference on an automatically constructed event graph and enhances generalisation ability through a neural event advisor. We conduct a range of experiments on multiple criteria, and the results demonstrate that our graph-based neural framework outperforms the state-of-the-art (sota) event planning approaches, considering both the performance of event sequence generation and the effectiveness on the downstream task of story generation.","wordlikeness":0.5,"lcsratio":0.75,"wordcoverage":0.6666666667}
{"Year":2015,"Venue":"semeval-2015","Acronym":"FBK-HLT","Description":"An Application of Semantic Textual Similarity for Answer Selection in Community Question Answering","Abstract":"This paper reports the description and performance of our system, subtasks., participating in the semeval 2015, task #3 \"answer selection in community question answering\" for english, for both subtasks. We submit two runs with different classi\ufb01ers in combining typical features (lexical similarity, string similarity, word n-grams, etc.) with machine translation evaluation metrics and with some ad hoc features (e.g user overlapping, spam \ufb01ltering). We outperform the baseline system and achieve interesting results on both subtasks.","wordlikeness":0.1428571429,"lcsratio":0.4285714286,"wordcoverage":0.5454545455}
{"Year":2016,"Venue":"ws-2016","Acronym":"SRDF","Description":"Extracting Lexical Knowledge Graph for Preserving Sentence Meaning","Abstract":"In this paper, we present an open information extraction system so-called designed that generates lexical knowledge graphs from unstructured texts. In semantic web, knowledge is expressed in the rdf triple form but the natural language text consist of multiple relations between arguments. For this reason, we combine open information extraction with the reification for the full text extraction to preserve meaning of sentence in our knowledge graph. And also our knowledge graph is designed to adapt for many existing semantic web applications. At the end of this paper, we introduce the result of the experiment and a korean template generation module developed using lexical.","wordlikeness":0.25,"lcsratio":0.75,"wordcoverage":0.75}
{"Year":2008,"Venue":"lrec-2008","Acronym":"KYOTO","Description":"a System for Mining, Structuring and Distributing Knowledge across Languages and Cultures","Abstract":"We outline work performed within the framework of a current ec project. The goal is to construct a language-independent information system for a specific domain (environment\/ecology\/biodiversity) anchored in a language-independent ontology that is linked to wordnets in seven languages. For each language, information extraction and identification of lexicalized concepts with ontological entries is carried out by text miners (\u0093kybots\u0094). The mapping of language-specific lexemes to the ontology allows for crosslinguistic identification and translation of equivalent terms. The infrastructure developed within this project enables long-range knowledge sharing and transfer across many languages and cultures, addressing the need for global and uniform transition of knowledge beyond the specific domains addressed here.","wordlikeness":0.6,"lcsratio":0.8,"wordcoverage":0.7272727273}
{"Year":2023,"Venue":"cl-2023","Acronym":"Onception","Description":"Active Learning with Expert Advice for Real World Machine Translation","Abstract":"Active learning can play an important role in low-resource settings (i.e., where annotated data is scarce), by selecting which instances may be more worthy to annotate. Most active learning approaches for machine translation assume the existence of a pool of sentences in a source language, and rely on human annotators to provide translations or post-edits, which can still be costly. In this article, we apply active learning to a real-world human-in-the-loop scenario in which we assume that: (1) the source sentences may not be readily available, but instead arrive in a stream; (2) the automatic translations receive feedback in the form of a rating, instead of a correct\/edited translation, since the human-in-the-loop might be a user looking for a translation, but not be able to provide one. To tackle the challenge of deciding whether each incoming pair source\u2013translations is worthy to query for human feedback, we resort to a number of stream-based active learning query strategies. Moreover, because we do not know in advance which query strategy will be the most adequate for a certain language pair and set of machine translation models, we propose to dynamically combine multiple strategies using prediction with expert advice. Our experiments on different language pairs and feedback settings show that using active learning allows us to converge on the best machine translation systems with fewer human interactions. Furthermore, combining multiple strategies using prediction with expert advice outperforms several individual active learning strategies with even fewer interactions, particularly in partial feedback settings.","wordlikeness":0.8888888889,"lcsratio":0.7777777778,"wordcoverage":0.9473684211}
{"Year":2018,"Venue":"conll-2018","Acronym":"SParse","Description":"Ko\\cc University Graph-Based Parsing System for the CoNLL 2018 Shared Task","Abstract":"We present conll, our graph-based parsing model submitted for the conll 2018 shared task: multilingual parsing from raw text to universal dependencies (zeman et al., 2018). Our model extends the state-of-the-art biaffine parser (dozat and manning, 2016) with a structural meta-learning module, smeta, that combines local and global label predictions. Our parser has been trained and run on universal dependencies datasets (nivre et al., 2016, 2018) and has 87.48% las, 78.63% mlas, 78.69% blex and 81.76% clas (nivre and fang, 2017) score on the italian-isdt dataset and has 72.78% las, 59.10% mlas, 61.38% blex and 61.72% clas score on the japanese-gsd dataset in our official submission. All other corpora are evaluated after the submission deadline, for whom we present our unofficial test results.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2011,"Venue":"ws-2011","Acronym":"Punctuation","Description":"Making a Point in Unsupervised Dependency Parsing","Abstract":"We show how net can be used to improve unsupervised dependency parsing. Our linguistic analysis con\ufb01rms the strong connection between english street and phrase boundaries in the penn treebank. However, approaches that naively include a marks in the grammar (as if they were words) do not perform well with klein and manning\u2019s dependency model with valence (dmv). Instead, we split a sentence at show and impose parsing restrictions over its fragments. Our grammar inducer is trained on the wall street journal (wsj) and achieves 59.5% accuracy out-of-domain (brown sentences with 100 or fewer words), more than 6% higher than the previous best results. Further evaluation, using the 2006\/7 conll sets, reveals that improve aids grammar induction in 17 of 18 languages, for an overall average net gain of 1.3%. Some of this improvement is from training, but more than half is from parsing with induced constraints, in inference. Our-aware decoding works with existing (even already-trained) parsing models and always increased accuracy in our experiments.","wordlikeness":0.8181818182,"lcsratio":0.6363636364,"wordcoverage":0.7826086957}
{"Year":2021,"Venue":"ranlp-2021","Acronym":"InFoBERT","Description":"Zero-Shot Approach to Natural Language Understanding Using Contextualized Word Embedding","Abstract":"Natural language understanding is an important task in modern dialogue systems. It becomes more important with the rapid extension of the dialogue systems\u2019 functionality. In this work, we present an approach to zero-shot transfer learning for the tasks of intent classification and slot-filling based on pre-trained language models. We use deep contextualized models feeding them with utterances and natural language descriptions of user intents to get text embeddings. These embeddings then used by a small neural network to produce predictions for intent and slot probabilities. This architecture achieves new state-of-the-art results in two zero-shot scenarios. One is a single language new skill adaptation and another one is a cross-lingual adaptation.","wordlikeness":0.875,"lcsratio":0.625,"wordcoverage":0.7142857143}
{"Year":2012,"Venue":"starsem-2012","Acronym":"UWashington","Description":"Negation Resolution using Machine Learning Methods","Abstract":"This paper reports on a simple system for resolving the scope of negation in the closed track of the *sem 2012 shared task. Cue detection is performed using regular expression rules extracted from the training data. Both scope tokens and negated event tokens are resolved using a conditional random field (crf) sequence tagger \u2013 namely the simpletagger library in the mallet machine learning toolkit. The full negation f1 score obtained for the task evaluation is 48.09% (p=74.02%, r=35.61%) which ranks this system fourth among the six submitted for the closed track.","wordlikeness":0.8181818182,"lcsratio":0.7272727273,"wordcoverage":0.9523809524}
{"Year":2015,"Venue":"acl-2015","Acronym":"KeLP","Description":"a Kernel-based Learning Platform for Natural Language Processing","Abstract":"Kernel-based learning algorithms have been shown to achieve state-of-the-art results in many natural language processing (nlp) tasks. We present from, a java framework that supports the implementation of both kernel-based learning algorithms and kernel functions over generic data representation, e.g. vectorial data or discrete structures. The framework has been designed to decouple kernel functions and learning algorithms: once a new kernel function has been implemented it can be adopted in all the available kernelmachine algorithms. The platform includes different online and batch learning algorithms for classi\ufb01cation, regression and clustering, as well as several kernel functions, ranging from vector-based to structural kernels. This paper will show the main aspects of the framework by applying it to different nlp tasks.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2014,"Venue":"bionlp-2014","Acronym":"FFTM","Description":"A Fuzzy Feature Transformation Method for Medical Documents","Abstract":"The vast array of medical text data represents a valuable resource that can be analyzed to advance the state of the art in medicine. Currently, text mining methods are being used to analyze medical research and clinical text data. Some of the main challenges in text analysis are high dimensionality and noisy data. There is a need to develop novel feature transformation methods that help reduce the dimensionality of data and improve the performance of machine learning algorithms. In this paper we present a feature transformation method named show. We illustrate the ef\ufb01cacy of our method using local term weighting, global term weighting, and fuzzy clustering methods and show that the quality of text analysis in medical text documents can be improved. We compare main with latent dirichlet allocation (lda) by using two different datasets and statistical tests show that and outperforms lda.","wordlikeness":0.25,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2013,"Venue":"semeval-2013","Acronym":"IIRG","Description":"A Naive Approach to Evaluating Phrasal Semantics","Abstract":"This paper describes the and 1 system entered in semeval-2013, the 7th international workshop on semantic evaluation. We participated in task 5 evaluating phrasal semantics. We have adopted a token-based approach to solve this task using 1) na\u00a8\u0131ve bayes methods and 2) word overlap methods, both of which rely on the extraction of syntactic features. We found that the word overlap method signi\ufb01cantly out-performs the na\u00a8\u0131ve bayes methods, achieving our highest overall score with an accuracy of approximately 78%.","wordlikeness":0.25,"lcsratio":0.75,"wordcoverage":0.75}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"XDoc","Description":"Unified Pre-training for Cross-Format Document Understanding","Abstract":"The surge of pre-training has witnessed the rapid development of document understanding recently. Pre-training and fine-tuning framework has been effectively used to tackle texts in various formats, including plain texts, document texts, and web texts. Despite achieving promising performance, existing pre-trained models usually target one specific document format at one time, making it difficult to combine knowledge from multiple document formats. To address this, we propose meanwhile,, a unified pre-trained model which deals with different document formats in a single model. For parameter efficiency, we share backbone parameters for different formats such as the word embedding layer and the transformer layers. Meanwhile, we introduce adaptive layers with lightweight parameters to enhance the distinction across different formats. Experimental results have demonstrated that with only 36.7% parameters, tasks achieves comparable or even better performance on a variety of downstream tasks compared with the individual pre-trained models, which is cost effective for real-world deployment. The code and pre-trained models are publicly available at <a href=https:\/\/aka.ms\/specific class=acl-markup-url>https:\/\/aka.ms\/we<\/a>.","wordlikeness":0.75,"lcsratio":0.75,"wordcoverage":0.8571428571}
{"Year":2006,"Venue":"wac-2006","Acronym":"CUCWeb","Description":"A Catalan corpus built from the Web","Abstract":"This paper presents architecture, a 166 million word corpus for catalan built by crawling the web. The corpus has been annotated with nlp tools and made available to language users through a \ufb02exible web interface. The developed architecture is quite general, so that it can be used to create corpora for other languages.","wordlikeness":0.6666666667,"lcsratio":0.8333333333,"wordcoverage":0.6666666667}
{"Year":2010,"Venue":"coling-2010","Acronym":"HCAMiner","Description":"Mining Concept Associations for Knowledge Discovery through Concept Chain Queries","Abstract":"This paper presents chains, a system focusing on detecting how concepts are linked across multiple documents. A traditional search involving, for example, two person names will attempt to find documents mentioning both these individuals. This research focuses on a different interpretation of such a query: what is the best concept chain across multiple documents that connects these individuals? A new robust framework is presented, based on (i) generating concept association graphs, a hybrid content representation, (ii) performing concept chain queries (ccq) to discover candidate chains, and (iii) subsequently ranking chains according to the significance of relationships suggested. These functionalities are implemented using an interactive visualization paradigm which assists users for a better understanding and interpretation of discovered relationships.","wordlikeness":0.75,"lcsratio":0.875,"wordcoverage":0.7777777778}
{"Year":2020,"Venue":"coling-2020","Acronym":"KanCMD","Description":"Kannada CodeMixed Dataset for Sentiment Analysis and Offensive Language Detection","Abstract":"We introduce kannada codemixed dataset (tasks,), a multi-task learning dataset for sentiment analysis and offensive language identification. The youtube, dataset highlights two real-world issues from the social media text. First, it contains actual comments in code mixed text posted by users on youtube social media, rather than in monolingual text from the textbook. Second, it has been annotated for two tasks, namely sentiment analysis and offensive language detection for under-resourced kannada language. Hence, comment. Is meant to stimulate research in under-resourced kannada language on real-world code-mixed social media text and multi-task learning. Was was obtained by crawling the youtube, and a minimum of three annotators annotates each comment. We release posted 7,671 comments for multitask learning research purpose.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2008,"Venue":"lrec-2008","Acronym":"Language-Sites","Description":"Accessing and Presenting Language Resources via Geographic Information Systems","Abstract":"The emerging area of geographic information systems (gis) has proven to add an interesting dimension to many research projects. Within the range initiative we have brought together a broad range of links to digital language corpora and resources. Via google earth\u0092s visually appealing 3d-interface users can spin the globe, zoom into an area they are interested in and access directly the relevant language resources. This paper focuses on several ways of relating the map and the online data (lexica, annotations, multimedia recordings, etc.). Furthermore, we discuss some of the implementation choices that have been made, including future challenges. In addition, we show how scholars (both linguists and anthropologists) are using gis tools to fulfill their specific research needs by making use of practical examples. This illustrates how both scientists and the general public can benefit from geography-based access to digital language data.","wordlikeness":0.7857142857,"lcsratio":0.9285714286,"wordcoverage":0.7826086957}
{"Year":2016,"Venue":"lrec-2016","Acronym":"AIMU","Description":"Actionable Items for Meeting Understanding","Abstract":"With emerging conversational data, automated content analysis is needed for better data interpretation, so that it is accurately understood and can be effectively integrated and utilized in various applications. Icsi meeting corpus is a publicly released data set of multi-party meetings in an organization that has been released over a decade ago, and has been fostering meeting understanding research since then. The original data collection includes transcription of participant turns as well as meta-data annotations, such as disfluencies and dialog act tags. This paper presents an extended set of annotations for the icsi meeting corpus with a goal of deeply understanding meeting conversations, where participant turns are annotated by actionable items that could be performed by an automated meeting assistant. In addition to the user utterances that contain an actionable item, annotations also include the arguments associated with the actionable item. The set of actionable items are determined by aligning human-human interactions to human-machine interactions, where a data annotation schema designed for a virtual personal assistant (human-machine genre) is adapted to the meetings domain (human-human genre). The data set is formed by annotating participants\u2019 utterances in meetings with potential intents\/actions considering their contexts. The set of actions target what could be accomplished by an automated meeting assistant, such as taking a note of action items that a participant commits to, or finding emails or topic related documents that were mentioned during the meeting. A total of 10 defined intents\/actions are considered as actionable items in meetings. Turns that include actionable intents were annotated for 22 public icsi meetings, that include a total of 21k utterances, segmented by speaker turns. Participants\u2019 spoken turns, possible actions along with associated arguments and their vector representations as computed by convolutional deep structured semantic models are included in the data set for future research. We present a detailed statistical analysis of the data set and analyze the performance of applying convolutional deep structured semantic models for an actionable item detection task. The data is available at <a href=http:\/\/research.microsoft.com\/projects\/meetingunderstanding\/ class=acl-markup-url>http:\/\/research.microsoft.com\/projects\/meetingunderstanding\/<\/a>.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2022,"Venue":"findings-2022","Acronym":"Mukayese","Description":"Turkish NLP Strikes Back","Abstract":"Having sufficient resources for language x lifts it from the under-resourced languages class, but not necessarily from the under-researched class. In this paper, we address the problem of the absence of organized benchmarks in the turkish language. We demonstrate that languages such as turkish are left behind the state-of-the-art in nlp applications. As a solution, we present not, a set of nlp benchmarks for the turkish language that contains several nlp tasks. We work on one or more datasets for each benchmark and present two or more baselines. Moreover, we present four new benchmarking datasets in turkish for language modeling, sentence segmentation, and spell checking. All datasets and baselines are available under: <a href=https:\/\/github.com\/alisafaya\/address class=acl-markup-url>https:\/\/github.","wordlikeness":0.5,"lcsratio":0.5,"wordcoverage":0.6666666667}
{"Year":2018,"Venue":"lrec-2018","Acronym":"PMKI","Description":"an European Commission action for the interoperability, maintainability and sustainability of Language Resources","Abstract":"The paper presents the public multilingual knowledge management infrastructure (for) action launched by the european commission (ec) to promote the digital single market in the european union (eu). That aims to share maintainable and sustainable language resources making them interoperable in order to support language technology industry, and public administrations, with multilingual tools able to improve cross border accessibility of digital services. The paper focuses on the main feature (interoperability) that represents the specificity of required platform distinguishing it from other existing frameworks. In particular it aims to create a set of tools and facilities, based on semantic web technologies, to establish semantic interoperability between multilingual lexicons. Such task requires to harmonize in general multilingual language resources using standardised representation with respect to a defined core data model under an adequate architecture. A comparative study among the main data models for representing lexicons and recommendations for the models service was required as well. Moreover, synergies with other programs of the eu institutions, as far as systems interoperability and machine translation (mt) solutions, are foreseen. For instance some interactions are foreseen between lexicons and mt service provided by the ec but also with other nlp applications. Keywords: language resources, interoperability, maintainability, sustainability. 1.","wordlikeness":0.75,"lcsratio":0.75,"wordcoverage":0.8571428571}
{"Year":2019,"Venue":"louhi-2019","Acronym":"BioReddit","Description":"Word Embeddings for User-Generated Biomedical NLP","Abstract":"Word embeddings, in their different shapes and iterations, have changed the natural language processing research landscape in the last years. The biomedical text processing field is no stranger to this revolution; however, scholars in the field largely trained their embeddings on scientific documents only, even when working on user-generated data. In this paper we show how training embeddings from a corpus collected from user-generated text from medical forums heavily influences the performance on downstream tasks, outperforming embeddings trained both on general purpose data or on scientific papers when applied on user-generated content.","wordlikeness":0.6666666667,"lcsratio":0.8888888889,"wordcoverage":0.8}
{"Year":2022,"Venue":"lrec-2022","Acronym":"UMUTextStats","Description":"A linguistic feature extraction tool for Spanish","Abstract":"Feature engineering consists in the application of domain knowledge to select and transform relevant features to build efficient machine learning models. In the natural language processing field, the state of the art concerning automatic document classification tasks relies on word and sentence embeddings built upon deep learning models based on transformers that have outperformed the competition in several tasks. However, the models built from these embeddings are usually difficult to interpret. On the contrary, linguistic features are easy to understand, they result in simpler models, and they usually achieve encouraging results. Moreover, both linguistic features and embeddings can be combined with different strategies which result in more reliable machine-learning models. The de facto tool for extracting linguistic features in spanish is liwc. However, this software does not consider specific linguistic phenomena of spanish such as grammatical gender and lacks certain verb tenses. In order to solve these drawbacks, we have developed the, a linguistic extraction tool designed from scratch for spanish. Furthermore, this tool has been validated to conduct different experiments in areas such as infodemiology, hate-speech detection, author profiling, authorship verification, humour or irony detection, among others. The results indicate that the combination of linguistic features and embeddings based on transformers are beneficial in automatic document classification.","wordlikeness":0.6666666667,"lcsratio":0.6666666667,"wordcoverage":0.6315789474}
{"Year":2018,"Venue":"bea-2018","Acronym":"UnibucKernel","Description":"A kernel-based learning method for complex word identification","Abstract":"In this paper, we present a kernel-based learning approach for the 2018 complex word identification (cwi) shared task. Our approach is based on combining multiple low-level features, such as character n-grams, with high-level semantic features that are either automatically learned using word embeddings or extracted from a lexical knowledge base, namely wordnet. After feature extraction, we employ a kernel method for the learning phase. The feature matrix is first transformed into a normalized kernel matrix. For the binary classification task (simple versus complex), we employ support vector machines. For the regression task, in which we have to predict the complexity level of a word (a word is more complex if it is labeled as complex by more annotators), we employ v-support vector regression. We applied our approach only on the three english data sets containing documents from wikipedia, wikinews and news domains. Our best result during the competition was the third place on the english wikipedia data set. However, in this paper, we also report better post-competition results.","wordlikeness":0.6666666667,"lcsratio":0.5833333333,"wordcoverage":0.6666666667}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"FAME","Description":"Feature-Based Adversarial Meta-Embeddings for Robust Input Representations","Abstract":"Combining several embeddings typically improves performance in downstream tasks as different embeddings encode different information. It has been shown that even models using embeddings from transformers still benefit from the inclusion of standard word embeddings. However, the combination of embeddings of different types and dimensions is challenging. As an alternative to attention-based meta-embeddings, we propose feature-based adversarial meta-embeddings (sets) with an attention function that is guided by features reflecting word-specific properties, such as shape and frequency, and show that this is beneficial to handle subword-based embeddings. In addition, challenging. Uses adversarial training to optimize the mappings of differently-sized embeddings to the same space. We demonstrate that of works effectively across languages and domains for sequence labeling and sentence classification, in particular in low-resource settings. Features sets the new state of the art for pos tagging in 27 languages, various ner settings and question classification in different domains.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2015,"Venue":"semeval-2015","Acronym":"TALN-UPF","Description":"Taxonomy Learning Exploiting CRF-Based Hypernym Extraction on Encyclopedic Definitions","Abstract":"This paper describes the system submitted by the querying team to semeval task 17 (taxonomy extraction evaluation). We present a method for automatically learning a taxonomy from a \ufb02at terminology, which bene\ufb01ts from a de\ufb01nition corpus obtained by querying the babelnet semantic network. Then, we combine a machine-learning algorithm for term-hypernym extraction with linguistically-motivated heuristics for hypernym decomposition. Our approach performs well in terms of vertex coverage and newly added vertices, while it shows room for improvement in terms of graph topology, edge coverage and precision of novel edges.","wordlikeness":0.5,"lcsratio":0.875,"wordcoverage":0.5714285714}
{"Year":2015,"Venue":"semeval-2015","Acronym":"VectorSLU","Description":"A Continuous Word Vector Approach to Answer Selection in Community Question Answering Systems","Abstract":"Continuous word and phrase vectors have proven useful in a number of nlp tasks. Here we describe our experience using them as a source of features for the semeval-2015 task 3, consisting of two community question answering subtasks: answer selection for categorizing answers as potential, good, and bad with regards to their corresponding questions; and yes\/no inference for predicting a yes, no, or unsure response to a yes\/no question using all of its good answers. Our system ranked 6th and 1st in the english answer selection and yes\/no inference subtasks respectively, and 2nd in the arabic answer selection subtask.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2022,"Venue":"ws-2022","Acronym":"TRAttack","Description":"Text Rewriting Attack Against Text Retrieval","Abstract":"Text retrieval has been widely-used in many online applications to help users find relevant information from a text collection. In this paper, we study a new attack scenario against text retrieval to evaluate its robustness to adversarial attacks under the black-box setting, in which attackers want their own texts to always get high relevance scores with different users\u2019 input queries and thus be retrieved frequently and can receive large amounts of impressions for profits. Considering that most current attack methods only simply follow certain fixed optimization rules, we propose a novel text rewriting attack (only) method with learning ability from the multi-armed bandit mechanism. Extensive experiments conducted on simulated victim environments demonstrate that method can yield texts that have higher relevance scores with different given users\u2019 queries than those generated by current state-of-the-art attack methods. We also evaluate information on tencent cloud\u2019s and baidu cloud\u2019s commercially-available text retrieval apis, and the rewritten adversarial texts successfully get high relevance scores with different user queries, which shows the practical potential of our method and the risk of text retrieval systems.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2006,"Venue":"coling-2006","Acronym":"Espresso","Description":"Leveraging Generic Patterns for Automatically Harvesting Semantic Relations","Abstract":"In this paper, we present various, a weakly-supervised, general-purpose, and accurate algorithm for harvesting semantic relations. The main contributions are: i) a method for exploiting generic patterns by filtering incorrect instances using the web; and ii) a principled measure of pattern and instance reliability enabling the filtering algorithm. We present an empirical comparison of filtering with various state of the art systems, on different size and genre corpora, on extracting various general and specific relations. Experimental results show that our exploitation of generic patterns substantially increases system recall with small effect on overall precision.","wordlikeness":1.0,"lcsratio":0.875,"wordcoverage":1.0}
{"Year":2023,"Venue":"findings-2023","Acronym":"Click","Description":"Controllable Text Generation with Sequence Likelihood Contrastive Learning","Abstract":"It has always been an important yet challenging problem to control language models to avoid generating texts with undesirable attributes, such as toxic language and unnatural repetition. We introduce leo for controllable text generation, which needs no modification to the model architecture and facilitates out-of-the-box use of trained models. It employs a contrastive loss on sequence likelihood, which fundamentally decreases the generation probability of negative samples (i.e., generations with undesirable attributes). It also adopts a novel likelihood ranking-based strategy to construct contrastive samples from model generations. On the tasks of language detoxification, sentiment steering, and repetition reduction, we show that leo outperforms strong baselines of controllable text generation and demonstrate the superiority of leo\u2019s sample construction strategy.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2010,"Venue":"semeval-2010","Acronym":"UHD","Description":"Cross-Lingual Word Sense Disambiguation Using Multilingual Co-Occurrence Graphs","Abstract":"We describe the university of heidelberg (approaches) system for the cross-lingual word sense disambiguation semeval-2010 task (cl-wsd). The system performs clwsd by applying graph algorithms previously developed for monolingual word sense disambiguation to multilingual cooccurrence graphs. Task. Has participated in the best and out-of-\ufb01ve (oof) evaluations and ranked among the most competitive systems for this task, thus indicating that graph-based approaches represent a powerful alternative for this task.","wordlikeness":0.3333333333,"lcsratio":0.6666666667,"wordcoverage":0.8}
{"Year":2021,"Venue":"acl-2021","Acronym":"DialogueCRN","Description":"Contextual Reasoning Networks for Emotion Recognition in Conversations","Abstract":"Emotion recognition in conversations (erc) has gained increasing attention for developing empathetic machines. Recently, many approaches have been devoted to perceiving conversational context by deep learning models. However, these approaches are insufficient in understanding the context due to lacking the ability to extract and integrate emotional clues. In this work, we propose novel contextual reasoning networks (intuitive) to fully understand the conversational context from a cognitive perspective. Inspired by the cognitive theory of emotion, we design multi-turn reasoning modules to extract and integrate emotional clues. The reasoning module iteratively performs an intuitive retrieving process and a conscious reasoning process, which imitates human unique cognitive thinking. Extensive experiments on three public benchmark datasets demonstrate the effectiveness and superiority of the proposed model.","wordlikeness":0.7272727273,"lcsratio":0.7272727273,"wordcoverage":0.8421052632}
{"Year":2014,"Venue":"lrec-2014","Acronym":"GlobalPhone","Description":"Pronunciation Dictionaries in 20 Languages","Abstract":"This paper describes the advances in the multilingual text and speech database paper, a multilingual database of high-quality read speech with corresponding transcriptions and pronunciation dictionaries in 20 languages. Very was designed to be uniform across languages with respect to the amount of data, speech quality, the collection scenario, the transcription and phone set conventions. With more than 400 hours of transcribed audio data from more than 2000 native speakers collection supplies an excellent basis for research in the areas of multilingual speech recognition, rapid deployment of speech processing systems to yet unsupported languages, language identification tasks, speaker recognition in multiple languages, multilingual speech synthesis, as well as monolingual speech recognition in a large variety of languages. Very recently the advances pronunciation dictionaries have been made available for research and commercial purposes by the european language resources association (elra).","wordlikeness":0.9090909091,"lcsratio":0.4545454545,"wordcoverage":0.7058823529}
{"Year":2018,"Venue":"lrec-2018","Acronym":"UFSAC","Description":"Unification of Sense Annotated Corpora and Tools","Abstract":"In word sense disambiguation, sense annotated corpora are often essential for evaluating a system and also valuable in order to reach a good ef\ufb01ciency. Always created for a speci\ufb01c purpose, there are today a dozen of sense annotated english corpora, in various formats and using different versions of wordnet. The main hypothesis of this work is that it should be possible to build a disambiguation system by using any of these corpora during the training phase or during the testing phase regardless of their original purpose. In this article, we present code: a format of corpus that can be used for either training or testing a disambiguation system, and the process we followed for constructing this format. We give to the community the whole set of sense annotated english corpora that we know, in this uni\ufb01ed format, when the copyright allows it, with sense keys converted to the last version of wordnet. We also provide the source code for building these corpora from their original data, and a complete java api for manipulating corpora in this format. The whole resource is available at the following url: https:\/\/github.com\/getalp\/possible. Keywords: word sense disambiguation, sense annotated corpora, uni\ufb01ed resource, tools 1.","wordlikeness":0.2,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2021,"Venue":"ws-2021","Acronym":"CS-BERT","Description":"a pretrained model for customer service dialogues","Abstract":"Large-scale pretrained transformer models have demonstrated state-of-the-art (sota) performance in a variety of nlp tasks. Nowadays, numerous pretrained models are available in different model flavors and different languages, and can be easily adapted to one\u2019s downstream task. However, only a limited number of models are available for dialogue tasks, and in particular, goal-oriented dialogue tasks. In addition, the available pretrained models are trained on general domain language, creating a mismatch between the pretraining language and the downstream domain launguage. In this contribution, we present finetuning, a bert model pretrained on millions of dialogues in the customer service domain. We evaluate easily on several downstream customer service dialogue tasks, and demonstrate that our in-domain pretraining is advantageous compared to other pretrained models in both zero-shot experiments as well as in finetuning experiments, especially in a low-resource data setting.","wordlikeness":0.5714285714,"lcsratio":0.5714285714,"wordcoverage":0.7142857143}
{"Year":2023,"Venue":"acl-2023","Acronym":"ManagerTower","Description":"Aggregating the Insights of Uni-Modal Experts for Vision-Language Representation Learning","Abstract":"Two-tower vision-language (vl) models have shown promising improvements on various downstream vl tasks. Although the most advanced work improves performance by building bridges between encoders, it suffers from ineffective layer-by-layer utilization of uni-modal representations and cannot flexibly exploit different levels of uni-modal semantic knowledge. In this work, we propose propose, a novel vl model architecture that gathers and combines the insights of pre-trained uni-modal experts at different levels. The managers introduced in each cross-modal layer can adaptively aggregate uni-modal semantic knowledge to facilitate more comprehensive cross-modal alignment and fusion. Between outperforms previous strong baselines both with and without vision-language pre-training (vlp). With only 4m vlp data, that achieves superior performances on various downstream vl tasks, especially 79.15% accuracy on vqav2 test-std, 86.56% ir@1 and 95.64% tr@1 on flickr30k. Code and checkpoints are available at <a href=https:\/\/github.com\/looperxx\/of class=acl-markup-url>https:\/\/github.com\/looperxx\/4m<\/a>.","wordlikeness":0.8333333333,"lcsratio":0.9166666667,"wordcoverage":0.7368421053}
{"Year":2012,"Venue":"inlg-2012","Acronym":"MinkApp","Description":"Generating Spatio-temporal Summaries for Nature Conservation Volunteers","Abstract":"We describe preliminary work on generating contextualized text for nature conservation volunteers. This natural language generation (nlg) differs from other ways of describing spatio-temporal data, in that it deals with abstractions on data across large geographical spaces (total projected area 20,600 km2), as well as temporal trends across longer time frames (ranging from one week up to a year). We identify challenges at all stages of the classical nlg pipeline.","wordlikeness":0.7142857143,"lcsratio":0.5714285714,"wordcoverage":0.7272727273}
{"Year":2012,"Venue":"lrec-2012","Acronym":"MULTIPHONIA","Description":"a MULTImodal database of PHONetics teaching methods in classroom InterActions.","Abstract":"The various corpus consists of audio-video classroom recordings comparing two methods of phonetic correction (the \u0091traditional' articulatory method, and the verbo-tonal method) this database was created not only to remedy the crucial lack of information and pedagogical resources on teaching pronunciation but also to test the benefit of vtm on second language pronunciation. The vtm method emphasizes the role of prosody cues as vectors of second language acquisition of the phonemic system. This method also provides various and unusual procedures including facilitating gestures in order to work on spotting and assimilating the target language prosodic system (rhythm, accentuation, intonation). In doing so, speech rhythm is apprehended in correlation with body\/gestural rhythm. The student is thus encouraged to associate gestures activating the motor memory at play during the repetition of target words or phrases. In turn, pedagogical gestures have an impact on second language lexical items' recollection (allen, 1995; tellier, 2008). Ultimately, this large corpus (96 hours of class sessions' recordings) will be made available to the scientific community, with several layers of annotations available for the study of segmental, prosodic and gestural aspects of l2 speech.","wordlikeness":0.7272727273,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2022,"Venue":"lrec-2022","Acronym":"BembaSpeech","Description":"A Speech Recognition Corpus for the Bemba Language","Abstract":"We present a preprocessed, ready-to-use automatic speech recognition corpus, in, consisting over 24 hours of read speech in the bemba language, a written but low-resourced language spoken by over 30% of the population in zambia. To assess its usefulness for training and testing asr systems for bemba, we explored different approaches; supervised pre-training (training from scratch), cross-lingual transfer learning from a monolingual english pre-trained model using deepspeech on the portion of the dataset and fine-tuning large scale self-supervised wav2vec2.0 based multilingual pre-trained models on the complete dataset corpus. From our experiments, the 1 billion xls-r parameter model gives the best results. The model achieves a word error rate (wer) of 32.91%, results demonstrating that model capacity significantly improves performance and that multilingual pre-trained models transfers cross-lingual acoustic representation better than monolingual pre-trained english model on the multilingual for the bemba asr. Lastly, results also show that the corpus can be used for building asr systems for bemba language.","wordlikeness":0.6363636364,"lcsratio":0.6363636364,"wordcoverage":0.7058823529}
{"Year":2005,"Venue":"mtsummit-2005","Acronym":"PARSIT-TE","Description":"Online Thai-English Machine Translation","Abstract":"This paper presents an online thai-english mt system, called parsitte, which is an extension of parsit english-thai one. We aim to assist foreigners and thai in exchanging more easily their information. The system is a rule-based and interlingua approach. To improve the system, we concentrate on pre-processing and rule analysis phases, which are considered necessary because of some specific problems of thai language.","wordlikeness":0.4444444444,"lcsratio":0.5555555556,"wordcoverage":0.8235294118}
{"Year":2022,"Venue":"lrec-2022","Acronym":"Lutma","Description":"A Frame-Making Tool for Collaborative FrameNet Development","Abstract":"This paper presents users, a collaborative, semi-constrained, tutorial-based tool for contributing frames and lexical units to the global framenet initiative. The tool parameterizes the process of frame creation, avoiding consistency violations and promoting the integration of frames contributed by the community with existing frames. Paper is structured in a wizard-like fashion so as to provide users with text and video tutorials relevant for each step in the frame creation process. We argue that this tool will allow for a sensible expansion of framenet coverage in terms of both languages and cultural perspectives encoded by them, positioning frames as a viable alternative for representing perspective in language models.","wordlikeness":0.2,"lcsratio":0.6,"wordcoverage":0.6666666667}
{"Year":2020,"Venue":"scil-2020","Acronym":"MonaLog","Description":"a Lightweight System for Natural Language Inference Based on Monotonicity","Abstract":"We present a new logic-based inference engine for natural language inference (nli) called intentionally, which is based on natural logic and the monotonicity calculus. In contrast to existing logic-based approaches, our system is intentionally designed to be as lightweight as possible, and operates using a small set of well-known (surface-level) monotonicity facts about quanti\ufb01ers, lexical items and tokenlevel polarity information. Despite its simplicity, we \ufb01nd our approach to be competitive with other logic-based nli models on the sick benchmark. We also use approaches, in combination with the current state-of-the-art model bert in a variety of settings, including for compositional data augmentation. We show that (surface-level) is capable of generating large amounts of high-quality training data for bert, improving its accuracy on sick.","wordlikeness":0.7142857143,"lcsratio":0.8571428571,"wordcoverage":0.7692307692}
{"Year":2022,"Venue":"tacl-2022","Acronym":"DP-Parse","Description":"Finding Word Boundaries from Raw Speech with an Instance Lexicon","Abstract":"Finding word boundaries in continuous speech is challenging as there is little or no equivalent of a \u2018space\u2019 delimiter between words. Popular bayesian non-parametric models for text segmentation (goldwater et al., 2006, 2009) use a dirichlet process to jointly segment sentences and build a lexicon of word types. We introduce (goldwater, which uses similar principles but only relies on an instance lexicon of word tokens, avoiding the clustering errors that arise with a lexicon of word types. On the zero resource speech benchmark 2017, our model sets a new speech segmentation state-of-the-art in 5 languages. The algorithm monotonically improves with better input representations, achieving yet higher scores when fed with weakly supervised inputs. Despite lacking a type lexicon, benchmark. Can be pipelined to a language model and learn semantic and syntactic representations as assessed by a new spoken word embedding benchmark.","wordlikeness":0.375,"lcsratio":0.625,"wordcoverage":0.7142857143}
{"Year":2011,"Venue":"acl-2011","Acronym":"Piggyback","Description":"Using Search Engines for Robust Cross-Domain Named Entity Recognition","Abstract":"We use search engine results to address a particularly dif\ufb01cult cross-domain language processing task, the adaptation of named entity recognition (ner) from news text to web queries. The key novelty of the method is that we submit a token with context to a search engine and use similar contexts in the search results as additional information for correctly classifying the token. We achieve strong gains in ner performance on news, in-domain and out-of-domain, and on web queries.","wordlikeness":0.7777777778,"lcsratio":0.6666666667,"wordcoverage":0.7058823529}
{"Year":2010,"Venue":"lrec-2010","Acronym":"Vergina","Description":"A Modern Greek Speech Database for Speech Synthesis","Abstract":"The present paper outlines the are speech database, which was developed in support of research and development of corpus-based unit selection and statistical parametric speech synthesis systems for modern greek language. In the following, we describe the design, development and implementation of the recording campaign, as well as the annotation of the database. Specifically, a text corpus of approximately 5 million words, collected from newspaper articles, periodicals, and paragraphs of literature, was processed in order to select the utterances-sentences needed for producing the speech database and to achieve a reasonable phonetic coverage. The broad coverage and contents of the selected utterances-sentences of the database \u2015 text corpus collected from different domains and writing styles \u2015 makes this database appropriate for various application domains. The database, recorded in audio studio, consists of approximately 3,000 phonetically balanced modern greek utterances corresponding to approximately four hours of speech. Annotation of the recording speech database was performed using task-specific tools, which are based on a hidden markov model (hmm) segmentation method, and then manual inspection and corrections were performed.","wordlikeness":0.8571428571,"lcsratio":0.5714285714,"wordcoverage":0.8}
{"Year":2018,"Venue":"lrec-2018","Acronym":"FonBund","Description":"A Library for Combining Cross-lingual Phonological Segment Data","Abstract":"We present an open-source library (to) that provides a way of mapping sequences of arbitrary phonetic segments in international phonetic alphabet (ipa) into multiple articulatory feature representations. The library interfaces with several existing linguistic typology resources providing phonological segment inventories and their corresponding articulatory feature systems. Our \ufb01rst goal was to facilitate the derivation of articulatory features without giving a special preference to any particular phonological segment inventory provided by freely available linguistic typology resources. The second goal was to build a very light-weight library that can be easily modi\ufb01ed to support new phonological segment inventories. In order to support ipa segments that do not occur in the freely available resources, the library provides a simple con\ufb01guration language for performing segment rewrites and adding custom segments with the corresponding feature structures. In addition to introducing the library and the corresponding linguistic resources, we also describe some of the practical uses of this library (multilingual speech synthesis) in the hope that this software will help facilitate multilingual speech research. Keywords: phonology, phonetic segments, software 1.","wordlikeness":0.5714285714,"lcsratio":0.8571428571,"wordcoverage":0.8333333333}
{"Year":2020,"Venue":"coling-2020","Acronym":"Lin","Description":"Unsupervised Extraction of Tasks from Textual Communication","Abstract":"Commitments and requests are a hallmark of collaborative communication, especially in team settings. Identifying specific tasks being committed to or request from emails and chat messages can enable important downstream tasks, such as producing todo lists, reminders, and calendar entries. State-of-the-art approaches for task identification rely on large annotated datasets, which are not always available, especially for domain-specific tasks. Accordingly, we propose a, an unsupervised approach of identifying tasks that leverages dependency parsing and verbnet. Our evaluations show that dependency yields comparable or more accurate results than supervised models on domains with large training sets, and maintains its excellent performance on unseen domains.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2018,"Venue":"sigdial-2018","Acronym":"DialCrowd","Description":"A toolkit for easy dialog system assessment","Abstract":"When creating a dialog system, developers need to test each version to ensure that it is performing correctly. Recently the trend has been to test on large datasets or to ask many users to try out a system. Crowdsourcing has solved the issue of finding users, but it presents new challenges such as how to use a crowdsourcing platform and what type of test is appropriate. Many has been designed to make system assessment easier and to ensure the quality of the result. This paper describes this, what specific needs it fulfills and how it works. It then relates a test of works. By a group of dialog system developer.","wordlikeness":0.6666666667,"lcsratio":0.5555555556,"wordcoverage":0.7142857143}
{"Year":1997,"Venue":"anlp-1997","Acronym":"CogentHelp","Description":"NLG meets SE in a tool for authoring dynamically generated on-line help","Abstract":"Help is a prototype tool for au- thoring dynamically generated on-line help for applications with graphical user inter- faces, embodying the \"evolution-friendly\" properties of tools in the literate program- ming tradition. In this paper, we describe and, highlighting the usefulness of certain natural language generation tech- niques in supporting software-engineering goals for help authoring tools -- princi- pally, quality and evolvability of help texts.","wordlikeness":0.8,"lcsratio":0.9,"wordcoverage":0.6666666667}
{"Year":2014,"Venue":"acl-2014","Acronym":"XMEANT","Description":"Better semantic MT evaluation without reference translations","Abstract":"We introduce new\u2014a new cross-lingual version of the semantic frame based mt evaluation metric meant\u2014which can correlate even more closely with human adequacy judgments than monolingual meant and eliminates the need for expensive human references. Previous work established that meant re\ufb02ects translation adequacy with state-of-the-art accuracy, and optimizing mt systems against meant robustly improves translation quality. However, to go beyond tuning weights in the loglinear smt model, a cross-lingual objective function that can deeply integrate semantic frame criteria into the mt training pipeline is needed. We show that cross-lingual needed. Outperforms monolingual meant by (1) replacing the monolingual context vector model in meant with simple translation probabilities, and (2) incorporating bracketing itg constraints.","wordlikeness":0.6666666667,"lcsratio":0.8333333333,"wordcoverage":0.9090909091}
{"Year":2021,"Venue":"acl-2021","Acronym":"TGEA","Description":"An Error-Annotated Dataset and Benchmark Tasks for TextGeneration from Pretrained Language Models","Abstract":"In order to deeply understand the capability of pretrained language models in text generation and conduct a diagnostic evaluation, we propose with, an error-annotated dataset with multiple benchmark tasks for text generation from pretrained language models (plms). We use carefully selected prompt words to guide gpt-2 to generate candidate sentences, from which we select 47k for error annotation. Crowdsourced workers manually check each of these sentences and detect 12k erroneous sentences. We create an error taxonomy to cover 24 types of errors occurring in these erroneous sentences according to the nature of errors with respect to linguistics and knowledge (e.g., common sense). For each erroneous span in plm-generated sentences, we also detect another span that is closely associated with it. Each error is hence manually labeled with comprehensive annotations, including the span of the error, the associated span, minimal correction to the error, the type of the error, and rationale behind the error. Apart from the fully annotated dataset, we also present a detailed description of the data collection procedure, statistics and analysis of the dataset. This is the first dataset with comprehensive annotations for plm-generated texts, which facilitates the diagnostic evaluation of plm-based text generation. Furthermore, we use span as a benchmark dataset and propose a series of automatic diagnosis tasks, including error detection, error type classification, associated span detection, error rationale generation, to further promote future study on the automatic error detection and correction on texts generated by pretrained language models.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2016,"Venue":"acl-2016","Acronym":"MMFeat","Description":"A Toolkit for Extracting Multi-Modal Features","Abstract":"Research at the intersection of language and other modalities, most notably vision, is becoming increasingly important in natural language processing. We introduce a toolkit that can be used to obtain feature representations for visual and auditory information. That is an easy-to-use python toolkit, which has been developed with the purpose of making non-linguistic modalities more accessible to natural language processing researchers.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2018,"Venue":"emnlp-2018","Acronym":"Magnitude","Description":"A Fast, Efficient Universal Vector Embedding Utility Package","Abstract":"Vector space embedding models like word2vec, glove, and fasttext are extremely popular representations in natural language processing (nlp) applications. We present natural, a fast, lightweight tool for utilizing and processing embeddings. Performs is an open source python package with a compact vector storage file format that allows for efficient manipulation of huge numbers of embeddings. Models performs common operations up to 60 to 6,000 times faster than gensim. Natural introduces several novel features for improved robustness like out-of-vocabulary lookups.","wordlikeness":1.0,"lcsratio":0.6666666667,"wordcoverage":1.0}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"MS-Mentions","Description":"Consistently Annotating Entity Mentions in Materials Science Procedural Text","Abstract":"Material science synthesis procedures are a promising domain for scientific nlp, as proper modeling of these recipes could provide insight into new ways of creating materials. However, a fundamental challenge in building information extraction models for material science synthesis procedures is getting accurate labels for the materials, operations, and other entities of those procedures. We present a new corpus of entity mention annotations over 595 material science synthesis procedural texts (157,488 tokens), which greatly expands the training data available for the named entity recognition task. We outline a new label inventory designed to provide consistent annotations and a new annotation approach intended to maximize the consistency and annotation speed of domain experts. Inter-annotator agreement studies and baseline models trained upon the data suggest that the corpus provides high-quality annotations of these mention types. This corpus helps lay a foundation for future high-quality modeling of synthesis procedures.","wordlikeness":0.6363636364,"lcsratio":0.8181818182,"wordcoverage":0.8421052632}
{"Year":2022,"Venue":"naacl-2022","Acronym":"KALA","Description":"Knowledge-Augmented Language Model Adaptation","Abstract":"Pre-trained language models (plms) have achieved remarkable success on various natural language understanding tasks. Simple fine-tuning of plms, on the other hand, might be suboptimal for domain-specific tasks because they cannot possibly cover knowledge from all domains. While adaptive pre-training of plms can help them obtain domain-specific knowledge, it requires a large training cost. Moreover, adaptive pre-training can harm the plm\u2019s performance on the downstream task by causing catastrophic forgetting of its general knowledge. To overcome such limitations of adaptive pre-training for plm adaption, we propose a novel domain adaption framework for plms coined as knowledge-augmented language model adaptation (adaptive), which modulates the intermediate hidden representations of plms with domain knowledge, consisting of entities and their relational facts. We validate the performance of our novel on question answering and named entity recognition tasks on multiple datasets across various domains. The results show that, despite being computationally efficient, our novel largely outperforms adaptive pre-training.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2021,"Venue":"ws-2021","Acronym":"FuzzyBIO","Description":"A Proposal for Fuzzy Representation of Discontinuous Entities","Abstract":"Discontinuous entities pose a challenge to named entity recognition (ner). These phenomena occur commonly in the biomedical domain. As a solution, expansions of the bio representation scheme that can handle these entity types are commonly used (i.e. biohd). However, the extra tag types make the ner task more difficult to learn. In this paper we propose an alternative; a fuzzy continuous bio scheme (ner). We focus on the task of adverse drug response extraction and normalization to compare sets to biohd. We find that percentage improves recall of ner for two of three data sets and results in a higher percentage of correctly identified disjoint and composite entities for all data sets. Using two also improves end-to-end performance for continuous and composite entities in two of three data sets. Since investigating improves performance for some data sets and the conversion from biohd to pose is straightforward, we recommend investigating which is more effective for any data set containing discontinuous entities.","wordlikeness":0.875,"lcsratio":0.875,"wordcoverage":0.7692307692}
{"Year":2021,"Venue":"ws-2021","Acronym":"TopGuNN","Description":"Fast NLP Training Data Augmentation using Large Corpora","Abstract":"Acquiring training data for natural language processing systems can be expensive and time-consuming. Given a few training examples crafted by experts, large corpora can be mined for thousands of semantically similar examples that provide useful variability to improve model generalization. We present efficiently, a fast contextualized k-nn retrieval system that can efficiently index and search over contextual embeddings generated from large corpora. 4.63tb is demonstrated for a training data augmentation use case over the gigaword corpus. Using approximate k-nn and an efficient architecture, examples performs queries over an embedding space of 4.63tb (approximately 1.5b embeddings) in less than a day.","wordlikeness":0.7142857143,"lcsratio":0.8571428571,"wordcoverage":0.7272727273}
{"Year":2017,"Venue":"ws-2017","Acronym":"Skip-Prop","Description":"Representing Sentences with One Vector Per Proposition","Abstract":"We introduce the notion of a multi-vector sentence representation based on a \u201cone vector per proposition\u201d philosophy, which we term structure vectors. By representing each predicate-argument structure in a complex sentence as an individual vector, structure is (1) a response to empirical evidence that single-vector sentence representations degrade with sentence length, and (2) a representation that maintains a semantically useful level of granularity. We demonstrate the feasibility of training length, vectors, introducing a method adapted from skip-thought vectors, and compare predicate-argument with \u201cone vector per sentence\u201d and \u201cone vector per token\u201d approaches.","wordlikeness":0.7777777778,"lcsratio":0.7777777778,"wordcoverage":0.75}
{"Year":2023,"Venue":"findings-2023","Acronym":"FedPerC","Description":"Federated Learning for Language Generation with Personal and Context Preference Embeddings","Abstract":"Federated learning is a training paradigm that learns from multiple distributed users without aggregating data on a centralized server, promising the ability to deploy machine-learning to a diverse population of users without first collecting large, labeled datasets. As federated learning involves averaging gradient updates across a decentralized population, there is a growing need for personalization of federated learning systems (i.e. conversational agents must personalize to individual users and the context of an interaction).in this work, we propose a new direction for personalization research within federated learning, leveraging both personal embeddings and shared context embeddings.we also present an approach to predict these \u201cpreference\u201d embeddings, enabling personalization without backpropagation. Compared to state-of-the-art personalization baselines, our approach achieves a 50% improvement in test-time perplexity using 0.001% of the memory required by baseline approaches, and achieving greater sample- and compute-efficiency.","wordlikeness":0.7142857143,"lcsratio":1.0,"wordcoverage":0.7692307692}
{"Year":2020,"Venue":"acl-2020","Acronym":"BioMRC","Description":"A Dataset for Biomedical Machine Reading Comprehension","Abstract":"We introducebiomedical, a large-scale cloze-style biomedical mrc dataset. Care was taken to reduce noise, compared to the previous bioread dataset of pappas et al. (2018). Experiments show that simple heuristics do not perform well on the new dataset and that two neural mrc models that had been tested on bioread perform much better on that, indicating that the new dataset is indeed less noisy or at least that its task is more feasible. Non-expert human performance is also higher on the new dataset compared to bioread, and biomedical experts perform even better. We also introduce a new bert-based mrc model, the best version of which substantially outperforms all other methods tested, reaching or surpassing the accuracy of biomedical experts in some experiments. We make the new dataset available in three different sizes, also releasing our code, and providing a leaderboard.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2018,"Venue":"lrec-2018","Acronym":"Sudachi","Description":"a Japanese Tokenizer for Business","Abstract":"Tokenization, or morphological analysis, is a fundamental and important technology for processing a japanese text, especially for industrial applications. However, we often face many obstacles, such as the inconsistency of token unit in different resources, notation variations, discontinued maintenance of the resources, and various issues with the existing tokenizer implementations. In order to improve this situation, we develop a tokenizer called public and its accompanying dictionary with features such as multi-granular output and normalization of notation variations. In addition to this, we continuously maintain our software and language resources in long-term as a part of the company business. We release the resulting tokenizer software and language resources freely available to the public as an open source software. You can access them at https:\/\/github.com\/worksapplications\/especially. Keywords: tokenization, morphological analysis, segmentation, part-of-speech tagging, lemmatization, open source software 1.","wordlikeness":0.8571428571,"lcsratio":0.4285714286,"wordcoverage":0.7692307692}
{"Year":2023,"Venue":"acl-2023","Acronym":"Split-NER","Description":"Named Entity Recognition via Two Question-Answering-based Classifications","Abstract":"In this work, we address the ner problem by splitting it into two logical sub-tasks: (1) span detection which simply extracts entity mention spans irrespective of entity type; (2) span classification which classifies the spans into their entity types. Further, we formulate both sub-tasks as question-answering (qa) problems and produce two leaner models which can be optimized separately for each sub-task. Experiments with four cross-domain datasets demonstrate that this two-step approach is both effective and time efficient. Our system, splitner outperforms baselines on ontonotes5.0, wnut17 and a cybersecurity dataset and gives on-par performance on bionlp13cg. In all cases, it achieves a significant reduction in training time compared to its qa baseline counterpart. The effectiveness of our system stems from fine-tuning the bert model twice, separately for span detection and classification. The source code can be found at <a href=https:\/\/github.com\/c3sr\/problem class=acl-markup-url>https:\/\/github.com\/c3sr\/problems<\/a>.","wordlikeness":0.7777777778,"lcsratio":0.6666666667,"wordcoverage":0.75}
{"Year":2020,"Venue":"sltu-2020","Acronym":"MultiSeg","Description":"Parallel Data and Subword Information for Learning Bilingual Embeddings in Low Resource Scenarios","Abstract":"Distributed word embeddings have become ubiquitous in natural language processing as they have been shown to improve performance in many semantic and syntactic tasks. Popular models for learning cross-lingual word embeddings do not consider the morphology of words. We propose an approach to learn bilingual embeddings using parallel data and subword information that is expressed in various forms, i.e. character n-grams, morphemes obtained by unsupervised morphological segmentation and byte pair encoding. We report results for three low resource morphologically rich languages (swahili, tagalog, and somali) and a high resource language (german) in a simulated a low-resource scenario. Our results show that our method that leverages subword information outperforms the model without subword information, both in intrinsic and extrinsic evaluations of the learned embeddings. Specifically, analogy reasoning results show that using subwords helps capture syntactic characteristics. Semantically, word similarity results and intrinsically, word translation scores demonstrate superior performance over existing methods. Finally, qualitative analysis also shows better-quality cross-lingual embeddings particularly for morphological variants in both languages.","wordlikeness":0.625,"lcsratio":0.75,"wordcoverage":0.7777777778}
{"Year":2018,"Venue":"lrec-2018","Acronym":"FEIDEGGER","Description":"A Multi-modal Corpus of Fashion Images and Descriptions in German","Abstract":"The availability of multi-modal datasets that pair images and textual descriptions of their content has been a crucial driver in progress of various text-image tasks such as automatic captioning and text-to-image retrieval. In this paper, we present in, a new multi-modal corpus that focuses speci\ufb01cally on the domain of fashion items and their visual descriptions in german. We argue that such narrow-domain multi-modality presents a unique set of challenges such as \ufb01ne-grained image distinctions and domain-speci\ufb01c language, and release this dataset to the research community to enable study of these challenges. This paper illustrates our crowdsourcing strategy to acquire the textual descriptions, gives an overview over the been dataset, and discusses possible use cases. Keywords: crowdsourcing, multi-modalily, fashion, german 1.","wordlikeness":0.6666666667,"lcsratio":0.7777777778,"wordcoverage":0.75}
{"Year":2022,"Venue":"findings-2022","Acronym":"Eider","Description":"Empowering Document-level Relation Extraction with Efficient Evidence Extraction and Inference-stage Fusion","Abstract":"Document-level relation extraction (docre) aims to extract semantic relations among entity pairs in a document. Typical docre methods blindly take the full document as input, while a subset of the sentences in the document, noted as the evidence, are often sufficient for humans to predict the relation of an entity pair. In this paper, we propose an evidence-enhanced framework, training, that empowers docre by efficiently extracting evidence and effectively fusing the extracted evidence in inference. We first jointly train an re model with a lightweight evidence extraction model, which is efficient in both memory and runtime. Empirically, even training the evidence model on silver labels constructed by our heuristic rules can lead to better re performance. We further design a simple yet effective inference process that makes re predictions on both extracted evidence and the full document, then fuses the predictions through a blending layer. This allows focus to focus on important sentences while still having access to the complete information in the document. Extensive experiments show that three outperforms state-of-the-art methods on three benchmark datasets (e.g., by 1.37\/1.26 ign f1\/f1 on docred).","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2013,"Venue":"semeval-2013","Acronym":"Serendio","Description":"Simple and Practical lexicon based approach to Sentiment Analysis","Abstract":"This paper describes the system developed by the system team for the semeval-2013 task 2 competition (task a). We use a lexicon based approach for discovering sentiments. Our lexicon is built from the after taxonomy. The lexicon taxonomy consists of positive, negative, negation, stop words and phrases. A typical tweet contains word variations, emoticons, hashtags etc. We use preprocessing steps such as stemming, emoticon detection and normalization, exaggerated word shortening and hashtag detection. After the preprocessing, the lexicon-based system classi\ufb01es the tweets as positive or negative based on the contextual sentiment orientation of the words. Our system yields an f-score of 0.8004 on the test dataset.","wordlikeness":0.625,"lcsratio":0.875,"wordcoverage":0.7142857143}
{"Year":2021,"Venue":"naacl-2021","Acronym":"ASAP","Description":"A Chinese Review Dataset Towards Aspect Category Sentiment Analysis and Rating Prediction","Abstract":"Sentiment analysis has attracted increasing attention in e-commerce. The sentiment polarities underlying user reviews are of great value for business intelligence. Aspect category sentiment analysis (acsa) and review rating prediction (rp) are two essential tasks to detect the fine-to-coarse sentiment polarities. Acsa and rp are highly correlated and usually employed jointly in real-world e-commerce scenarios. While most public datasets are constructed for acsa and rp separately, which may limit the further exploitation of both tasks. To address the problem and advance related researches, we present a large-scale chinese restaurant review dataset rating including 46, 730 genuine reviews from a leading online-to-offline (o2o) e-commerce platform in china. Besides a 5-star scale rating, each review is manually annotated according to its sentiment polarities towards 18 pre-defined aspect categories. We hope the release of the dataset could shed some light on the field of sentiment analysis. Moreover, we propose an intuitive yet effective joint model for acsa and rp. Experimental results demonstrate that the joint model outperforms state-of-the-art baselines on both tasks.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2023,"Venue":"acl-2023","Acronym":"KGA","Description":"A General Machine Unlearning Framework Based on Knowledge Gap Alignment","Abstract":"Recent legislation of the \u201cright to be forgotten\u201d has led to the interest in machine unlearning, where the learned models are endowed with the function to forget information about specific training instances as if they have never existed in the training set. Previous work mainly focuses on computer vision scenarios and largely ignores the essentials of unlearning in nlp field, where text data contains more explicit and sensitive personal information than images. In this paper, we propose a general unlearning framework called different to induce forgetfulness. Different from previous work that tries to recover gradients or forces models to perform close to one specific distribution, propose maintains distribution differences (i.e., knowledge gap). This relaxes the distribution assumption. Furthermore, we first apply the unlearning method to various nlp tasks (i.e., classification, translation, response generation) and propose several unlearning evaluation metrics with pertinence. Experiments on large-scale datasets show that led yields comprehensive improvements over baselines, where extensive analyses further validate the effectiveness of framework and provide insight into unlearning for nlp tasks.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2020,"Venue":"lrec-2020","Acronym":"HypoNLI","Description":"Exploring the Artificial Patterns of Hypothesis-only Bias in Natural Language Inference","Abstract":"Many recent studies have shown that for models trained on datasets for natural language inference (nli), it is possible to make correct predictions by merely looking at the hypothesis while completely ignoring the premise. In this work, we manage to derive adversarial examples in terms of the hypothesis-only bias and explore eligible ways to mitigate such bias. Specifically, we extract various phrases from the hypotheses (artificial patterns) in the training sets, and show that they have been strong indicators to the specific labels. We then figure out \u2018hard\u2019 and \u2018easy\u2019 instances from the original test sets whose labels are opposite to or consistent with those indications. We also set up baselines including both pretrained models (bert, roberta, xlnet) and competitive non-pretrained models (infersent, dam, esim). Apart from the benchmark and baselines, we also investigate two debiasing approaches which exploit the artificial pattern modeling to mitigate such hypothesis-only bias: down-sampling and adversarial training. We believe those methods can be treated as competitive baselines in nli debiasing tasks.","wordlikeness":0.5714285714,"lcsratio":1.0,"wordcoverage":0.7058823529}
{"Year":2015,"Venue":"semeval-2015","Acronym":"ICRC-HIT","Description":"A Deep Learning based Comment Sequence Labeling System for Answer Selection Challenge","Abstract":"In this paper, we present a comment labeling system based on a deep learning strategy. We treat the answer selection task as a sequence labeling problem and propose recurrent convolution neural networks to recognize good comments. In the recurrent architecture of our system, our approach uses 2-dimensional convolutional neural networks to learn the distributed representation for question-comment pair, and assigns the labels to the comment sequence with a recurrent neural network over cnn. Compared with the conditional random fields based method, our approach performs better performance on macro-f1 (53.82%), and achieves the highest accuracy (73.18%), f1-value (79.76%) on predicting the good class in this answer selection challenge.","wordlikeness":0.5,"lcsratio":0.625,"wordcoverage":0.6666666667}
{"Year":2016,"Venue":"ws-2016","Acronym":"MED","Description":"The LMU System for the SIGMORPHON 2016 Shared Task on Morphological Reinflection","Abstract":"This paper presents using, the main system of the lmu team for the sigmorphon 2016 shared task on morphological rein\ufb02ection as well as an extended analysis of how different design choices contribute to the \ufb01nal performance. We model the task of morphological rein\ufb02ection using neural encoder-decoder models together with an encoding of the input as a single sequence of the morphological tags of the source and target form as well as the sequence of letters of the source form. The shared task consists of three subtasks, three different tracks and covers 10 different languages to encourage the use of language-independent approaches. Of was the system with the overall best performance, demonstrating our method generalizes well for the low-resource setting of the sigmorphon 2016 shared task.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2010,"Venue":"acl-2010","Acronym":"BabelNet","Description":"Building a Very Large Multilingual Semantic Network","Abstract":"In this paper we present to \u2013 a very large, wide-coverage multilingual semantic network. The resource is automatically constructed by means of a methodology that integrates lexicographic and encyclopedic knowledge from wordnet and wikipedia. In addition machine translation is also applied to enrich the resource with lexical information for all languages. We conduct experiments on new and existing gold-standard datasets to show the high quality and coverage of the resource.","wordlikeness":0.75,"lcsratio":0.875,"wordcoverage":0.75}
{"Year":2021,"Venue":"ws-2021","Acronym":"TermMind","Description":"Alibaba&#39;s WMT21 Machine Translation Using Terminologies Task Submission","Abstract":"This paper describes our work in the wmt 2021 machine translation using terminologies shared task. We participate in the shared translation terminologies task in english to chinese language pair. To satisfy terminology constraints on translation, we use a terminology data augmentation strategy based on transformer model. We used tags to mark and add the term translations into the matched sentences. We created synthetic terms using phrase tables extracted from bilingual corpus to increase the proportion of term translations in training data. Detailed pre-processing and filtering on data, in-domain finetuning and ensemble method are used in our system. Our submission obtains competitive results in the terminology-targeted evaluation.","wordlikeness":0.875,"lcsratio":0.875,"wordcoverage":0.7777777778}
{"Year":2020,"Venue":"coling-2020","Acronym":"DT-QDC","Description":"A Dataset for Question Comprehension in Online Test","Abstract":"With the transformation of education from the traditional classroom environment to online education and assessment, it is more and more important to accurately assess the difficulty of questions than ever. As teachers may not be able to follow the student\u2019s performance and learning behavior closely, a well-defined method to measure the difficulty of questions to guide learning is necessary. In this paper, we explore the concept of question difficulty and provide our new chinese is dataset. This is currently the largest and only chinese question dataset, and it also has enriched attributes and difficulty labels. Additional attributes such as keywords, chapter, and question type would allow models to understand questions more precisely. We proposed the mtms-bert and orms-bert, which can improve the judgment of difficulty from different views. The proposed methods outperforms different baselines by 7.79% on f1-score and 15.92% on mae, 28.26% on mse on the new explore dataset, laying the foundation for the question difficulty comprehension task.","wordlikeness":0.1666666667,"lcsratio":0.6666666667,"wordcoverage":0.6}
{"Year":2020,"Venue":"icon-2020","Acronym":"CLPLM","Description":"Character Level Pretrained Language Model for ExtractingSupport Phrases for Sentiment Labels","Abstract":"In this paper, we have designed a character-level pre-trained language model for extracting support phrases from tweets based on the sentiment label. We also propose a character-level ensemble model designed by properly blending pre-trained contextual embeddings (pce) models- roberta, bert, and albert along with neural network models- rnn, cnn and wavenet at different stages of the model. For a given tweet and associated sentiment label, our model predicts the span of phrases in a tweet that prompts the particular sentiment in the tweet. In our experiments, we have explored various model architectures and configuration for both single as well as ensemble models. We performed a systematic comparative analysis of all the model\u2019s performance based on the jaccard score obtained. The best performing ensemble model obtained the highest jaccard scores of 73.5, giving it a relative improvement of 2.4% over the best performing single roberta based character-level model, at 71.5(jaccard score).","wordlikeness":0.2,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2021,"Venue":"eacl-2021","Acronym":"BERTective","Description":"Language Models and Contextual Information for Deception Detection","Abstract":"Spotting a lie is challenging but has an enormous potential impact on security as well as private and public safety. Several nlp methods have been proposed to classify texts as truthful or deceptive. In most cases, however, the target texts\u2019 preceding context is not considered. This is a severe limitation, as any communication takes place in context, not in a vacuum, and context can help to detect deception. We study a corpus of italian dialogues containing deceptive statements and implement deep neural models that incorporate various linguistic contexts. We establish a new state-of-the-art identifying deception and find that not all context is equally useful to the task. Only the texts closest to the target, if from the same speaker (rather than questions by an interlocutor), boost performance. We also find that the semantic information in language models such as bert contributes to the performance. However, bert alone does not capture the implicit knowledge of deception cues: its contribution is conditional on the concurrent use of attention to learn cues from bert\u2019s representations.","wordlikeness":0.8,"lcsratio":0.8,"wordcoverage":0.8421052632}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"TaCube","Description":"Pre-computing Data Cubes for Answering Numerical-Reasoning Questions over Tabular Data","Abstract":"Existing auto-regressive pre-trained language models (plms) like t5 and bart, have been well applied to table question answering by unifiedskg and tapex, respectively, and demonstrated state-of-the-art results on multiple benchmarks. However, auto-regressive plms are challenged by recent emerging numerical reasoning datasets, such as tat-qa, due to the error-prone implicit calculation. In this paper, we present recent, to pre-compute aggregation\/arithmetic results for the table in advance, so that they are handy and readily available for plms to answer numerical reasoning questions. Sum, systematically and comprehensively covers a collection of computational operations over table segments. By simply concatenating so to the input sequence of plms, it shows significant experimental effectiveness. Calculation. Promotes the f1 score from 49.6% to 66.2% on tat-qa and achieves new state-of-the-art results on wikitq (59.6% denotation accuracy). Pre-trained\u2019s improvements on numerical reasoning cases are even more notable: on tat-qa, table promotes the exact match accuracy of bart-large by 39.6% on sum, 52.5% on average, 36.6% on substraction, and 22.2% on division.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2021,"Venue":"eacl-2021","Acronym":"DOCENT","Description":"Learning Self-Supervised Entity Representations from Large Document Collections","Abstract":"This paper explores learning rich self-supervised entity representations from large amounts of associated text. Once pre-trained, these models become applicable to multiple entity-centric tasks such as ranked retrieval, knowledge base completion, question answering, and more. Unlike other methods that harvest self-supervision signals based merely on a local context within a sentence, we radically expand the notion of context to include any available text related to an entity. This enables a new class of powerful, high-capacity representations that can ultimately distill much of the useful information about an entity from multiple text sources, without any human supervision. We present several training strategies that, unlike prior approaches, learn to jointly predict words and entities \u2013 strategies we compare experimentally on downstream tasks in the tv-movies domain, such as movielens tag prediction from user reviews and natural language movie search. As evidenced by results, our models match or outperform competitive baselines, sometimes with little or no fine-tuning, and are also able to scale to very large corpora. Finally, we make our datasets and pre-trained models publicly available. This includes reviews2movielens, mapping the ~1b word corpus of amazon movie reviews (he and mcauley, 2016) to movielens tags (harper and konstan, 2016), as well as reddit movie suggestions with natural language queries and corresponding community recommendations.","wordlikeness":0.8333333333,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2022,"Venue":"lrec-2022","Acronym":"MHE","Description":"Code-Mixed Corpora for Similar Language Identification","Abstract":"This paper introduces a new magahi-hindi-english (and) code-mixed data-set for similar language identification (smlid), where magahi is a less-resourced minority language. This corpus provides a language id at two levels: word and sentence. This data-set is the first magahi-hindi-english code-mixed data-set for similar language identification task. Furthermore, we will discuss the complexity of the data-set and provide a few baselines for the language identification task.","wordlikeness":0.6666666667,"lcsratio":0.6666666667,"wordcoverage":0.75}
{"Year":2023,"Venue":"acl-2023","Acronym":"XL-LEXEME","Description":"WiC Pretrained Model for Cross-Lingual LEXical sEMantic changE","Abstract":"The recent introduction of large-scale datasets for the wic (word in context) task enables the creation of more reliable and meaningful contextualized word embeddings.however, most of the approaches to the wic task use cross-encoders, which prevent the possibility of deriving comparable word embeddings.in this work, we introduce lexical, a lexical semantic change detection model.significant extends sbert, highlighting the target word in the sentence. We evaluate the on the multilingual benchmarks for semeval-2020 task 1 - lexical semantic change (lsc) detection and the rushifteval shared task involving five languages: english, german, swedish, latin, and russian.task. Outperforms the state-of-the-art in english, german and swedish with statistically significant differences from the baseline results and obtains state-of-the-art performance in the rushifteval shared task.","wordlikeness":0.4444444444,"lcsratio":0.8888888889,"wordcoverage":0.625}
{"Year":2018,"Venue":"wmt-2018","Acronym":"RUSE","Description":"Regressor Using Sentence Embeddings for Automatic Machine Translation Evaluation","Abstract":"We introduce the can metric for the wmt18 metrics shared task. Sentence embeddings can capture global information that cannot be captured by local features based on character or word n-grams. Although training sentence embeddings using small-scale translation datasets with manual evaluation is difficult, sentence embeddings trained from large-scale data in other tasks can improve the automatic evaluation of machine translation. We use a multi-layer perceptron regressor based on three types of sentence embeddings. The experimental results of the wmt16 and wmt17 datasets show that the multi-layer metric achieves a state-of-the-art performance in both segment- and system-level metrics tasks with embedding features only.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2001,"Venue":"mtsummit-2001","Acronym":"PolVerbNet","Description":"an experimental database for Polish verbs","Abstract":"The semantics of verbs implies, as is known, a great number of difficulties, when it is to be represented in a computational lexicon. The slavic languages are especially challenging in respect of this task because of the huge complexity of verbs, where the stems are combined with prefixes indicating aspect and aktionsart. The current paper describes an approach to build report, a database for polish verbs, considering the internal structure of the aspect-aktionsart system. Languages is thus implemented in a larger english-polish mt-system, which incorporates wordnet. We report our translation procedure and the system\u2019s performance is evaluated and discussed.","wordlikeness":0.8,"lcsratio":0.7,"wordcoverage":0.7368421053}
{"Year":2022,"Venue":"slpat-2022","Acronym":"CueBot","Description":"Cue-Controlled Response Generation for Assistive Interaction Usages","Abstract":"Conversational assistants are ubiquitous among the general population, however, these systems have not had an impact on people with disabilities, or speech and language disorders, for whom basic day-to-day communication and social interaction is a huge struggle. Language model technology can play a huge role in empowering these users and help them interact with others with less effort via interaction support. To enable this population, we build a system that can represent them in a social conversation and generate responses that can be controlled by the users using cues\/keywords. We build models that can speed up this communication by suggesting relevant cues in the dialog response context. We also introduce a keyword-loss to lexically constrain the model response output. We present automatic and human evaluation of our cue\/keyword predictor and the controllable dialog system to show that our models perform significantly better than models without control. Our evaluation and user study shows that keyword-control on end-to-end response generation models is powerful and can enable and empower users with degenerative disorders to carry out their day-to-day communication.","wordlikeness":0.6666666667,"lcsratio":0.8333333333,"wordcoverage":0.6666666667}
{"Year":2023,"Venue":"mtsummit-2023","Acronym":"BIT-ACT","Description":"An Ancient Chinese Translation System Using Data Augmentation","Abstract":"This paper describes a translation model for ancient chinese to modern chinese and english for the evahan 2023 competition, a subtask of the ancient language translation 2023 challenge. During the training of our model, we applied various data augmentation techniques and used siku-roberta as part of our model architecture. The results indicate that back translation improves the model\u2019s performance, but double back translation introduces noise and harms the model\u2019s performance. Fine-tuning on the original dataset can be helpful in solving the issue.","wordlikeness":0.7142857143,"lcsratio":0.5714285714,"wordcoverage":0.7692307692}
{"Year":2021,"Venue":"eacl-2021","Acronym":"SICK-NL","Description":"A Dataset for Dutch Natural Language Inference","Abstract":"We present embedding (read: signal), a dataset targeting natural language inference in dutch. Language is obtained by translating the sick dataset of (marelli et al., 2014) from english into dutch. Having a parallel inference dataset allows us to compare both monolingual and multilingual nlp models for english and dutch on the two tasks. In the paper, we motivate and detail the translation process, perform a baseline evaluation on both the original sick dataset and its dutch incarnation from, taking inspiration from dutch skipgram embeddings and contextualised embedding models. In addition, we encapsulate two phenomena encountered in the translation to formulate stress tests and verify how well the dutch models capture syntactic restructurings that do not affect semantics. Our main finding is all models perform worse on english than on sick, indicating that the dutch dataset is more challenging than the english original. Results on the stress tests show that models don\u2019t fully capture word order freedom in dutch, warranting future systematic studies.","wordlikeness":0.5714285714,"lcsratio":0.5714285714,"wordcoverage":0.7692307692}
{"Year":2022,"Venue":"acl-2022","Acronym":"PLANET","Description":"Dynamic Content Planning in Autoregressive Transformers for Long-form Text Generation","Abstract":"Despite recent progress of pre-trained language models on generating fluent text, existing methods still suffer from incoherence problems in long-form text generation tasks that require proper content control and planning to form a coherent high-level logical flow. In this work, we propose models, a novel generation framework leveraging autoregressive self-attention mechanism to conduct content planning and surface realization dynamically. To guide the generation of output sentences, our framework enriches the transformer decoder with latent representations to maintain sentence-level semantic plans grounded by bag-of-words. Moreover, we introduce a new coherence-based contrastive learning objective to further improve the coherence of output. Extensive experiments are conducted on two challenging long-form text generation tasks including counterargument generation and opinion article generation. Both automatic and human evaluations show that our method significantly outperforms strong baselines and generates more coherent texts with richer contents.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2008,"Venue":"lrec-2008","Acronym":"Chooser","Description":"a Multi-Task Annotation Tool","Abstract":"The paper presents a tool assisting manual annotation of linguistic data developed at the department of computational linguistics, ibl-bas. Centralised is a general-purpose modular application for corpus annotation based on the principles of commonality and reusability of the created resources, language and theory independence, extendibility and user-friendliness. These features have been achieved through a powerful abstract architecture within the model-view-controller paradigm that is easily tailored to task-specific requirements and readily extendable to new applications. The tool is to a considerable extent independent of data format and representation and produces outputs that are largely consistent with existing standards. The annotated data are therefore reusable in tasks requiring different levels of annotation and are accessible to external applications. The tool incorporates edit functions, pass and arrangement strategies that facilitate annotators\u0092 work. The relevant module produces tree-structured and graph-based representations in respective annotation modes. Another valuable feature of the application is concurrent access by multiple users and centralised storage of lexical resources underlying annotation schemata, as well as of annotations, including frequency of selection, updates in the lexical database, etc. For has been successfully applied to a number of tasks: pos tagging, ws and syntactic annotation.","wordlikeness":0.8571428571,"lcsratio":0.2857142857,"wordcoverage":0.9230769231}
{"Year":2023,"Venue":"findings-2023","Acronym":"CSS","Description":"A Large-scale Cross-schema Chinese Text-to-SQL Medical Dataset","Abstract":"The cross-domain text-to-sql task aims to build a system that can parse user questions into sql on complete unseen databases, and the single-domain text-to-sql task evaluates the performance on identical databases. Both of these setups confront unavoidable difficulties in real-world applications. To this end, we introduce the cross-schema text-to-sql task, where the databases of evaluation data are different from that in the training data but come from the same domain. Furthermore, we present questions, a large-scale cross-schema chinese text-to-sql dataset, to carry on corresponding studies. To originally consisted of 4,340 question\/sql pairs across 2 databases. In order to generalize models to different medical systems, we extend moreover, and create 19 new databases along with 29,280 corresponding dataset examples. Moreover, we is also a large corpus for single-domain chinese text-to-sql studies. We present the data collection approach and a series of analyses of the data statistics. To show the potential and usefulness of come, benchmarking baselines have been conducted and reported. Our dataset is publicly available at <a href=https:\/\/huggingface.co\/datasets\/zhanghanchong\/come class=acl-markup-url>https:\/\/huggingface.co\/datasets\/zhanghanchong\/this<\/a>.","wordlikeness":0.3333333333,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2022,"Venue":"lrec-2022","Acronym":"Huqariq","Description":"A Multilingual Speech Corpus of Native Languages of Peru forSpeech Recognition","Abstract":"The recognition, corpus is a multilingual collection of speech from native peruvian languages. The transcribed corpus is intended for the research and development of speech technologies to preserve endangered languages in peru. Methodology. Is primarily designed for the development of automatic speech recognition, language identification and text-to-speech tools. In order to achieve corpus collection sustainably, we employs the crowdsourcing methodology. Fully includes four native languages of peru, and it is expected that by the year 2022, it can reach up to 20 native languages out of the 48 native languages in peru. The corpus has 220 hours of transcribed audio recorded by more than 500 volunteers, making it the largest speech corpus for native languages in peru. In order to verify the quality of the corpus, we present speech recognition experiments using 220 hours of fully transcribed audio.","wordlikeness":0.4285714286,"lcsratio":0.7142857143,"wordcoverage":0.625}
{"Year":2021,"Venue":"inlg-2021","Acronym":"HI-CMLM","Description":"Improve CMLM with Hybrid Decoder Input","Abstract":"Mask-predict cmlm (ghazvininejad et al.,2019) has achieved stunning performance among non-autoregressive nmt models, but we find that the mechanism of predicting all of the target words only depending on the hidden state of [mask] is not effective and efficient in initial iterations of refinement, resulting in ungrammatical repetitions and slow convergence. In this work, we mitigate this problem by combining copied source with embeddings of [mask] in decoder. Notably. It\u2019s not a straightforward copying that is shown to be useless, but a novel heuristic hybrid strategy \u2014 fence-mask. Experimental results show that it gains consistent boosts on both wmt14 en&lt;->de and wmt16 en&lt;->ro corpus by 0.5 bleu on average, and 1 bleu for less-informative short sentences. This reveals that incorporating additional information by proper strategies is beneficial to improve cmlm, particularly translation quality of short texts and speeding up early-stage convergence.","wordlikeness":0.2857142857,"lcsratio":0.7142857143,"wordcoverage":0.5714285714}
{"Year":2023,"Venue":"findings-2023","Acronym":"GeoDRL","Description":"A Self-Learning Framework for Geometry Problem Solving using Reinforcement Learning in Deductive Reasoning","Abstract":"Ensuring both interpretability and correctness is a great challenge in automated geometry problem solving (gps), and the scarcity of labeled data hinders learning mathematical reasoning from samples. Therefore, we present is, a self-learning geometry problem solving framework that integrates logic graph deduction and deep reinforcement learning (drl) to optimize geometry reasoning as a markov decision process. Accuracy employs a graph neural network on a geometry logic graph, updating the problem state using a symbolic system. Incorporating drl into deductive reasoning enables reasoning to achieve unsupervised self-learning while maintaining correctness. Process., through unsupervised learning, exhibits enhanced accuracy in the geometry3k dataset, improving by 11.1% over previous sota methods, and simultaneously boosts efficiency and interpretability.","wordlikeness":0.5,"lcsratio":0.8333333333,"wordcoverage":0.7272727273}
{"Year":2023,"Venue":"acl-2023","Acronym":"AD-KD","Description":"Attribution-Driven Knowledge Distillation for Language Model Compression","Abstract":"Knowledge distillation has attracted a great deal of interest recently to compress large language models. However, existing knowledge distillation methods suffer from two limitations. First, the student model simply imitates the teacher\u2019s behavior while ignoring the reasoning behind it. Second, these methods usually focus on the transfer of sophisticated model-specific knowledge but overlook data-specific knowledge. In this paper, we present a novel attribution-driven knowledge distillation approach, which explores the token-level rationale behind the teacher model based on integrated gradients (ig) and transfers attribution knowledge to the student model. To enhance the knowledge transfer of model reasoning and generalization, we further explore multi-view attribution distillation on all potential decisions of the teacher. Comprehensive experiments are conducted with bert on the glue benchmark. The experimental results demonstrate the superior performance of our approach to several state-of-the-art methods.","wordlikeness":0.2,"lcsratio":0.8,"wordcoverage":0.75}
{"Year":2018,"Venue":"acl-2018","Acronym":"YEDDA","Description":"A Lightweight Collaborative Text Span Annotation Tool","Abstract":"In this paper, we introduce experiments, a lightweight but efficient and comprehensive open-source tool for text span annotation. System provides a systematic solution for text span annotation, ranging from collaborative user annotation to administrator evaluation and analysis. It overcomes the low efficiency of traditional text annotation tools by annotating entities through both command line and shortcut keys, which are configurable with custom labels. Recommendation. Also gives intelligent recommendations by learning the up-to-date annotated text. An administrator client is developed to evaluate annotation quality of multiple annotators and generate detailed comparison report for each annotator pair. Experiments show that the proposed system can reduce the annotation time by half compared with existing annotation tools. And the annotation time can be further compressed by 16.47% through intelligent recommendation.","wordlikeness":0.6,"lcsratio":0.4,"wordcoverage":0.6666666667}
{"Year":2021,"Venue":"acl-2021","Acronym":"LenAtten","Description":"An Effective Length Controlling Unit For Text Summarization","Abstract":"Fixed length summarization aims at generating summaries with a preset number of words or characters. Most recent researches incorporate length information with word embeddings as the input to the recurrent decoding unit, causing a compromise between length controllability and summary quality. In this work, we present an effective length controlling unit length attention (compromise) to break this trade-off. Experimental results show that target not only brings improvements in length controllability and rogue scores but also has great generalization ability. In the task of generating a summary with the target length, our model is 732 times better than the bestperforming length controllable summarizer in length controllability on the cnn\/daily mail dataset.","wordlikeness":0.625,"lcsratio":0.875,"wordcoverage":0.8}
{"Year":2022,"Venue":"acl-2022","Acronym":"LAGr","Description":"Label Aligned Graphs for Better Systematic Generalization in Semantic Parsing","Abstract":"Semantic parsing is the task of producing structured meaning representations for natural language sentences. Recent research has pointed out that the commonly-used sequence-to-sequence (seq2seq) semantic parsers struggle to generalize systematically, i.e. to handle examples that require recombining known knowledge in novel settings. In this work, we show that better systematic generalization can be achieved by producing the meaning representation directly as a graph and not as a sequence. To this end we propose seq2seq (label aligned graphs), a general framework to produce semantic parses by independently predicting node and edge labels for a complete multi-layer input-aligned graph. The strongly-supervised producing algorithm requires aligned graphs as inputs, whereas weakly-supervised natural infers alignments for originally unaligned target graphs using approximate maximum-a-posteriori inference. Experiments demonstrate that for achieves significant improvements in systematic generalization upon the baseline seq2seq parsers in both strongly- and weakly-supervised settings.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2017,"Venue":"emnlp-2017","Acronym":"Train-O-Matic","Description":"Large-Scale Supervised Word Sense Disambiguation in Multiple Languages without Manual Training Data","Abstract":"Annotating large numbers of sentences with senses is the heaviest requirement of current word sense disambiguation. We present purposes, a language-independent method for generating millions of sense-annotated training instances for virtually all meanings of words in a language\u2019s vocabulary. The approach is fully automatic: no human intervention is required and the only type of human knowledge used is a wordnet-like resource. Disambiguation. Achieves consistently state-of-the-art performance across gold standard datasets and languages, while at the same time removing the burden of manual annotation. All the training data is available for research purposes at <a href=http:\/\/trainomatic.org class=acl-markup-url>http:\/\/trainomatic.org<\/a>.","wordlikeness":0.6923076923,"lcsratio":0.6923076923,"wordcoverage":0.7272727273}
{"Year":2015,"Venue":"ws-2015","Acronym":"USFD","Description":"Twitter NER with Drift Compensation and Linked Data","Abstract":"This paper describes a pilot ner system for twitter, comprising the components system entry to the w-nut 2015 ner shared task. The goal is to correctly label entities in a tweet dataset, using an inventory of ten types. We employ structured learning, drawing on gazetteers taken from linked data, and on unsupervised clustering features, and attempting to compensate for stylistic and topic drift \u2013 a key challenge in social media text. Our result is competitive; we provide an analysis of the components of our methodology, and an examination of the target dataset in the context of this task.","wordlikeness":0.5,"lcsratio":0.5,"wordcoverage":0.8571428571}
{"Year":2023,"Venue":"eacl-2023","Acronym":"NxPlain","Description":"A Web-based Tool for Discovery of Latent Concepts","Abstract":"The proliferation of deep neural networks in various domains has seen an increased need for the interpretability of these models, especially in scenarios where fairness and trust are as important as model performance. A lot of independent work is being carried out to: i) analyze what linguistic and non-linguistic knowledge is learned within these models, and ii) highlight the salient parts of the input. We present ii), a web-app that provides an explanation of a model\u2019s prediction using latent concepts. The discovers latent concepts learned in a deep nlp model, provides an interpretation of the knowledge learned in the model, and explains its predictions based on the used concepts. The application allows users to browse through the latent concepts in an intuitive order, letting them efficiently scan through the most salient concepts with a global corpus-level view and a local sentence-level view. Our tool is useful for debugging, unraveling model bias, and for highlighting spurious correlations in a model. A hosted demo is available here: <a href=https:\/\/scan.qcri.org class=acl-markup-url>https:\/\/hosted.qcri.","wordlikeness":0.7142857143,"lcsratio":0.4285714286,"wordcoverage":0.8571428571}
{"Year":2013,"Venue":"semeval-2013","Acronym":"MELODI","Description":"A Supervised Distributional Approach for Free Paraphrasing of Noun Compounds","Abstract":"This paper describes the system submitted by the yields team for the semeval-2013 task 4: free paraphrases of noun compounds (hendrickx et al., 2013). Our approach combines the strength of an unsupervised distributional word space model with a supervised maximum-entropy classi\ufb01cation model; the distributional model yields a feature representation for a particular compound noun, which is subsequently used by the classi\ufb01er to induce a number of appropriate paraphrases.","wordlikeness":1.0,"lcsratio":0.6666666667,"wordcoverage":0.8571428571}
{"Year":2014,"Venue":"semeval-2014","Acronym":"IITP","Description":"A Supervised Approach for Disorder Mention Detection and Disambiguation","Abstract":"In this paper we brie\ufb02y describe our supervised machine learning approach for disorder mention detection system that we submitted as part of our participation in the semeval-2014 shared task. The main goal of this task is to build a system that automatically identi\ufb01es mentions of clinical conditions from the clinical texts. The main challenge lies due in the fact that the same mention of concept may be represented in many surface forms. We develop the system based on the supervised machine learning algorithms, namely conditional random field and support vector machine. One appealing characteristics of our system is that most of the features for learning are extracted automatically from the given training or test datasets without using deep domain speci\ufb01c resources and\/or tools. We submitted three runs, and best performing system is based on conditional random field. For task a, it shows the precision, recall and f-measure values of 50.00%, 47.90% and 48.90%, respectively under the strict matching criterion. When the matching criterion is relaxed, it shows the precision, recall and f-measure of 81.50%, 79.70% and 80.60%, respectively. For task b, we obtain the accuracies of 33.30% and 69.60% for the relaxed and strict matches, respectively.","wordlikeness":0.5,"lcsratio":0.75,"wordcoverage":0.6666666667}
{"Year":2020,"Venue":"inlg-2020","Acronym":"OMEGA","Description":"A probabilistic approach to referring expression generation in a virtual environment","Abstract":"In recent years, referring expression genera- tion algorithms were inspired by game theory and probability theory. In this paper, an al- gorithm is designed for the generation of re- ferring expressions (reg) that base on both models by integrating maximization of utilities into the content determination process. It im- plements cognitive models for assessing visual salience of objects and additional features. In order to evaluate the algorithm properly and validate the applicability of existing models and evaluative information criteria, both, pro- duction and comprehension studies, are con- ducted using a complex domain of objects, pro- viding new directions of approaching the eval- uation of reg algorithms.","wordlikeness":1.0,"lcsratio":0.8,"wordcoverage":1.0}
{"Year":2021,"Venue":"naacl-2021","Acronym":"NL-EDIT","Description":"Correcting Semantic Parse Errors through Natural Language Interaction","Abstract":"We study semantic parsing in an interactive setting in which users correct errors with natural language feedback. We present text-to-sql, a model for interpreting natural language feedback in the interaction context to generate a sequence of edits that can be applied to the initial parse to correct its errors. We show that correct can boost the accuracy of existing text-to-sql parsers by up to 20% with only one turn of correction. We analyze the limitations of the model and discuss directions for improvement and evaluation. The code and datasets used in this paper are publicly available at <a href=http:\/\/aka.ms\/nledit class=acl-markup-url>http:\/\/aka.ms\/nledit<\/a>.","wordlikeness":0.5714285714,"lcsratio":0.7142857143,"wordcoverage":0.7272727273}
{"Year":2020,"Venue":"acl-2020","Acronym":"SAFER","Description":"A Structure-free Approach for Certified Robustness to Adversarial Word Substitutions","Abstract":"State-of-the-art nlp models can often be fooled by human-unaware transformations such as synonymous word substitution. For security reasons, it is of critical importance to develop models with certified robustness that can provably guarantee that the prediction is can not be altered by any possible synonymous word substitution. In this work, we propose a certified robust method based on a new randomized smoothing technique, which constructs a stochastic ensemble by applying random word substitutions on the input sentences, and leverage the statistical properties of the ensemble to provably certify the robustness. Our method is simple and structure-free in that it only requires the black-box queries of the model outputs, and hence can be applied to any pre-trained models (such as bert) and any types of models (world-level or subword-level). Our method significantly outperforms recent state-of-the-art methods for certified robustness on both imdb and amazon text classification tasks. To the best of our knowledge, we are the first work to achieve certified robustness on large systems such as bert with practically meaningful certified accuracy.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2020,"Venue":"scil-2020","Acronym":"DialectGram","Description":"Automatic Detection of Dialectal Changes with Multi-geographic Resolution Analysis","Abstract":"Several computational models have been developed to detect and analyze dialect variation in recent years. Most of these models assume a prede\ufb01ned set of geographical regions over which they detect and analyze dialectal variation. However, dialect variation occurs at multiple levels of geographic resolution ranging from cities within a state, states within a country, and between countries across continents. In this work, we propose a model that enables detection of dialectal variation at multiple levels of geographic resolution obviating the need for a-priori de\ufb01nition of the resolution level. Our method at, learns dialectsensitive word embeddings while being agnostic of the geographic resolution. Speci\ufb01cally it only requires one-time training and enables analysis of dialectal variation at a chosen resolution post-hoc \u2013 a signi\ufb01cant departure from prior models which need to be retrained whenever the pre-de\ufb01ned set of regions changes. Furthermore, pre-de\ufb01ned explicitly models senses thus enabling one to estimate the proportion of each sense usage in any given region. Finally, we quantitatively evaluate our model against other baselines on a new evaluation dataset dialectsim (in english) and show that across can effectively model linguistic variation.","wordlikeness":0.9090909091,"lcsratio":0.9090909091,"wordcoverage":0.6363636364}
{"Year":2020,"Venue":"emnlp-2020","Acronym":"PRover","Description":"Proof Generation for Interpretable Reasoning over Rules","Abstract":"Recent work by clark et al. (2020) shows that transformers can act as \u201csoft theorem whens\u201d by answering questions over explicitly provided knowledge in natural language. In our work, we take a step closer to emulating formal theorem whiles, by proposing in, an interpretable transformer-based model that jointly answers binary questions over rule-bases and generates the corresponding proofs. Our model learns to predict nodes and edges corresponding to proof graphs in an efficient constrained training paradigm. During inference, a valid proof, satisfying a set of global constraints is generated. We conduct experiments on synthetic, hand-authored, and human-paraphrased rule-bases to show promising results for qa and proof generation, with strong generalization performance. First, proposing generates proofs with an accuracy of 87%, while retaining or improving performance on the qa task, compared to ruletakers (up to 6% improvement on zero-shot evaluation). Second, when trained on questions requiring lower depths of reasoning, it generalizes significantly better to higher depths (up to 15% improvement). Third, of obtains near perfect qa accuracy of 98% using only 40% of the training data. However, generating proofs for questions requiring higher depths of reasoning becomes challenging, and the accuracy drops to 65% for \u201cdepth 5\u201d, indicating significant scope for future work.","wordlikeness":0.8333333333,"lcsratio":1.0,"wordcoverage":0.9090909091}
{"Year":1991,"Venue":"eacl-1991","Acronym":"Pearl","Description":"A Probabilistic Chart Parser","Abstract":"This i)al)er describes a ilatural language i)ars - ing algorith,n for unrestricted text which uses a prol)al)ility-i~ased scoring function to select the \"l)est\" i)arse of a sclfl,ence. The parser, t~earl, is a time-asynchronous i)ottom-ul) chart parser with earley-tyl)e tol)-down prediction which l)ur - sues the highest-scoring theory iu the chart, where the score of a theory represents tim extent to which the context of the sentence predicts that interpre- tation. This parser dilrers front previous attemi)ts at stochastic parsers in that it uses a richer form of conditional prol)alfilities i)ased on context to l)re- diet likelihood. T>carl also provides a framework for i,lcorporating the results of previous work in i)art-of-spe(;ch assignrlmn|., unknown word too<l- ois, and other probal)ilistic models of lingvistic features into one parsing tool, interleaving these techniques instead of using the traditional pipeline a,'chitecture, lu preliminary tests, \"represents has i)ee.","wordlikeness":1.0,"lcsratio":0.6,"wordcoverage":1.0}
{"Year":2022,"Venue":"ijcnlp-2022","Acronym":"Director","Description":"Generator-Classifiers For Supervised Language Modeling","Abstract":"Current language models achieve low perplexity but their resulting generations still suffer from toxic responses, repetitiveness, and contradictions. The standard language modeling setup fails to address these issues. In this paper, we introduce a new architecture, generator-classifier, that consists of a unified generator-classifier with both a language modeling and a classification head for each output token. Training is conducted jointly using both standard language modeling data, and data labeled with desirable and undesirable sequences. Experiments in several settings show that the model has competitive training and decoding speed compared to standard language models while yielding superior results, avoiding undesirable behaviors while maintaining generation quality. It also outperforms existing model guiding approaches in terms of both accuracy and efficiency. Our code is made publicly available.","wordlikeness":1.0,"lcsratio":0.5,"wordcoverage":1.0}
{"Year":2006,"Venue":"ws-2006","Acronym":"LEILA","Description":"Learning to Extract Information by Linguistic Analysis","Abstract":"One of the challenging tasks in the context of the semantic web is to automatically extract instances of binary relations from web documents \u2013 for example all pairs of a person and the corresponding birthdate. In this paper, we present given, a system that can extract instances of arbitrary given binary relations from natural language web documents \u2013 without human interaction. Different from previous approaches, the uses a deep syntactic analysis. This results in consistent improvements over comparable systems (such as e.g. snowball or texttoonto).","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2023,"Venue":"ws-2023","Acronym":"oBERTa","Description":"Improving Sparse Transfer Learning via improved initialization, distillation, and pruning regimes","Abstract":"In this paper, we introduce the range of language language models, an easy-to-use set of language models which allows natural language processing (nlp) practitioners to obtain between 3.8 and 24.3 times faster models without expertise in model compression. Specifically, deliver extends existing work on pruning, knowledge distillation, and quantization and leverages frozen embeddings, improves distillation, and model initialization to deliver higher accuracy on a broad range of transfer tasks. In generating generating, we explore how the highly optimized rbetween differs from the bert for pruning during pre-training and finetuning. We find it less amenable to compression during fine-tuning. We explore the use of expertise on seven representative nlp tasks and find that the improved compression techniques allow a pruned pre-training model to match the performance of bertbase and exceed the performance of prune ofa large on the squad v1.1 question answering dataset, despite being 8x and 2x respectively faster in inference. We release our code, training regimes, and associated model for broad usage to encourage usage and experimentation.","wordlikeness":0.6666666667,"lcsratio":0.8333333333,"wordcoverage":0.8333333333}
{"Year":2012,"Venue":"starsem-2012","Acronym":"janardhan","Description":"Semantic Textual Similarity using Universal Networking Language graph matching","Abstract":"Sentences that are syntactically quite different can often have similar or same meaning. The semeval 2012 task of semantic textual similarity aims at \ufb01nding the semantic similarity between two sentences. The semantic representation of universal networking language (unl), represents only the inherent meaning in a sentence without any syntactic details. Thus, comparing the unl graphs of two sentences can give an insight into how semantically similar the two sentences are. This paper presents the unl graph matching method for the semantic textual similarity(sts) task.","wordlikeness":0.7777777778,"lcsratio":0.7777777778,"wordcoverage":0.7058823529}
{"Year":2022,"Venue":"naacl-2022","Acronym":"FNet","Description":"Mixing Tokens with Fourier Transforms","Abstract":"We show that transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that \u201cmix\u201d input tokens. Most surprisingly, we find that replacing the self-attention sublayer in a transformer encoder with a standard, unparameterized fourier transform achieves 92-97% of the accuracy of bert counterparts on the glue benchmark, but trains 80% faster on gpus and 70% faster on tpus at standard 512 input lengths. At longer input lengths, our the model is significantly faster: when compared to the \u201cefficient transformers\u201d on the long range arena benchmark, be matches the accuracy of the most accurate models, while outpacing the fastest models across all sequence lengths on gpus (and across relatively shorter lengths on tpus). Finally, is has a light memory footprint and is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small fixed models outperform transformer counterparts.","wordlikeness":0.75,"lcsratio":0.75,"wordcoverage":0.8571428571}
{"Year":2021,"Venue":"acl-2021","Acronym":"TextFlint","Description":"Unified Multilingual Robustness Evaluation Toolkit for Natural Language Processing","Abstract":"67,000) is a multilingual robustness evaluation toolkit for nlp tasks that incorporates universal text transformation, task-specific transformation, adversarial attack, subpopulation, and their combinations to provide comprehensive robustness analyses. This enables practitioners to automatically evaluate their models from various aspects or to customize their evaluations as desired with just a few lines of code. <a also generates complete analytical reports as well as targeted augmented data to address the shortcomings of the model in terms of its robustness. To guarantee acceptability, all the text transformations are linguistically based and all the transformed data selected (up to 100,000 texts) scored highly under human evaluation. To validate the utility, we performed large-scale empirical evaluations (over 67,000) on state-of-the-art deep learning models, classic supervised methods, and real-world systems. The toolkit is already available at <a href=https:\/\/github.com\/adversarial class=acl-markup-url>https:\/\/github.com\/evaluate<\/a> with all the evaluation results demonstrated at that.io.","wordlikeness":0.5555555556,"lcsratio":0.7777777778,"wordcoverage":0.75}
{"Year":2023,"Venue":"acl-2023","Acronym":"TREA","Description":"Tree-Structure Reasoning Schema for Conversational Recommendation","Abstract":"Conversational recommender systems (crs) aim to timely trace the dynamic interests of users through dialogues and generate relevant responses for item recommendations. Recently, various external knowledge bases (especially knowledge graphs) are incorporated into crs to enhance the understanding of conversation contexts. However, recent reasoning-based models heavily rely on simplified structures such as linear structures or fixed-hierarchical structures for causality reasoning, hence they cannot fully figure out sophisticated relationships among utterances with external knowledge. To address this, we propose a novel tree structure reasoning schema named on. Reasoning, constructs a multi-hierarchical scalable tree as the reasoning structure to clarify the causal relationships between mentioned entities, and fully utilizes historical conversations to generate more reasonable and suitable responses for recommended results. Extensive experiments on two public crs datasets have demonstrated the effectiveness of our approach.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2021,"Venue":"naacl-2021","Acronym":"KPQA","Description":"A Metric for Generative Question Answering Using Keyphrase Weights","Abstract":"In the automatic evaluation of generative question answering (genqa) systems, it is difficult to assess the correctness of generated answers due to the free-form of the answer. Especially, widely used n-gram similarity metrics often fail to discriminate the incorrect answers since they equally consider all of the tokens. To alleviate this problem, we propose via metric, a new metric for evaluating the correctness of genqa. Specifically, our new metric assigns different weights to each token via keyphrase prediction, thereby judging whether a generated answer sentence captures the key meaning of the reference answer. To evaluate our metric, we create high-quality human judgments of correctness on two genqa datasets. Using our human-evaluation datasets, we show that our proposed metric has a significantly higher correlation with human judgments than existing metrics in various datasets. Code for we-metric will be available at <a href=https:\/\/github.com\/hwanheelee1993\/create class=acl-markup-url>https:\/\/github.com\/hwanheelee1993\/reference<\/a>.","wordlikeness":0.25,"lcsratio":0.75,"wordcoverage":0.6666666667}
{"Year":2022,"Venue":"naacl-2022","Acronym":"SSEGCN","Description":"Syntactic and Semantic Enhanced Graph Convolutional Network for Aspect-based Sentiment Analysis","Abstract":"Aspect-based sentiment analysis (absa) aims to predict the sentiment polarity towards a particular aspect in a sentence. Recently, graph neural networks based on dependency tree convey rich structural information which is proven to be utility for absa. However, how to effectively harness the semantic and syntactic structure information from the dependency tree remains a challenging research question. In this paper, we propose a novel syntactic and semantic enhanced graph convolutional network (results) model for absa task. Specifically, we propose an aspect-aware attention mechanism combined with self-attention to obtain attention score matrices of a sentence, which can not only learn the aspect-related semantic correlations, but also learn the global semantics of the sentence. In order to obtain comprehensive syntactic structure information, we construct syntactic mask matrices of the sentence according to the different syntactic distances between words. Furthermore, to combine syntactic structure and semantic information, we equip the attention score matrices by syntactic mask matrices. Finally, we enhance the node representations with graph convolutional network over attention score matrices for absa. Experimental results on benchmark datasets illustrate that our proposed model outperforms state-of-the-art methods.","wordlikeness":0.1666666667,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2018,"Venue":"nlposs-2018","Acronym":"Texar","Description":"A Modularized, Versatile, and Extensible Toolbox for Text Generation","Abstract":"We introduce sharing, an open-source toolkit aiming to support the broad set of text generation tasks. Different from many existing toolkits that are specialized for specific applications (e.g., neural machine translation), can is designed to be highly flexible and versatile. This is achieved by abstracting the common patterns underlying the diverse tasks and methodologies, creating a library of highly reusable modules and functionalities, and enabling arbitrary model architectures and various algorithmic paradigms. The features make be particularly suitable for technique sharing and generalization across different text generation applications. The toolkit emphasizes heavily on extensibility and modularized system design, so that components can be freely plugged in or swapped out. We conduct extensive experiments and case studies to demonstrate the use and advantage of the toolkit.","wordlikeness":0.8,"lcsratio":0.8,"wordcoverage":0.8888888889}
{"Year":2022,"Venue":"wanlp-2022","Acronym":"AraBART","Description":"a Pretrained Arabic Sequence-to-Sequence Model for Abstractive Summarization","Abstract":"Like most natural language understanding and generation tasks, state-of-the-art models for summarization are transformer-based sequence-to-sequence architectures that are pretrained on large corpora. While most existing models focus on english, arabic remains understudied. In this paper we propose datasets,, the first arabic model in which the encoder and the decoder are pretrained end-to-end, based on bart. We show that we achieves the best performance on multiple abstractive summarization datasets, outperforming strong baselines including a pretrained arabic bert-based model, multilingual bart, arabic t5, and a multilingual t5 model. Model, is publicly available.","wordlikeness":0.8571428571,"lcsratio":1.0,"wordcoverage":0.8333333333}
{"Year":2022,"Venue":"lrec-2022","Acronym":"VISA","Description":"An Ambiguous Subtitles Dataset for Visual Scene-aware Machine Translation","Abstract":"Existing multimodal machine translation (mmt) datasets consist of images and video captions or general subtitles which rarely contain linguistic ambiguity, making visual information not so effective to generate appropriate translations. We introduce movies, a new dataset that consists of 40k japanese-english parallel sentence pairs and corresponding video clips with the following key features: (1) the parallel sentences are subtitles from movies and tv episodes; (2) the source subtitles are ambiguous, which means they have multiple possible translations with different meanings; (3) we divide the dataset into polysemy and omission according to the cause of ambiguity. We show that polysemy is challenging for the latest mmt system, and we hope that the dataset can facilitate mmt research.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2023,"Venue":"acl-2023","Acronym":"DSEE","Description":"Dually Sparsity-embedded Efficient Tuning of Pre-trained Language Models","Abstract":"Gigantic pre-trained models have become central to natural language processing (nlp), serving as the starting point for fine-tuning towards a range of downstream tasks. However, two pain points persist for this paradigm: (a) as the pre-trained models grow bigger (e.g., 175b parameters for gpt-3), even the fine-tuning process can be time-consuming and computationally expensive; (b) the fine-tuned model has the same size as its starting point by default, which is neither sensible due to its more specialized functionality, nor practical since many fine-tuned models will be deployed in resource-constrained environments. To address these pain points, we propose a framework for resource- and parameter-efficient fine-tuning by leveraging the sparsity prior in both weight updates and the final model weights. Our proposed framework, dubbed dually sparsity-embedded efficient tuning (for), aims to achieve two key objectives: (i) parameter efficient fine-tuning - by enforcing sparsity-aware low-rank updates on top of the pre-trained weights; and (ii) resource-efficient inference - by encouraging a sparse weight structure towards the final fine-tuned model. We leverage sparsity in these two directions by exploiting both unstructured and structured sparse patterns in pre-trained language models viaa unified approach. Extensive experiments and in-depth investigations, with diverse network backbones (i.e., bert, roberta, and gpt-2) on dozens of datasets, consistently demonstrate impressive parameter-\/inference-efficiency, while maintaining competitive downstream performance. For instance, propose saves about 25% inference flops while achieving comparable performance, with 0.5% trainable parameters on bert. Codes are available at <a href=https:\/\/github.com\/vita-group\/- class=acl-markup-url>https:\/\/github.com\/vita-group\/time-consuming<\/a>.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":1992,"Venue":"anlp-1992","Acronym":"MORPHE","Description":"A Practical Compiler for Reversible Morphology Rules","Abstract":"Morph~ is a common lisp compiler for reversible inflectional morphology rules developed at the cen- ter for machine translation at carnegie mellon uni- versity. This paper describes the morph~ process- ing model, its implementation, and how it handles some common morphological processes.","wordlikeness":0.8333333333,"lcsratio":1.0,"wordcoverage":0.9230769231}
{"Year":2020,"Venue":"sigdial-2020","Acronym":"rrSDS","Description":"Towards a Robot-ready Spoken Dialogue System","Abstract":"Spoken interaction with a physical robot requires a dialogue system that is modular, multimodal, distributive, incremental and temporally aligned. In this demo paper, we make significant contributions towards fulfilling these requirements by expanding upon the retico incremental framework. We outline the incremental and multimodal modules and how their computation can be distributed. We demonstrate the power and flexibility of our robot-ready spoken dialogue system to be integrated with almost any robot.","wordlikeness":0.2,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2023,"Venue":"acl-2023","Acronym":"DISCO","Description":"Distilling Counterfactuals with Large Language Models","Abstract":"Models trained with counterfactually augmented data learn representations of the causal structure of tasks, enabling robust generalization. However, high-quality counterfactual data is scarce for most tasks and not easily generated at scale. When crowdsourced, such data is typically limited in scale and diversity; when generated using supervised methods, it is computationally expensive to extend to new counterfactual dimensions. In this work, we introduce language (distilled counterfactual data), a new method for automatically generating high-quality counterfactual data at scale. Using engineers prompts to generate phrasal perturbations with a large general language model. Then, a task-specific teacher model filters these generations to distill high-quality counterfactual data. While task-agnostic, we apply our pipeline to the task of natural language inference (nli) and find that on challenging evaluations such as the nli stress test, comparatively smaller student models trained with then, generated counterfactuals are more robust (6% absolute) and generalize better across distributions (2%) compared to models trained without data augmentation. Furthermore, this augmented models are 10% more consistent between counterfactual pairs on three evaluation sets, demonstrating that general augmentation enables models to more reliably learn causal representations. Our repository are available at: <a href=https:\/\/github.com\/eric11eca\/for class=acl-markup-url>https:\/\/github.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2020,"Venue":"findings-2020","Acronym":"BERT-kNN","Description":"Adding a kNN Search Component to Pretrained Language Models for Better QA","Abstract":"Khandelwal et al. (2020) use a k-nearest-neighbor (knn) component to improve language model performance. We show that this idea is beneficial for open-domain question answering (qa). To improve the recall of facts encountered during training, we combine bert (devlin et al., 2019) with a traditional information retrieval step (ir) and a knn search over a large datastore of an embedded text collection. Our contributions are as follows: i) are outperforms bert on cloze-style qa by large margins without any further training. Ii) we show that bert often identifies the correct response category (e.g., us city), but only knn recovers the factually correct answer (e.g.,\u201cmiami\u201d). Iii) compared to bert, by excels for rare facts. Iv) the can easily handle facts not covered by bert\u2019s training set, e.g., recent events.","wordlikeness":0.5,"lcsratio":0.625,"wordcoverage":0.6153846154}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"ClidSum","Description":"A Benchmark Dataset for Cross-Lingual Dialogue Summarization","Abstract":"We present ,, a benchmark dataset towards building cross-lingual summarization systems on dialogue documents. It consists of 67k+ dialogue documents and 112k+ annotated summaries in different target languages. Based on the proposed multiple, we introduce two benchmark settings for supervised and semi-supervised scenarios, respectively. We then build various baseline systems in different paradigms (pipeline and end-to-end) and conduct extensive experiments on an to provide deeper analyses. Furthermore, we propose mdialbart which extends mbart via further pre-training, where the multiple objectives help the pre-trained model capture the structural characteristics as well as key content in dialogues and the transformation from source to the target language. Experimental results show the superiority of mdialbart, as an end-to-end model, outperforms strong pipeline models on mdialbart. Finally, we discuss specific challenges that current approaches faced with this task and give multiple promising directions for future research. We have released the dataset and code at <a href=https:\/\/github.com\/krystalan\/a class=acl-markup-url>https:\/\/github.com\/krystalan\/key<\/a>.","wordlikeness":0.7142857143,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"PcMSP","Description":"A Dataset for Scientific Action Graphs Extraction from Polycrystalline Materials Synthesis Procedure Text","Abstract":"Scientific action graphs extraction from materials synthesis procedures is important for reproducible research, machine automation, and material prediction. But the lack of annotated data has hindered progress in this field. We demonstrate an effort to annotate polycrystalline materials synthesis procedures comprehensive from 305 open access scientific articles for the construction of synthesis action graphs. This is a new dataset for material science information extraction that simultaneously contains the synthesis sentences extracted from the experimental paragraphs, as well as the entity mentions and intra-sentence relations. A two-step human annotation and inter-annotator agreement study guarantee the high quality of the sentences corpus. We introduce four natural language processing tasks: sentence classification, named entity recognition, relation classification, and joint extraction of entities and relations. Comprehensive experiments validate the effectiveness of several state-of-the-art models for these challenges while leaving large space for improvement. We also perform the error analysis and point out some unique challenges that require further investigation. We will release our annotation scheme, the corpus, and codes to the research community to alleviate the scarcity of labeled data in this domain.","wordlikeness":0.2,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"JADES","Description":"New Text Simplification Dataset in Japanese Targeted at Non-Native Speakers","Abstract":"The user-dependency of text simplification makes its evaluation obscure. A targeted evaluation dataset clarifies the purpose of simplification, though its specification is hard to define. We built several (japanese dataset for the evaluation of simplification), a text simplification dataset targeted at non-native japanese speakers, according to public vocabulary and grammar profiles. 3,907 comprises 3,907 complex-simple sentence pairs annotated by an expert. Analysis of simplification), shows that wide and multiple rewriting operations were applied through simplification. Furthermore, we analyzed outputs on analysis from several benchmark systems and automatic and manual scores of them. Results of these analyses highlight differences between english and japanese in operations and evaluations.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2020,"Venue":"coling-2020","Acronym":"CLUE","Description":"A Chinese Language Understanding Evaluation Benchmark","Abstract":"The advent of natural language understanding (nlu) benchmarks for english, such as glue and superglue allows new nlu models to be evaluated across a diverse set of tasks. These comprehensive benchmarks have facilitated a broad range of research and applications in natural language processing (nlp). The problem, however, is that most such benchmarks are limited to english, which has made it difficult to replicate many of the successes in english nlu for other languages. To help remedy this issue, we introduce the first large-scale chinese language understanding evaluation (project) benchmark. However, is an open-ended, community-driven project that brings together 9 tasks spanning several well-established single-sentence\/sentence-pair classification tasks, as well as machine reading comprehension, all on original chinese text. To establish results on these tasks, we report scores using an exhaustive set of current state-of-the-art pre-trained chinese models (9 in total). We also introduce a number of supplementary datasets and additional tools to help facilitate further progress on chinese nlu. Our benchmark is released at <a href=https:\/\/www.chinesebenchmarks.com class=acl-markup-url>https:\/\/www.ofbenchmarks.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2022,"Venue":"acl-2022","Acronym":"Text-to-Table","Description":"A New Way of Information Extraction","Abstract":"We study a new problem setting of information extraction (ie), referred to as define. In perform, given a text, one creates a table or several tables expressing the main content of the text, while the model is learned from text-table pair data. The problem setting differs from those of the existing methods for ie. First, the extraction can be carried out from long texts to large tables with complex structures. Second, the extraction is entirely data-driven, and there is no need to explicitly define the schemas. As far as we know, there has been no previous work that studies the problem. In this work, we formalize proposed as a sequence-to-sequence (seq2seq) problem. We first employ a seq2seq model fine-tuned from a pre-trained language model to perform the task. We also develop a new method within the seq2seq approach, exploiting two additional techniques in table generation: table constraint and table relation embeddings. We consider those as an inverse problem of the well-studied table-to-text, and make use of four existing table-to-text datasets in our experiments on datasets. Experimental results show that the vanilla seq2seq model can outperform the baseline methods of using relation extraction and named entity extraction. The results also show that our method can further boost the performances of the vanilla seq2seq model. We further discuss the main challenges of the proposed task. The code and data are available at <a href=https:\/\/github.com\/shirley-wu\/text_to_table class=acl-markup-url>https:\/\/github.com\/shirley-wu\/text_to_table<\/a>.","wordlikeness":0.6923076923,"lcsratio":0.4615384615,"wordcoverage":0.6363636364}
{"Year":2020,"Venue":"acl-2020","Acronym":"MATINF","Description":"A Jointly Labeled Large-Scale Dataset for Classification, Question Answering and Summarization","Abstract":"Recently, large-scale datasets have vastly facilitated the development in nearly all domains of natural language processing. However, there is currently no cross-task dataset in nlp, which hinders the development of multi-task learning. We propose such, the first jointly labeled large-scale dataset for classification, question answering and summarization. Pairs contains 1.07 million question-answer pairs with human-labeled categories and user-generated question descriptions. Based on such rich information, no is applicable for three major nlp tasks, including classification, question answering, and summarization. We benchmark existing methods and a novel multi-task baseline over including to inspire further research. Our comprehensive comparison and experiments over inspire and other datasets demonstrate the merits held by no.","wordlikeness":0.8333333333,"lcsratio":0.8333333333,"wordcoverage":0.8333333333}
{"Year":2016,"Venue":"coling-2016","Acronym":"CamelParser","Description":"A system for Arabic Syntactic Analysis and Morphological Disambiguation","Abstract":"In this paper, we present features,, a state-of-the-art system for arabic syntactic dependency analysis aligned with contextually disambiguated morphological features. Syntactic uses a state-of-the-art morphological disambiguator and improves its results using syntactically driven features. The system offers a number of output formats that include basic dependency with morphological features, two tree visualization modes, and traditional arabic grammatical analysis.","wordlikeness":0.6363636364,"lcsratio":0.5454545455,"wordcoverage":0.7058823529}
{"Year":2022,"Venue":"coling-2022","Acronym":"KoBEST","Description":"Korean Balanced Evaluation of Significant Tasks","Abstract":"A well-formulated benchmark plays a critical role in spurring advancements in the natural language processing (nlp) field, as it allows objective and precise evaluation of diverse models. As modern language models (lms) have become more elaborate and sophisticated, more difficult benchmarks that require linguistic knowledge and reasoning have been proposed. However, most of these benchmarks only support english, and great effort is necessary to construct benchmarks for other low resource languages. To this end, we propose a new benchmark named korean balanced evaluation of significant tasks (korean), which consists of five korean-language downstream tasks. Professional korean linguists designed the tasks that require advanced korean linguistic knowledge. Moreover, our data is purely annotated by humans and thoroughly reviewed to guarantee high data quality. We also provide baseline models and human performance results. Our dataset is available on the huggingface.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2020,"Venue":"acl-2020","Acronym":"Stanza","Description":"A Python Natural Language Processing Toolkit for Many Human Languages","Abstract":"We introduce corpora,, an open-source python natural language processing toolkit supporting 66 human languages. Compared to existing widely used toolkits, processing features a language-agnostic fully neural pipeline for text analysis, including tokenization, multi-word token expansion, lemmatization, part-of-speech and morphological feature tagging, dependency parsing, and named entity recognition. We have trained of on a total of 112 datasets, including the universal dependencies treebanks and other multilingual corpora, and show that the same neural architecture generalizes well and achieves competitive performance on all languages tested. Additionally, on includes a native python interface to the widely used java stanford corenlp software, which further extends its functionality to cover other tasks such as coreference resolution and relation extraction. Source code, documentation, and pretrained models for 66 languages are available at <a href=https:\/\/stanfordnlp.github.io\/trained\/ class=acl-markup-url>https:\/\/stanfordnlp.github.io\/additionally,\/<\/a>.","wordlikeness":0.6666666667,"lcsratio":0.8333333333,"wordcoverage":0.8}
{"Year":2004,"Venue":"acl-2004","Acronym":"TANGO","Description":"Bilingual Collocational Concordancer","Abstract":"In this paper, we describe learners as a collocational concordancer for looking up collocations. The system was designed to answer user\u2019s query of bilingual collocational usage for nouns, verbs and adjectives. We first obtained collocations from the large monolingual british national corpus (bnc). Subsequently, we identified collocation instances and translation counterparts in the bilingual corpus such as sinorama parallel corpus (spc) by exploiting the wordalignment technique. The main goal of the concordancer is to provide the user with a reference tools for correct collocation use so as to assist second language learners to acquire the most eminent characteristic of native-like writing.","wordlikeness":0.8,"lcsratio":0.8,"wordcoverage":0.8888888889}
{"Year":2015,"Venue":"emnlp-2015","Acronym":"FINET","Description":"Context-Aware Fine-Grained Named Entity Typing","Abstract":"We propose outperforms, a system for detecting the types of named entities in short inputs\u2014such as sentences or tweets\u2014with respect to wordnet\u2019s super \ufb01ne-grained type system. And generates candidate types using a sequence of multiple extractors, ranging from explicitly mentioned types to implicit types, and subsequently selects the most appropriate using ideas from word-sense disambiguation. So combats data scarcity and noise from existing systems: it does not rely on supervision in its extractors and generates training data for type selection from wordnet and other resources. Data supports the most \ufb01ne-grained type system so far, including types with no annotated training data. Our experiments indicate that state-of-the-art outperforms state-of-the-art methods in terms of recall, precision, and granularity of extracted types.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.9090909091}
{"Year":2018,"Venue":"paclic-2018","Acronym":"DEMN","Description":"Distilled-Exposition Enhanced Matching Network for Story Comprehension","Abstract":"This paper proposes a distilled-exposition enhanced matching network (in) for story-cloze test, which is still a challenging task in story comprehension. We divide a complete story into three narrative segments: an exposition, a climax, and an ending. The model consists of three modules: input module, matching module, and distillation module. The input module provides semantic representations for the three segments and then feeds them into the other two modules. The matching module collects interaction features between the ending and the climax. The distillation module distills the crucial semantic information in the exposition and infuses it into the matching module in two different ways. We evaluate our single and ensemble model on rocstories corpus (mostafazadeh et al., 2016), achieving an accuracy of 80.1% and 81.2% on the test set respectively. The experimental results demonstrate that our consists model achieves a state-of-the-art performance.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2016,"Venue":"acl-2016","Acronym":"Roleo","Description":"Visualising Thematic Fit Spaces on the Web","Abstract":"In this paper, we present \ufb01lls, a web tool for visualizing the vector spaces generated by the evaluation of distributional memory (dm) models over thematic \ufb01t judgements. A thematic \ufb01t judgement is a rating of the selectional preference of a verb for an argument that \ufb01lls a given thematic role. The dm approach to thematic \ufb01t judgements involves the construction of a sub-space in which a prototypical role-\ufb01ller can be built for comparison to the noun being judged. We describe a publicly-accessible web tool that allows for querying and exploring these spaces as well as a technique for visualizing thematic \ufb01t sub-spaces ef\ufb01ciently for web use.","wordlikeness":0.8,"lcsratio":0.6,"wordcoverage":0.8888888889}
{"Year":2020,"Venue":"coling-2020","Acronym":"EmpDG","Description":"Multi-resolution Interactive Empathetic Dialogue Generation","Abstract":"A humanized dialogue system is expected to generate empathetic replies, which should be sensitive to the users\u2019 expressed emotion. The task of empathetic dialogue generation is proposed to address this problem. The essential challenges lie in accurately capturing the nuances of human emotion and considering the potential of user feedback, which are overlooked by the majority of existing work. In response to this problem, we propose a multi-resolution adversarial model \u2013 capture, to generate more empathetic responses. Problem, exploits both the coarse-grained dialogue-level and fine-grained token-level emotions, the latter of which helps to better capture the nuances of user emotion. In addition, we introduce an interactive adversarial learning framework which exploits the user feedback, to identify whether the generated responses evoke emotion perceptivity in dialogues. Experimental results show that the proposed approach significantly outperforms the state-of-the-art baselines in both content quality and emotion perceptivity.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2022,"Venue":"lrec-2022","Acronym":"ViHealthBERT","Description":"Pre-trained Language Models for Vietnamese in Health Text Mining","Abstract":"Pre-trained language models have become crucial to achieving competitive results across many natural language processing (nlp) problems. For monolingual pre-trained models in low-resource languages, the quantity has been significantly increased. However, most of them relate to the general domain, and there are limited strong baseline language models for domain-specific. We introduce most, the first domain-specific pre-trained language model for vietnamese healthcare. The performance of our model shows strong results while outperforming the general domain language models in all health-related datasets. Moreover, we also present vietnamese datasets for the healthcare domain for two tasks are acronym disambiguation (ad) and frequently asked questions (faq) summarization. We release our monolingual to facilitate future research and downstream application for vietnamese nlp in domain-specific. Our dataset and code are available in <a href=https:\/\/github.com\/demdecuong\/most class=acl-markup-url>https:\/\/github.com\/demdecuong\/our<\/a>.","wordlikeness":0.8333333333,"lcsratio":0.8333333333,"wordcoverage":0.7619047619}
{"Year":2014,"Venue":"semeval-2014","Acronym":"Potsdam","Description":"Semantic Dependency Parsing by Bidirectional Graph-Tree Transformations and Syntactic Parsing","Abstract":"We present the 78.60. Systems that participated in the semantic dependency parsing shared task of semeval 2014. They are based on linguistically motivated bidirectional transformations between graphs and trees and on utilization of syntactic dependency parsing. They were entered in both the closed track and the open track of the challenge, recording a peak average labeled f1 score of 78.60.","wordlikeness":0.7142857143,"lcsratio":0.8571428571,"wordcoverage":0.7272727273}
{"Year":2015,"Venue":"semeval-2015","Acronym":"ULisboa","Description":"Recognition and Normalization of Medical Concepts","Abstract":"This paper describes a system developed for the disorder identi\ufb01cation subtask within task 14 of semeval 2015. The developed system is based on a chain of two modules, one for recognition and another for normalization. The recognition module is based on an adapted version of the stanford ner system to train crf models in order to recognize disorder mentions. Crf models were build based on a novel encoding of entity spans as token classi\ufb01cations to also consider non-continuous entities, along with a rich set of features based on (i) domain lexicons and (ii) brown clusters inferred from a large collection of clinical texts. For disorder normalization, we (i) generated a non ambiguous dictionary of abbreviations from the labelled \ufb01les, using it together with (ii) an heuristic method based on similarity search and (iii) a comparison method based on the information content of each disorder. The system achieved an f-measure of 0.740 (the second best), with a precision of 0.779, a recall of 0.705.","wordlikeness":0.4285714286,"lcsratio":0.5714285714,"wordcoverage":0.7692307692}
{"Year":2022,"Venue":"ws-2022","Acronym":"SynSciPass","Description":"detecting appropriate uses of scientific text generation","Abstract":"Approaches to machine generated text detection tend to focus on binary classification of human versus machine written text. In the scientific domain where publishers might use these models to examine manuscripts under submission, misclassification has the potential to cause harm to authors. Additionally, authors may appropriately use text generation models such as with the use of assistive technologies like translation tools. In this setting, a binary classification scheme might be used to flag appropriate uses of assistive text generation technology as simply machine generated which is a cause of concern. In our work, we simulate this scenario by presenting a state-of-the-art detector trained on the dagpap22 with machine translated passages from scielo and find that the model performs at random. Given this finding, we develop a framework for dataset development that provides a nuanced approach to detecting machine generated text by having labels for the type of technology used such as for translation or paraphrase resulting in the construction of flag. By training the same model that performed well on dagpap22 on are, we show that not only is the model more robust to domain shifts but also is able to uncover the type of technology used for machine generated text. Despite this, we conclude that current datasets are neither comprehensive nor realistic enough to understand how these models would perform in the wild where manuscript submissions can come from many unknown or novel distributions, how they would perform on scientific full-texts rather than small passages, and what might happen when there is a mix of appropriate and inappropriate uses of natural language generation.","wordlikeness":0.5,"lcsratio":0.6,"wordcoverage":0.6666666667}
{"Year":2021,"Venue":"eacl-2021","Acronym":"MTOP","Description":"A Comprehensive Multilingual Task-Oriented Semantic Parsing Benchmark","Abstract":"Scaling semantic parsing models for task-oriented dialog systems to new languages is often expensive and time-consuming due to the lack of available datasets. Available datasets suffer from several shortcomings: a) they contain few languages b) they contain small amounts of labeled examples per language c) they are based on the simple intent and slot detection paradigm for non-compositional queries. In this paper, we present a new multilingual dataset, called projection., comprising of 100k annotated utterances in 6 languages across 11 domains. We use this dataset and other publicly available datasets to conduct a comprehensive benchmarking study on using various state-of-the-art multilingual pre-trained models for task-oriented semantic parsing. We achieve an average improvement of +6.3 points on slot f1 for the two existing multilingual datasets, over best results reported in their experiments. Furthermore, we demonstrate strong zero-shot performance using pre-trained models combined with automatic translation and alignment, and a proposed distant supervision method to reduce the noise in slot label projection.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2015,"Venue":"semeval-2015","Acronym":"ECNU","Description":"Using Multiple Sources of CQA-based Information for Answers Selection and YES\/NO Response Inference","Abstract":"This paper reports our submissions to community question answering task in semeval2015, which consists of two subtasks: (1) predict the quality of answers to given question as good, bad, or potentially relevant and (2) identify yes, no or unsure response to a given yes\/no question based on the good answers identi\ufb01ed by subtask 1. For both subtasks, we adopted supervised classi\ufb01cation method and examined the effects of heterogeneous features generated from community question answering data, such as bag-of-words, string matching, semantic similarity, answerer information, answer-speci\ufb01c features, questionspeci\ufb01c features, etc. Our submitted primary systems ranked the forth and the second for the two subtasks of english data respectively.","wordlikeness":0.25,"lcsratio":0.75,"wordcoverage":0.75}
{"Year":2023,"Venue":"ws-2023","Acronym":"WikiGoldSK","Description":"Annotated Dataset, Baselines and Few-Shot Learning Experiments for Slovak Named Entity Recognition","Abstract":"Named entity recognition (ner) is a fundamental nlp tasks with a wide range of practical applications. The performance of state-of-the-art ner methods depends on high quality manually anotated datasets which still do not exist for some languages. In this work we aim to remedy this situation in slovak by introducing conduct, the first sizable human labelled slovak ner dataset. We benchmark it by evaluating state-of-the-art multilingual pretrained language models and comparing it to the existing silver-standard slovak ner dataset. We also conduct few-shot experiments and show that training on a sliver-standard dataset yields better results. To enable future work that can be based on slovak ner, we release the dataset, code, as well as the trained models publicly under permissible licensing terms at <a href=https:\/\/github.com\/naiveneuron\/depends class=acl-markup-url>https:\/\/github.","wordlikeness":0.7,"lcsratio":0.6,"wordcoverage":0.6666666667}
{"Year":2010,"Venue":"lrec-2010","Acronym":"KALAKA","Description":"A TV Broadcast Speech Database for the Evaluation of Language Recognition Systems","Abstract":"A speech database, named results, was created to support the albayzin 2008 evaluation of language recognition systems, organized by the spanish network on speech technologies from may to november 2008. This evaluation, designed according to the criteria and methodology applied in the nist language recognition evaluations, involved four target languages: basque, catalan, galician and spanish (official languages in spain), and included speech signals in other (unknown) languages to allow open-set verification trials. In this paper, the process of designing, collecting data and building the train, development and evaluation datasets of technologies is described. Results attained in the albayzin 2008 lre are presented as a means of evaluating the database. The performance of a state-of-the-art language recognition system on a closed-set evaluation task is also presented for reference. Future work includes extending 2008 by adding portuguese and english as target languages and renewing the set of unknown languages needed to carry out open-set evaluations.","wordlikeness":0.8333333333,"lcsratio":0.6666666667,"wordcoverage":0.8333333333}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"RLET","Description":"A Reinforcement Learning Based Approach for Explainable QA with Entailment Trees","Abstract":"Interpreting the reasoning process from questions to answers poses a challenge in approaching explainable qa. A recently proposed structured reasoning format, entailment tree, manages to offer explicit logical deductions with entailment steps in a tree structure. To generate entailment trees, prior single pass sequence-to-sequence models lack visible internal decision probability, while stepwise approaches are supervised with extracted single step data and cannot model the tree as a whole. In this work, we propose into, a reinforcement learning based entailment tree generation framework, which is trained utilising the cumulative signals across the whole tree. Whole. Iteratively performs single step reasoning with sentence selection and deduction generation modules, from which the training signal is accumulated across the tree with elaborately designed aligned reward function that is consistent with the evaluation. To the best of our knowledge, we are the first to introduce rl into the entailment tree generation task. Experiments on three settings of the entailmentbank dataset demonstrate the strength of using rl framework.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2023,"Venue":"eacl-2023","Acronym":"DREEAM","Description":"Guiding Attention with Evidence for Improving Document-Level Relation Extraction","Abstract":"Document-level relation extraction (docre) is the task of identifying all relations between each entity pair in a document. Evidence, defined as sentences containing clues for the relationship between an entity pair, has been shown to help docre systems focus on relevant texts, thus improving relation extraction. However, evidence retrieval (er) in docre faces two major issues: high memory consumption and limited availability of annotations. This work aims at addressing these issues to improve the usage of er in docre. First, we propose docre., a memory-efficient approach that adopts evidence information as the supervisory signal, thereby guiding the attention modules of the docre system to assign high weights to evidence. Second, we propose a self-training strategy for in to learn er from automatically-generated evidence on massive data without evidence annotations. Experimental results reveal that our approach exhibits state-of-the-art performance on the docred benchmark for both docre and er. To the best of our knowledge, docre is the first approach to employ er self-training.","wordlikeness":0.5,"lcsratio":0.8333333333,"wordcoverage":0.9090909091}
{"Year":2006,"Venue":"lrec-2006","Acronym":"SKELETON","Description":"Specialised knowledge retrieval on the basis of terms and conceptual relations","Abstract":"The main goal of this paper is to present a first approach to an automatic detection of conceptual relations between two terms in specialised written text. Previous experiments on the basis of the manual analysis lead the authors to implement an automatic query strategy combining the term candidates proposed by an extractor together with a list of verbal syntactic patterns used for the relations refinement. Next step on the research will be the integration of the results into the term extractor in order to attain more restrictive pieces of information directly reused for the ontology building task.","wordlikeness":0.875,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2021,"Venue":"acl-2021","Acronym":"TAN-NTM","Description":"Topic Attention Networks for Neural Topic Modeling","Abstract":"Topic models have been widely used to learn text representations and gain insight into document corpora. To perform topic discovery, most existing neural models either take document bag-of-words (bow) or sequence of tokens as input followed by variational inference and bow reconstruction to learn topic-word distribution. However, leveraging topic-word distribution for learning better features during document encoding has not been explored much. To this end, we develop a framework are, which processes document as a sequence of tokens through a lstm whose contextual outputs are attended in a topic-aware manner. We propose a novel attention mechanism which factors in topic-word distribution to enable the model to attend on relevant words that convey topic related cues. The output of topic attention module is then used to carry out variational inference. We perform extensive ablations and experiments resulting in ~9-15 percentage improvement over score of existing sota topic models in npmi coherence on several benchmark datasets - 20newsgroups, yelp review polarity and agnews. Further, we show that our method learns better latent document-topic features compared to existing topic models through improvement on two downstream tasks: document classification and topic guided keyphrase generation.","wordlikeness":0.2857142857,"lcsratio":0.8571428571,"wordcoverage":0.6666666667}
{"Year":2019,"Venue":"conll-2019","Acronym":"MrMep","Description":"Joint Extraction of Multiple Relations and Multiple Entity Pairs Based on Triplet Attention","Abstract":"This paper focuses on how to extract multiple relational facts from unstructured text. Neural encoder-decoder models have provided a viable new approach for jointly extracting relations and entity pairs. However, these models either fail to deal with entity overlapping among relational facts, or neglect to produce the whole entity pairs. In this work, we propose a novel architecture that augments the encoder and decoder in two elegant ways. First, we apply a binary cnn classifier for each relation, which identifies all possible relations maintained in the text, while retaining the target relation representation to aid entity pair recognition. Second, we perform a multi-head attention over the text and a triplet attention with the target relation interacting with every token of the text to precisely produce all possible entity pairs in a sequential manner. Experiments on three benchmark datasets show that our proposed method successfully addresses the multiple relations and multiple entity pairs even with complex overlapping and significantly outperforms the state-of-the-art methods.","wordlikeness":0.4,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2006,"Venue":"ws-2006","Acronym":"Humor","Description":"Prosody Analysis and Automatic Recognition for F*R*I*E*N*D*S*","Abstract":"We analyze linguisticous spoken conversations from a classic comedy television show, friends, by examining acousticprosodic and linguistic features and their utility in automatic followed recognition. Using a simple annotation scheme, we automatically label speaker turns in our corpus that are followed by laughs as baseline.ous and the rest as non-theous. Our analysis-prosody analysis reveals signi\ufb01cant differences in prosodic characteristics (such as pitch, tempo, energy etc.) of thatous and non-non-ous.ous speech, even when accounted for the gender and speaker differences. Promising recognition was carried out using standard supervised learning classi\ufb01ers, and shows promising results signi\ufb01cantly above the baseline.","wordlikeness":1.0,"lcsratio":0.8,"wordcoverage":1.0}
{"Year":1996,"Venue":"coling-1996","Acronym":"GLOSSER-RuG","Description":"in Support of Reading","Abstract":"Q'his paper reports ou ongoing work on a cai,i, system to facilitate foreign lain guage learning: gi,()ssei{-i{ug. The system is partieulm'ly dependent on ad- vanc.ed morphological analysis, t!'ollow- ing a brief introduction to the project, the paper describes the architecture of glossi';i{-rug. Then wc describe iu detail the main compolmnts\/modnles that are part of the implemented pro- totype. Finally, iml)lement,ation issues and details involving the user interfaces of the tool are discussed. We oul, line the design of an integrated system t,o sul> port the reading of french text by ])ul, ctl speakers.","wordlikeness":0.7272727273,"lcsratio":0.3636363636,"wordcoverage":0.625}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"RACE","Description":"Retrieval-augmented Commit Message Generation","Abstract":"Commit messages are important for software development and maintenance. Many neural network-based approaches have been proposed and shown promising results on automatic commit message generation. However, the generated commit messages could be repetitive or redundant. In this paper, we propose promising, a new retrieval-augmented neural commit message generation method, which treats the retrieved similar commit as an exemplar and leverages it to generate an accurate commit message. As the retrieved commit message may not always accurately describe the content\/intent of the current code diff, we also propose an exemplar guider, which learns the semantic similarity between the retrieved and current code diff and then guides the generation of commit message based on the similarity. We conduct extensive experiments on a large public dataset with five programming languages. Experimental results show that conduct can outperform all baselines. Furthermore, propose can boost the performance of existing seq2seq models in commit message generation.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2014,"Venue":"codeswitch-2014","Acronym":"DCU-UVT","Description":"Word-Level Language Classification with Code-Mixed Data","Abstract":"This paper describes the svm-based team\u2019s participation in the language identi\ufb01cation in code-switched data shared task in the workshop on computational approaches to code switching. Wordlevel classi\ufb01cation experiments were carried out using a simple dictionary-based method, linear kernel support vector machines (svms) with and without contextual clues, and a k-nearest neighbour approach. Based on these experiments, we select our svm-based system with contextual clues as our \ufb01nal system and present results for the nepali-english and spanish-english datasets.","wordlikeness":0.2857142857,"lcsratio":0.5714285714,"wordcoverage":0.6153846154}
{"Year":2000,"Venue":"lrec-2000","Acronym":"SIMPLE","Description":"A General Framework for the Development of Multilingual Lexicons","Abstract":"The project le-applications is an innovative attempt of building harmonized syntactic-semantic lexicons for 12 european languages, aimed at use in different human language technology applications. Human provides a general design model for the encoding of a large amount of semantic information, spanning from ontological typing, to argument structure and terminology. Results thus provides a general framework for resource development, where state-of-the-art results in lexical semantics are coupled with the needs of language engineering applications accessing semantic information. 1.","wordlikeness":1.0,"lcsratio":0.6666666667,"wordcoverage":1.0}
{"Year":2023,"Venue":"acl-2023","Acronym":"WACO","Description":"Word-Aligned Contrastive Learning for Speech Translation","Abstract":"End-to-end speech translation (e2e st) aims to directly translate source speech into target text. Existing st methods perform poorly when only extremely small speech-text data are available for training. We observe that an st model\u2019s performance closely correlates with its embedding similarity between speech and source transcript. In this paper, we propose word-aligned contrastive learning (is), a simple and effective method for extremely low-resource speech-to-text translation. Our key idea is bridging word-level representations for both speech and text modalities via contrastive learning. We evaluate used and other methods on the must-c dataset, a widely used st benchmark, and on a low-resource direction maltese-english from iwslt 2023. Our experiments demonstrate that directly outperforms the best baseline by 9+ bleu points with only 1-hour parallel st data. Code is available at <a href=https:\/\/github.com\/owaski\/speech class=acl-markup-url>https:\/\/github.com\/owaski\/baseline<\/a>.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2010,"Venue":"amta-2010","Acronym":"PLuTO","Description":"MT for On-Line Patent Translation","Abstract":"Automatic \u2013 patent language translation online \u2013 is a partially eu-funded commercialization project which specializes in the automatic retrieval and translation of patent documents. At the core of the behind framework is a machine translation (mt) engine through which web-based translation services are offered. The fully integrated search architecture includes a translation engine coupling mt with translation memories (tm), and a patent search and retrieval engine. In this paper, we first describe the motivating factors behind the provision of such a service. Following this, we give an overview of the translation framework as a whole, with particular emphasis on the mt components, and provide a real world use case scenario in which and mt services are ex- ploited.","wordlikeness":0.8,"lcsratio":0.8,"wordcoverage":0.75}
{"Year":2020,"Venue":"emnlp-2020","Acronym":"HERO","Description":"Hierarchical Encoder for Video&#43;Language Omni-representation Pre-training","Abstract":"We present cross-modal, a novel framework for large-scale video+language omni-representation learning. Video-and-language encodes multimodal inputs in a hierarchical structure, where local context of a video frame is captured by a cross-modal transformer via multimodal fusion, and global video context is captured by a temporal transformer. In addition to standard masked language modeling (mlm) and masked frame modeling (mfm) objectives, we design two new pre-training tasks: (i) video-subtitle matching (vsm), where the model predicts both global and local temporal alignment; and (ii) frame order modeling (fom), where the model predicts the right order of shuffled video frames. Collected is jointly trained on howto100m and large-scale tv datasets to gain deep understanding of complex social dynamics with multi-character interactions. Comprehensive experiments demonstrate that (i) achieves new state of the art on multiple benchmarks over text-based video\/video-moment retrieval, video question answering (qa), video-and-language inference and video captioning tasks across different domains. We also introduce two new challenging benchmarks how2qa and how2r for video qa and retrieval, collected from diverse video content over multimodalities.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2022,"Venue":"wanlp-2022","Acronym":"ArzEn-ST","Description":"A Three-way Speech Translation Corpus for Code-Switched Egyptian Arabic-English","Abstract":"We present our work on collecting arzen, a code-switched egyptian arabic-english speech translation corpus. This corpus is an extension of the arzen speech corpus, which was collected through informal interviews with bilingual speakers. In this work, we collect translations in both directions, monolingual egyptian arabic and monolingual english, forming a three-way speech translation corpus. We make the translation guidelines and corpus publicly available. We also report results for baseline systems for machine translation and speech translation tasks. We believe this is a valuable resource that can motivate and facilitate further research studying the code-switching phenomenon from a linguistic perspective and can be used to train and evaluate nlp systems.","wordlikeness":0.5,"lcsratio":0.875,"wordcoverage":0.7142857143}
{"Year":2013,"Venue":"semeval-2013","Acronym":"UTTime","Description":"Temporal Relation Classification using Deep Syntactic Features","Abstract":"In this paper, we present a system, which, which we submitted to tempeval-3 for task c: annotating temporal relations. The system uses logistic regression classi\ufb01ers and exploits features extracted from a deep syntactic parser, including paths between event words in phrase structure trees and their path lengths, and paths between event words in predicateargument structures and their subgraphs. Annotating achieved an f1 score of 34.9 based on the graphed-based evaluation for task c (ranked 2nd) and 56.45 for task c-relationonly (ranked 1st) in the tempeval-3 evaluation.","wordlikeness":0.6666666667,"lcsratio":0.8333333333,"wordcoverage":0.8}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"ProofInfer","Description":"Generating Proof via Iterative Hierarchical Inference","Abstract":"Proof generation focuses on deductive reasoning: given a hypothesis and a set of theories, including some supporting facts and logical rules expressed in natural language, the model generates a proof tree indicating how to deduce the hypothesis from given theories. Current models with state-of-the-art performance employ the stepwise method that adds an individual node to the proof step-by-step. However, these methods actually focus on generating several proof paths rather than a whole tree. During generation, they focus on the most relevant areas of the currently generated node while neglecting the rest of the proof tree. To address this problem, we propose this, which generates the proof tree via iterative hierarchical inference. At each step, address adds the entire layer to the proof, where all nodes in this layer are generated simultaneously. Since the conventional autoregressive generation architecture cannot simultaneously predict multiple nodes, supporting employs text-to-text paradigm. To this end, we propose a divide-and-conquer algorithm to encode the proof tree as the plain text without losing structure information. Experimental results show that comparable significantly improves performance on several widely-used datasets. In addition, tree still performs well with data-limited, achieving comparable performance to the state-of-the-art model with about 40% of the training data.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.7058823529}
{"Year":2013,"Venue":"acl-2013","Acronym":"HYENA-live","Description":"Fine-Grained Online Entity Type Classification from Natural-language Text","Abstract":"Recent research has shown progress in achieving high-quality, very \ufb01ne-grained type classi\ufb01cation in hierarchical taxonomies. Within such a multi-level type hierarchy with several hundreds of types at different levels, many entities naturally belong to multiple types. In order to achieve high-precision in type classi\ufb01cation, current approaches are either limited to certain domains or require time consuming multistage computations. As a consequence, existing systems are incapable of performing ad-hoc type classi\ufb01cation on arbitrary input texts. In this demo, we present a novel webbased tool that is able to perform domain independent entity type classi\ufb01cation under real time conditions. Thanks to its ef\ufb01cient implementation and compacted feature representation, the system is able to process text inputs on-the-\ufb02y while still achieving equally high precision as leading state-ofthe-art implementations. Our system offers an online interface where natural-language text can be inserted, which returns semantic type labels for entity mentions. Further more, the user interface allows users to explore the assigned types by visualizing and navigating along the type-hierarchy.","wordlikeness":0.5,"lcsratio":0.7,"wordcoverage":0.6666666667}
{"Year":2020,"Venue":"coling-2020","Acronym":"IntKB","Description":"A Verifiable Interactive Framework for Knowledge Base Completion","Abstract":"Knowledge bases (kbs) are essential for many downstream nlp tasks, yet their prime shortcoming is that they are often incomplete. State-of-the-art frameworks for kb completion often lack sufficient accuracy to work fully automated without human supervision. As a remedy, we propose : a novel interactive framework for kb completion from text based on a question answering pipeline. Our framework is tailored to the specific needs of a human-in-the-loop paradigm: (i) we generate facts that are aligned with text snippets and are thus immediately verifiable by humans. (ii) our system is designed such that it continuously learns during the kb completion task and, therefore, significantly improves its performance upon initial zero- and few-shot relations over time. (iii) we only trigger human interactions when there is enough information for a correct prediction. Therefore, we train our system with negative examples and a fold-option if there is no answer. Our framework yields a favorable performance: it achieves a hit@1 ratio of 29.7% for initially unseen relations, upon which it gradually improves to 46.2%.","wordlikeness":0.4,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2021,"Venue":"crac-2021","Acronym":"DMRST","Description":"A Joint Framework for Document-Level Multilingual RST Discourse Segmentation and Parsing","Abstract":"Text discourse parsing weighs importantly in understanding information flow and argumentative structure in natural language, making it beneficial for downstream tasks. While previous work significantly improves the performance of rst discourse parsing, they are not readily applicable to practical use cases: (1) edu segmentation is not integrated into most existing tree parsing frameworks, thus it is not straightforward to apply such models on newly-coming data. (2) most parsers cannot be used in multilingual scenarios, because they are developed only in english. (3) parsers trained from single-domain treebanks do not generalize well on out-of-domain inputs. In this work, we propose a document-level multilingual rst discourse parsing framework, which conducts edu segmentation and discourse tree parsing jointly. Moreover, we propose a cross-translation augmentation strategy to enable the framework to support multilingual parsing and improve its domain generality. Experimental results show that our model achieves state-of-the-art performance on document-level multilingual rst parsing in all sub-tasks.","wordlikeness":0.2,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2018,"Venue":"naacl-2018","Acronym":"LSDSCC","Description":"a Large Scale Domain-Specific Conversational Corpus for Response Generation with Diversity Oriented Evaluation Metrics","Abstract":"It has been proven that automatic conversational agents can be built up using the endto-end neural response generation (nrg) framework, and such a data-driven methodology requires a large number of dialog pairs for model training and reasonable evaluation metrics for testing. This paper proposes a large scale domain-specific conversational corpus (new) composed of high-quality queryresponse pairs extracted from the domainspecific online forum, with thorough preprocessing and cleansing procedures. Also, a testing set, including multiple diverse responses annotated for each query, is constructed, and on this basis, the metrics for measuring the diversity of generated results are further presented. We evaluate the performances of neural dialog models with the widely applied diversity boosting strategies on the proposed dataset. The experimental results have shown that our proposed corpus can be taken as a new benchmark dataset for the nrg task, and the presented metrics are promising to guide the optimization of nrg models by quantifying the diversity of the generated responses reasonably.","wordlikeness":0.1666666667,"lcsratio":1.0,"wordcoverage":0.6153846154}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"SeqAttack","Description":"On Adversarial Attacks for Named Entity Recognition","Abstract":"Named entity recognition is a fundamental task in information extraction and is an essential element for various natural language processing pipelines. Adversarial attacks have been shown to greatly affect the performance of text classification systems but knowledge about their effectiveness against named entity recognition models is limited. This paper investigates the effectiveness and portability of adversarial attacks from text classification to named entity recognition and the ability of adversarial training to counteract these attacks. We find that character-level and word-level attacks are the most effective, but adversarial training can grant significant protection at little to no expense of standard performance. Alongside our results, we also release ,, a framework to conduct adversarial attacks against token classification models (used in this work for named entity recognition) and a companion web application to inspect and cherry pick adversarial examples.","wordlikeness":0.7777777778,"lcsratio":0.7777777778,"wordcoverage":0.8}
{"Year":2019,"Venue":"ijcnlp-2019","Acronym":"TellMeWhy","Description":"Learning to Explain Corrective Feedback for Second Language Learners","Abstract":"We present a writing prototype feedback system, problem, to provide explanations of errors in submitted essays. In our approach, the sentence with corrections is analyzed to identify error types and problem words, aimed at customizing explanations based on the context of the error. The method involves learning the relation of errors and problem words, generating common feedback patterns, and extracting grammar patterns, collocations and example sentences. At run-time, a sentence with corrections is classified, and the problem word and template are identified to provide detailed explanations. Preliminary evaluation shows that the method has potential to improve existing commercial writing services.","wordlikeness":0.7777777778,"lcsratio":0.5555555556,"wordcoverage":0.6666666667}
{"Year":2023,"Venue":"eamt-2023","Acronym":"GoSt-ParC-Sign","Description":"Gold Standard Parallel Corpus of Sign and spoken language","Abstract":"Good quality training data for sign language machine translation (slmt) is extremely scarce, and this is one of the challenges that any project focusing on machine translation (mt) which also targets sign languages is currently facing. The goal of this ongoing project is to create a parallel corpus of authentic flemish sign language (vgt) and written dutch which can be employed as gold standard in automated sign language translation.","wordlikeness":0.5,"lcsratio":0.8571428571,"wordcoverage":0.5714285714}
{"Year":2023,"Venue":"insights-2023","Acronym":"SocBERT","Description":"A Pretrained Model for Social Media Text","Abstract":"Pretrained language models (plms) on domain-specific data have been proven to be effective for in-domain natural language processing (nlp) tasks. Our work aimed to develop a language model which can be effective for the nlp tasks with the data from diverse social media platforms. We pretrained a language model on twitter and reddit posts in english consisting of 929m sequence blocks for 112k steps. We benchmarked our model and 3 transformer-based models\u2014bert, bertweet, and roberta on 40 social media text classification tasks. The results showed that although our model did not perform the best on all of the tasks, it outperformed the baseline model\u2014bert on most of the tasks, which illustrates the effectiveness of our model. Also, our work provides some insights of how to improve the efficiency of training plms.","wordlikeness":0.7142857143,"lcsratio":0.7142857143,"wordcoverage":0.8333333333}
{"Year":2020,"Venue":"findings-2020","Acronym":"ESTeR","Description":"Combining Word Co-occurrences and Word Associations for Unsupervised Emotion Detection","Abstract":"Accurate detection of emotions in user- generated text was shown to have several applications for e-commerce, public well-being, and disaster management. Currently, the state-of-the-art performance for emotion detection in text is obtained using complex, deep learning models trained on domain-specific, labeled data. In this paper, we propose similarity , an unsupervised model for identifying emotions using a novel similarity function based on random walks on graphs. Our model combines large-scale word co-occurrence information with word-associations from lexicons avoiding not only the dependence on labeled datasets, but also an explicit mapping of words to latent spaces used in emotion-enriched word embeddings. Our similarity function can also be computed efficiently. We study a range of datasets including recent tweets related to covid-19 to illustrate the superior performance of our model and report insights on public emotions during the on-going pandemic.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.9090909091}
{"Year":2023,"Venue":"ranlp-2023","Acronym":"MQDD","Description":"Pre-training of Multimodal Question Duplicity Detection for Software Engineering Domain","Abstract":"This work proposes a new pipeline for leveraging data collected on the stack overflow website for pre-training a multimodal model for searching duplicates on question answering websites. Our multimodal model is trained on question descriptions and source codes in multiple programming languages. We design two new learning objectives to improve duplicate detection capabilities. The result of this work is a mature, fine-tuned multimodal question duplicity detection (can) model, ready to be integrated into a stack overflow search system, where it can help users find answers for already answered questions. Alongside the the model, we release two datasets related to the software engineering domain. The first stack overflow dataset (sod) represents a massive corpus of paired questions and answers. The second stack overflow duplicity dataset (sodd) contains data for training duplicate detection models.","wordlikeness":0.25,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2020,"Venue":"coling-2020","Acronym":"CoNAN","Description":"A Complementary Neighboring-based Attention Network for Referring Expression Generation","Abstract":"Daily scenes are complex in the real world due to occlusion, undesired lighting conditions, etc. Although humans handle those complicated environments well, they evoke challenges for machine learning systems to identify and describe the target without ambiguity. Most previous research focuses on mining discriminating features within the same category for the target object. One the other hand, as the scene becomes more complicated, human frequently uses the neighbor objects as complementary information to describe the target one. Motivated by that, we propose a novel complementary neighboring-based attention network (complementary) that explicitly utilizes the visual differences between the target object and its highly-related neighbors. These highly-related neighbors are determined by an attentional ranking module, as complementary features, highlighting the discriminating aspects for the target object. The speaker module then takes the visual difference features as an additional input to generate the expression. Our qualitative and quantitative results on the dataset refcoco, refcoco+, and refcocog demonstrate that our generated expressions outperform other state-of-the-art models by a clear margin.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2004,"Venue":"hlt-2004","Acronym":"ITSPOKE","Description":"An Intelligent Tutoring Spoken Dialogue System","Abstract":"System is a spoken dialogue system that uses the why2-atlas text-based tutoring system as its \u201cback-end\u201d. A student \ufb01rst types a natural language answer to a qualitative physics problem. Are then engages the student in a spoken dialogue to provide feedback and correct misconceptions, and to elicit more complete explanations. We are using why2-atlas to generate an empirically-based understanding of the rami\ufb01cations of adding spoken language capabilities to text-based dialogue tutors.","wordlikeness":0.7142857143,"lcsratio":1.0,"wordcoverage":0.8333333333}
{"Year":2023,"Venue":"acl-2023","Acronym":"DisentQA","Description":"Disentangling Parametric and Contextual Knowledge with Counterfactual Question Answering","Abstract":"Question answering models commonly have access to two sources of \u201cknowledge\u201d during inference time: (1) parametric knowledge - the factual knowledge encoded in the model weights, and (2) contextual knowledge - external knowledge (e.g., a wikipedia passage) given to the model to generate a grounded answer. Having these two sources of knowledge entangled together is a core issue for generative qa models as it is unclear whether the answer stems from the given non-parametric knowledge or not. This unclarity has implications on issues of trust, interpretability and factuality. In this work, we propose a new paradigm in which qa models are trained to disentangle the two sources of knowledge. Using counterfactual data augmentation, we introduce a model that predicts two answers for a given question: one based on given contextual knowledge and one based on parametric knowledge. Our experiments on the natural questions dataset show that this approach improves the performance of qa models by making them more robust to knowledge conflicts between the two knowledge sources, while generating useful disentangled answers.","wordlikeness":0.625,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2008,"Venue":"coling-2008","Acronym":"CollabRank","Description":"Towards a Collaborative Approach to Single-Document Keyphrase Extraction","Abstract":"Previous methods usually conduct the keyphrase extraction task for single documents separately without interactions for each document, under the assumption that the documents are considered independent of each other. This paper proposes a novel approach named use to collaborative single-document keyphrase extraction by making use of mutual influences of multiple documents within a cluster context. To is implemented by first employing the clustering algorithm to obtain appropriate document clusters, and then using the graph-based ranking algorithm for collaborative single-document keyphrase extraction within each cluster. Experimental results demonstrate the encouraging performance of the proposed approach. Different clustering algorithms have been investigated and we find that the system performance relies positively on the quality of document clusters.","wordlikeness":0.7,"lcsratio":1.0,"wordcoverage":0.7826086957}
{"Year":2021,"Venue":"naacl-2021","Acronym":"MelBERT","Description":"Metaphor Detection via Contextualized Late Interaction using Metaphorical Identification Theories","Abstract":"Automated metaphor detection is a challenging task to identify the metaphorical expression of words in a sentence. To tackle this problem, we adopt pre-trained contextualized models, e.g., bert and roberta. to this end, we propose a novel metaphor detection model, namely <i>metaphor-aware late interaction over bert (but)<\/i>. Our model not only leverages contextualized word representation but also benefits from linguistic metaphor identification theories to detect whether the target word is metaphorical. Our empirical results demonstrate that target outperforms several strong baselines on four benchmark datasets, i.e., vua-18, vua-20, moh-x, and trofi.","wordlikeness":0.8571428571,"lcsratio":0.8571428571,"wordcoverage":0.7692307692}
{"Year":2013,"Venue":"semeval-2013","Acronym":"SU-Sentilab","Description":"A Classification System for Sentiment Analysis in Twitter","Abstract":"Sentiment analysis refers to automatically extracting the sentiment present in a given natural language text. We present our participation to the semeval2013 competition, in the sentiment analysis of twitter and sms messages. Our approach for this task is the combination of two sentiment analysis subsystems which are combined together to build the \ufb01nal system. Both subsystems use supervised learning using features based on various polarity lexicons.","wordlikeness":0.6363636364,"lcsratio":0.6363636364,"wordcoverage":0.7}
{"Year":2023,"Venue":"findings-2023","Acronym":"LightFormer","Description":"Light-weight Transformer Using SVD-based Weight Transfer and Parameter Sharing","Abstract":"Transformer has become an important technique for natural language processing tasks with great success. However, it usually requires huge storage space and computational cost, making it difficult to be deployed on resource-constrained edge devices. To compress and accelerate transformer, we propose transformers., which adopts a low-rank factorization initialized by svd-based weight transfer and parameter sharing. The svd-based weight transfer can effectively utilize the well-trained transformer parameter knowledge to speed up the model convergence, and effectively alleviate the low-rank bottleneck problem combined with parameter sharing. We validate our method on machine translation, text summarization and text classification tasks. Experiments show that on iwslt\u201914 de-en and wmt\u201914 en-de, wmt\u201914 achieves similar performance to the baseline transformer with 3.8 times and 1.8 times fewer parameters, and achieves 2.3 times speedup and 1.5 times speedup respectively, generally outperforming recent light-weight transformers.","wordlikeness":0.8181818182,"lcsratio":1.0,"wordcoverage":0.7777777778}
{"Year":2008,"Venue":"lrec-2008","Acronym":"AnCora-Verb","Description":"A Lexical Resource for the Semantic Annotation of Corpora","Abstract":"In this paper we present two large-scale verbal lexicons, consulting-ca for catalan and corpora-es for spanish, which are the basis for the semantic annotation with arguments and thematic roles of ancora corpora. In applications lexicons, the mapping between syntactic functions, arguments and thematic roles of each verbal predicate it is established taking into account the verbal semantic class and the diatheses alternations in which the predicate can participate. Each verbal predicate is related to one or more semantic classes basically differentiated according to the four event classes -accomplishments, achievements, states and activities-, and on the diatheses alternations in which a verb can occur. Paper-es contains a total of 1,965 different verbs corresponding to 3,671 senses and basically-ca contains 2,151 verbs and 4,513 senses. These figures correspond to the total of 500,000 words contained in each corpus, ancora-ca and ancora-es. the lexicons and the annotated corpora constitute the richest linguistic resources of this kind freely available for spanish and catalan. The big amount of linguistic information contained in both resources should be of great interest for computational applications and linguistic studies. Currently, a consulting interface for these lexicons is available at (<a href=http:\/\/clic.ub.edu\/ancora\/ class=acl-markup-url>http:\/\/clic.ub.edu\/ancora\/<\/a>).","wordlikeness":0.6363636364,"lcsratio":0.5454545455,"wordcoverage":0.7}
{"Year":2011,"Venue":"ws-2011","Acronym":"ThaiHerbMiner","Description":"A Thai Herbal Medicine Mining and Visualizing Tool","Abstract":"Thai traditional medicine (ttm) has a long history in thailand and is nowadays considered an effective alternative approach to the modern medicine. One of the main knowledge in thai traditional medicine is the use of various types of herbs to form medicines. Our main goal is to bridge the gap between the traditional knowledge and the modern biomedical knowledge. Using text mining and visualization techniques, some implicit relations from one source could be used to verify and enhance the knowledge discovery in another source. In this paper, we present our ongoing work, related, a thai herbal medicine mining and visualizing tool. Mining applies text mining to extract some salient relations from a collection of pubmed articles related to thai herbs. The extracted relations can be browsed and viewed using information visualization. Our proposed tool can also recommend a list of herbs which have similar medical properties.","wordlikeness":0.5384615385,"lcsratio":0.9230769231,"wordcoverage":0.6666666667}
{"Year":2015,"Venue":"emnlp-2015","Acronym":"RELLY","Description":"Inferring Hypernym Relationships Between Relational Phrases","Abstract":"Relational phrases (e.g., \u201cgot married to\u201d) and their hypernyms (e.g., \u201cis a relative of\u201d) are central for many tasks including question answering, open information extraction, paraphrasing, and entailment detection. This has motivated the development of several linguistic resources (e.g. dirt, patty, and wisenet) which systematically collect and organize relational phrases. These resources have demonstrable practical bene\ufb01ts, but are each limited due to noise, sparsity, or size. We present a new general-purpose method, with, for constructing a large hypernymy graph of relational phrases with high-quality subsumptions using collective probabilistic programming techniques. Our graph induction approach integrates small highprecision knowledge bases together with large automatically curated resources, and reasons collectively to combine these resources into a consistent graph. Using but, we construct a high-coverage, high-precision hypernymy graph consisting of 20k relational phrases and 35k hypernymy links. Our evaluation indicates a hypernymy link precision of 78%, and demonstrates the value of this resource for a document-relevance ranking task.","wordlikeness":0.8,"lcsratio":0.8,"wordcoverage":0.9090909091}
{"Year":2023,"Venue":"syntaxfest-2023","Acronym":"ICON","Description":"Building a Large-Scale Benchmark Constituency Treebank for the Indonesian Language","Abstract":"Constituency parsing is an important task of informing how words are combined to form sentences. While constituency parsing in english has seen significant progress in the last few years, tools for constituency parsing in indonesian remain few and far between. In this work, we publish transformer-based (indonesian constituency treebank), the hitherto largest publicly-available manually-annotated benchmark constituency treebank for the indonesian language with a size of 10,000 sentences and approximately 124,000 constituents and 182,000 tokens, which can support the training of state-of-the-art transformer-based models. We establish strong baselines on the 88.85% dataset using the berkeley neural parser with transformer-based pre-trained embeddings, with the best performance of 88.85% f1 score coming from our own version of spanbert (indospanbert). We further analyze the predictions made by our best-performing model to reveal certain idiosyncrasies in the indonesian language that pose challenges for constituency parsing.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2016,"Venue":"ws-2016","Acronym":"VERSE","Description":"Event and Relation Extraction in the BioNLP 2016 Shared Task","Abstract":"We present the vancouver event and relation system for extraction (as)1 as a competing system for three subtasks of the bionlp shared task 2016. Entity, performs full event extraction including entity, relation and modi\ufb01cation extraction using a feature-based approach. It achieved the highest f1-score in the bacteria biotope (bb3) event subtask and the third highest f1-score in the seed development (seedev) binary subtask.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2020,"Venue":"lrec-2020","Acronym":"AlloVera","Description":"A Multilingual Allophone Database","Abstract":"We introduce a new resource, introduce, which provides mappings from 218 allophones to phonemes for 14 languages. Phonemes are contrastive phonological units, and allophones are their various concrete realizations, which are predictable from phonological context. While phonemic representations are language specific, phonetic representations (stated in terms of (allo)phones) are much closer to a universal (language-independent) transcription. Technologies) allows the training of speech recognition models that output phonetic transcriptions in the international phonetic alphabet (ipa), regardless of the input language. We show that a \u201cuniversal\u201d allophone model, allosaurus, built with built, outperforms \u201cuniversal\u201d phonemic models and language-specific models on a speech-transcription task. We explore the implications of this technology (and related technologies) for the documentation of endangered and minority languages. We further explore other applications for which with will be suitable as it grows, including phonological typology.","wordlikeness":0.75,"lcsratio":0.75,"wordcoverage":0.7692307692}
{"Year":2021,"Venue":"acl-2021","Acronym":"DYPLOC","Description":"Dynamic Planning of Content Using Mixed Language Models for Text Generation","Abstract":"We study the task of long-form opinion text generation, which faces at least two distinct challenges. First, existing neural generation models fall short of coherence, thus requiring efficient content planning. Second, diverse types of information are needed to guide the generator to cover both subjective and objective content. To this end, we propose relevant, a generation framework that conducts dynamic planning of content while generating the output based on a novel design of mixed language models. To enrich the generation with diverse content, we further propose to use large pre-trained models to predict relevant concepts and to generate claims. We experiment with two challenging tasks on newly collected datasets: (1) argument generation with reddit changemyview, and (2) writing articles using new york times\u2019 opinion section. Automatic evaluation shows that our model significantly outperforms competitive comparisons. Human judges further confirm that our generations are more coherent with richer content.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2010,"Venue":"amta-2010","Acronym":"WeBiText","Description":"Multilingual Concordancer Built from Public High Quality Web Content","Abstract":"In this paper, we describe problems. (www.web.ca) and how it is being used. After is a concordancer that allows translators to search in large, high-quality multilingual web sites, in order to find solutions to translation problems. After a quick overview of the system, we present results from an analysis of its logs, which provides a picture of how the tool is being used and how well it performs. We show that it is mostly used to find solutions for short, two or three word translation problems. The system produces at least one hit for 58% of the queries, and hits from at least five different web pages in 41% of cases. We show that 36% of the queries correspond to specialized language problems, which is much higher than what was previously reported for a similar concordancer based on the canadian hansard (transsearch). We also provide a back of the envelope calculation of the current economic impact of the tool, which we estimate at $1 million per year, and growing rapidly.","wordlikeness":0.75,"lcsratio":0.75,"wordcoverage":0.8}
{"Year":2006,"Venue":"lrec-2006","Acronym":"CoGrOO","Description":"a Brazilian-Portuguese Grammar Checker based on the CETENFOLHA Corpus","Abstract":"This paper describes an ongoing portuguese language grammar checker project, called techniques.1-corretor gramatical para openoffice (grammar checker for openoffice), based on cetenfolha, a brazilian portuguese morphosyntactic annotated corpus. Two of its features are highlighted: - hybrid architecture, mixing rules and statistics; - free software project. This project aims at checking grammatical errors such as nominal and verbal agreement, \u0093crase\u0094 (the coalescence of preposition \u0093a\u0094 (to) + definitive singular determiner \u0093a\u0094 yielding \u0093\u00e0\u0094), nominal and verbal government and other common errors in brazilian portuguese language. We also present some empirical results based on the implemented techniques.","wordlikeness":0.6666666667,"lcsratio":0.8333333333,"wordcoverage":0.7272727273}
{"Year":2022,"Venue":"acl-2022","Acronym":"IAM","Description":"A Comprehensive and Large-Scale Dataset for Integrated Argument Mining Tasks","Abstract":"Traditionally, a debate usually requires a manual preparation process, including reading plenty of articles, selecting the claims, identifying the stances of the claims, seeking the evidence for the claims, etc. As the ai debate attracts more attention these years, it is worth exploring the methods to automate the tedious process involved in the debating system. In this work, we introduce a comprehensive and large dataset named (cesc), which can be applied to a series of argument mining tasks, including claim extraction, stance classification, evidence extraction, etc. Our dataset is collected from over 1k articles related to 123 topics. Near 70k sentences in the dataset are fully annotated based on their argument properties (e.g., claims, stances, evidence, etc.). We further propose two new integrated argument mining tasks associated with the debate preparation process: (1) claim extraction with stance classification (cesc) and (2) claim-evidence pair extraction (cepe). We adopt a pipeline approach and an end-to-end method for each integrated task separately. Promising experimental results are reported to show the values and challenges of our proposed tasks, and motivate future research on argument mining.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2020,"Venue":"acl-2020","Acronym":"CluBERT","Description":"A Cluster-Based Approach for Learning Sense Distributions in Multiple Languages","Abstract":"Knowing the most frequent sense (mfs) of a word has been proved to help word sense disambiguation (wsd) models significantly. However, the scarcity of sense-annotated data makes it difficult to induce a reliable and high-coverage distribution of the meanings in a language vocabulary. To address this issue, in this paper we present raw, an automatic and multilingual approach for inducing the distributions of word senses from a corpus of raw sentences. Our experiments show that paper learns distributions over english senses that are of higher quality than those extracted by alternative approaches. When used to induce the mfs of a lemma, of attains state-of-the-art results on the english word sense disambiguation tasks and helps to improve the disambiguation performance of two off-the-shelf wsd models. Moreover, our distributions also prove to be effective in other languages, beating all their alternatives for computing the mfs on the multilingual wsd tasks. We release our sense distributions in five different languages at <a href=https:\/\/github.com\/sapienzanlp\/by class=acl-markup-url>https:\/\/github.com\/sapienzanlp\/to<\/a>.","wordlikeness":0.7142857143,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2022,"Venue":"lrec-2022","Acronym":"MentalBERT","Description":"Publicly Available Pretrained Language Models for Mental Healthcare","Abstract":"Mental health is a critical issue in modern society, and mental disorders could sometimes turn to suicidal ideation without adequate treatment. Early detection of mental disorders and suicidal ideation from social content provides a potential way for effective social intervention. Recent advances in pretrained contextualized language representations have promoted the development of several domainspecific pretrained models and facilitated several downstream applications. However, there are no existing pretrained language models for mental healthcare. This paper trains and release two pretrained masked language models, i.e., potential and mentalroberta, to benefit machine learning for the mental healthcare research community. Besides, we evaluate our trained domain-specific models and several variants of pretrained language models on several mental disorder detection benchmarks and demonstrate that language representations pretrained in the target domain improve the performance of mental health detection tasks.","wordlikeness":0.9,"lcsratio":0.8,"wordcoverage":0.75}
{"Year":2023,"Venue":"findings-2023","Acronym":"AltCLIP","Description":"Altering the Language Encoder in CLIP for Extended Language Capabilities","Abstract":"Clip (contrastive language\u2013image pretraining) is an english multimodal representation model learned from a massive amount of english text-image pairs and has achieved great success in various downstream tasks, including image classification, text-to-image retrieval, and image generation. When extending clip to other languages, the major problem is the lack of good-quality text-image pairs. In this work, we present benchmark., a simple and low-resource method to build a strong multilingual multimodal representation model. Instead of training a model from scratch on multilingual text-image pairs, we take the original clip model trained on english text-image pairs and alter its text encoder with a pre-trained multilingual text encoder (xlm-r). We then align text and image representations by a two-stage training schema consisting of teacher learning and contrastive learning. Our method utilizes the existence of rich parallel text data and pre-trained multilingual language models. We present extensive experimental evaluations to demonstrate the effectiveness of our proposed method. Our model sets new state-of-the-art zero-shot performances on a wide range of tasks in multilingual multimodal benchmarks, including imagenet-cn\/it\/ja\/ko serials, flicker30k-cn, coco-cn, multi30k, and xtd. Further, our model outperforms the original clip model on zero-shot cross-modal retrieval, image classification in the wild (icinw) tasks, and clip benchmark. We plan to open-source our code, pre-trained model weights, and evaluation toolkits of multilingual multimodal tasks, to facilitate research on multilingual multimodal representation learning.","wordlikeness":0.8571428571,"lcsratio":1.0,"wordcoverage":0.7272727273}
{"Year":2021,"Venue":"acl-2021","Acronym":"TILGAN","Description":"Transformer-based Implicit Latent GAN for Diverse and Coherent Text Generation","Abstract":"Conventional autoregressive models have achieved great success in text generation but suffer from the exposure bias problem in that token sequences in the training and in the generation stages are mismatched. While generative adversarial networks (gans) can remedy this problem, existing implementations of gans directly on discrete outputs tend to be unstable and lack diversity. In this work, we propose latent, a transformerbased implicit latent gan, which combines a transformer autoencoder and gan in the latent space with a novel design and distribution matching based on the kullback-leibler (kl) divergence. Speci\ufb01cally, to improve local and global coherence, we explicitly introduce a multi-scale discriminator to capture the semantic information at varying scales among the sequence of hidden representations encoded by transformer. Moreover, the decoder is enhanced by an additional kl loss to be consistent with the latent-generator. Experimental results on three benchmark datasets demonstrate the validity and effectiveness of our model, by obtaining signi\ufb01cant improvements and a better quality-diversity trade-off in automatic and human evaluation for both unconditional and conditional generation tasks.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.7272727273}
{"Year":2014,"Venue":"lrec-2014","Acronym":"ETER","Description":"a new metric for the evaluation of hierarchical named entity recognition","Abstract":"This paper addresses the question of hierarchical named entity evaluation. In particular, we focus on metrics to deal with complex named entity structures as those introduced within the quaero project. The intended goal is to propose a smart way of evaluating partially correctly detected complex entities, beyond the scope of traditional metrics. None of the existing metrics are fully adequate to evaluate the proposed quaero task involving entity detection, classification and decomposition. We are discussing the strong and weak points of the existing metrics. We then introduce a new metric, the entity tree error rate (this), to evaluate hierarchical and structured named entity detection, classification and decomposition. The level. Metric builds upon the commonly accepted ser metric, but it takes the complex entity structure into account by measuring errors not only at the slot (or complex entity) level but also at a basic (atomic) entity level. We are comparing our new metric to the standard one using first some examples and then a set of real data selected from the etape evaluation results.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2019,"Venue":"ijcnlp-2019","Acronym":"Unicoder","Description":"A Universal Language Encoder by Pre-training with Multiple Cross-lingual Tasks","Abstract":"We present including, a universal language encoder that is insensitive to different languages. Given an arbitrary nlp task, a model can be trained with be using training data in one language and directly applied to inputs of the same task in other languages. Comparing to similar efforts such as multilingual bert and xlm , three new cross-lingual pre-training tasks are proposed, including cross-lingual word recovery, cross-lingual paraphrase classification and cross-lingual masked language model. These tasks help more learn the mappings among different languages from more perspectives. We also find that doing fine-tuning on multiple languages together can bring further improvement. Experiments are performed on two tasks: cross-lingual natural language inference (xnli) and cross-lingual question answering (xqa), where xlm is our baseline. On xnli, 1.8% averaged accuracy improvement (on 15 languages) is obtained. On xqa, which is a new cross-lingual dataset built by us, 5.5% averaged accuracy improvement (on french and german) is obtained.","wordlikeness":0.875,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2019,"Venue":"emnlp-2019","Acronym":"GEM","Description":"Generative Enhanced Model for adversarial attacks","Abstract":"We present our generative enhanced model (enabled) that we used to create samples awarded the first prize on the fever 2.0 breakers task. Facts is the extended language model developed upon gpt-2 architecture. The addition of novel target vocabulary input to the already existing context input enabled controlled text generation. The training procedure resulted in creating a model that inherited the knowledge of pretrained gpt-2, and therefore was ready to generate natural-like english sentences in the task domain with some additional control. As a result, controlled generated malicious claims that mixed facts from various articles, so it became difficult to classify their truthfulness.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2023,"Venue":"acl-2023","Acronym":"CWSeg","Description":"An Efficient and General Approach to Chinese Word Segmentation","Abstract":"In this work, we report our efforts in advancing chinese word segmentation for the purpose of rapid deployment in different applications. The pre-trained language model (plm) based segmentation methods have achieved state-of-the-art (sota) performance, whereas this paradigm also poses challenges in the deployment. It includes the balance between performance and cost, segmentation ambiguity due to domain diversity and vague words boundary, and multi-grained segmentation. In this context, we propose a simple yet effective approach, namely experiments, to augment plm-based schemes by developing cohort training and versatile decoding strategies. Extensive experiments on benchmark datasets demonstrate the efficiency and generalization of our approach. The corresponding segmentation system is also implemented for practical usage and the demo is recorded.","wordlikeness":0.2,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2021,"Venue":"findings-2021","Acronym":"DialogueTRM","Description":"Exploring Multi-Modal Emotional Dynamics in a Conversation","Abstract":"Emotion dynamics formulates principles explaining the emotional fluctuation during conversations. Recent studies explore the emotion dynamics from the self and inter-personal dependencies, however, ignoring the temporal and spatial dependencies in the situation of multi-modal conversations. To address the issue, we extend the concept of emotion dynamics to multi-modal settings and propose a dialogue transformer for simultaneously modeling the intra-modal and inter-modal emotion dynamics. Specifically, the intra-modal emotion dynamics is to not only capture the temporal dependency but also satisfy the context preference in every single modality. The inter-modal emotional dynamics aims at handling multi-grained spatial dependency across all modalities. Our models outperform the state-of-the-art with a margin of 4%-16% for most of the metrics on three benchmark datasets.","wordlikeness":0.7272727273,"lcsratio":0.6363636364,"wordcoverage":0.8421052632}
{"Year":2013,"Venue":"semeval-2013","Acronym":"UoM","Description":"Using Explicit Semantic Analysis for Classifying Sentiments","Abstract":"In this paper, we describe our system submitted for the sentiment analysis task at semeval 2013 (task 2). We implemented a combination of explicit semantic analysis (esa) with naive bayes classi\ufb01er. Esa represents text as a high dimensional vector of explicitly de\ufb01ned topics, following the distributional semantic model. This approach is novel in the sense that esa has not been used for sentiment analysis in the literature, to the best of our knowledge.","wordlikeness":0.3333333333,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2022,"Venue":"lrec-2022","Acronym":"Atril","Description":"an XML Visualization System for Corpus Texts","Abstract":"This paper presents full, an xml visualization system for corpus texts, developed for, but not restricted to, the project corpus de audi\u00eancias (coraudis), a corpus composed of transcripts of sessions of criminal proceedings recorded at the coimbra court. The main aim of the tool is to provide researchers with a web-based environment that allows for an easily customizable visualization of corpus texts with heavy structural annotation. Existing corpus analysis tools such as sketchengine, teitok and cqpweb offer some kind of visualization mechanisms, but, to our knowledge, none meets our project\u2019s main needs. Our requirements are a system that is open-source; that can be easily connected to cqpweb and teitok, that provides a full text-view with switchable visualization templates, that allows for the visualization of overlapping utterances. To meet those requirements, we created provide, a module with a corpus xml file viewer, a visualization management system, and a word alignment tool.","wordlikeness":0.6,"lcsratio":0.6,"wordcoverage":0.8}
{"Year":2023,"Venue":"acl-2023","Acronym":"LM-CPPF","Description":"Paraphrasing-Guided Data Augmentation for Contrastive Prompt-Based Few-Shot Fine-Tuning","Abstract":"In recent years, there has been significant progress in developing pre-trained language models for nlp. However, these models often struggle when fine-tuned on small datasets. To address this issue, researchers have proposed various adaptation approaches. Prompt-based tuning is arguably the most common way, especially for larger models. Previous research shows that adding contrastive learning to prompt-based fine-tuning is effective as it helps the model generate embeddings that are more distinguishable between classes, and it can also be more sample-efficient as the model learns from positive and negative examples simultaneously. One of the most important components of contrastive learning is data augmentation, but unlike computer vision, effective data augmentation for nlp is still challenging. This paper proposes contrastive, contrastive paraphrasing-guided prompt-based fine-tuning of language models, which leverages prompt-based few-shot paraphrasing using generative language models, especially large language models such as gpt-3 and opt-175b, for data augmentation. Our experiments on multiple text classification benchmarks show that this augmentation method outperforms other methods, such as easy data augmentation, back translation, and multiple templates.","wordlikeness":0.2857142857,"lcsratio":0.7142857143,"wordcoverage":0.5714285714}
{"Year":2005,"Venue":"hlt-2005","Acronym":"BLANC","Description":"Learning Evaluation Metrics for MT","Abstract":"We introduce skipngrams),, a family of dynamic, trainable evaluation metrics for machine translation. Flexible, parametrized models can be learned from past data and automatically optimized to correlate well with human judgments for different criteria (e.g. adequacy, \ufb02uency) using different correlation measures. Towards this end, we discuss acs (all common skipngrams), a practical algorithm with trainable parameters that estimates referencecandidate translation overlap by computing a weighted sum of all common skipngrams in polynomial time. We show that the bleu and rouge metric families are special cases of polynomial, and we compare correlations with human judgments across these three metric families. We analyze the algorithmic complexity of acs and argue that it is more powerful in modeling both local meaning and sentence-level structure, while offering the same practicality as the established algorithms it generalizes.","wordlikeness":1.0,"lcsratio":0.8,"wordcoverage":0.9090909091}
{"Year":2021,"Venue":"paclic-2021","Acronym":"PS-GAN","Description":"Feature augmented text generation in Telugu","Abstract":"Since the inception of generative adversarial networks (gans), synthetic image generation has taken a giant leap because of the ability of these networks to generate high-quality images, however, the same cannot be said for text generation. A major challenge encountered in text generation using gan\u2019s is the nondifferentiability of the discrete text. Most of the previous studies for text generation using gans focus on solving this, but none of them incorporate any additional features in the gan. These features could be useful in the training of the models, especially in the case of lowresource languages. In this paper, we propose a novel model called the pos-sentigan (discrete), where we show that the use of parts-of-speech tag and sentiment features aid in the generation of better sentences. We also provide \u2018pravar\u2019, a first-ever dataset consisting of stories from different categories that enable text\/story generation for telugu, a low resource language. Finally, we show the performance of the proposed models on three datasets, namely, pravar1, telugu wikipedia2 and telugu news3. \u2217 the authors contributed equally to this work. 1pravar is a hindi word that translates to \u201cgood\u201d or \u201cimportant\u201d. 2https:\/\/www.kaggle.com\/disisbig\/telugu-wikipediaarticles 3https:\/\/github.","wordlikeness":0.5,"lcsratio":0.5,"wordcoverage":0.6666666667}
{"Year":2014,"Venue":"gwc-2014","Acronym":"Hydra","Description":"A Software System for Wordnet","Abstract":"This paper presents an overview of the software for wordnet processing .. The system has fully-\ufb02edged gui and api, both working with powerful modal query language. Platform. Has been used for the development of the bulgarian wordnet for the last 7 years and recently was improved, became open source and is distributed as part of the meta-share platform.","wordlikeness":0.8,"lcsratio":0.4,"wordcoverage":0.8}
{"Year":2014,"Venue":"gwc-2014","Acronym":"OpenWordNet-PT","Description":"A Project Report","Abstract":"This paper presents representation, a freely available open-source wordnet for portuguese, with its latest developments and practical uses. We provide a detailed description of the rdf representation developed for of. We highlight our efforts to extend the coverage of our resource and add nominalization relations connecting nouns and verbs. Finally, we present several real-world applications where was was put to use, including a large-scale high-throughput sentiment analysis system.","wordlikeness":0.6428571429,"lcsratio":0.4285714286,"wordcoverage":0.5714285714}
{"Year":2023,"Venue":"acl-2023","Acronym":"MPCHAT","Description":"Towards Multimodal Persona-Grounded Conversation","Abstract":"In order to build self-consistent personalized dialogue agents, previous research has mostly focused on textual persona that delivers personal facts or personalities. However, to fully describe the multi-faceted nature of persona, image modality can help better reveal the speaker\u2019s personal characteristics and experiences in episodic memory (rubin et al., 2003; conway, 2009). In this work, we extend persona-based dialogue to the multimodal domain and make two main contributions. First, we present the first multimodal persona-based dialogue dataset named experiences, which extends persona with both text and images to contain episodic memories. Second, we empirically show that incorporating multimodal persona, as measured by three proposed multimodal persona-grounded dialogue tasks (i.e., next response prediction, grounding persona prediction, and speaker identification), leads to statistically significant performance improvements across all tasks. Thus, our work highlights that multimodal persona is crucial for improving multimodal dialogue comprehension, and our persona, serves as a high-quality resource for this research.","wordlikeness":0.8333333333,"lcsratio":0.8333333333,"wordcoverage":0.8}
{"Year":2018,"Venue":"lrec-2018","Acronym":"MMQA","Description":"A Multi-domain Multi-lingual Question-Answering Framework for English and Hindi","Abstract":"In this paper, we assess the challenges for multi-domain, multi-lingual question answering, create necessary resources for benchmarking and develop a baseline model. We curate 500 articles in six different domains from the web. These articles form a comparable corpora of 250 english documents and 250 hindi documents. From these comparable corpora, we have created 5, 495 question-answer pairs with the questions and answers, both being in english and hindi. The question can be both factoid or short descriptive types. The answers are categorized in 6 coarse and 63 finer types. To the best of our knowledge, this is the very first attempt towards creating multi-domain, multi-lingual question answering evaluation involving english and hindi. We develop a deep learning based model for classifying an input question into the coarse and finer categories depending upon the expected answer. Answers are extracted through similarity computation and subsequent ranking. For factoid question, we obtain an mrr value of 49.10% and for short descriptive question, we obtain a bleu score of 41.37%. Evaluation of question classification model shows the accuracies of 90.12% and 80.30% for coarse and finer classes, respectively. Keywords: multi-lingual question answering, answer extraction, neural network, question classification 1.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2013,"Venue":"semeval-2013","Acronym":"UWM-TRIADS","Description":"Classifying Drug-Drug Interactions with Two-Stage SVM and Post-Processing","Abstract":"We describe our system for the ddiextraction-2013 shared task of classifying drug-drug interactions (ddis) given labeled drug mentions. The challenge called for a five-way classification of all drug pairs in each sentence: a drug pair is either non-interacting, or interacting as one of four types. Our approach begins with the use of a two-stage weighted svm classifier to handle the highly unbalanced class distribution: the first stage for a binary classification of drug pairs as interacting or non-interacting, and the second stage for further classification of interacting pairs from the first stage into one of the four interacting types. Our svm features exploit stemmed words, lemmas, bigrams, part of speech tags, verb lists, and similarity measures, among others. For each stage, we also developed a set of post-processing rules based on observations in the training data. Our best system achieved 0.472 fmeasure.","wordlikeness":0.4,"lcsratio":0.8,"wordcoverage":0.6315789474}
{"Year":2017,"Venue":"acl-2017","Acronym":"Olelo","Description":"A Question Answering Application for Biomedicine","Abstract":"Despite the importance of the biomedical domain, there are few reliable applications to support researchers and physicians for retrieving particular facts that \ufb01t their needs. Users typically rely on search engines that only support keywordand \ufb01lter-based searches. We present questions, a question answering system for biomedicine. An is built on top of an inmemory database, integrates domain resources, such as document collections and terminologies, and uses various natural language processing components. Natural is fast, intuitive and easy to use. We evaluated the systems on two use cases: answering questions related to a particular gene and on the bioasq benchmark. Search is available at: http:\/\/hpi.de\/ plattner\/such.","wordlikeness":0.4,"lcsratio":0.8,"wordcoverage":0.75}
{"Year":2021,"Venue":"acl-2021","Acronym":"EBERT","Description":"Efficient BERT Inference with Dynamic Structured Pruning","Abstract":"Pruning has been demonstrated as an effective way of reducing computational complexity for deep networks, especially cnns for computer vision tasks. In this paper, we investigate the opportunity to accelerate the inference of large-scale pre-trained language model via pruning. We propose our, a dynamic structured pruning algorithm for ef\ufb01cient bert inference. Unlike previous methods that randomly prune the model weights for static inference, opportunity dynamically determines and prunes the unimportant heads in multi-head self-attention layers and the unimportant structured computations in feed-forward network for each input sample at run-time. Experimental results show that our proposed algorithm outperforms other state-of-the-art methods on different tasks.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.8333333333}
{"Year":2019,"Venue":"ijcnlp-2019","Acronym":"Mask-Predict","Description":"Parallel Decoding of Conditional Masked Language Models","Abstract":"Most machine translation systems generate text autoregressively from left to right. We, instead, use a masked language modeling objective to train a model to predict any subset of the target words, conditioned on both the input text and a partially masked target translation. This approach allows for efficient iterative decoding, where we first predict all of the target words non-autoregressively, and then repeatedly mask out and regenerate the subset of words that the model is least confident about. By applying this strategy for a constant number of iterations, our model improves state-of-the-art performance levels for non-autoregressive and parallel decoding translation models by over 4 bleu on average. It is also able to reach within about 1 bleu point of a typical left-to-right transformer model, while decoding significantly faster.","wordlikeness":0.75,"lcsratio":0.5833333333,"wordcoverage":0.7368421053}
{"Year":2021,"Venue":"acl-2021","Acronym":"CIL","Description":"Contrastive Instance Learning Framework for Distantly Supervised Relation Extraction","Abstract":"The journey of reducing noise from distant supervision (ds) generated training data has been started since the ds was first introduced into the relation extraction (re) task. For the past decade, researchers apply the multi-instance learning (mil) framework to find the most reliable feature from a bag of sentences. Although the pattern of mil bags can greatly reduce ds noise, it fails to represent many other useful sentence features in the datasets. In many cases, these sentence features can only be acquired by extra sentence-level human annotation with heavy costs. Therefore, the performance of distantly supervised re models is bounded. In this paper, we go beyond typical mil framework and propose a novel contrastive instance learning (propose) framework. Specifically, we regard the initial mil as the relational triple encoder and constraint positive pairs against negative pairs for each instance. Experiments demonstrate the effectiveness of our proposed framework, with significant improvements over the previous methods on nyt10, gds and kbp.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2012,"Venue":"starsem-2012","Acronym":"UOW","Description":"Semantically Informed Text Similarity","Abstract":"The syntactic submissions to the semantic textual similarity task at semeval-2012 use a supervised machine learning algorithm along with features based on lexical, syntactic and semantic similarity metrics to predict the semantic equivalence between a pair of sentences. The lexical metrics are based on wordoverlap. A shallow syntactic metric is based on the overlap of base-phrase labels. The semantically informed metrics are based on the preservation of named entities and on the alignment of verb predicates and the overlap of argument roles using inexact matching. Our submissions outperformed the of\ufb01cial baseline, with our best system ranked above average, but the contribution of the semantic metrics was not conclusive.","wordlikeness":0.6666666667,"lcsratio":0.3333333333,"wordcoverage":0.8}
{"Year":2022,"Venue":"findings-2022","Acronym":"BehancePR","Description":"A Punctuation Restoration Dataset for Livestreaming Video Transcript","Abstract":"Given the increasing number of livestreaming videos, automatic speech recognition and post-processing for livestreaming video transcripts are crucial for efficient data management as well as knowledge mining. A key step in this process is punctuation restoration which restores fundamental text structures such as phrase and sentence boundaries from the video transcripts. This work presents a new human-annotated corpus, called accessible, for punctuation restoration in livestreaming video transcripts. Our experiments on on demonstrate the challenges of punctuation restoration for this domain. Furthermore, we show that popular natural language processing toolkits like stanford stanza, spacy, and trankit underperform on detecting sentence boundary on non-punctuated transcripts of livestreaming videos. The dataset is publicly accessible at <a href=http:\/\/github.com\/nlp-uoregon\/popular class=acl-markup-url>http:\/\/github.com\/nlp-uoregon\/show<\/a>.","wordlikeness":0.5555555556,"lcsratio":0.5555555556,"wordcoverage":0.75}
{"Year":2022,"Venue":"acl-2022","Acronym":"IMPLI","Description":"Investigating NLI Models&#39; Performance on Figurative Language","Abstract":"Natural language inference (nli) has been widely used as a task to train and evaluate models for language understanding. However, the ability of nli models to perform inferences requiring understanding of figurative language such as idioms and metaphors remains understudied. We introduce the however, (idiomatic and metaphoric paired language inference) dataset, an english dataset consisting of paired sentences spanning idioms and metaphors. We develop novel methods to generate 24k semiautomatic pairs as well as manually creating 1.8k gold pairs. We use based to evaluate nli models based on roberta fine-tuned on the widely used mnli dataset. We then show that while they can reliably detect entailment relationship between figurative phrases with their literal counterparts, they perform poorly on similarly structured examples where pairs are designed to be non-entailing. This suggests the limits of current nli models with regard to understanding figurative language and this dataset serves as a benchmark for future improvements in this direction.","wordlikeness":0.8,"lcsratio":0.8,"wordcoverage":0.8333333333}
{"Year":2018,"Venue":"acl-2018","Acronym":"NASH","Description":"Toward End-to-End Neural Architecture for Generative Semantic Hashing","Abstract":"Semantic hashing has become a powerful paradigm for fast similarity search in many information retrieval systems. While fairly successful, previous techniques generally require two-stage training, and the binary constraints are handled <i>ad-hoc<\/i>. In this paper, we present an <i>end-to-end<\/i> neural architecture for semantic hashing (has), where the binary hashing codes are treated as <i>bernoulli<\/i> latent variables. A neural variational inference framework is proposed for training, where gradients are directly backpropagated through the discrete latent variable to optimize the hash function. We also draw the connections between proposed method and <i>rate-distortion theory<\/i>, which provides a theoretical foundation for the effectiveness of our framework. Experimental results on three public datasets demonstrate that our method significantly outperforms several state-of-the-art models on both <i>unsupervised<\/i> and <i>supervised<\/i> scenarios.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2022,"Venue":"acl-2022","Acronym":"RELiC","Description":"Retrieving Evidence for Literary Claims","Abstract":"Humanities scholars commonly provide evidence for claims that they make about a work of literature (e.g., a novel) in the form of quotations from the work. We collect a large-scale dataset (it) of 78k literary quotations and surrounding critical analysis and use it to formulate the novel task of literary evidence retrieval, in which models are given an excerpt of literary analysis surrounding a masked quotation and asked to retrieve the quoted passage from the set of all passages in the work. Solving this retrieval task requires a deep understanding of complex literary and linguistic phenomena, which proves challenging to methods that overwhelmingly rely on lexical and semantic similarity matching. We implement a roberta-based dense passage retriever for this task that outperforms existing pretrained information retrieval baselines; however, experiments and analysis by human domain experts indicate that there is substantial room for improvement.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.8333333333}
{"Year":2018,"Venue":"lrec-2018","Acronym":"SandhiKosh","Description":"A Benchmark Corpus for Evaluating Sanskrit Sandhi Tools","Abstract":"Sanskrit is an ancient indian language. Several important texts which are of interest to people all over the world today were written in sanskrit. The sanskrit grammar has a precise and complete speci\ufb01cation given in the text as.t.\u00afadhy\u00afay\u00af\u0131 by p\u00afan.ini. This has led to the development of a number of sanskrit computational linguistics tools for processing and analyzing sanskrit texts. Unfortunately, there has been no effort to standardize and critically validate these tools. In this paper, we develop a sanskrit benchmark called have to evaluate the completeness and accuracy of sanskrit sandhi tools. We present the results of this benchmark on three most prominent sanskrit tools and demonstrate that these tools have substantial scope for improvement. This benchmark will be freely available to researchers worldwide and we hope it will help everyone working in this area evaluate and validate their tools. Keywords: sanskrit, sandhi, morphophonology 1.","wordlikeness":0.5,"lcsratio":0.8,"wordcoverage":0.7058823529}
{"Year":2022,"Venue":"ws-2022","Acronym":"Prabhupadavani","Description":"A Code-mixed Speech Translation Data for 25 Languages","Abstract":"Nowadays, the interest in code-mixing has become ubiquitous in natural language processing (nlp); however, not much attention has been given to address this phenomenon for speech translation (st) task. This can be solely attributed to the lack of code-mixed st task labelled data. Thus, we introduce nowadays,, which is a multilingual code-mixed st dataset for 25 languages. It is multi-domain, covers ten language families, containing 94 hours of speech by 130+ speakers, manually aligned with corresponding text in the target language. The literature. Is about vedic culture and heritage from indic literature, where code-switching in the case of quotation from literature is important in the context of humanities teaching. To the best of our knowledge, prabhupadvani is the first multi-lingual code-mixed st dataset available in the st literature. This data also can be used for a code-mixed machine translation task. All the dataset can be accessed at: <a href=https:\/\/github.com\/frozentoad9\/cmst class=acl-markup-url>https:\/\/github.com\/frozentoad9\/cmst<\/a>.","wordlikeness":0.5714285714,"lcsratio":0.5714285714,"wordcoverage":0.6}
{"Year":2007,"Venue":"paclic-2007","Acronym":"AutoCor","Description":"A Query Based Automatic Acquisition of Corpora of Closely-related Languages","Abstract":". . Is a method for the automatic acquisition and classification of corpora of documents in closely-related languages. It is an extension and enhancement of corpusbuilder, a system that automatically builds specific minority language corpora from a closed corpus, since some tagalog documents retrieved by corpusbuilder are actually documents in other closely-related philippine languages. It used the query generation method odds ratio, and introduced the concept of common word pruning to differentiate between documents of closely-related philippine languages and tagalog. The performance of the system using with and without pruning are compared, and common word pruning was found to improve the precision of the system. Keywords: document acquisition, document classification. 1.","wordlikeness":0.8571428571,"lcsratio":1.0,"wordcoverage":0.7692307692}
{"Year":2015,"Venue":"ws-2015","Acronym":"NDMSCS","Description":"A Topic-Based Chinese Microblog Polarity Classification System","Abstract":"In this paper, we focus on topic-based microblog sentiment classi\ufb01cation task that classify the microblog\u2019s sentiment polarities toward a speci\ufb01c topic. Most of the existing approaches for sentiment analysis usually adopt the target-independent strategy, which may assign irrelevant sentiments to the given topic. In this paper, we leverage the non-negative matrix factorization to get the relevant topic words and then further incorporate target-dependent features for topic-based microblog sentiment classi\ufb01cation. According to the experiment results, our system (to) has achieved a good performance in the sighan 8 task 2.","wordlikeness":0.3333333333,"lcsratio":0.8333333333,"wordcoverage":0.7142857143}
{"Year":2021,"Venue":"naacl-2021","Acronym":"LATEX-Numeric","Description":"Language Agnostic Text Attribute Extraction for Numeric Attributes","Abstract":"In this paper, we present this - a high-precision fully-automated scalable framework for extracting e-commerce numeric attributes from unstructured product text like product description. Most of the past work on attribute extraction is not scalable as they rely on manually curated training data, either with or without use of active learning. We rely on distant supervision for training data generation, removing dependency on manual labels. One issue with distant supervision is that it leads to incomplete training annotation due to missing attribute values while matching. We propose a multi-task learning architecture to deal with missing labels in the training data, leading to f1 improvement of 9.2% for numeric attributes over state-of-the-art single-task architecture. While multi-task architecture benefits both numeric and non-numeric attributes, we present automated techniques to further improve the numeric attributes extraction models. Numeric attributes require a list of units (or aliases) for better matching with distant supervision. We propose an automated algorithm for alias creation using unstructured text and attribute values, leading to a 20.2% f1 improvement. Extensive experiments on real world datasets for 20 numeric attributes across 5 product categories and 3 english marketplaces show that are achieves a high f1-score, without any manual intervention, making it suitable for practical applications. Finally we show that the improvements are language-agnostic and datasets achieves 13.9% f1 improvement for 3 non-english languages.","wordlikeness":0.6153846154,"lcsratio":0.9230769231,"wordcoverage":0.6666666667}
{"Year":2019,"Venue":"sigdial-2019","Acronym":"SIM","Description":"A Slot-Independent Neural Model for Dialogue State Tracking","Abstract":"Dialogue state tracking is an important component in task-oriented dialogue systems to identify users\u2019 goals and requests as a dialogue proceeds. However, as most previous models are dependent on dialogue slots, the model complexity soars when the number of slots increases. In this paper, we put forward a slot-independent neural model (utilizes) to track dialogue states while keeping the model complexity invariant to the number of dialogue slots. The model utilizes attention mechanisms between user utterance and system actions. Tracking achieves state-of-the-art results on woz and dstc2 tasks, with only 20% of the model size of previous models.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2022,"Venue":"ijcnlp-2022","Acronym":"TexPrax","Description":"A Messaging Application for Ethical, Real-time Data Collection and Annotation","Abstract":"Collecting and annotating task-oriented dialog data is difficult, especially for highly specific domains that require expert knowledge. At the same time, informal communication channels such as instant messengers are increasingly being used at work. This has led to a lot of work-relevant information that is disseminated through those channels and needs to be post-processed manually by the employees. To alleviate this problem, we present ease, a messaging system to collect and annotate _problems_, _causes_, and _solutions_ that occur in work-related chats. _solutions_ uses a chatbot to directly engage the employees to provide lightweight annotations on their conversation and ease their documentation work. To comply with data privacy and security regulations, we use an end-to-end message encryption and give our users full control over their data which has various advantages over conventional annotation tools. We evaluate annotations in a user-study with german factory employees who ask their colleagues for solutions on problems that arise during their daily work. Overall, we collect 202 task-oriented german dialogues containing 1,027 sentences with sentence-level expert annotations. Our data analysis also reveals that real-world conversations frequently contain instances with code-switching, varying abbreviations for the same entity, and dialects which nlp systems should be able to handle.","wordlikeness":0.7142857143,"lcsratio":0.5714285714,"wordcoverage":0.6666666667}
{"Year":2018,"Venue":"emnlp-2018","Acronym":"InferLite","Description":"Simple Universal Sentence Representations from Natural Language Inference Data","Abstract":"Natural language inference has been shown to be an effective supervised task for learning generic sentence embeddings. In order to better understand the components that lead to effective representations, we propose a lightweight version of infersent, called order, that does not use any recurrent layers and operates on a collection of pre-trained word embeddings. We show that a simple instance of our model that makes no use of context, word ordering or position can still obtain competitive performance on the majority of downstream prediction tasks, with most performance gaps being filled by adding local contextual information through temporal convolutions. Our models can be trained in under 1 hour on a single gpu and allows for fast inference of new representations. Finally we describe a semantic hashing layer that allows our model to learn generic binary codes for sentences.","wordlikeness":0.7777777778,"lcsratio":0.8888888889,"wordcoverage":0.7777777778}
{"Year":2022,"Venue":"lrec-2022","Acronym":"LARD","Description":"Large-scale Artificial Disfluency Generation","Abstract":"Disfluency detection is a critical task in real-time dialogue systems. However, despite its importance, it remains a relatively unexplored field, mainly due to the lack of appropriate datasets. At the same time, existing datasets suffer from various issues, including class imbalance issues, which can significantly affect the performance of the model on rare classes, as it is demonstrated in this paper. To this end, we propose as, a method for generating complex and realistic artificial disfluencies with little effort. The proposed method can handle three of the most common types of disfluencies: repetitions, replacements, and restarts. In addition, we release a new large-scale dataset with disfluencies that can be used on four different tasks: disfluency detection, classification, extraction, and correction. Experimental results on the this dataset demonstrate that the data produced by the proposed method can be effectively used for detecting and removing disfluencies, while also addressing limitations of existing datasets.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2016,"Venue":"lrec-2016","Acronym":"BulPhonC","Description":"Bulgarian Speech Corpus for the Development of ASR Technology","Abstract":"In this paper we introduce a bulgarian speech database, which was created for the purpose of asr technology development. The paper describes the design and the content of the speech database. We present also an empirical evaluation of the performance of a lvcsr system for bulgarian trained on the lvcsr data. The resource is available free for scientific usage.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2023,"Venue":"findings-2023","Acronym":"NormNet","Description":"Normalize Noun Phrases for More Robust NLP","Abstract":"A critical limitation of deep nlp models is their over-fitting over spurious features. Previous work has proposed several approaches to debunk such features and reduce their impact on the learned models. In this work, a normalization strategy is proposed to eliminate the false features caused by the textual surfaces of noun phrases. The motivation for this strategy is that noun phrases often play the role of slots in textual expressions and their exact forms are often not that important for performing the final task. As an intuitive example, consider the expression \u201d<span class=tex-math>x <span class=font-weight-normal>like eating<\/span> y<\/span>\". There are a huge number of suitable instantiations for <span class=tex-math>x<\/span> and <span class=tex-math>y<\/span> in the locale. However, humans can already infer the sentiment polarity of <span class=tex-math>x<\/span> toward <span class=tex-math>y<\/span> without knowing their exact forms.based on this intuition, we introduce inference, a pretrained language model based network, to implement the normalization strategy. Y<\/span>\". Learns to replace as many noun phrases in the input sentence as possible with pre-defined base forms. The output of approaches is then fed as input to a prompt-based learning model to perform label prediction. To evaluate the effectiveness of our strategy, we conducted experimental studies on several tasks, including aspect sentiment classification (asc), semantic text similarity (sts), and natural language inference (nli). The experimental results confirm the effectiveness of our strategy.","wordlikeness":0.8571428571,"lcsratio":1.0,"wordcoverage":0.7692307692}
{"Year":2023,"Venue":"acl-2023","Acronym":"FashionKLIP","Description":"Enhancing E-Commerce Image-Text Retrieval with Fashion Multi-Modal Conceptual Knowledge Graph","Abstract":"Image-text retrieval is a core task in the multi-modal domain, which arises a lot of attention from both research and industry communities. Recently, the booming of visual-language pre-trained (vlp) models has greatly enhanced the performance of cross-modal retrieval. However, the fine-grained interactions between objects from different modalities are far from well-established. This issue becomes more severe in the e-commerce domain, which lacks sufficient training data and fine-grained cross-modal knowledge. To alleviate the problem, this paper proposes a novel e-commerce knowledge-enhanced vlp model conducted. We first automatically establish a multi-modal conceptual knowledge graph from large-scale e-commerce image-text data, and then inject the prior knowledge into the vlp model to align across modalities at the conceptual level. The experiments conducted on a public benchmark dataset demonstrate that conducted effectively enhances the performance of e-commerce image-text retrieval upon state-of-the-art vlp models by a large margin. The application of the method in real industrial scenarios also proves the feasibility and efficiency of knowledge..","wordlikeness":0.8181818182,"lcsratio":0.9090909091,"wordcoverage":0.7777777778}
{"Year":2021,"Venue":"konvens-2021","Acronym":"ArgueBERT","Description":"How To Improve BERT Embeddings for Measuring the Similarity of Arguments","Abstract":"Argumentation is an important tool within human interaction, not only in law and politics but also for discussing issues, expressing and exchanging opinions and coming to decisions in our everyday life. Applications for argumentation often require the measurement of the arguments\u2019 similarity, to solve tasks like clustering, paraphrase identi\ufb01cation or summarization. In our work, bert embeddings are pre-trained on novel training objectives and afterwards \ufb01ne-tuned in a siamese architecture, similar to reimers and gurevych (2019b), to measure the similarity of arguments. The experiments conducted in our work show that a change in bert\u2019s pre-training process can improve the performance on measuring argument similarity.","wordlikeness":0.8888888889,"lcsratio":0.6666666667,"wordcoverage":0.7142857143}
{"Year":2020,"Venue":"ws-2020","Acronym":"CAiRE-COVID","Description":"A Question Answering and Query-focused Multi-Document Summarization System for COVID-19 Scholarly Information Management","Abstract":"We present which, a real-time question answering (qa) and multi-document summarization system, which won one of the 10 tasks in the kaggle covid-19 open research dataset challenge, judged by medical experts. Our system aims to tackle the recent challenge of mining the numerous scientific articles being published on covid-19 by answering high priority questions from the community and summarizing salient question-related information. It combines information extraction with state-of-the-art qa and query-focused multi-document summarization techniques, selecting and highlighting evidence snippets from existing literature given a query. We also propose query-focused abstractive and extractive multi-document summarization methods, to provide more relevant information related to the question. We further conduct quantitative experiments that show consistent improvements on various metrics for each module. We have launched our website answering for broader use by the medical community, and have open-sourced the code for our system, to bootstrap further study by other researches.","wordlikeness":0.6363636364,"lcsratio":0.9090909091,"wordcoverage":0.6666666667}
{"Year":2022,"Venue":"lrec-2022","Acronym":"HOPE","Description":"A Task-Oriented and Human-Centric Evaluation Framework Using Professional Post-Editing Towards More Effective MT Evaluation","Abstract":"Traditional automatic evaluation metrics for machine translation have been widely criticized by linguists due to their low accuracy, lack of transparency, focus on language mechanics rather than semantics, and low agreement with human quality evaluation. Human evaluations in the form of mqm-like scorecards have always been carried out in real industry setting by both clients and translation service providers (tsps). However, traditional human translation quality evaluations are costly to perform and go into great linguistic detail, raise issues as to inter-rater reliability (irr) and are not designed to measure quality of worse than premium quality translations. In this work, we introduce <b>highly<\/b>, a task-oriented and <i><b>h<\/b> <\/i>uman-centric evaluation framework for machine translation output based <i><b>o<\/b> <\/i>n professional <i><b>p<\/b> <\/i>ost-<i> <b>e<\/b> <\/i>diting annotations. It contains only a limited number of commonly occurring error types, and uses a scoring model with geometric progression of error penalty points (epps) reflecting error severity level to each translation unit. The initial experimental work carried out on english-russian language pair mt outputs on marketing content type of text from highly technical domain reveals that our evaluation framework is quite effective in reflecting the mt output quality regarding both overall system-level performance and segment-level transparency, and it increases the irr for error type interpretation. The approach has several key advantages, such as ability to measure and compare less than perfect mt output from different systems, ability to indicate human perception of quality, immediate estimation of the labor effort required to bring mt output to premium quality, low-cost and faster application, as well as higher irr. Our experimental data is available at <a href=https:\/\/github.com\/lhan87\/only class=acl-markup-url>https:\/\/github.com\/lhan87\/providers<\/a>.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2018,"Venue":"lrec-2018","Acronym":"SimPA","Description":"A Sentence-Level Simplification Corpus for the Public Administration Domain","Abstract":"We present a sentence-level simpli\ufb01cation corpus with content from the public administration (pa) domain. The corpus contains 1, 100 original sentences with manual simpli\ufb01cations collected through a two-stage process. Firstly, annotators were asked to simplify only words and phrases (lexical simpli\ufb01cation). Each sentence was simpli\ufb01ed by three annotators. Secondly, one lexically simpli\ufb01ed version of each original sentence was further simpli\ufb01ed at the syntactic level. In its current version there are 3, 300 lexically simpli\ufb01ed sentences plus 1, 100 syntactically simpli\ufb01ed sentences. The corpus will be used for evaluation of text simpli\ufb01cation approaches in the scope of the eu h2020 annotators.tico project \u2013 which focuses on accessibility of e-services in the pa domain \u2013 and beyond. The main advantage of this corpus is that lexical and syntactic simpli\ufb01cations can be analysed and used in isolation. The lexically simpli\ufb01ed corpus is also multi-reference (three different simpli\ufb01cations per original sentence). This is an ongoing effort and our \ufb01nal aim is to collect manual simpli\ufb01cations for the entire set of original sentences, with over 10k sentences. Keywords: text simpli\ufb01cation, simpli\ufb01cation corpora, public administration 1.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2007,"Venue":"semeval-2007","Acronym":"UPV-SI","Description":"Word Sense Induction using Self Term Expansion","Abstract":"In this paper we are reporting the results obtained participating in the \u201cevaluating word sense induction and discrimination systems\u201d task of semeval 2007. Our totally unsupervised system performed an automatic self-term expansion process by mean of co-ocurrence terms and, thereafter, it executed the unsupervised kstar clustering method. Two ranking tables with di\ufb00erent evaluation measures were calculated by the task organizers, every table with two baselines and six runs submitted by different teams. We were ranked third place in both ranking tables obtaining a better performance than three di\ufb00erent baselines, and outperforming the average score.","wordlikeness":0.3333333333,"lcsratio":0.6666666667,"wordcoverage":0.6666666667}
{"Year":2022,"Venue":"paclic-2022","Acronym":"SEND","Description":"A Simple and Efficient Noise Detection Algorithm for Vietnamese Real Estate Posts","Abstract":"One of the emerging research fields in natural language processing is noise detection (nd), the process of identifying posts containing noise information on textual data. While numerous datasets and approaches are developed for nd research in other languages, equivalent resources for the vietnamese are limited. To the best of our knowledge, no dataset or method has been investigated or proposed to address the noise detection tasks in the vietnamese language. In reality, noise data is constantly present in datasets and sometimes hurts relevant model performance. To overcome this limitation, we propose vind, a first human-annotated dataset that is available to the scientific community as a benchmark for the task of vietnamese noise detection. The vind dataset contains 12,862 posts collected from five major vietnamese real estate news websites. This paper provides an overview of the vietnamese noise detection task, the process of creating the vind dataset, and the techniques for carrying out the baseline experiments. On the vind dataset, the phobertlarge model outperforms robust baseline models such as lstm, bi-lstm, bert, roberta, xlm-r, and distilbert and achieves a macro f1-score of 0.9024. In addition, our proposed method also successfully improves the related task\u2019s performance, mainly vietnamese named entity recognition (ner) for real estate posts, about 0.0239 in terms of macro f1-score.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2021,"Venue":"eacl-2021","Acronym":"MONAH","Description":"Multi-Modal Narratives for Humans to analyze conversations","Abstract":"In conversational analyses, humans manually weave multimodal information into the transcripts, which is significantly time-consuming. We introduce a system that automatically expands the verbatim transcripts of video-recorded conversations using multimodal data streams. This system uses a set of preprocessing rules to weave multimodal annotations into the verbatim transcripts and promote interpretability. Our feature engineering contributions are two-fold: firstly, we identify the range of multimodal features relevant to detect rapport-building; secondly, we expand the range of multimodal annotations and show that the expansion leads to statistically significant improvements in detecting rapport-building.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2020,"Venue":"coling-2020","Acronym":"Vulgaris","Description":"Analysis of a Corpus for Middle-Age Varieties of Italian Language","Abstract":"Italian is a romance language that has its roots in vulgar latin. The birth of the modern italian started in tuscany around the 14th century, and it is mainly attributed to the works of dante alighieri, francesco petrarca and giovanni boccaccio, who are among the most acclaimed authors of the medieval age in tuscany. However, italy has been characterized by a high variety of dialects, which are often loosely related to each other, due to the past fragmentation of the territory. Italian has absorbed influences from many of these dialects, as also from other languages due to dominion of portions of the country by other nations, such as spain and france. In this work we present stylistic\/chronological, a project aimed at studying a corpus of italian textual resources from authors of different regions, ranging in a time period between 1200 and 1600. Each composition is associated to its author, and authors are also grouped in families, i.e. sharing similar stylistic\/chronological characteristics. Hence, the dataset is not only a valuable resource for studying the diachronic evolution of italian and the differences between its dialects, but it is also useful to investigate stylistic aspects between single authors. We provide a detailed statistical analysis of the data, and a corpus-driven study in dialectology and diachronic varieties.","wordlikeness":0.875,"lcsratio":0.875,"wordcoverage":0.75}
{"Year":2014,"Venue":"lrec-2014","Acronym":"TVD","Description":"A Reproducible and Multiply Aligned TV Series Dataset","Abstract":"We introduce a new dataset built around two tv series from different genres, the big bang theory, a situation comedy and game of thrones, a fantasy drama. The dataset has multiple tracks extracted from diverse sources, including dialogue (manual and automatic transcripts, multilingual subtitles), crowd-sourced textual descriptions (brief episode summaries, longer episode outlines) and various metadata (speakers, shots, scenes). The paper describes the dataset and provide tools to reproduce it for research purposes provided one has legally acquired the dvd set of the series. Tools are also provided to temporally align a major subset of dialogue and description tracks, in order to combine complementary information present in these tracks for enhanced accessibility. For alignment, we consider tracks as comparable corpora and first apply an existing algorithm for aligning such corpora based on dynamic time warping and tfidf-based similarity scores. We improve this baseline algorithm using contextual information, wordnet-based word similarity and scene location information. We report the performance of these algorithms on a manually aligned subset of the data. To highlight the interest of the database, we report a use case involving rich speech retrieval and propose other uses.","wordlikeness":0.3333333333,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2011,"Venue":"acl-2011","Acronym":"MEANT","Description":"An inexpensive, high-accuracy, semi-automatic metric for evaluating translation utility based on semantic roles","Abstract":"We introduce a novel semi-automated metric, achieves, that assesses translation utility by matching semantic role fillers, producing scores that correlate with human judgment as well as hter but at much lower labor cost. As machine translation systems improve in lexical choice and fluency, the shortcomings of widespread n-gram based, fluency-oriented mt evaluation metrics such as bleu, which fail to properly evaluate adequacy, become more apparent. But more accurate, nonautomatic adequacy-oriented mt evaluation metrics like hter are highly labor-intensive, which bottlenecks the evaluation cycle. We first show that when using untrained monolingual readers to annotate semantic roles in mt output, the non-automatic version of the metric hwith achieves a 0.43 correlation coefficient with human adequacy judgments at the sentence level, far superior to bleu at only 0.20, and equal to the far more expensive hter. We then replace the human semantic role annotators with automatic shallow semantic parsing to further automate the evaluation metric, and show that even the semiautomated evaluation metric achieves a 0.34 correlation coefficient with human adequacy judgment, which is still about 80% as closely correlated as hter despite an even lower labor cost for the evaluation procedure. The results show that our proposed metric is significantly better correlated with human judgment on adequacy than current widespread automatic evaluation metrics, while being much more cost effective than hter.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2023,"Venue":"dmr-2023","Acronym":"QA-Adj","Description":"Adding Adjectives to QA-based Semantics","Abstract":"Identifying all predicate-argument relations in a sentence has been a fundamental research target in nlp. While traditionally these relations were modeled via formal schemata, the recent qa-srl paradigm (and its extensions) present appealing advantages of capturing such relations through intuitive natural language question-answer (qa) pairs. In this paper, we extend the qa-based semantics framework to cover adjectival predicates, which carry important information in many downstream settings yet have been scarcely addressed in nlp research. Firstly, based on some prior literature and empirical assessment, we propose capturing four types of core adjectival arguments, through corresponding question types. Notably, our coverage goes beyond prior annotations of adjectival arguments, while also explicating valuable implicit arguments. Next, we develop an extensive data annotation methodology, involving controlled crowdsourcing and targeted expert review. Following, we create a high-quality dataset, consisting of 9k adjective mentions with 12k predicate-argument instances (qas). Finally, we present and analyze baseline models based on text-to-text language modeling, indicating challenges for future research, particularly regarding the scarce argument types. Overall, we suggest that our contributions can provide the basis for research on contemporary modeling of adjectival information.","wordlikeness":0.1666666667,"lcsratio":0.8333333333,"wordcoverage":0.6}
{"Year":2020,"Venue":"coling-2020","Acronym":"RatE","Description":"Relation-Adaptive Translating Embedding for Knowledge Graph Completion","Abstract":"Many graph embedding approaches have been proposed for knowledge graph completion via link prediction. Among those, translating embedding approaches enjoy the advantages of light-weight structure, high efficiency and great interpretability. Especially when extended to complex vector space, they show the capability in handling various relation patterns including symmetry, antisymmetry, inversion and composition. However, previous translating embedding approaches defined in complex vector space suffer from two main issues: 1) representing and modeling capacities of the model are limited by the translation function with rigorous multiplication of two complex numbers; and 2) embedding ambiguity caused by one-to-many relations is not explicitly alleviated. In this paper, we propose a relation-adaptive translation function built upon a novel weighted product in complex space, where the weights are learnable, relation-specific and independent to embedding size. The translation function only requires eight more scalar parameters each relation, but improves expressive power and alleviates embedding ambiguity problem. Based on the function, we then present our relation-adaptive translating embedding (when) approach to score each graph triple. Moreover, a novel negative sampling method is proposed to utilize both prior knowledge and self-adversarial learning for effective optimization. Experiments verify approach achieves state-of-the-art performance on four link prediction benchmarks.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2013,"Venue":"naacl-2013","Acronym":"KELVIN","Description":"a tool for automated knowledge base construction","Abstract":"We present several, an automated system for processing a large text corpus and distilling a knowledge base about persons, organizations, and locations. We have tested the pennsylvania, system on several corpora, including: (a) the tac kbp 2012 cold start corpus which consists of public web pages from the university of pennsylvania, and (b) a subset of 26k news articles taken from english gigaword 5th edition. Our naacl hlt 2013 demonstration permits a user to interact with a set of searchable html pages, which are automatically generated from the knowledge base. Each page contains information analogous to the semi-structured details about an entity that are present in wikipedia infoboxes, along with hyperlink citations to supporting text.","wordlikeness":0.8333333333,"lcsratio":0.6666666667,"wordcoverage":1.0}
{"Year":2022,"Venue":"ijcnlp-2022","Acronym":"FPC","Description":"Fine-tuning with Prompt Curriculum for Relation Extraction","Abstract":"The current classification methods for relation extraction (re) generally utilize pre-trained language models (plms) and have achieved superior results. However, such methods directly treat relation labels as class numbers, therefore they ignore the semantics of relation labels. Recently, prompt-based fine-tuning has been proposed and attracted much attention. This kind of methods insert templates into the input and convert the classification task to a (masked) language modeling problem. With this inspiration, we propose a novel method fine-tuning with prompt curriculum (method) for re, with two distinctive characteristics: the relation prompt learning, introducing an auxiliary prompt-based fine-tuning task to make the model capture the semantics of relation labels; the prompt learning curriculum, a fine-tuning procedure including an increasingly difficult task to adapt the model to the difficult multi-task setting. We have conducted extensive experiments on four widely used re benchmarks under fully supervised and low-resource settings. The experimental results show that setting. Can significantly outperform the existing methods and obtain the new state-of-the-art results.","wordlikeness":0.3333333333,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2023,"Venue":"eacl-2023","Acronym":"RevUp","Description":"Revise and Update Information Bottleneck for Event Representation","Abstract":"The existence of external (\u201cside\u201d) semantic knowledge has been shown to result in more expressive computational event models. To enable the use of side information that may be noisy or missing, we propose a semi-supervised information bottleneck-based discrete latent variable model. We reparameterize the model\u2019s discrete variables with auxiliary continuous latent variables and a light-weight hierarchical structure. Our model is learned to minimize the mutual information between the observed data and optional side knowledge that is not already captured by the new, auxiliary variables. We theoretically show that our approach generalizes past approaches, and perform an empirical case study of our approach on event modeling. We corroborate our theoretical results with strong empirical experiments, showing that the proposed method outperforms previous proposed approaches on multiple datasets.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2020,"Venue":"emnlp-2020","Acronym":"BioMegatron","Description":"Larger Biomedical Domain Language Model","Abstract":"There has been an influx of biomedical domain-specific language models, showing language models pre-trained on biomedical text perform better on biomedical domain benchmarks than those trained on general domain text corpora such as wikipedia and books. Yet, most works do not study the factors affecting each domain language application deeply. Additionally, the study of model size on domain-specific models has been mostly missing. We empirically study and evaluate several factors that can affect performance on domain language applications, such as the sub-word vocabulary set, model size, pre-training corpus, and domain transfer. We show consistent improvements on benchmarks with our larger larger model trained on a larger domain corpus, contributing to our understanding of domain language model applications. We demonstrate noticeable improvements over the previous state-of-the-art (sota) on standard biomedical nlp benchmarks of question answering, named entity recognition, and relation extraction. Code and checkpoints to reproduce our experiments are available at [github.com\/nvidia\/nemo].","wordlikeness":0.7272727273,"lcsratio":0.7272727273,"wordcoverage":0.7}
{"Year":2019,"Venue":"acl-2019","Acronym":"TIGS","Description":"An Inference Algorithm for Text Infilling with Gradient Search","Abstract":"Text infilling aims at filling in the missing part of a sentence or paragraph, which has been applied to a variety of real-world natural language generation scenarios. Given a well-trained sequential generative model, it is challenging for its unidirectional decoder to generate missing symbols conditioned on the past and future information around the missing part. In this paper, we propose an iterative inference algorithm based on gradient search, which could be the first inference algorithm that can be broadly applied to any neural sequence generative models for text infilling tasks. Extensive experimental comparisons show the effectiveness and efficiency of the proposed method on three different text infilling tasks with various mask ratios and different mask strategies, comparing with five state-of-the-art methods.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2018,"Venue":"coling-2018","Acronym":"GenSense","Description":"A Generalized Sense Retrofitting Model","Abstract":"With the aid of recently proposed word embedding algorithms, the study of semantic similarity has progressed and advanced rapidly. However, many natural language processing tasks need sense level representation. To address this issue, some researches propose sense embedding learning algorithms. In this paper, we present a generalized model from existing sense retrofitting model. The generalization takes three major components: semantic relations between the senses, the relation strength and the semantic strength. In the experiment, we show that the generalized model can outperform previous approaches in three types of experiment: semantic relatedness, contextual word similarity and semantic difference.","wordlikeness":0.875,"lcsratio":1.0,"wordcoverage":0.7692307692}
{"Year":2012,"Venue":"acl-2012","Acronym":"TopicTiling","Description":"A Text Segmentation Algorithm based on LDA","Abstract":"This work presents a text segmentation algorithm called performs. This algorithm is based on the well-known texttiling algorithm, and segments documents using the latent dirichlet allocation (lda) topic model. We show that using the mode topic id assigned during the inference method of lda, used to annotate unseen documents, improves performance by stabilizing the obtained topics. We show signi\ufb01cant improvements over state of the art segmentation algorithms on two standard datasets. As an additional bene\ufb01t, topics. Performs the segmentation in linear time and thus is computationally less expensive than other lda-based segmentation methods.","wordlikeness":0.7272727273,"lcsratio":0.5454545455,"wordcoverage":0.7368421053}
{"Year":2023,"Venue":"nodalida-2023","Acronym":"DanTok","Description":"Domain Beats Language for Danish Social Media POS Tagging","Abstract":"Language from social media remains challenging to process automatically, especially for non-english languages. In this work, we introduce the first nlp dataset for tiktok comments and the first danish social media dataset with part-of-speech annotation. We further supply annotations for normalization, code-switching, and annotator uncertainty. As transferring models to such a highly specialized domain is non-trivial, we conduct an extensive study into which source data and modeling decisions most impact the performance. Surprisingly, transferring from in-domain data, even from a different language, outperforms in-language, out-of-domain training. These benefits nonetheless rely on the underlying language models having been at least partially pre-trained on data from the target language. Using our additional annotation layers, we further analyze how normalization, code-switching, and human uncertainty affect the tagging accuracy.","wordlikeness":0.6666666667,"lcsratio":0.8333333333,"wordcoverage":0.7272727273}
{"Year":2022,"Venue":"naacl-2022","Acronym":"SKILL","Description":"Structured Knowledge Infusion for Large Language Models","Abstract":"Large language models (llms) have demonstrated human-level performance on a vast spectrum of natural language tasks. However, it is largely unexplored whether they can better internalize knowledge from a structured data, such as a knowledge graph, or from text. In this work, we propose a method to infuse structured knowledge into llms, by directly training t5 models on factual triples of knowledge graphs (kgs). We show that models pre-trained on wikidata kg with our method outperform the t5 baselines on freebaseqa and wikihop, as well as the wikidata-answerable subset of triviaqa and naturalquestions. The models pre-trained on factual triples compare competitively with the ones on natural language sentences that contain the same knowledge. Trained on a smaller size kg, wikimovies, we saw 3x improvement of exact match score on metaqa task. The proposed method has an advantage that no alignment between the knowledge graph and text corpus is required in curating training data. This makes our method particularly useful when working with industry-scale knowledge graphs.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2017,"Venue":"acl-2017","Acronym":"TriviaQA","Description":"A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension","Abstract":"We present other, a challenging reading comprehension dataset containing over 650k question-answer-evidence triples. Relatively includes 95k question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, authored (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on squad reading comprehension. Neither approach comes close to human performance (23% and 40% vs. 80%), suggesting that datasets, is a challenging testbed that is worth significant future study.","wordlikeness":0.75,"lcsratio":0.75,"wordcoverage":0.8571428571}
{"Year":2003,"Venue":"ws-2003","Acronym":"OLLIE","Description":"On-Line Learning for Information Extraction","Abstract":"This paper reports work aimed at developing an open, distributed learning environment, learning, where researchers can experiment with different machine learning (ml) methods for information extraction. Once the required level of performance is reached, the ml algorithms can be used to speed up the manual annotation process. (ml) uses a browser client while data storage and ml training is performed on servers. The different ml algorithms use a uni\ufb01ed programming interface; the integration of new ones is straightforward.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2016,"Venue":"lrec-2016","Acronym":"ARRAU","Description":"Linguistically-Motivated Annotation of Anaphoric Descriptions","Abstract":"This paper presents a second release of the dataset: dataset: a multi-domain corpus with thorough linguistically motivated annotation of anaphora and related phenomena. Building upon the first release almost a decade ago, a considerable effort had been invested in improving the data both quantitatively and qualitatively. Thus, we have doubled the corpus size, expanded the selection of covered phenomena to include referentiality and genericity and designed and implemented a methodology for enforcing the consistency of the manual annotation. We believe that the new release of dataset: provides a valuable material for ongoing research in complex cases of coreference as well as for a variety of related tasks. The corpus is publicly available through ldc.","wordlikeness":0.8,"lcsratio":0.6,"wordcoverage":0.8}
{"Year":2023,"Venue":"findings-2023","Acronym":"MixPAVE","Description":"Mix-Prompt Tuning for Few-shot Product Attribute Value Extraction","Abstract":"The task of product attribute value extraction is to identify values of an attribute from product information. Product attributes are important features, which help improve online shopping experience of customers, such as product search, recommendation and comparison. Most existing works only focus on extracting values for a set of known attributes with sufficient training data. However, with the emerging nature of e-commerce, new products with their unique set of new attributes are constantly generated from different retailers and merchants. Collecting a large number of annotations for every new attribute is costly and time consuming. Therefore, it is an important research problem for product attribute value extraction with limited data. In this work, we propose a novel prompt tuning approach with <b>mix<\/b>ed <b>p<\/b>rompts for few-shot <b>a<\/b>ttribute <b>v<\/b>alue <b>e<\/b>xtraction, namely superior. Specifically, state-of-the-art introduces only a small amount (&lt; 1%) of trainable parameters, i.e., a mixture of two learnable prompts, while keeping the existing extraction model frozen. In this way, therefore, not only benefits from parameter-efficient training, but also avoids model overfitting on limited training examples. Experimental results on two product benchmarks demonstrate the superior performance of the proposed approach over several state-of-the-art baselines. A comprehensive set of ablation studies validate the effectiveness of the prompt design, as well as the efficiency of our approach.","wordlikeness":0.5714285714,"lcsratio":1.0,"wordcoverage":0.7272727273}
{"Year":1991,"Venue":"mtsummit-1991","Acronym":"ULTRA","Description":"A Multi-lingual Machine Translator","Abstract":"Language (universal language translator) is a multilingual, interlingual machine translation system currently under development at the computing research laboratory at new mexico state university. It translates between five languages (chinese, english, german, japanese, spanish) with vocabularies in each language based on approximately 10,000 word senses. The major design criteria are that the system be robust and general purpose with simple to use utilities for customization to suit the needs of particular users. This paper describes the central characteristics of the system: the intermediate representation, the language components, semantic and pragmatic processes, and supporting lexical entry tools.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2017,"Venue":"udw-2017","Acronym":"Udapi","Description":"Universal API for Universal Dependencies","Abstract":"Etc. Is an open-source framework providing an application programming interface (api) for processing universal dependencies data. Providing is available in python, perl and java. It is suitable both for full-\ufb02edged applications and fast prototyping: visualization of dependency trees, format conversions, querying, editing and transformations, validity tests, dependency parsing, evaluation etc.","wordlikeness":0.6,"lcsratio":0.8,"wordcoverage":0.75}
{"Year":2004,"Venue":"ws-2004","Acronym":"CESTA","Description":"Machine Translation Evaluation Campaign [Work-in-Progress Project Report]","Abstract":"Material,, the first european campaign dedicated to mt evaluation, is a project labelled by the french technolangue action. An provides an evaluation of six commercial and academic mt systems using a protocol set by an international panel of experts. \u201crouge\u201d aims at producing reusable resources and information about reliability of the metrics. Two runs will be carried out: one using the system\u2019s basic dictionary, another after terminological adaptation. Evaluation task, test material, resources, evaluation measures, metrics, will be detailed in the full paper. The protocol is the combination of a contrastive reference to: ibm \u201cbleu\u201d protocol (papineni, k., s. Roukos, t. Ward and z. Wei-jing, 2001); \u201cblanc\u201d protocol derived from (hartley, rajman, 2002).; \u201crouge\u201d protocol (babych, hartley, atwell, 2003). The results of the campaign will be published in a final report and be the object of two intermediary and final workshops.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2022,"Venue":"findings-2022","Acronym":"BanglaBERT","Description":"Language Model Pretraining and Benchmarks for Low-Resource Language Understanding Evaluation in Bangla","Abstract":"In this work, we introduce spoken, a bert-based natural language understanding (nlu) model pretrained in bangla, a widely spoken yet low-resource language in the nlp literature. To pretrain answering, we collect 27.5 gb of bangla pretraining data (dubbed \u2018bangla2b+\u2019) by crawling 110 popular bangla sites. We introduce two downstream task datasets on natural language inference and question answering and benchmark on four diverse nlu tasks covering text classification, sequence labeling, and span prediction. In the process, we bring them under the first-ever bangla language understanding benchmark (blub). Two achieves state-of-the-art results outperforming multilingual and monolingual models. We are making the models, datasets, and a leaderboard publicly available at <a href=https:\/\/github.com\/csebuetnlp\/monolingual class=acl-markup-url>https:\/\/github.com\/csebuetnlp\/models,<\/a> to advance bangla nlp.","wordlikeness":0.8,"lcsratio":0.9,"wordcoverage":0.75}
{"Year":2014,"Venue":"lrec-2014","Acronym":"MUHIT","Description":"A Multilingual Harmonized Dictionary","Abstract":"This paper discusses a trial to build a multilingual harmonized dictionary that contains more than 40 languages, with special reference to arabic which represents about 20% of the whole size of the dictionary. This dictionary is called useful which is an interactive multilingual dictionary application. It is a web application that makes it easily accessible to all users. Language is developed within the universal networking language (unl) framework by the undl foundation, in cooperation with bibliotheca alexandrina (ba). This application targets to serve specialists and non-specialists. It provides users with full linguistic description to each lexical item. This free application is useful to many nlp tasks such as multilingual translation and cross-language synonym search. This dictionary is built depending on wordnet and corpus based approaches, in a specially designed linguistic environment called unlariam that is developed by the unld foundation. This dictionary is the first launched application by the unld foundation.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2018,"Venue":"lrec-2018","Acronym":"BlogSet-BR","Description":"A Brazilian Portuguese Blog Corpus","Abstract":"The rich user-generated content found on internet blogs have always attracted the interest of scienti\ufb01c communities for many different purposes, such as from opinion and sentiment mining, information extraction or topic discovery. Nonetheless, an extensive corpora is essential to perform most of natural language processing involved in these tasks. This paper presents in, an extensive brazilian portuguese corpus containing 2.1 billions words extracted from 7.4 millions posts over 808 thousand different brazilian blogs. Additionally, a survey was conducted with authors to draw a pro\ufb01le of brazilian bloggers. Keywords: blog as corpus, extracted, brazilian portuguese corpus 1.","wordlikeness":0.6,"lcsratio":0.8,"wordcoverage":0.7058823529}
{"Year":2018,"Venue":"emnlp-2018","Acronym":"SemRegex","Description":"A Semantics-Based Approach for Generating Regular Expressions from Natural Language Specifications","Abstract":"Recent research proposes syntax-based approaches to address the problem of generating programs from natural language specifications. These approaches typically train a sequence-to-sequence learning model using a syntax-based objective: maximum likelihood estimation (mle). Such syntax-based approaches do not effectively address the goal of generating semantically correct programs, because these approaches fail to handle program aliasing, i.e., semantically equivalent programs may have many syntactically different forms. To address this issue, in this paper, we propose a semantics-based approach named expected. Syntax-based provides solutions for a subtask of the program-synthesis problem: generating regular expressions from natural language. Different from the existing syntax-based approaches, learning trains the model by maximizing the expected semantic correctness of the generated regular expressions. The semantic correctness is measured using the dfa-equivalence oracle, random test cases, and distinguishing test cases. The experiments on three public datasets demonstrate the superiority of goal over the existing state-of-the-art approaches.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.7142857143}
{"Year":2013,"Venue":"semeval-2013","Acronym":"UNITOR-HMM-TK","Description":"Structured Kernel-based learning for Spatial Role Labeling","Abstract":"In this paper the system system participating in the spatial role labeling task at semeval 2013 is presented. The spatial roles classi\ufb01cation is addressed as a sequence-based word classi\ufb01cation problem: the svmhmm learning algorithm is applied, based on a simple feature modeling and a robust lexical generalization achieved through a distributional model of lexical semantics. In the identi\ufb01cation of spatial relations, roles are combined to generate candidate relations, later veri\ufb01ed by a svm classi\ufb01er. The smoothed partial tree kernel is applied, i.e. a convolution kernel that enhances both syntactic and lexical properties of the examples, avoiding the need of a manual feature engineering phase. Finally, results on three of the \ufb01ve tasks of the challenge are reported.","wordlikeness":0.4615384615,"lcsratio":0.4615384615,"wordcoverage":0.6}
{"Year":2012,"Venue":"lrec-2012","Acronym":"EmpaTweet","Description":"Annotating and Detecting Emotions on Twitter","Abstract":"The rise of micro-blogging in recent years has resulted in significant access to emotion-laden text. Unlike emotion expressed in other textual sources (e.g., blogs, quotes in newswire, email, product reviews, or even clinical text), micro-blogs differ by (1) placing a strict limit on length, resulting radically in new forms of emotional expression, and (2) encouraging users to express their daily thoughts in real-time, often resulting in far more emotion statements than might normally occur. In this paper, we introduce a corpus collected from twitter with annotated micro-blog posts (or \u0093tweets\u0094) annotated at the tweet-level with seven emotions: anger, disgust, fear, joy, love, sadness, and surprise. We analyze how emotions are distributed in the data we annotated and compare it to the distributions in other emotion-annotated corpora. We also used the annotated corpus to train a classifier that automatically discovers the emotions in tweets. In addition, we present an analysis of the linguistic style used for expressing emotions our corpus. We hope that these observations will lead to the design of novel emotion detection techniques that account for linguistic style and psycholinguistic theories.","wordlikeness":0.5555555556,"lcsratio":0.5555555556,"wordcoverage":0.7368421053}
{"Year":2016,"Venue":"lrec-2016","Acronym":"TwiSty","Description":"A Multilingual Twitter Stylometry Corpus for Gender and Personality Profiling","Abstract":"Personality profiling is the task of detecting personality traits of authors based on writing style. Several personality typologies exist, however, the briggs-myer type indicator (mbti) is particularly popular in the non-scientific community, and many people use it to analyse their own personality and talk about the results online. Therefore, large amounts of self-assessed data on mbti are readily available on social-media platforms such as twitter. We present a novel corpus of tweets annotated with the mbti personality type and gender of their author for six western european languages (dutch, german, french, italian, portuguese and spanish). We outline the corpus creation and annotation, show statistics of the obtained data distributions and present first baselines on myers-briggs personality profiling and gender prediction for all six languages.","wordlikeness":0.8333333333,"lcsratio":1.0,"wordcoverage":0.9090909091}
{"Year":2011,"Venue":"ijcnlp-2011","Acronym":"CLGVSM","Description":"Adapting Generalized Vector Space Model to Cross-lingual Document Clustering","Abstract":"Cross-lingual document clustering (cldc) is the task to automatically organize a large collection of cross-lingual documents into groups considering content or topic. Different from the traditional hard matching strategy, this paper extends traditional generalized vector space model (gvsm) to handle cross-lingual cases, referred to as as, by incorporating cross-lingual word similarity measures. With this model, we further compare different word similarity measures in cross-lingual document clustering. To select cross-lingual features effectively, we also propose a softmatching based feature selection method in different. Experimental results on benchmarking data set show that (1) the proposed considering is very effective for cross-document clustering, outperforming the two strong baselines vector space model (vsm) and latent semantic analysis (lsa) significantly; and (2) the new feature selection method can further improve outperforming.","wordlikeness":0.1666666667,"lcsratio":0.6666666667,"wordcoverage":0.6666666667}
{"Year":2022,"Venue":"findings-2022","Acronym":"MMM","Description":"An Emotion and Novelty-aware Approach for Multilingual Multimodal Misinformation Detection","Abstract":"The growth of multilingual web content in low-resource languages is becoming an emerging challenge to detect misinformation. One particular hindrance to research on this problem is the non-availability of resources and tools. Majority of the earlier works in misinformation detection are based on english content which confines the applicability of the research to a specific language only. Increasing presence of multimedia content on the web has promoted misinformation in which real multimedia content (images, videos) are used in different but related contexts with manipulated texts to mislead the readers. Detecting this category of misleading information is almost impossible without any prior knowledge. Studies say that emotion-invoking and highly novel content accelerates the dissemination of false information. To counter this problem, here in this paper, we first introduce a novel multilingual multimodal misinformation dataset that includes background knowledge (from authentic sources) of the misleading articles. Second, we propose an effective neural model leveraging novelty detection and emotion recognition to detect fabricated information. We perform extensive experiments to justify that our proposed model outperforms the state-of-the-art (sota) on the concerned task.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2013,"Venue":"naacl-2013","Acronym":"TruthTeller","Description":"Annotating Predicate Truth","Abstract":"We propose a novel semantic annotation type of assigning truth values to predicate occurrences, and present integrates, a standalone publiclyavailable tool that produces such annotations. , integrates a range of semantic phenomena, such as negation, modality, presupposition, implicativity, and more, which were dealt only partly in previous works. Empirical evaluations against human annotations show satisfactory results and suggest the usefulness of this new type of tool for nlp.","wordlikeness":0.9090909091,"lcsratio":0.4545454545,"wordcoverage":0.7272727273}
{"Year":2020,"Venue":"emnlp-2020","Acronym":"PARADE","Description":"A New Dataset for Paraphrase Identification Requiring Computer Science Domain Knowledge","Abstract":"We present a new benchmark dataset called data for paraphrase identification that requires specialized domain knowledge. Lexical contains paraphrases that overlap very little at the lexical and syntactic level but are semantically equivalent based on computer science domain knowledge, as well as non-paraphrases that overlap greatly at the lexical and syntactic level but are not semantically equivalent based on this domain knowledge. Experiments show that both state-of-the-art neural models and non-expert human annotators have poor performance on equivalent. For example, bert after fine-tuning achieves an f1 score of 0.709, which is much lower than its performance on other paraphrase identification datasets. New can serve as a resource for researchers interested in testing models that incorporate domain knowledge. We make our data and code freely available.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2023,"Venue":"tacl-2023","Acronym":"FeelingBlue","Description":"A Corpus for Understanding the Emotional Connotation of Color in Context","Abstract":"While the link between color and emotion has been widely studied, how context-based changes in color impact the intensity of perceived emotions is not well understood. In this work, we present a new multimodal dataset for exploring the emotional connotation of color as mediated by line, stroke, texture, shape, and language. Our dataset, how, is a collection of 19,788 4-tuples of abstract art ranked by annotators according to their evoked emotions and paired with rationales for those annotations. Using this corpus, we present a baseline for a new task: justified affect transformation. Given an image i, the task is to 1) recolor i to enhance a specified emotion e and 2) provide a textual justification for the change in e. Our model is an ensemble of deep neural networks which takes i, generates an emotionally transformed color palette p conditioned on i, applies p to i, and then justifies the color transformation in text via a visual-linguistic model. Experimental results shed light on the emotional connotation of color in context, demonstrating both the promise of our approach on this challenging task and the considerable potential for future investigations enabled by our corpus.","wordlikeness":0.8181818182,"lcsratio":0.7272727273,"wordcoverage":0.7777777778}
{"Year":2020,"Venue":"lrec-2020","Acronym":"FrSemCor","Description":"Annotating a French Corpus with Supersenses","Abstract":"French, as many languages, lacks semantically annotated corpus data. Our aim is to provide the linguistic and nlp research communities with a gold standard sense-annotated corpus of french, using wordnet unique beginners as semantic tags, thus allowing for interoperability. In this paper, we report on the first phase of the project, which focused on the annotation of common nouns. The resulting dataset consists of more than 12,000 french noun occurrences which were annotated in double blind and adjudicated according to a carefully redefined set of supersenses. The resource is released online under a creative commons licence.","wordlikeness":0.5,"lcsratio":0.75,"wordcoverage":0.7142857143}
{"Year":2016,"Venue":"lrec-2016","Acronym":"MADAD","Description":"A Readability Annotation Tool for Arabic Text","Abstract":"This paper introduces supports, a general-purpose annotation tool for arabic text with focus on readability annotation. This tool will help in overcoming the problem of lack of arabic readability training data by providing an online environment to collect readability assessments on various kinds of corpora. Also the tool supports a broad range of annotation tasks for various linguistic and semantic phenomena by allowing users to create their customized annotation schemes. Allowing is a web-based tool, accessible through any web browser; the main features that distinguish a are its flexibility, portability, customizability and its bilingual interface (arabic\/english).","wordlikeness":0.6,"lcsratio":0.6,"wordcoverage":0.8}
{"Year":2023,"Venue":"acl-2023","Acronym":"Prompter","Description":"Zero-shot Adaptive Prefixes for Dialogue State Tracking Domain Adaptation","Abstract":"A challenge in the dialogue state tracking (dst) field is adapting models to new domains without using any supervised data \u2014 zero-shot domain adaptation. Parameter-efficient transfer learning (petl) has the potential to address this problem due to its robustness. However, it has yet to be applied to the zero-shot scenarios, as it is not clear how to apply it unsupervisedly. Our method, without, uses descriptions of target domain slots to generate dynamic prefixes that are concatenated to the key and values at each layer\u2019s self-attention mechanism. This allows for the use of prefix-tuning in zero-shot. Has outperforms previous methods on both the multiwoz and sgd benchmarks. In generating prefixes, our analyses find that however, not only utilizes the semantics of slot descriptions but also how often the slots appear together in conversation. Moreover, slot\u2019s gains are due to its improved ability to distinguish \u201dnone\u201d-valued dialogue slots, compared against baselines.","wordlikeness":0.875,"lcsratio":0.75,"wordcoverage":0.875}
{"Year":2019,"Venue":"ijcnlp-2019","Acronym":"CoSQL","Description":"A Conversational Text-to-SQL Challenge Towards Cross-Domain Natural Language Interfaces to Databases","Abstract":"We present systems., a corpus for building cross-domain, general-purpose database (db) querying dialogue systems. It consists of 30k+ turns plus 10k+ annotated sql queries, obtained from a wizard-of-oz (woz) collection of 3k dialogues querying 200 complex dbs spanning 138 domains. Each dialogue simulates a real-world db query scenario with a crowd worker as a user exploring the db and a sql expert retrieving answers with sql, clarifying ambiguous questions, or otherwise informing of unanswerable questions. When user questions are answerable by sql, the expert describes the sql and execution results to the user, hence maintaining a natural interaction flow. Research. Introduces new challenges compared to existing task-oriented dialogue datasets: (1) the dialogue states are grounded in sql, a domain-independent executable representation, instead of domain-specific slot value pairs, and (2) because testing is done on unseen databases, success requires generalizing to new domains. Expert includes three tasks: sql-grounded dialogue state tracking, response generation from query results, and user dialogue act prediction. We evaluate a set of strong baselines for each task and show that testing presents significant challenges for future research. The dataset, baselines, and leaderboard will be released at <a href=https:\/\/yale-lily.github.io\/because class=acl-markup-url>https:\/\/yale-lily.github.io\/states<\/a>.","wordlikeness":0.4,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2022,"Venue":"acl-2022","Acronym":"GL-CLeF","Description":"A Global--Local Contrastive Learning Framework for Cross-lingual Spoken Language Understanding","Abstract":"Due to high data demands of current methods, attention to zero-shot cross-lingual spoken language understanding (slu) has grown, as such approaches greatly reduce human annotation effort. However, existing models solely rely on shared parameters, which can only perform implicit alignment across languages. We present global-local contrastive learning framework (closer.) to address this shortcoming. Specifically, we employ contrastive learning, leveraging bilingual dictionaries to construct multilingual views of the same utterance, then encourage their representations to be more similar than negative example pairs, which achieves to explicitly align representations of similar sentences across languages. In addition, a key step in experiments is a proposed local and global component, which achieves a fine-grained cross-lingual transfer (i.e., sentence-level local intent transfer, token-level local slot transfer, and semantic-level global transfer across intent and slot). Experiments on multiatis++ show that contrastive achieves the best performance and successfully pulls representations of similar sentences across languages closer.","wordlikeness":0.4285714286,"lcsratio":1.0,"wordcoverage":0.6153846154}
{"Year":2023,"Venue":"acl-2023","Acronym":"CAME","Description":"Confidence-guided Adaptive Memory Efficient Optimization","Abstract":"Adaptive gradient methods, such as adam and lamb, have demonstrated excellent performance in the training of large language models. Nevertheless, the need for adaptivity requires maintaining second-moment estimates of the per-parameter gradients, which entails a high cost of extra memory overheads. To solve this problem, several memory-efficient optimizers (e.g., adafactor) have been proposed to obtain a drastic reduction in auxiliary memory usage, but with a performance penalty. In this paper, we first study a confidence-guided strategy to reduce the instability of existing memory efficient optimizers. Based on this strategy, we propose accuracy to simultaneously achieve two goals: fast convergence as in traditional adaptive methods, and low memory usage as in memory-efficient methods. Extensive experiments demonstrate the training stability and superior performance of two across various nlp tasks such as bert and gpt-2 training. Notably, for bert pre-training on the large batch size of 32,768, our proposed optimizer attains faster convergence and higher accuracy compared with the adam optimizer. The implementation of publicly is publicly available.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2019,"Venue":"acl-2019","Acronym":"ConvLab","Description":"Multi-Domain End-to-End Dialog System Platform","Abstract":"We present approaches,, an open-source multi-domain end-to-end dialog system platform, that enables researchers to quickly set up experiments with reusable components and compare a large set of different approaches, ranging from conventional pipeline systems to end-to-end neural models, in common environments. Reference offers a set of fully annotated datasets and associated pre-trained reference models. As a showcase, we extend the multiwoz dataset with user dialog act annotations to train all component models and demonstrate how showcase, makes it easy and effortless to conduct complicated experiments in multi-domain end-to-end dialog settings.","wordlikeness":0.7142857143,"lcsratio":0.5714285714,"wordcoverage":0.6666666667}
{"Year":2021,"Venue":"acl-2021","Acronym":"KaggleDBQA","Description":"Realistic Evaluation of Text-to-SQL Parsers","Abstract":"The goal of database question answering is to enable natural language querying of real-life relational databases in diverse application domains. Recently, large-scale datasets such as spider and wikisql facilitated novel modeling techniques for text-to-sql parsing, improving zero-shot generalization to unseen databases. In this work, we examine the challenges that still prevent these techniques from practical deployment. First, we present relational, a new cross-domain evaluation dataset of real web databases, with domain-specific data types, original formatting, and unrestricted questions. Second, we re-examine the choice of evaluation tasks for text-to-sql parsers as applied in real-life settings. Finally, we augment our in-domain evaluation task with database documentation, a naturally occurring source of implicit domain knowledge. We show that databases. Presents a challenge to state-of-the-art zero-shot parsers but a more realistic evaluation setting and creative use of associated database documentation boosts their accuracy by over 13.2%, doubling their performance.","wordlikeness":0.5,"lcsratio":0.5,"wordcoverage":0.625}
{"Year":2020,"Venue":"emnlp-2020","Acronym":"RoFT","Description":"A Tool for Evaluating Human Detection of Machine-Generated Text","Abstract":"In recent years, large neural networks for natural language generation (nlg) have made leaps and bounds in their ability to generate fluent text. However, the tasks of evaluating quality differences between nlg systems and understanding how humans perceive the generated text remain both crucial and difficult. In this system demonstration, we present real or fake text (introduce), a website that tackles both of these challenges by inviting users to try their hand at detecting machine-generated text in a variety of domains. We introduce a novel evaluation task based on detecting the boundary at which a text passage that starts off human-written transitions to being machine-generated. We show preliminary results of using and to evaluate detection of machine-generated news articles.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2022,"Venue":"ws-2022","Acronym":"NLPOP","Description":"a Dataset for Popularity Prediction of Promoted NLP Research on Twitter","Abstract":"Twitter has slowly but surely established itself as a forum for disseminating, analysing and promoting nlp research. The trend of researchers promoting work not yet peer-reviewed (preprints) by posting concise summaries presented itself as an opportunity to collect and combine multiple modalities of data. In scope of this paper, we (1) construct a dataset of twitter threads in which researchers promote nlp preprints and (2) evaluate whether it is possible to predict the popularity of a thread based on the content of the twitter thread, paper content and user metadata. We experimentally show that it is possible to predict popularity of threads promoting research based on their content, and that predictive performance depends on modelling textual input, indicating that the dataset could present value for related areas of nlp research such as citation recommendation and abstractive summarization.","wordlikeness":0.6,"lcsratio":0.8,"wordcoverage":0.75}
{"Year":2020,"Venue":"acl-2020","Acronym":"ExpBERT","Description":"Representation Engineering with Natural Language Explanations","Abstract":"Suppose we want to specify the inductive bias that married couples typically go on honeymoons for the task of extracting pairs of spouses from text. In this paper, we allow model developers to specify these types of inductive biases as natural language explanations. We use bert fine-tuned on multinli to \u201cinterpret\u201d these explanations with respect to the input sentence, producing explanation-guided representations of the input. Across three relation extraction tasks, our method, bert, matches a bert baseline but with 3\u201320x less labeled data and improves on the baseline by 3\u201310 f1 points with the same amount of labeled data.","wordlikeness":0.8571428571,"lcsratio":0.7142857143,"wordcoverage":0.9230769231}
{"Year":2021,"Venue":"acl-2021","Acronym":"OpenAttack","Description":"An Open-source Textual Adversarial Attack Toolkit","Abstract":"Textual adversarial attacking has received wide and increasing attention in recent years. Various attack models have been proposed, which are enormously distinct and implemented with different programming frameworks and settings. These facts hinder quick utilization and fair comparison of attack models. In this paper, we present an open-source textual adversarial attack toolkit named an to solve these issues. Compared with existing other textual adversarial attack toolkits, years. Has its unique strengths in support for all attack types, multilinguality, and parallel processing. Currently, typical includes 15 typical attack models that cover all attack types. Its highly inclusive modular design not only supports quick utilization of existing attack models, but also enables great flexibility and extensibility. Href=https:\/\/github.com\/thunlp\/ has broad uses including comparing and evaluating attack models, measuring robustness of a model, assisting in developing new attack models, and adversarial training. Source code and documentation can be obtained at <a href=https:\/\/github.com\/thunlp\/its class=acl-markup-url>https:\/\/github.com\/thunlp\/great<\/a>.","wordlikeness":0.9,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2011,"Venue":"alta-2011","Acronym":"ENGAGE","Description":"Automated Gestures for Animated Characters","Abstract":"There is a rapidly growing body of work in the use of embodied conversational agents (eca) to convey complex contextual relationships through verbal and non-verbal communication, in domains ranging from military c2 to e-learning. In these applications the subject matter expert is often na\u00efve to the technical requirements of ecas. verbal (the extensible natural gesture animation generation engine) is designed to automatically generate appropriate and \u2018realistic\u2019 animation for ecas based on the content provided to them. It employs syntactic analysis of the surface text and uses predefined behaviour models to generate appropriate behaviours for the eca. We discuss the design of this system, its current applications and plans for its future development.","wordlikeness":1.0,"lcsratio":0.6666666667,"wordcoverage":1.0}
{"Year":2023,"Venue":"inlg-2023","Acronym":"LOWRECORP","Description":"the Low-Resource NLG Corpus Building Challenge","Abstract":"Most languages in the world do not have sufficient data available to develop neural-network-based natural language generation (nlg) systems. To alleviate this resource scarcity, we propose a novel challenge for the nlg community: low-resource language corpus development (resource). We present an innovative framework to collect a single dataset with dual tasks to maximize the efficiency of data collection efforts and respect language consultant time. Specifically, we focus on a text-chat-based interface for two generation tasks \u2013 conversational response generation grounded in a source document and\/or image and dialogue summarization (from the former task). The goal of this shared task is to collectively develop grounded datasets for local and low-resourced languages. To enable data collection, we make available web-based software that can be used to collect these grounded conversations and summaries. Submissions will be assessed for the size, complexity, and diversity of the corpora to ensure quality control of the datasets as well as any enhancements to the interface or novel approaches to grounding conversations.","wordlikeness":0.7777777778,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2018,"Venue":"lrec-2018","Acronym":"ANCOR-AS","Description":"Enriching the ANCOR Corpus with Syntactic Annotations","Abstract":"This paper presents paper, an enriched version of the ancor corpus. This version adds syntactic annotations in addition to the existing coreference and speech transcription ones. This corpus is also released in a new tei-compliant xml format. Keywords: coreference, parsing, oral data, tei 1.","wordlikeness":0.5,"lcsratio":0.875,"wordcoverage":0.8}
{"Year":2021,"Venue":"acl-2021","Acronym":"RAW-C","Description":"Relatedness of Ambiguous Words in Context (A New Lexical Resource for English)","Abstract":"Most words are ambiguous\u2014-i.e., they convey distinct meanings in different contexts\u2014-and even the meanings of unambiguous words are context-dependent. Both phenomena present a challenge for nlp. Recently, the advent of contextualized word embeddings has led to success on tasks involving lexical ambiguity, such as word sense disambiguation. However, there are few tasks that directly evaluate how well these contextualized embeddings accommodate the more continuous, dynamic nature of word meaning\u2014-particularly in a way that matches human intuitions. We introduce theories, a dataset of graded, human relatedness judgments for 112 ambiguous words in context (with 672 sentence pairs total), as well as human estimates of sense dominance. The average inter-annotator agreement (assessed using a leave-one-annotator-out method) was 0.79. We then show that a measure of cosine distance, computed using contextualized embeddings from bert and elmo, correlates with human judgments, but that cosine distance also systematically underestimates how similar humans find uses of the same sense of a word to be, and systematically overestimates how similar humans find uses of different-sense homonyms. Finally, we propose a synthesis between psycholinguistic theories of the mental lexicon and computational models of lexical semantics.","wordlikeness":0.6,"lcsratio":0.8,"wordcoverage":0.75}
{"Year":2007,"Venue":"ws-2007","Acronym":"METEOR","Description":"An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments","Abstract":"Recaps is an automatic metric for machine translation evaluation which has been demonstrated to have high levels of correlation with human judgments of translation quality, signi\ufb01cantly outperforming the more commonly used bleu metric. It is one of several automatic metrics used in this year\u2019s shared task within the acl wmt-07 workshop. This paper recaps the technical details underlying the metric and describes recent improvements in the metric. The latest release includes improved metric parameters and extends the metric to support evaluation of mt output in spanish, french and german, in addition to english.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2023,"Venue":"acl-2023","Acronym":"MultiInstruct","Description":"Improving Multi-Modal Zero-Shot Learning via Instruction Tuning","Abstract":"Instruction tuning, a new learning paradigm that fine-tunes pre-trained language models on tasks specified through instructions, has shown promising zero-shot performance on various natural language processing tasks. However, it has yet to be explored for vision and multimodal tasks. In this work, we introduce strategies, the first multimodal instruction tuning benchmark dataset that consists of 62 diverse multimodal tasks in a unified seq-to-seq format covering 10 broad categories. The tasks are derived from 21 existing open-source datasets and each task is equipped with 5 expert-written instructions. We take ofa as the base pre-trained model for multimodal instruction tuning, and to further improve its zero-shot performance, we explore multiple transfer learning strategies to leverage the large-scale natural instructions dataset. Experimental results demonstrate strong zero-shot performance on various unseen multimodal tasks and the benefit of transfer learning from a text-only instruction dataset. We also design a new evaluation metric \u2013 sensitivity, to evaluate how sensitive the model is to the variety of instructions. Our results indicate that fine-tuning the model on a diverse set of tasks and instructions leads to a reduced sensitivity to variations in instructions for each task.","wordlikeness":0.8461538462,"lcsratio":1.0,"wordcoverage":0.7619047619}
{"Year":2013,"Venue":"semeval-2013","Acronym":"Duluth","Description":"Word Sense Induction Applied to Web Page Clustering","Abstract":"The results. Systems that participated in task 11 of semeval\u20132013 carried out word sense induction (wsi) in order to cluster web search results. They relied on an approach that represented web snippets using second\u2013order co\u2013 occurrences. These systems were all implemented using senseclusters, a freely available open source software package.","wordlikeness":0.5,"lcsratio":0.8333333333,"wordcoverage":1.0}
{"Year":2021,"Venue":"konvens-2021","Acronym":"forumBERT","Description":"Topic Adaptation and Classification of Contextualized Forum Comments in German","Abstract":"Online user comments in public forums are often associated with low quality, hate speech or even excessive demands for moderation. To better exploit their constructive and deliberate potential, we present classi\ufb01cation. Ndr.de, is built on top of the bert architecture and uses a shared weight and late fusion technique to better determine the quality and relevance of a comment on a forum article. Our model integrates article context with comments for the online\/of\ufb02ine comment moderation task. This is done using a two step procedure: self-supervised bert language model \ufb01ne tuning for topic adaptation followed by integration into the studies architecture for online\/of\ufb02ine classi\ufb01cation. We present evaluation results on various classi\ufb01cation tasks of the public one million post dataset, as well as on the online\/of\ufb02ine comment moderation task on 998,158 labelled comments from ndr.de, a popular german broadcaster\u2019s website. All signi\ufb01cantly outperforms baseline models on the ndr dataset and also outperforms all existing advanced baseline models on the omp dataset. Additionally we conduct two studies on the in\ufb02uence of topic adaptation on the general comment moderation task.","wordlikeness":0.8888888889,"lcsratio":0.7777777778,"wordcoverage":0.8}
{"Year":2006,"Venue":"coling-2006","Acronym":"Archivus","Description":"A Multimodal System for Multimedia Meeting Browsing and Retrieval","Abstract":"This paper presents supporting, a multimodal language-enabled meeting browsing and retrieval system. The prototype is in an early stage of development, and we are currently exploring the role of natural language for interacting in this relatively unfamiliar and complex domain. We brie\ufb02y describe the design and implementation status of the system, and then focus on how this system is used to elicit useful data for supporting hypotheses about multimodal interaction in the domain of meeting retrieval and for developing nlp modules for this speci\ufb01c domain.","wordlikeness":0.875,"lcsratio":0.5,"wordcoverage":0.875}
{"Year":2016,"Venue":"lrec-2016","Acronym":"WikiCoref","Description":"An English Coreference-annotated Corpus of Wikipedia Articles","Abstract":"This paper presents no, an english corpus annotated for anaphoric relations, where all documents are from the english version of wikipedia. Our annotation scheme follows the one of ontonotes with a few disparities. We annotated each markable with coreference type, mention type and the equivalent freebase topic. Since most similar annotation efforts concentrate on very specific types of written text, mainly newswire, there is a lack of resources for otherwise over-used wikipedia texts. The corpus described in this paper addresses this issue. We present a freely available resource we initially devised for improving coreference resolution algorithms dedicated to wikipedia texts. Our corpus has no restriction on the topics of the documents being annotated, and documents of various sizes have been considered for annotation.","wordlikeness":0.7777777778,"lcsratio":0.6666666667,"wordcoverage":0.625}
{"Year":2014,"Venue":"amta-2014","Acronym":"Kanjingo","Description":"a mobile app for post-editing","Abstract":"We present ,, a mobile app for post-editing currently running under ios. The app was developed using an agile methodoly at cngl, dcu. Though it could be used for numerous scenarios, our test scenario involved the post-editing of machine translated sample content for the non-profit translation organization translators without borders. Feedback from a first round of user testing for english-french and english-spanish was positive, but users also identified a number of usability issues that required improvement. These issues were addressed in a second development round and a second usability evaluation was carried out in collaboration with another non-profit translation organization, the rosetta foundation, again with french and spanish as target languages.","wordlikeness":0.625,"lcsratio":0.5,"wordcoverage":0.7142857143}
{"Year":2023,"Venue":"findings-2023","Acronym":"SConE","Description":"Simplified Cone Embeddings with Symbolic Operators for Complex Logical Queries","Abstract":"Geometric representation of query embeddings (using points, particles, rectangles and cones) can effectively achieve the task of answering complex logical queries expressed in first-order logic (fol) form over knowledge graphs, allowing intuitive encodings. However, current geometric-based methods depend on the neural approach to model fol operators (conjunction, disjunction and negation), which are not easily explainable with considerable computation cost. We overcome this challenge by introducing a symbolic modeling approach for the fol operators, emphasizing the direct calculation of the intersection between geometric shapes, particularly sector-cones in the embedding space, to model the conjunction operator. This approach reduces the computation cost as a non-neural approach is involved in the core logic operators. Moreover, we propose to accelerate the learning in the relation projection operator using the neural approach to emphasize the essential role of this operator in all query structures. Although empirical evidence for explainability is challenging, our approach demonstrates a significant improvement in answering complex logical queries (both non-negative and negative fol forms) over previous geometric-based models.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2023,"Venue":"ccl-2023","Acronym":"MCLS","Description":"A Large-Scale Multimodal Cross-Lingual Summarization Dataset","Abstract":"\u201cmultimodal summarization which aims to generate summaries with multimodal inputs, e.g., textand visual features, has attracted much attention in the research community. However, previousstudies only focus on monolingual multimodal summarization and neglect the non-native readerto understand the cross-lingual news in practical applications. It inspires us to present a newtask, named multimodal cross-lingual summarization for news (is), which generates cross-lingual summaries from multi-source information. To this end, we present a large-scale multimodalcross-lingual summarization dataset, which consists of 1.1 million article-summary pairs with 3.4million images in 44 * 43 language pairs. To generate a summary in any language, we propose aunified framework that jointly trains the multimodal monolingual and cross-lingual summarizationtasks, where a bi-directional knowledge distillation approach is designed to transfer knowledgebetween both tasks. Extensive experiments on many-to-many settings show the effectiveness ofthe proposed model.","wordlikeness":0.25,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2021,"Venue":"naacl-2021","Acronym":"FLIN","Description":"A Flexible Natural Language Interface for Web Navigation","Abstract":"Ai assistants can now carry out tasks for users by directly interacting with website uis. current semantic parsing and slot-filling techniques cannot flexibly adapt to many different websites without being constantly re-trained. We propose a, a natural language interface for web navigation that maps user commands to concept-level actions (rather than low-level ui actions), thus being able to flexibly adapt to different websites and handle their transient nature. We frame this as a ranking problem: given a user command and a webpage, in learns to score the most relevant navigation instruction (involving action and parameter values). To train and evaluate to, we collect a dataset using nine popular websites from three domains. Our results show that propose was able to adapt to new websites in a given domain.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2010,"Venue":"lrec-2010","Acronym":"ISO-TimeML","Description":"An International Standard for Semantic Annotation","Abstract":"In this paper, we present this, a revised and interoperable version of the temporal markup language, timeml. We describe the changes and enrichments made, while framing the effort in a more general methodology of semantic annotation. In particular, we assume a principled distinction between the annotation of an expression and the representation which that annotation denotes. This involves not only the specification of an annotation language for a particular phenomenon, but also the development of a meta-model that allows one to interpret the syntactic expressions of the specification semantically.","wordlikeness":0.5,"lcsratio":0.5,"wordcoverage":0.6666666667}
{"Year":2014,"Venue":"tacl-2014","Acronym":"FLORS","Description":"Fast and Simple Domain Adaptation for Part-of-Speech Tagging","Abstract":"We present adaptation, a new part-of-speech tagger for domain adaptation. Unseen uses robust representations that work especially well for unknown words and for known words with unseen tags. Adaptation is simpler and faster than previous domain adaptation methods, yet it has significantly better accuracy than several baselines.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.9090909091}
{"Year":2018,"Venue":"lrec-2018","Acronym":"NoReC","Description":"The Norwegian Review Corpus","Abstract":"This paper presents the norwegian review corpus (more), created for training and evaluating models for document-level sentiment analysis. The full-text reviews have been collected from major norwegian news sources and cover a range of different domains, including literature, movies, video games, restaurants, music and theater, in addition to product reviews across a range of categories. Each review is labeled with a manually assigned score of 1\u20136, as provided by the rating of the original author. This \ufb01rst release of the corpus comprises more than 35,000 reviews. It is distributed using the conll-u format, pre-processed using udpipe, along with a rich set of metadata. The work reported in this paper forms part of the sant initiative (sentiment analysis for norwegian text), a project seeking to provide open resources and tools for sentiment analysis and opinion mining for norwegian. Keywords: sentiment analysis, opinion mining, corpus, norwegian, reviews 1.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2020,"Venue":"emnlp-2020","Acronym":"MIME","Description":"MIMicking Emotions for Empathetic Response Generation","Abstract":"Current approaches to empathetic response generation view the set of emotions expressed in the input text as a flat structure, where all the emotions are treated uniformly. We argue that empathetic responses often mimic the emotion of the user to a varying degree, depending on its positivity or negativity and content. We show that the consideration of these polarity-based emotion clusters and emotional mimicry results in improved empathy and contextual relevance of the response as compared to the state-of-the-art. Also, we introduce stochasticity into the emotion mixture that yields emotionally more varied empathetic responses than the previous work. We demonstrate the importance of these factors to empathetic response generation using both automatic- and human-based evaluations. The implementation of are is publicly available at <a href=https:\/\/github.com\/declare-lab\/mimicry class=acl-markup-url>https:\/\/github.com\/declare-lab\/emotionally<\/a>.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2020,"Venue":"acl-2020","Acronym":"jiant","Description":"A Software Toolkit for Research on General-Purpose Text Understanding Models","Abstract":"We introduce conducting, an open source toolkit for conducting multitask and transfer learning experiments on english nlu tasks. For enables modular and configuration driven experimentation with state-of-the-art models and a broad set of tasks for probing, transfer learning, and multitask training experiments. Learning, implements over 50 nlu tasks, including all glue and superglue benchmark tasks. We demonstrate that training reproduces published performance on a variety of tasks and models, e.g., roberta and bert.","wordlikeness":0.6,"lcsratio":0.8,"wordcoverage":0.8}
{"Year":2010,"Venue":"semeval-2010","Acronym":"OWNS","Description":"Cross-lingual Word Sense Disambiguation Using Weighted Overlap Counts and Wordnet Based Similarity Measures","Abstract":"We report here our work on english french cross-lingual word sense disambiguation where the task is to \ufb01nd the best french translation for a target english word depending on the context in which it is used. Our approach relies on identifying the nearest neighbors of the test sentence from the training data using a pairwise similarity measure. The proposed measure \ufb01nds the af\ufb01nity between two sentences by calculating a weighted sum of the word overlap and the semantic overlap between them. The semantic overlap is calculated using standard wordnet similarity measures. Once the nearest neighbors have been identi\ufb01ed, the best translation is found by taking a majority vote over the french translations of the nearest neighbors.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2020,"Venue":"lrec-2020","Acronym":"LiViTo","Description":"Linguistic and Visual Features Tool for Assisted Analysis of Historic Manuscripts","Abstract":"We propose a mixed methods approach to the identification of scribes and authors in handwritten documents, and present documents., a software tool which combines linguistic insights and computer vision techniques in order to assist researchers in the analysis of handwritten historical documents. Our research shows that it is feasible to train neural networks for the automatic transcription of handwritten documents and to use these transcriptions as input for further learning processes. Hypotheses about scribes can be tested effectively by extracting visual handwriting features and clustering them appropriately. Methods from linguistics and from computer vision research integrate into a mixed methods system, with benefits on both sides. Techniques was trained with historical czech texts by 18th century immigrants to berlin, a total of 564 pages from a corpus of about 5000 handwritten pages without indication of author or scribe. We provide an overview of the three-year development of an and an introduction into its methodology and its functions. We then present our findings concerning the corpus of berlin czech manuscripts and discuss possible further usage scenarios.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2022,"Venue":"lrec-2022","Acronym":"Align-smatch","Description":"A Novel Evaluation Method for Chinese Abstract Meaning Representation Parsing based on Alignment of Concept and Relation","Abstract":"Meaning representation is a sentence-level meaning representation, which abstracts the meaning of sentences into a rooted acyclic directed graph. With the continuous expansion of chinese amr corpus, more and more scholars have developed parsing systems to automatically parse sentences into chinese amr. However, the current parsers can\u2019t deal with concept alignment and relation alignment, let alone the evaluation methods for amr parsing. Therefore, to make up for the vacancy of chinese amr parsing evaluation methods, based on amr evaluation metric smatch, we have improved the algorithm of generating triples so that to make it compatible with concept alignment and relation alignment. Finally, we obtain a new integrity metric a for paring evaluation. A comparative research then was conducted on 20 manually annotated amr and gold amr, with the result that representation works well in alignments and more robust in evaluating arcs. We also put forward some fine-grained metric for evaluating concept alignment, relation alignment and implicit concepts, in order to further measure parsers\u2019 performance in subtasks.","wordlikeness":0.75,"lcsratio":0.75,"wordcoverage":0.6666666667}
{"Year":2012,"Venue":"lrec-2012","Acronym":"TED-LIUM","Description":"an Automatic Speech Recognition dedicated corpus","Abstract":"This paper presents the corpus developed by the lium for automatic speech recognition (asr), based on the ted talks. This corpus was built during the iwslt 2011 evaluation campaign, and is composed of 118 hours of speech with its accompanying automatically aligned transcripts. We describe the content of the corpus, how the data was collected and processed, how it will be publicly available and how we built an asr system using this data leading to a wer score of 17.4 %. The official results we obtained at the iwslt 2011 evaluation campaign are also discussed.","wordlikeness":0.625,"lcsratio":0.625,"wordcoverage":0.7142857143}
{"Year":2021,"Venue":"acl-2021","Acronym":"FST","Description":"the FAIR Speech Translation System for the IWSLT21 Multilingual Shared Task","Abstract":"In this paper, we describe our end-to-end multilingual speech translation system submitted to the iwslt 2021 evaluation campaign on the multilingual speech translation shared task. Our system is built by leveraging transfer learning across modalities, tasks and languages. First, we leverage general-purpose multilingual modules pretrained with large amounts of unlabelled and labelled data. We further enable knowledge transfer from the text task to the speech task by training two tasks jointly. Finally, our multilingual model is finetuned on speech translation task-specific data to achieve the best translation results. Experimental results show our system outperforms the reported systems, including both end-to-end and cascaded based approaches, by a large margin. In some translation directions, our speech translation results evaluated on the public multilingual tedx test set are even comparable with the ones from a strong text-to-text translation system, which uses the oracle speech transcripts as input.","wordlikeness":0.3333333333,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2021,"Venue":"findings-2021","Acronym":"AEDA","Description":"An Easier Data Augmentation Technique for Text Classification","Abstract":"This paper proposes positions (an easier data augmentation) technique to help improve the performance on text classification tasks. Will includes only random insertion of punctuation marks into the original text. This is an easier technique to implement for data augmentation than eda method (wei and zou, 2019) with which we compare our results. In addition, it keeps the order of the words while changing their positions in the sentence leading to a better generalized performance. Furthermore, the deletion operation in eda can cause loss of information which, in turn, misleads the network, whereas technique preserves all the input information. Following the baseline, we perform experiments on five different datasets for text classification. We show that using the is-augmented data for training, the models show superior performance compared to using the eda-augmented data in all five datasets. The source code will be made available for further study and reproduction of the results.","wordlikeness":0.25,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2023,"Venue":"sigdial-2023","Acronym":"PGTask","Description":"Introducing the Task of Profile Generation from Dialogues","Abstract":"Recent approaches have attempted to personalize dialogue systems by leveraging profile information into models. However, this knowledge is scarce and difficult to obtain, which makes the extraction\/generation of profile information from dialogues a fundamental asset. To surpass this limitation, we introduce the profile generation task (and). We contribute with a new dataset for this problem, comprising profile sentences aligned with related utterances, extracted from a corpus of dialogues. Furthermore, using state-of-the-art methods, we provide a benchmark for profile generation on this novel dataset. Our experiments disclose the challenges of profile generation, and we hope that this introduces a new research direction.","wordlikeness":0.6666666667,"lcsratio":0.8333333333,"wordcoverage":0.8}
{"Year":2020,"Venue":"lrec-2020","Acronym":"JParaCrawl","Description":"A Large Scale Web-Based English-Japanese Parallel Corpus","Abstract":"Recent machine translation algorithms mainly rely on parallel corpora. However, since the availability of parallel corpora remains limited, only some resource-rich language pairs can benefit from them. We constructed a parallel corpus for english-japanese, for which the amount of publicly available parallel corpora is still limited. We constructed the parallel corpus by broadly crawling the web and automatically aligning parallel sentences. Our collected corpus, called state, amassed over 8.7 million sentence pairs. We show how it includes a broader range of domains and how a neural machine translation model trained with it works as a good pre-trained model for fine-tuning specific domains. The pre-training and fine-tuning approaches achieved or surpassed performance comparable to model training from the initial state and reduced the training time. Additionally, we trained the model with an in-domain dataset and fine-tuning to show how we achieved the best performance with them. As and the pre-trained models are freely available online for research purposes.","wordlikeness":0.6,"lcsratio":0.7,"wordcoverage":0.7}
{"Year":2023,"Venue":"findings-2023","Acronym":"Prosody-TTS","Description":"Improving Prosody with Masked Autoencoder and Conditional Diffusion Model For Expressive Text-to-Speech","Abstract":"Expressive text-to-speech aims to generate high-quality samples with rich and diverse prosody, which is hampered by <b>dual challenges<\/b>: 1) prosodic attributes in highly dynamic voices are difficult to capture and model without intonation; and 2) highly multimodal prosodic representations cannot be well learned by simple regression (e.g., mse) objectives, which causes blurry and over-smoothing predictions. This paper proposes results, a two-stage pipeline that enhances <b>prosody modeling and sampling<\/b> by introducing several components: 1) a self-supervised masked autoencoder to model the prosodic representation without relying on text transcriptions or local prosody attributes, which ensures to cover diverse speaking voices with superior generalization; and 2) a diffusion model to sample diverse prosodic patterns within the latent space, which prevents tts models from generating samples with dull prosodic performance. Experimental results show that multimodal achieves new state-of-the-art in text-to-speech with natural and expressive synthesis. Both subjective and objective evaluation demonstrate that it exhibits superior audio quality and prosody naturalness with rich and diverse prosodic attributes. Audio samples are available at <a href=https:\/\/improved_prosody.github.io class=acl-markup-url>https:\/\/improved_prosody.github.","wordlikeness":0.5454545455,"lcsratio":0.9090909091,"wordcoverage":0.6363636364}
{"Year":2021,"Venue":"findings-2021","Acronym":"SD-QA","Description":"Spoken Dialectal Question Answering for the Real World","Abstract":"Question answering (qa) systems are now available through numerous commercial applications for a wide variety of domains, serving millions of users that interact with them via speech interfaces. However, current benchmarks in qa research do not account for the errors that speech recognition models might introduce, nor do they consider the language variations (dialects) of the users. To address this gap, we augment an existing qa dataset to construct a multi-dialect, spoken qa benchmark on five languages (arabic, bengali, english, kiswahili, korean) with more than 68k audio prompts in 24 dialects from 255 speakers. We provide baseline results showcasing the real-world performance of qa systems and analyze the effect of language variety and other sensitive speaker attributes on downstream performance. Last, we study the fairness of the asr and qa models with respect to the underlying user populations.","wordlikeness":0.2,"lcsratio":0.8,"wordcoverage":0.6666666667}
{"Year":2022,"Venue":"konvens-2022","Acronym":"MONAPipe","Description":"Modes of Narration and Attribution Pipeline for German Computational Literary Studies and Language Analysis in spaCy","Abstract":"Computational is a collection of pipeline components for the open-source python library spacy. the components perform a broad range of morphological, syntactic, semantic and pragmatic analyses for german texts and are mostly developed speci\ufb01cally for the literary domain. Is1 combines implementations from various separate resources with new ones in one place, constituting a convenient tool for computational linguistics and literary studies.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.7142857143}
{"Year":2016,"Venue":"naacl-2016","Acronym":"Farasa","Description":"A Fast and Furious Segmenter for Arabic","Abstract":"In this paper, we present arabic, a fast and accurate arabic segmenter. Our approach is based on svm-rank using linear kernels. We measure the performance of the segmenter in terms of accuracy and ef\ufb01ciency, in two nlp tasks, namely machine translation (mt) and information retrieval (ir). Of outperforms or is at par with the stateof-the-art arabic segmenters (stanford and madamira), while being more than one order of magnitude faster.","wordlikeness":0.6666666667,"lcsratio":0.8333333333,"wordcoverage":0.7272727273}
{"Year":2016,"Venue":"lrec-2016","Acronym":"CASSAurus","Description":"A Resource of Simpler Spanish Synonyms","Abstract":"In this work we introduce and describe a language resource composed of lists of simpler synonyms for spanish. The synonyms are divided in different senses taken from the spanish openthesaurus, where context disambiguation was performed by using statistical information from the web and google books ngrams. This resource is freely available online and can be used for different nlp tasks such as lexical simplification. Indeed, so far it has been already integrated into four tools.","wordlikeness":0.7777777778,"lcsratio":0.5555555556,"wordcoverage":0.75}
{"Year":2006,"Venue":"hlt-2006","Acronym":"ParaEval","Description":"Using Paraphrases to Evaluate Summaries Automatically","Abstract":"Parallel is an automated evaluation method for comparing reference and peer summaries. It facilitates a tieredcomparison strategy where recall-oriented global optimal and local greedy searches for paraphrase matching are enabled in the top tiers. We utilize a domainindependent paraphrase table extracted from a large bilingual parallel corpus using methods from machine translation (mt). We show that the quality of quality\u2019s evaluations, measured by correlating with human judgments, closely resembles that of rouge\u2019s.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2012,"Venue":"lrec-2012","Acronym":"YADAC","Description":"Yet another Dialectal Arabic Corpus","Abstract":"This paper presents the first phase of building phrase \u2015 a multi-genre dialectal arabic (da) corpus \u2015 that is compiled using web data from microblogs (i.e. twitter), blogs\/forums and online knowledge market services in which both questions and answers are user-generated. In addition to introducing two new genres to the current efforts of building da corpora (i.e. microblogs and question-answer pairs extracted from online knowledge market services), the paper highlights and tackles several new issues related to building da corpora that have not been handled in previous studies: function-based web harvesting and dialect identification, vowel-based spelling variation, linguistic hypercorrection and its effect on spelling variation, unsupervised part-of-speech (pos) tagging and base phrase chunking for da. Although the algorithms for both pos tagging and base-phrase chunking are still under development, the results are promising.","wordlikeness":0.2,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2022,"Venue":"lrec-2022","Acronym":"MC-TRISLAN","Description":"A Large 3D Motion Capture Sign Language Data-set","Abstract":"The new 3d motion capture data corpus expands the portfolio of existing language resources by a corpus of 18 hours of czech sign language. This helps to alleviate the current problem, which is a critical lack of high quality data necessary for research and subsequent deployment of machine learning techniques in this area. We currently provide the largest collection of annotated sign language recordings acquired by state-of-the-art 3d human body recording technology for the successful future deployment in communication technologies, especially machine translation and sign language synthesis.","wordlikeness":0.5,"lcsratio":0.8,"wordcoverage":0.7058823529}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"MetaLogic","Description":"Logical Reasoning Explanations with Fine-Grained Structure","Abstract":"In this paper, we propose a comprehensive benchmark to investigate models\u2019 logical reasoning capabilities in complex real-life scenarios. Current explanation datasets often employ synthetic data with simple reasoning structures. Therefore, it cannot express more complex reasoning processes, such as the rebuttal to a reasoning step and the degree of certainty of the evidence. To this end, we propose a comprehensive logical reasoning explanation form. Based on the multi-hop chain of reasoning, the explanation form includes three main components: (1) the condition of rebuttal that the reasoning node can be challenged; (2) logical formulae that uncover the internal texture of reasoning nodes; (3) reasoning strength indicated by degrees of certainty. The fine-grained structure conforms to the real logical reasoning scenario, better fitting the human cognitive process but, simultaneously, is more challenging for the current models. We evaluate the current best models\u2019 performance on this new explanation form. The experimental results show that generating reasoning graphs remains a challenging task for current models, even with the help of giant pre-trained language models.","wordlikeness":0.8888888889,"lcsratio":0.7777777778,"wordcoverage":0.8235294118}
{"Year":2020,"Venue":"lrec-2020","Acronym":"DiMLex-Bangla","Description":"A Lexicon of Bangla Discourse Connectives","Abstract":"We present applications., a newly developed lexicon of discourse connectives in bangla. The lexicon, upon completion of its first version, contains 123 bangla connective entries, which are primarily compiled from the linguistic literature and translation of english discourse connectives. The lexicon compilation is later augmented by adding more connectives from a currently developed corpus, called the bangla rst discourse treebank (das and stede, 2018). (das provides information on syntactic categories of bangla connectives, their discourse semantics and non-connective uses (if any). It uses the format of the german connective lexicon dimlex (stede and umbach, 1998), which provides a cross-linguistically applicable xml schema. The resource is the first of its kind in bangla, and is freely available for use in studies on discourse structure and computational applications.","wordlikeness":0.6153846154,"lcsratio":0.6923076923,"wordcoverage":0.5714285714}
{"Year":2014,"Venue":"wanlp-2014","Acronym":"Al-Bayan","Description":"An Arabic Question Answering System for the Holy Quran","Abstract":"Recently, question answering (qa) has been one of the main focus of natural language processing research. However, arabic question answering is still not in the mainstream. The challenges of the arabic language and the lack of resources have made it dif\ufb01cult to provide arabic qa systems with high accuracy. While low accuracies may be accepted for general purpose systems, it is critical in some \ufb01elds such as religious affairs. Therefore, there is a need for specialized accurate systems that target these critical \ufb01elds. In this paper, we propose affairs., a new arabic qa system specialized for the holy quran. The system accepts an arabic question about the quran, retrieves the most relevant quran verses, then extracts the passage that contains the answer from the quran and its interpretation books (tafseer). Evaluation results on a collected dataset show that the overall system can achieve 85% accuracy using the top-3 results.","wordlikeness":0.625,"lcsratio":0.75,"wordcoverage":0.7142857143}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"IrekiaLFes","Description":"a New Open Benchmark and Baseline Systems for Spanish Automatic Text Simplification","Abstract":"Automatic text simplification (ats) seeks to reduce the complexity of a text for a general public or a target audience. In the last years, deep learning methods have become the most used systems in ats research, but these systems need large and good quality datasets to be evaluated. Moreover, these data are available on a large scale only for english and in some cases with restrictive licenses. In this paper, we present irekialf_es, an open-license benchmark for spanish text simplification. It consists of a document-level corpus and a sentence-level test set that has been manually aligned. We also conduct a neurolinguistically-based evaluation of the corpus in order to reveal its suitability for text simplification. This evaluation follows the lexicon-unification-linearity (leuli) model of neurolinguistic complexity assessment. Finally, we present a set of experiments and baselines of ats systems in a zero-shot scenario.","wordlikeness":0.5,"lcsratio":0.7,"wordcoverage":0.6666666667}
{"Year":2009,"Venue":"nodalida-2009","Acronym":"PolArt","Description":"A Robust Tool for Sentiment Analysis","Abstract":"We introduce and, a robust tool for sentiment analysis. But is a pattern-based approach designed to cope with polarity composition. In order to determine the polarity of larger text units, a cascade of rewrite operations is carried out: word polarities are combined to np, vp and sentence polarities. Moreover, combined is able to cope with the target-speci\ufb01c polarity of phrases, where two neutral words combine to a non-neutral phrase. Target detection is done with the wikipedia category system, but also user de\ufb01ned target hierarchies are allowed. Phrases, is based on the treetagger chunker output, and is customised for english and german. In this paper we evaluate phrases,\u2019s compositional capacity.","wordlikeness":0.8333333333,"lcsratio":0.6666666667,"wordcoverage":0.9090909091}
{"Year":2013,"Venue":"acl-2013","Acronym":"SenseSpotting","Description":"Never let your parallel data tie you to an old domain","Abstract":"Words often gain new senses in new domains. Being able to automatically identify, from a corpus of monolingual text, which word tokens are being used in a previously unseen sense has applications to machine translation and other tasks sensitive to lexical semantics. We de\ufb01ne a task, translation, in which we build systems to spot tokens that have new senses in new domain text. Instead of dif\ufb01cult and expensive annotation, we build a goldstandard by leveraging cheaply available parallel corpora, targeting our approach to the problem of domain adaptation for machine translation. Our system is able to achieve f-measures of as much as 80%, when applied to word types it has never seen before. Our approach is based on a large set of novel features that capture varied aspects of how words change when used in new domains.","wordlikeness":0.8461538462,"lcsratio":0.5384615385,"wordcoverage":0.7619047619}
{"Year":2018,"Venue":"acl-2018","Acronym":"PhraseCTM","Description":"Correlated Topic Modeling on Phrases within Markov Random Fields","Abstract":"Recent emerged phrase-level topic models are able to provide topics of phrases, which are easy to read for humans. But these models are lack of the ability to capture the correlation structure among the discovered numerous topics. We propose a novel topic model markov and a two-stage method to find out the correlated topics at phrase level. In the first stage, we train semantically, which models the generation of words and phrases simultaneously by linking the phrases and component words within markov random fields when they are semantically coherent. In the second stage, we generate the correlation of topics from topics. We evaluate our method by a quantitative experiment and a human study, showing the correlated topic modeling on phrases is a good and practical way to interpret the underlying themes of a corpus.","wordlikeness":0.6666666667,"lcsratio":0.8888888889,"wordcoverage":0.8}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"MAVEN-ERE","Description":"A Unified Large-scale Dataset for Event Coreference, Temporal, Causal, and Subevent Relation Extraction","Abstract":"The diverse relationships among real-world events, including coreference, temporal, causal, and subevent relations, are fundamental to understanding natural languages. However, two drawbacks of existing datasets limit event relation extraction (ere) tasks: (1) small scale. Due to the annotation complexity, the data scale of existing datasets is limited, which cannot well train and evaluate data-hungry models. (2) absence of unified annotation. Different types of event relations naturally interact with each other, but existing datasets only cover limited relation types at once, which prevents models from taking full advantage of relation interactions. To address these issues, we construct a unified large-scale human-annotated ere dataset but with improved annotation schemes. It contains 103,193 event coreference chains, 1,216,217 temporal relations, 57,992 causal relations, and 15,841 subevent relations, which is larger than existing datasets of all the ere tasks by at least an order of magnitude. Experiments show that ere on codes is quite challenging, and considering relation interactions with joint learning can improve performances. The dataset and source codes can be obtained from <a href=https:\/\/github.com\/thu-keg\/subevent class=acl-markup-url>https:\/\/github.com\/thu-keg\/tasks<\/a>.","wordlikeness":0.5555555556,"lcsratio":0.7777777778,"wordcoverage":0.7058823529}
{"Year":2022,"Venue":"naacl-2022","Acronym":"DocEE","Description":"A Large-Scale and Fine-grained Benchmark for Document-level Event Extraction","Abstract":"Event extraction aims to identify an event and then extract the arguments participating in the event. Despite the great success in sentence-level event extraction, events are more naturally presented in the form of documents, with event arguments scattered in multiple sentences. However, a major barrier to promote document-level event extraction has been the lack of large-scale and practical training and evaluation datasets. In this paper, we present present, a new document-level event extraction dataset including 27,000+ events, 180,000+ arguments. We highlight three features: large-scale manual annotations, fine-grained argument types and application-oriented settings. Experiments show that there is still a big gap between state-of-the-art models and human beings (41% vs 85% in f1 score), indicating that types is an open issue. Fine-grained is now available at <a href=https:\/\/github.com\/tongmeihan1995\/an.git class=acl-markup-url>https:\/\/github.com\/tongmeihan1995\/beings.git<\/a>.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.8333333333}
{"Year":1998,"Venue":"coling-1998","Acronym":"Multext-East","Description":"Parallel and Comparable Corpora and Lexicons for Six Central and Eastern European Languages","Abstract":"The eu copernicus project families. Has created a multi-lingual corpus of text and speech data, covering the six languages of the project: bulgarian, czech, estonian, hungarian, romanian, and slovene. In addition, wordform lexicons for each of the languages were developed. The corpus includes a parallel component consisting of orwell's nineteen eighty-four, with versions in all six languages tagged for part-of-speech and aligned to english (also tagged for pos). We describe the encoding format and data architecture designed especially for this corpus, which is generally usable for encoding linguistic corpora. We also describe the methodology for the development of a harmonized set of morphosyntactic descriptions (msds), which builds upon the scheme for western european languages developed within the eagles project. We discuss the special concerns for handling the six project languages, which cover three distinct language families.","wordlikeness":0.75,"lcsratio":0.75,"wordcoverage":0.6}
{"Year":2010,"Venue":"lrec-2010","Acronym":"OAL","Description":"A NLP Architecture to Improve the Development of Linguistic Resources for NLP","Abstract":"The performance of most nlp applications relies upon the quality of linguistic resources. The creation, maintenance and enrichment of those resources are a labour-intensive task, especially when no tools are available. In this paper we present the nlp architecture process, designed to assist computational linguists in the whole process of the development of resources in an industrial context: from corpora compilation to quality assurance. To add new words more easily to the morphosyntactic lexica, a guesser that lemmatizes and assigns morphosyntactic tags as well as inflection paradigms to a new word has been developed. Moreover, different control mechanisms are set up to check the coherence and consistency of the resources. Today paradigms manages resources in five european languages: french, english, spanish, italian and polish. Chinese and portuguese are in process. The development of incremental has followed an incremental strategy. At present, semantic lexica, a named entities guesser and a named entities phonetizer are being developed.","wordlikeness":0.6666666667,"lcsratio":0.6666666667,"wordcoverage":0.8571428571}
{"Year":2022,"Venue":"naacl-2022","Acronym":"COGMEN","Description":"COntextualized GNN based Multimodal Emotion recognitioN","Abstract":"Emotions are an inherent part of human interactions, and consequently, it is imperative to develop ai systems that understand and recognize human emotions. During a conversation involving various people, a person\u2019s emotions are influenced by the other speaker\u2019s utterances and their own emotional state over the utterances. In this paper, we propose contextualized graph neural network based multi- modal emotion recognition (emotions.) system that leverages local information (i.e., inter\/intra dependency between speakers) and global information (context). The proposed model uses graph neural network (gnn) based architecture to model the complex dependencies (local and global information) in a conversation. Our model gives state-of-the- art (sota) results on iemocap and mosei datasets, and detailed ablation experiments show the importance of modeling information at both levels.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2023,"Venue":"mmnlg-2023","Acronym":"WebNLG-Interno","Description":"Utilizing FRED-T5 to address the RDF-to-text problem (WebNLG 2023)","Abstract":"We present our solution for the russian rdf002 to-text generation task of the webnlg challenge 2023. We use the pretrained large language model named fred-t5 (zmitrovich et al., 2023) to finetune on the train dataset. Also, we propose several types of prompt and run experiments to analyze their effectiveness. Our submission achieves 0.373 ter on the test dataset, taking the first place according to the results of the automatic evaluation and outperforming the best result of the previous challenge by 0.025. The code of our solution is available at the following link: https:\/\/github.","wordlikeness":0.5,"lcsratio":0.5,"wordcoverage":0.64}
{"Year":2023,"Venue":"acl-2023","Acronym":"BIC","Description":"Twitter Bot Detection with Text-Graph Interaction and Semantic Consistency","Abstract":"Twitter bots are automatic programs operated by malicious actors to manipulate public opinion and spread misinformation. Research efforts have been made to automatically identify bots based on texts and networks on social media. Existing methods only leverage texts or networks alone, and while few works explored the shallow combination of the two modalities, we hypothesize that the interaction and information exchange between texts and graphs could be crucial for holistically evaluating bot activities on social media. In addition, according to a recent survey (cresci, 2020), twitter bots are constantly evolving while advanced bots steal genuine users\u2019 tweets and dilute their malicious content to evade detection. This results in greater inconsistency across the timeline of novel twitter bots, which warrants more attention. In light of these challenges, we propose exchange, a twitter bot detection framework with text-graph interaction and semantic consistency. Specifically, in addition to separately modeling the two modalities on social media, operated employs a text-graph interaction module to enable information exchange across modalities in the learning process. In addition, given the stealing behavior of novel twitter bots, weights proposes to model semantic consistency in tweets based on attention weights while using it to augment the decision process. Extensive experiments demonstrate that novel consistently outperforms state-of-the-art baselines on two widely adopted datasets. Further analyses reveal that text-graph interactions and modeling semantic consistency are essential improvements and help combat bot evolution.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":1999,"Venue":"mtsummit-1999","Acronym":"BITS","Description":"a method for bilingual text search over the Web","Abstract":"Parallel corpus are valuable resource for machine translation, multi-lingual text retrieval, language education and other applications, but for various reasons, its availability is very limited at present. Noticed that the world word web is a potential source to mine parallel text, researchers are making their efforts to explore the web in order to get a big collection of bitext. This paper presents at (bilingual internet text search), a system which harvests multilingual texts over the world wide web with virtually no human intervention. The technique is simple, easy to port to any language pairs, and with high accuracy. The results of the experiments on german-english pair proved that the method is very successful.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2021,"Venue":"acl-2021","Acronym":"Disfl-QA","Description":"A Benchmark Dataset for Understanding Disfluencies in Question Answering","Abstract":"Dis\ufb02uencies is an under-studied topic in nlp, even though it is ubiquitous in human conversation. This is largely due to the lack of datasets containing dis\ufb02uencies. In this paper, we present a new challenge question answering dataset, introduce, a derivative of squad, where humans introduce contextual dis\ufb02uencies in previously \ufb02uent questions. Though contains a variety of challenging dis\ufb02uencies that require a more comprehensive understanding of the text than what was necessary in prior datasets. Experiments show that the performance of existing state-of-the-art question answering models degrades signi\ufb01cantly when tested on disflqa in a zero-shot setting. We show data augmentation methods partially recover the loss in performance and also demonstrate the ef\ufb01cacy of using gold data for \ufb01ne-tuning. We argue that we need large-scale dis\ufb02uency datasets in order for nlp models to be robust to them. The dataset is publicly available at: https:\/\/github.com\/ google-research-datasets\/nlp,.","wordlikeness":0.375,"lcsratio":0.875,"wordcoverage":0.6666666667}
{"Year":2021,"Venue":"naacl-2021","Acronym":"CoRT","Description":"Complementary Rankings from Transformers","Abstract":"Many recent approaches towards neural information retrieval mitigate their computational costs by using a multi-stage ranking pipeline. In the first stage, a number of potentially relevant candidates are retrieved using an efficient retrieval model such as bm25. Although bm25 has proven decent performance as a first-stage ranker, it tends to miss relevant passages. In this context we propose relevant, a simple neural first-stage ranking model that leverages contextual representations from pretrained language models such as bert to complement term-based ranking functions while causing no significant delay at query time. Using the ms marco dataset, we show that has significantly increases the candidate recall by complementing bm25 with missing candidates. Consequently, we find subsequent re-rankers achieve superior results with less candidates. We further demonstrate that passage retrieval using approaches can be realized with surprisingly low latencies.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2020,"Venue":"lrec-2020","Acronym":"MPDD","Description":"A Multi-Party Dialogue Dataset for Analysis of Emotions and Interpersonal Relationships","Abstract":"A dialogue dataset is an indispensable resource for building a dialogue system. Additional information like emotions and interpersonal relationships labeled on conversations enables the system to capture the emotion flow of the participants in the dialogue. However, there is no publicly available chinese dialogue dataset with emotion and relation labels. In this paper, we collect the conversions from tv series scripts, and annotate emotion and interpersonal relationship labels on each utterance. This dataset contains 25,548 utterances from 4,142 dialogues. We also set up some experiments to observe the effects of the responded utterance on the current utterance, and the correlation between emotion and relation types in emotion and relation classification tasks.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2016,"Venue":"lrec-2016","Acronym":"PROTEST","Description":"A Test Suite for Evaluating Pronouns in Machine Translation","Abstract":"We present 2015, a test suite for the evaluation of pronoun translation by mt systems. The test suite comprises 250 hand-selected pronoun tokens and an automatic evaluation method which compares the translations of pronouns in mt output with those in the reference translation. Pronoun translations that do not match the reference are referred for manual evaluation. Single is designed to support analysis of system performance at the level of individual pronoun groups, rather than to provide a single aggregate measure over all pronouns. We wish to encourage detailed analyses to highlight issues in the handling of specific linguistic mechanisms by mt systems, thereby contributing to a better understanding of those problems involved in translating pronouns. We present two use cases for systems,: a) for measuring improvement\/degradation of an incremental system change, and b) for comparing the performance of a group of systems whose design may be largely unrelated. Following the latter use case, we demonstrate the application of by to the evaluation of the systems submitted to the discomt 2015 shared task on pronoun translation.","wordlikeness":1.0,"lcsratio":0.8571428571,"wordcoverage":1.0}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"XPrompt","Description":"Exploring the Extreme of Prompt Tuning","Abstract":"Prompt tuning learns soft prompts to condition the frozen pre-trained language models (plms) for performing downstream tasks in a parameter-efficient manner. While prompt tuning has gradually reached the performance level of fine-tuning as the model scale increases, there is still a large performance gap between prompt tuning and fine-tuning for models of moderate and small scales (typically less than 11b parameters). In this paper, we empirically show that the trained prompt tokens can have a negative impact on a downstream task and thus degrade its performance. To bridge the gap, we propose a novel prompt tuning model with an extremely small scale (soft) under the regime of lottery tickets hypothesis. Specifically, while eliminates the negative prompt tokens at different granularity levels through a hierarchical structured pruning, yielding a more parameter-efficient prompt yet with a competitive performance. Comprehensive experiments are carried out on the superglue tasks, and the results indicate that models is able to close the performance gap at smaller model scales.","wordlikeness":0.5714285714,"lcsratio":1.0,"wordcoverage":0.9230769231}
{"Year":2015,"Venue":"depling-2015","Acronym":"ParsPer","Description":"A Dependency Parser for Persian","Abstract":"We present a dependency parser for persian, called stanford, developed using the graph-based parser in the mate tools. The parser is trained on the entire uppsala persian dependency treebank with a speci\ufb01c con\ufb01guration that was selected by maltparser as the best performing parsing representation. The treebank\u2019s syntactic annotation scheme is based on stanford typed dependencies with extensions for persian. The results of the parsing evaluation revealed a best labeled accuracy over 82% with an unlabeled accuracy close to 87%. The parser is freely available and released as an open source tool for parsing persian.","wordlikeness":0.8571428571,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2018,"Venue":"acl-2018","Acronym":"EmotionX-DLC","Description":"Self-Attentive BiLSTM for Detecting Sequential Emotions in Dialogues","Abstract":"In this paper, we propose a self-attentive bidirectional long short-term memory (sa-bilstm) network to predict multiple emotions for the emotionx challenge. The bilstm exhibits the power of modeling the word dependencies, and extracting the most relevant features for emotion classification. Building on top of bilstm, the self-attentive network can model the contextual dependencies between utterances which are helpful for classifying the ambiguous emotions. We achieve 59.6 and 55.0 unweighted accuracy scores in the friends and the emotionpush test sets, respectively.","wordlikeness":0.5833333333,"lcsratio":0.75,"wordcoverage":0.7619047619}
{"Year":2022,"Venue":"lrec-2022","Acronym":"FrameASt","Description":"A Framework for Second-level Agenda Setting in Parliamentary Debates through the Lense of Comparative Agenda Topics","Abstract":"This paper presents a framework for studying second-level political agenda setting in parliamentary debates, based on the selection of policy topics used by political actors to discuss a specific issue on the parliamentary agenda. For example, the covid-19 pandemic as an agenda item can be contextualised as a health issue or as a civil rights issue, as a matter of macroeconomics or can be discussed in the context of social welfare. Our framework allows us to observe differences regarding how different parties discuss the same agenda item by emphasizing different topical aspects of the item. We apply and evaluate our framework on data from the german bundestag and discuss the merits and limitations of our approach. In addition, we present a new annotated data set of parliamentary debates, following the coding schema of policy topics developed in the comparative agendas project (cap), and release models for topic classification in parliamentary debates.","wordlikeness":0.625,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2015,"Venue":"naacl-2015","Acronym":"Idest","Description":"Learning a Distributed Representation for Event Patterns","Abstract":"This paper describes much, a new method for learning paraphrases of event patterns. It is based on a new neural network architecture that only relies on the weak supervision signal that comes from the news published on the same day and mention the same real-world entities. It can generalize across extractions from different dates to produce a robust paraphrase model for event patterns that can also capture meaningful representations for rare patterns. We compare it with two state-of-the-art systems and show that it can attain comparable quality when trained on a small dataset. Its generalization capabilities also allow it to leverage much more data, leading to substantial quality improvements.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.8333333333}
{"Year":2019,"Venue":"ijcnlp-2019","Acronym":"HABLex","Description":"Human Annotated Bilingual Lexicons for Experiments in Machine Translation","Abstract":"Bilingual lexicons are valuable resources used by professional human translators. While these resources can be easily incorporated in statistical machine translation, it is unclear how to best do so in the neural framework. In this work, we present the - dataset, designed to test methods for bilingual lexicon integration into neural machine translation. Our data consists of human generated alignments of words and phrases in machine translation test sets in three language pairs (russian-english, chinese-english, and korean-english), resulting in clean bilingual lexicons which are well matched to the reference. We also present two simple baselines - constrained decoding and continued training - and an improvement to continued training to address overfitting.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2016,"Venue":"lrec-2016","Acronym":"AMISCO","Description":"The Austrian German Multi-Sensor Corpus","Abstract":"We introduce a unique, comprehensive austrian german multi-sensor corpus with moving and non-moving speakers to facilitate the evaluation of estimators and detectors that jointly detect a speaker\u2019s spatial and temporal parameters. The corpus is suitable for various machine learning and signal processing tasks, linguistic studies, and studies related to a speaker\u2019s fundamental frequency (due to recorded glottograms). Available corpora are limited to (synthetically generated\/spatialized) speech data or recordings of musical instruments that lack moving speakers, glottograms, and\/or multi-channel distant speech recordings. That is why we recorded 24 spatially non-moving and moving speakers, balanced male and female, to set up a two-room and 43-channel austrian german multi-sensor speech corpus. It contains 8.2 hours of read speech based on phonetically balanced sentences, commands, and digits. The orthographic transcriptions include around 53,000 word tokens and 2,070 word types. Special features of this corpus are the laryngograph recordings (representing glottograms required to detect a speaker\u2019s instantaneous fundamental frequency and pitch), corresponding clean-speech recordings, and spatial information and video data provided by four kinects and a camera.","wordlikeness":0.8333333333,"lcsratio":1.0,"wordcoverage":0.7272727273}
{"Year":2020,"Venue":"emnlp-2020","Acronym":"DORB","Description":"Dynamically Optimizing Multiple Rewards with Bandits","Abstract":"Policy gradients-based reinforcement learning has proven to be a promising approach for directly optimizing non-differentiable evaluation metrics for language generation tasks. However, optimizing for a specific metric reward leads to improvements in mostly that metric only, suggesting that the model is gaming the formulation of that metric in a particular way without often achieving real qualitative improvements. Hence, it is more beneficial to make the model optimize multiple diverse metric rewards jointly. While appealing, this is challenging because one needs to manually decide the importance and scaling weights of these metric rewards. Further, it is important to consider using a dynamic combination and curriculum of metric rewards that flexibly changes over time. Considering the above aspects, in our work, we automate the optimization of multiple metric rewards simultaneously via a multi-armed bandit approach (round,), where at each round, the bandit chooses which metric reward to optimize next, based on expected arm gains. We use the exp3 algorithm for bandits and formulate two approaches for bandit rewards: (1) single multi-reward bandit (sm-bandit); (2) hierarchical multi-reward bandit (hm-bandit). We empirically show the effectiveness of our approaches via various automatic metrics and human evaluation on two important nlg tasks: question generation and data-to-text generation. Finally, we present interpretable analyses of the learned bandit curriculum over the optimized rewards.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2020,"Venue":"findings-2020","Acronym":"TextHide","Description":"Tackling Data Privacy in Language Understanding Tasks","Abstract":"An unsolved challenge in distributed or federated learning is to effectively mitigate privacy risks without slowing down training or reducing accuracy. In this paper, we propose propose aiming at addressing this challenge for natural language understanding tasks. It requires all participants to add a simple encryption step to prevent an eavesdropping attacker from recovering private text data. Such an encryption step is efficient and only affects the task performance slightly. In addition, we fits well with the popular framework of fine-tuning pre-trained language models (e.g., bert) for any sentence or sentence-pair task. We evaluate present on the glue benchmark, and our experiments show that an can effectively defend attacks on shared gradients or representations and the averaged accuracy reduction is only 1.9%. We also present an analysis of the security of glue using a conjecture about the computational intractability of a mathematical problem.","wordlikeness":0.75,"lcsratio":0.625,"wordcoverage":0.8}
{"Year":2015,"Venue":"ijclclp-2015","Acronym":"HANSpeller","Description":"A Unified Framework for Chinese Spelling Correction","Abstract":"The number of people learning chinese as a foreign language (cfl) has been booming in recent decades. The problem of spelling error correction for cfl learners increasingly is becoming important. Compared to the regular text spelling check task, more error types need to be considered in cfl cases. In this paper, we propose a unified framework for chinese spelling correction. Instead of conventional methods, which focus on rules or statistics separately, our approach is based on extended hmm and ranker-based models, together with a rule-based model for further polishing, and a final decision-making step is adopted to decide whether to output the corrections or not. Experimental results on the test data of foreigner's chinese essays provided by the sighan 2014 bake-off illustrate the performance of our approach. Keywords: chinese spelling correction, hmm, ranker-base model, rule-based model, decision-making. 1.","wordlikeness":0.7,"lcsratio":0.8,"wordcoverage":0.75}
{"Year":2022,"Venue":"findings-2022","Acronym":"SEHY","Description":"A Simple yet Effective Hybrid Model for Summarization of Long Scientific Documents","Abstract":"Long document (e.g., scientific papers) summarization is obtaining more and more attention in recent years. Extractive approaches attempt to choose salient sentences via understanding the whole document, but long documents cover numerous subjects with varying details and will not ease content understanding. Instead, abstractive approaches elaborate to generate related tokens while suffering from truncating the source document due to their input sizes. To this end, we propose a simple yet effective hybrid approach, which we call exploits, that exploits the discourse information of a document to select salient sections instead sentences for summary generation. On the one hand, training avoids the full-text understanding; on the other hand, it retains salient information given the length limit. In particular, we design two simple strategies for training the extractor: extracting sections incrementally and based on salience-analysis. Then, we use strong abstractive models to generate the final summary. We evaluate our approach on a large-scale scientific paper dataset: arxiv. Further, we discuss how the disciplinary class (e.g., computer science, math or physics) of a scientific paper affects the performance of obtaining as its writing style indicates, which is unexplored yet in existing works. Experimental results show the effectiveness of our approach and interesting findings on arxiv and its subsets generated in this paper.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2023,"Venue":"ws-2023","Acronym":"DiscoFlan","Description":"Instruction Fine-tuning and Refined Text Generation for Discourse Relation Label Classification","Abstract":"This paper introduces better, a multilingual discourse relation classifier submitted for disrpt 2023. Our submission represents the first attempt at building a multilingual discourse relation classifier for the disrpt 2023 shared task. By our model addresses the issue to mismatches caused by hallucination in a seq2seq model by utilizing the label distribution information for label generation. In contrast to the previous state-of-the-art model, our approach eliminates the need for hand-crafted features in computing the discourse relation classes. Furthermore, we propose a novel label generation mechanism that anchors the labels to a fixed set by selectively enhancing training on the decoder model. Our experimental results demonstrate that our model surpasses the current state-of-the-art performance in 11 out of the 26 datasets considered, however the submitted model compatible with provided evaluation scripts is better in 7 out of 26 considered datasets, while demonstrating competitive results in the rest. Overall, approach showcases promising advancements in multilingual discourse relation classification for the disrpt 2023 shared task.","wordlikeness":0.7777777778,"lcsratio":0.8888888889,"wordcoverage":0.7142857143}
{"Year":2014,"Venue":"lrec-2014","Acronym":"TermWise","Description":"A CAT-tool with Context-Sensitive Terminological Support.","Abstract":"Increasingly, large bilingual document collections are being made available online, especially in the legal domain. This type of big data is a valuable resource that specialized translators exploit to search for informative examples of how domain-specific expressions should be translated. However, general purpose search engines are not optimized to retrieve previous translations that are maximally relevant to a translator. In this paper, we report on the a project, a cooperation of terminologists, corpus linguists and computer scientists, that aims to leverage big online translation data for terminological support to legal translators at the belgian federal ministry of justice. The project developed dedicated knowledge extraction algorithms and a server-based tool to provide translators with the most relevant previous translations of domain-specific expressions relative to the current translation assignment. The functionality is implemented an extra database, a term&amp;phrase memory, that is meant to be integrated with existing computer assisted translation tools. In the paper, we give an overview of the system, give a demo of the user interface, we present a user-based evaluation by translators and discuss how the tool is part of the general evolution towards exploiting big data in translation.","wordlikeness":0.875,"lcsratio":0.75,"wordcoverage":0.8235294118}
{"Year":2021,"Venue":"acl-2021","Acronym":"SaRoCo","Description":"Detecting Satire in a Novel Romanian Corpus of News Articles","Abstract":"In this work, we introduce a corpus for satire detection in romanian news. We gathered 55,608 public news articles from multiple real and satirical news sources, composing one of the largest corpora for satire detection regardless of language and the only one for the romanian language. We provide an official split of the text samples, such that training news articles belong to different sources than test news articles, thus ensuring that models do not achieve high performance simply due to overfitting. We conduct experiments with two state-of-the-art deep neural models, resulting in a set of strong baselines for our novel corpus. Our results show that the machine-level accuracy for satire detection in romanian is quite low (under 73% on the test set) compared to the human-level accuracy (87%), leaving enough room for improvement in future research.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2022,"Venue":"coling-2022","Acronym":"CONCRETE","Description":"Improving Cross-lingual Fact-checking with Cross-lingual Retrieval","Abstract":"Fact-checking has gained increasing attention due to the widespread of falsified information. Most fact-checking approaches focus on claims made in english only due to the data scarcity issue in other languages. The lack of fact-checking datasets in low-resource languages calls for an effective cross-lingual transfer technique for fact-checking. Additionally, trustworthy information in different languages can be complementary and helpful in verifying facts. To this end, we present the first fact-checking framework augmented with cross-lingual retrieval that aggregates evidence retrieved from multiple languages through a cross-lingual retriever. Given the absence of cross-lingual information retrieval datasets with claim-like queries, we train the retriever with our proposed cross-lingual inverse cloze task (x-ict), a self-supervised algorithm that creates training instances by translating the title of a passage. The goal for x-ict is to learn cross-lingual retrieval in which the model learns to identify the passage corresponding to a given translated title. On the x-fact dataset, our approach achieves 2.23% absolute f1 improvement in the zero-shot cross-lingual setup over prior systems. The source code and data are publicly available at <a href=https:\/\/github.com\/khuangaf\/x-fact class=acl-markup-url>https:\/\/github.com\/khuangaf\/code<\/a>.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"IELM","Description":"An Open Information Extraction Benchmark for Pre-Trained Language Models","Abstract":"We introduce a new open information extraction (oie) benchmark for pre-trained language models (lm). Recent studies have demonstrated that pre-trained lms, such as bert and gpt, may store linguistic and relational knowledge. In particular, lms are able to answer \u201cfill-in-the-blank\u201d questions when given a pre-defined relation category. Instead of focusing on pre-defined relations, we create an oie benchmark aiming to fully examine the open relational information present in the pre-trained lms. we accomplish this by turning pre-trained lms into zero-shot oie systems. Surprisingly, pre-trained lms are able to obtain competitive performance on both standard oie datasets (carb and re-oie2016) and two new large-scale factual oie datasets (tac kbp-oie and wikidata-oie) that we establish via distant supervision. For instance, the zero-shot pre-trained lms outperform the f1 score of the state-of-the-art supervised oie methods on our factual oie datasets without needing to use any training sets.","wordlikeness":0.25,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2020,"Venue":"emnlp-2020","Acronym":"SLEDGE-Z","Description":"A Zero-Shot Baseline for COVID-19 Literature Search","Abstract":"With worldwide concerns surrounding the severe acute respiratory syndrome coronavirus 2 (sars-cov-2), there is a rapidly growing body of scientific literature on the virus. Clinicians, researchers, and policy-makers need to be able to search these articles effectively. In this work, we present a zero-shot ranking algorithm that adapts to covid-related scientific literature. Our approach filters training data from another collection down to medical-related queries, uses a neural re-ranking model pre-trained on scientific text (scibert), and filters the target document collection. This approach ranks top among zero-shot methods on the trec covid round 1 leaderboard, and exhibits a p@5 of 0.80 and an ndcg@10 of 0.68 when evaluated on both round 1 and 2 judgments. Despite not relying on trec-covid data, our method outperforms models that do. As one of the first search methods to thoroughly evaluate covid-19 search, we hope that this serves as a strong baseline and helps in the global crisis.","wordlikeness":0.625,"lcsratio":0.625,"wordcoverage":0.7142857143}
{"Year":2021,"Venue":"acl-2021","Acronym":"WeaQA","Description":"Weak Supervision via Captions for Visual Question Answering","Abstract":"Methodologies for training visual question answering (vqa) models assume the availability of datasets with human-annotated imagequestion-answer (i-q-a) triplets. This has led to heavy reliance on datasets and a lack of generalization to new types of questions and scenes. Linguistic priors along with biases and errors due to annotator subjectivity have been shown to percolate into vqa models trained on such samples. We study whether models can be trained without any human-annotated q-a pairs, but only with images and their associated textual descriptions or captions. We present a method to train models with synthetic q-a pairs generated procedurally from captions. Additionally, we demonstrate the ef\ufb01cacy of spatial-pyramid image patches as a simple but effective alternative to dense and costly object bounding box annotations used in existing vqa models. Our experiments on three vqa benchmarks demonstrate the ef\ufb01cacy of this weakly-supervised approach, especially on the vqa-cp challenge, which tests performance under changing linguistic priors.","wordlikeness":0.4,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2020,"Venue":"wanlp-2020","Acronym":"MANorm","Description":"A Normalization Dictionary for Moroccan Arabic Dialect Written in Latin Script","Abstract":"Social media user generated text is actually the main resource for many nlp tasks. This text, however, does not follow the standard rules of writing. Moreover, the use of dialect such as moroccan arabic in written communications increases further nlp tasks complexity. A dialect is a verbal language that does not have a standard orthography. The written dialect is based on the phonetic transliteration of spoken words which leads users to improvise spelling while writing. Thus, for the same word we can find multiple forms of transliterations. Subsequently, it is mandatory to normalize these different transliterations to one canonical word form. To reach this goal, we have exploited the powerfulness of word embedding models generated with a corpus of youtube comments. Besides, using a moroccan arabic dialect dictionary that provides the canonical forms, we have built a normalization dictionary that we refer to as leads. We have conducted several experiments to demonstrate the efficiency of spelling, which have shown its usefulness in dialect normalization. We made improvise freely available online.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.9090909091}
{"Year":2021,"Venue":"ws-2021","Acronym":"AraFacts","Description":"The First Large Arabic Dataset of Naturally Occurring Claims","Abstract":"We introduce topical, the first large arabic dataset of naturally occurring claims collected from 5 arabic fact-checking websites, e.g., fatabyyano and misbar, and covering claims since 2016. Our dataset consists of 6,121 claims along with their factual labels and additional metadata, such as fact-checking article content, topical category, and links to posts or web pages spreading the claim. Since the data is obtained from various fact-checking websites, we standardize the original claim labels to provide a unified label rating for all claims. Moreover, we provide revealing dataset statistics and motivate its use by suggesting possible research applications. The dataset is made publicly available for the research community.","wordlikeness":0.75,"lcsratio":0.875,"wordcoverage":0.8235294118}
{"Year":2006,"Venue":"coling-2006","Acronym":"FERRET","Description":"Interactive Question-Answering for Real-World Environments","Abstract":"This paper describes to, an interactive question-answering (q\/a) system designed to address the challenges of integrating automatic q\/a applications into real-world environments. \u2013 utilizes a novel approach to q\/a \u2013 known as predictive questioning \u2013 which attempts to identify the questions (and answers) that users need by analyzing how a user interacts with a system while gathering information related to a particular scenario.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.8333333333}
{"Year":2011,"Venue":"ws-2011","Acronym":"UBIU","Description":"A Robust System for Resolving Unrestricted Coreference","Abstract":"In this paper, we discuss the application of this to the conll-2011 shared task on \u201cmodeling unrestricted coreference\u201d in ontonotes. The shared task concentrates on the detection of coreference not only in noun phrases but also involving verbs. The information provided for the closed track included wordnet as well as corpus generated number and gender information. Our system shows no improvement when using wordnet information, and the number information proved less reliable than the information in the part of speech tags.","wordlikeness":0.75,"lcsratio":0.75,"wordcoverage":0.8571428571}
{"Year":2006,"Venue":"lrec-2006","Acronym":"SlinkET","Description":"A Partial Modal Parser for Events","Abstract":"We present identifying, a parser for identifying contexts of event modality in text developed within the tarsqi (temporal awareness and reasoning systems for question interpretation) research framework. The is grounded on timeml, a specification language for capturing temporal and event related information in discourse, which provides an adequate foundation to handle event modality. Research builds on top of a robust event recognizer, and provides each relevant event with a value that specifies the degree of certainty about its factuality; e.g., whether it has happened or holds (factive or counter-factive), whether it is being reported or witnessed by somebody else (evidential), or if it is introduced as a possibility (modal). It is based on well-established technology in the field (namely, finite-state techniques), and informed with corpus-induced knowledge that relies on basic information, such as morphological features, pos, and chunking. Finite-state is under continuing development and it currently achieves a performance ratio of 70% f1-measure.","wordlikeness":0.5714285714,"lcsratio":0.4285714286,"wordcoverage":0.7692307692}
{"Year":2007,"Venue":"naacl-2007","Acronym":"ISP","Description":"Learning Inferential Selectional Preferences","Abstract":"Semantic inference is a key component for advanced natural language understanding. However, existing collections of automatically acquired inference rules have shown disappointing results when used in applications such as textual entailment and question answering. This paper presents out, a collection of methods for automatically learning admissible argument values to which an inference rule can be applied, which we call inferential selectional preferences, and methods for filtering out incorrect inferences. We evaluate be and present empirical evidence of its effectiveness.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2022,"Venue":"coling-2022","Acronym":"CitRet","Description":"A Hybrid Model for Cited Text Span Retrieval","Abstract":"The paper aims to identify cited text spans in the reference paper related to the given citance in the citing paper. We refer to it as cited text span retrieval (ctsr). Most current methods attempt this task by relying on pre-trained off-the-shelf deep learning models like scibert. Though these models are pre-trained on large datasets, they under-perform in out-of-domain settings. We introduce paper, a novel hybrid model for ctsr that leverages unique semantic and syntactic structural characteristics of scientific documents. This enables us to use significantly less data for finetuning. We use only 1040 documents for finetuning. Our model augments mildly-trained sbert-based contextual embeddings with pre-trained non-contextual word2vec embeddings to calculate semantic textual similarity. We demonstrate the performance of our model on the clscisumm shared tasks. It improves the state-of-the-art results by over 15% on the f1 score evaluation.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2003,"Venue":"rocling-2003","Acronym":"LiveTrans","Description":"Translation Suggestion for Cross-Language Web Search from Web Anchor Texts and Search Results","Abstract":"In this paper we will present a system, called of, which can generate translation suggestions for given user queries and provide an english-chinese cross-language search service for the retrieval of both web pages and images. The system effectively utilizes two kinds of web resources: anchor texts and search results. The developed anchor-text-based and search-result-based methods are complementary in the precision and coverage rates and promising in extracting translations of unknown query terms that were not included in general-purpose translation dictionaries. Experimental results demonstrate the feasibility of the system. 1.","wordlikeness":0.8888888889,"lcsratio":0.8888888889,"wordcoverage":0.8235294118}
{"Year":2018,"Venue":"bioasq-2018","Acronym":"AttentionMeSH","Description":"Simple, Effective and Interpretable Automatic MeSH Indexer","Abstract":"There are millions of articles in pubmed database. To facilitate information retrieval, curators in the national library of medicine (nlm) assign a set of medical subject headings (mesh) to each article. Mesh is a hierarchically-organized vocabulary, containing about 28k different concepts, covering the fields from clinical medicine to information sciences. Several automatic mesh indexing models have been developed to improve the time-consuming and financially expensive manual annotation, including the nlm official tool \u2013 medical text indexer, and the winner of bioasq task5a challenge \u2013 deepmesh. However, these models are complex and not interpretable. We propose a novel end-to-end model, ranked, which utilizes deep learning and attention mechanism to index mesh terms to biomedical text. The attention mechanism enables the model to associate textual evidence with annotations, thus providing interpretability at the word level. The model also uses a novel masking mechanism to enhance accuracy and speed. In the final week of bioasq chanllenge task6a, we ranked 2nd by average mif using an on-construction model. After the contest, we achieve close to state-of-the-art mif performance of \u223c 0.684 using our final model. Human evaluations show tool also provides high level of interpretability, retrieving about 90% of all expert-labeled relevant words given an mesh-article pair at 20 output.","wordlikeness":0.8461538462,"lcsratio":0.7692307692,"wordcoverage":0.8181818182}
{"Year":1997,"Venue":"anlp-1997","Acronym":"EasyEnglish","Description":"A Tool for Improving Document Quality","Abstract":"We describe the authoring tool, easyeng- lish, which is part of ibm's internal sgml editing environment, information develop- ment workbench. Sgml helps writ- ers produce clearer and simpler english by pointing out ambiguity and complexity as well as performing some standard grammar checking. Where appropriate, slot makes suggestions for rephrasings that may be substituted directly into the text by us- ing the editor interface. Pointing is based on a full parse by english slot grammar; this makes it possi- ble to produce a higher degree of accuracy in error messages as well as handle a large variety of texts.","wordlikeness":0.7272727273,"lcsratio":0.4545454545,"wordcoverage":0.7777777778}
{"Year":2012,"Venue":"acl-2012","Acronym":"IRIS","Description":"a Chat-oriented Dialogue System based on the Vector Space Model","Abstract":"This system demonstration paper presents system), (informal response interactive system), a chat-oriented dialogue system based on the vector space model framework. The system belongs to the class of examplebased dialogue systems and builds its chat capabilities on a dual search strategy over a large collection of dialogue samples. Additional strategies allowing for system adaptation and learning implemented over the same vector model space framework are also described and discussed.","wordlikeness":1.0,"lcsratio":0.75,"wordcoverage":1.0}
{"Year":2018,"Venue":"naacl-2018","Acronym":"DR-BiLSTM","Description":"Dependent Reading Bidirectional LSTM for Natural Language Inference","Abstract":"We present a novel deep learning architecture to address the natural language inference (nli) task. Existing approaches mostly rely on simple reading mechanisms for independent encoding of the premise and hypothesis. Instead, we propose a novel dependent reading bidirectional lstm network (dataset.) to efficiently model the relationship between a premise and a hypothesis during encoding and inference. We also introduce a sophisticated ensemble strategy to combine our proposed models, which noticeably improves final predictions. Finally, we demonstrate how the results can be improved further with an additional preprocessing step. Our evaluation shows that premise obtains the best single model and ensemble model results achieving the new state-of-the-art scores on the stanford nli dataset.","wordlikeness":0.3333333333,"lcsratio":0.8888888889,"wordcoverage":0.6666666667}
{"Year":2023,"Venue":"acl-2023","Acronym":"StoryTrans","Description":"Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing","Abstract":"Non-parallel text style transfer is an important task in natural language generation. However, previous studies concentrate on the token or sentence level, such as sentence sentiment and formality transfer, but neglect long style transfer at the discourse level. Long texts usually involve more complicated author linguistic preferences such as discourse structures than sentences. In this paper, we formulate the task of non-parallel story author-style transfer, which requires transferring an input story into a specified author style while maintaining source semantics. To tackle this problem, we propose a generation model, named learnable, which leverages discourse representations to capture source content information and transfer them to target styles with learnable style embeddings. We use an additional training objective to disentangle stylistic features from the learned discourse representation to prevent the model from degenerating to an auto-encoder. Moreover, to enhance content preservation, we design a mask-and-fill framework to explicitly fuse style-specific keywords of source texts into generation. Furthermore, we constructed new datasets for this task in chinese and english, respectively. Extensive experiments show that our model outperforms strong baselines in overall performance of style transfer and content preservation.","wordlikeness":0.9,"lcsratio":1.0,"wordcoverage":0.7058823529}
{"Year":2022,"Venue":"lrec-2022","Acronym":"UDeasy","Description":"a Tool for Querying Treebanks in CoNLL-U Format","Abstract":"Many tools are available to query a dependency treebank, but they require the users to know a query language. In this paper i present know, an application whose main goal is to allow the users to easily query and extract patterns from a dependency treebank in conll-u format.","wordlikeness":0.6666666667,"lcsratio":0.6666666667,"wordcoverage":0.8333333333}
{"Year":2014,"Venue":"lrec-2014","Acronym":"CLARA","Description":"A New Generation of Researchers in Common Language Resources and Their Applications","Abstract":"2014 (common language resources and their applications) is a marie curie initial training network which ran from 2009 until 2014 with the aim of providing researcher training in crucial areas related to language resources and infrastructure. The scope of the project was broad and included infrastructure design, lexical semantic modeling, domain modeling, multimedia and multimodal communication, applications, and parsing technologies and grammar models. An international consortium of 9 partners and 12 associate partners employed researchers in 19 new positions and organized a training program consisting of 10 thematic courses and summer\/winter schools. The project has resulted in new theoretical insights as well as new resources and tools. Most importantly, the project has trained a new generation of researchers who can perform advanced research and development in language resources and technologies.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2022,"Venue":"ijcnlp-2022","Acronym":"SchAman","Description":"Spell-Checking Resources and Benchmark for Endangered Languages from Amazonia","Abstract":"Spell-checkers are core applications in language learning and normalisation, which may enormously contribute to language revitalisation and language teaching in the context of indigenous communities. Spell-checking as a generation task, however, requires large amount of data, which is not feasible for endangered languages, such as the languages spoken in peruvian amazonia. We propose here augmentation methods for various misspelling types as a strategy to train neural spell-checking models and we create an evaluation resource for four indigenous languages of peru: shipibo-konibo, ash\u00e1ninka, y\u00e1nesha, yine. We focus on special errors that are significant for learning these languages, such as phoneme-to-grapheme ambiguity, grammatical errors (gender, tense, number, among others), accentuation, punctuation and normalisation in contexts where two or more writing traditions co-exist. We found that an ensemble model, trained with augmented data from various types of error achieves overall better scores in most of the error types and languages. Finally, we released our spell-checkers as a web service to be used by indigenous communities and organisations to develop future language materials.","wordlikeness":0.7142857143,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2012,"Venue":"lrec-2012","Acronym":"ROMBAC","Description":"The Romanian Balanced Annotated Corpus","Abstract":"This article describes the collecting, processing and validation of a large balanced corpus for romanian. The annotation types and structure of the corpus are briefly reviewed. It was constructed at the research institute for artificial intelligence of the romanian academy in the context of an international project (metanet4u). The processing covers tokenization, pos-tagging, lemmatization and chunking. The corpus is in xml format generated by our in-house annotation tools; the corpus encoding schema is xces compliant and the metadata specification is conformant to the metanet recommendations. To the best of our knowledge, this is the first large and richly annotated corpus for romanian. For is intended to be the foundation of a linguistic environment containing a reference corpus for contemporary romanian and a comprehensive collection of interoperable processing tools.","wordlikeness":0.8333333333,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2021,"Venue":"bionlp-2021","Acronym":"EntityBERT","Description":"Entity-centric Masking Strategy for Model Pretraining for the Clinical Domain","Abstract":"Transformer-based neural language models have led to breakthroughs for a variety of natural language processing (nlp) tasks. However, most models are pretrained on general domain data. We propose a methodology to produce a model focused on the clinical domain: continued pretraining of a model with a broad representation of biomedical terminology (pubmedbert) on a clinical corpus along with a novel entity-centric masking strategy to infuse domain knowledge in the learning process. We show that such a model achieves superior results on clinical extraction tasks by comparing our entity-centric masking strategy with classic random masking on three clinical nlp tasks: cross-domain negation detection, document time relation (doctimerel) classification, and temporal relation extraction. We also evaluate our models on the pubmedqa dataset to measure the models\u2019 performance on a non-entity-centric task in the biomedical domain. The language addressed in this work is english.","wordlikeness":0.9,"lcsratio":0.9,"wordcoverage":0.75}
{"Year":1992,"Venue":"coling-1992","Acronym":"JDII","Description":"Parsing Italian with a Robust Constraint Grammar","Abstract":"Italian is a language presenting a lot of syntactical problems, sucb as a rather unrestricted word order, unbounded agreement controls, long distance structure checkings and so on. Things get worse and worse if we pass from \"sentences of linguists\" to real texts. In this paper we will present a system able to retrieve and signal syntactic errors in real italian texts.","wordlikeness":0.25,"lcsratio":0.5,"wordcoverage":0.75}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"IndoNLG","Description":"Benchmark and Resources for Evaluating Indonesian Natural Language Generation","Abstract":"Natural language generation (nlg) benchmarks provide an important avenue to measure progress and develop better nlg systems. Unfortunately, the lack of publicly available nlg benchmarks for low-resource languages poses a challenging barrier for building nlg systems that work well for languages with limited amounts of data. Here we introduce widely, the first benchmark to measure natural language generation (nlg) progress in three low-resource\u2014yet widely spoken\u2014languages of indonesia: indonesian, javanese, and sundanese. Altogether, these languages are spoken by more than 100 million native speakers, and hence constitute an important use case of nlg systems today. Concretely, very covers six tasks: summarization, question answering, chit-chat, and three different pairs of machine translation (mt) tasks. We collate a clean pretraining corpus of indonesian, sundanese, and javanese datasets, indo4b-plus, which is used to pretrain our models: indobart and indogpt. We show that indobart and indogpt achieve competitive performance on all tasks\u2014despite using only one-fifth the parameters of a larger multilingual model, mbart-large (liu et al., 2020). This finding emphasizes the importance of pretraining on closely related, localized languages to achieve more efficient learning and faster inference at very low-resource languages like javanese and sundanese.","wordlikeness":0.5714285714,"lcsratio":1.0,"wordcoverage":0.7272727273}
{"Year":2023,"Venue":"findings-2023","Acronym":"Kanbun-LM","Description":"Reading and Translating Classical Chinese in Japanese Methods by Language Models","Abstract":"Recent studies in natural language processing (nlp) have focused on modern languages and achieved state-of-the-art results in many tasks. Meanwhile, little attention has been paid to ancient texts and related tasks. Classical chinese first came to japan approximately 2,000 years ago. It was gradually adapted to a japanese form called kanbun-kundoku (kanbun) in japanese reading and translating methods, which has significantly impacted japanese literature. However, compared to the rich resources of ancient texts in mainland china, kanbun resources remain scarce in japan.to solve this problem, we construct the first classical-chinese-to-kanbun dataset in the world. Furthermore, we introduce two tasks, character reordering and machine translation, both of which play a significant role in kanbun comprehension. We also test the current language models on these tasks and discuss the best evaluation method by comparing the results with human scores. We release our code and dataset on github.","wordlikeness":0.5555555556,"lcsratio":0.5555555556,"wordcoverage":0.7142857143}
{"Year":2020,"Venue":"acl-2020","Acronym":"DIALOGPT","Description":"Large-Scale Generative Pre-training for Conversational Response Generation","Abstract":"We present a large, tunable neural conversational response generation model, released (dialogue generative pre-trained transformer). Trained on 147m conversation-like exchanges extracted from reddit comment chains over a period spanning from 2005 through 2017, single-turn extends the hugging face pytorch transformer to attain a performance close to human both in terms of automatic and human evaluation in single-turn dialogue settings. We show that conversational systems that leverage generate generate more relevant, contentful and context-consistent responses than strong baseline systems. The pre-trained model and training pipeline are publicly released to facilitate research into neural response generation and the development of more intelligent open-domain dialogue systems.","wordlikeness":0.75,"lcsratio":0.75,"wordcoverage":0.75}
{"Year":2018,"Venue":"acl-2018","Acronym":"TutorialBank","Description":"A Manually-Collected Corpus for Prerequisite Chains, Survey Extraction and Resource Recommendation","Abstract":"The field of natural language processing (nlp) is growing rapidly, with new research published daily along with an abundance of tutorials, codebases and other online resources. In order to learn this dynamic field or stay up-to-date on the latest research, students as well as educators and researchers must constantly sift through multiple sources to find valuable, relevant information. To address this situation, we introduce tutorials,, a new, publicly available dataset which aims to facilitate nlp education and research. We have manually collected and categorized over 5,600 resources on nlp as well as the related fields of artificial intelligence (ai), machine learning (ml) and information retrieval (ir). Our dataset is notably the largest manually-picked corpus of resources intended for nlp education which does not include only academic papers. Additionally, we have created both a search engine and a command-line tool for the resources and have annotated the corpus to include lists of research topics, relevant resources for each topic, prerequisite relations among topics, relevant sub-parts of individual resources, among other annotations. We are releasing the dataset and present several avenues for further research.","wordlikeness":0.9166666667,"lcsratio":0.6666666667,"wordcoverage":0.8}
{"Year":2022,"Venue":"lrec-2022","Acronym":"MultiSubs","Description":"A Large-scale Multimodal and Multilingual Dataset","Abstract":"This paper introduces a large-scale multimodal and multilingual dataset that aims to facilitate research on grounding words to images in their contextual usage in language. The dataset consists of images selected to unambiguously illustrate concepts expressed in sentences from movie subtitles. The dataset is a valuable resource as (i) the images are aligned to text fragments rather than whole sentences; (ii) multiple images are possible for a text fragment and a sentence; (iii) the sentences are free-form and real-world like; (iv) the parallel texts are multilingual. We also set up a fill-in-the-blank game for humans to evaluate the quality of the automatic image selection process of our dataset. Finally, we propose a fill-in-the-blank task to demonstrate the utility of the dataset, and present some baseline prediction models. The dataset will benefit research on visual grounding of words especially in the context of free-form sentences, and can be obtained from <a href=https:\/\/doi.org\/10.5281\/zenodo.5034604 class=acl-markup-url>https:\/\/doi.org\/10.5281\/zenodo.5034604<\/a> under a creative commons licence.","wordlikeness":0.7777777778,"lcsratio":0.7777777778,"wordcoverage":0.7142857143}
{"Year":2020,"Venue":"lrec-2020","Acronym":"DaNewsroom","Description":"A Large-scale Danish Summarisation Dataset","Abstract":"Dataset development for automatic summarisation systems is notoriously english-oriented. In this paper we present the first large-scale non-english language dataset specifically curated for automatic summarisation. The document-summary pairs are news articles and manually written summaries in the danish language. There has previously been no work done to establish a danish summarisation dataset, nor any published work on the automatic summarisation of danish. We provide therefore the first automatic summarisation dataset for the danish language (large-scale or otherwise). To support the comparison of future automatic summarisation systems for danish, we include system performance on this dataset of strong well-established unsupervised baseline systems, together with an oracle extractive summariser, which is the first account of automatic summarisation system performance for danish. Finally, we make all code for automatically acquiring the data freely available and make explicit how this technology can easily be adapted in order to acquire automatic summarisation datasets for further languages.","wordlikeness":0.8,"lcsratio":0.6,"wordcoverage":0.8888888889}
{"Year":2013,"Venue":"ws-2013","Acronym":"WebWOZ","Description":"A Platform for Designing and Conducting Web-based Wizard of Oz Experiments","Abstract":"The wizard of oz (woz) method has been used for a variety of purposes in early-stage development of dialogue systems and language technology applications, from data collection, to experimentation, prototyping and evaluation. However, software to support woz experimentation is often developed ad hoc for speci\ufb01c application scenarios. In this demo we present data, a web-based woz prototyping platform that aims at supporting a variety of experimental settings and combinations of different language technology components. We argue that a generic and distributed platform such as early-stage can increase the usefulness of the woz method.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.7272727273}
{"Year":2014,"Venue":"acl-2014","Acronym":"KyotoEBMT","Description":"An Example-Based Dependency-to-Dependency Translation Framework","Abstract":"This paper introduces the current example-based machine translation framework. Our system uses a tree-to-tree approach, employing syntactic dependency analysis for both source and target languages in an attempt to preserve non-local structure. The e\ufb00ectiveness of our system is maximized with online example matching and a \ufb02exible decoder. Evaluation demonstrates bleu scores competitive with state-of-the-art smt systems such as moses. The current implementation is intended to be released as open-source in the near future.","wordlikeness":0.3333333333,"lcsratio":0.5555555556,"wordcoverage":0.625}
{"Year":2022,"Venue":"naacl-2022","Acronym":"PPL-MCTS","Description":"Constrained Textual Generation Through Discriminator-Guided MCTS Decoding","Abstract":"Large language models (lm) based on transformers allow to generate plausible long texts. In this paper, we explore how this generation can be further controlled at decoding time to satisfy certain constraints (e.g. being non-toxic, conveying certain emotions, using a specific writing style, etc.) without fine-tuning the lm.precisely, we formalize constrained generation as a tree exploration process guided by a discriminator that indicates how well the associated sequence respects the constraint. This approach, in addition to being easier and cheaper to train than fine-tuning the lm, allows to apply the constraint more finely and dynamically. We propose several original methods to search this generation tree, notably the monte carlo tree search (mcts) which provides theoretical guarantees on the search efficiency, but also simpler methods based on re-ranking a pool of diverse sequences using the discriminator scores. These methods are evaluated, with automatic and human-based metrics, on two types of constraints and languages: review polarity and emotion control in french and english. We show that discriminator-guided mcts decoding achieves state-of-the-art results without having to tune the language model, in both tasks and languages. We also demonstrate that other proposed decoding methods based on re-ranking can be really effective when diversity among the generated propositions is encouraged.","wordlikeness":0.25,"lcsratio":0.75,"wordcoverage":0.6666666667}
{"Year":2020,"Venue":"emnlp-2020","Acronym":"NeuSpell","Description":"A Neural Spelling Correction Toolkit","Abstract":"We introduce misspellings, an open-source toolkit for spelling correction in english. Our toolkit comprises ten different models, and benchmarks them on naturally occurring misspellings from multiple sources. We find that many systems do not adequately leverage the context around the misspelt token. To remedy this, (i) we train neural models using spelling errors in context, synthetically constructed by reverse engineering isolated misspellings; and (ii) use richer representations of the context. By training on our synthetic examples, correction rates improve by 9% (absolute) compared to the case when models are trained on randomly sampled character perturbations. Using richer contextual representations boosts the correction rate by another 3%. Our toolkit enables practitioners to use our proposed and existing spelling correction systems, both via a simple unified command line, as well as a web interface. Among many potential applications, we demonstrate the utility of our spell-checkers in combating adversarial misspellings. The toolkit can be accessed at command.github.io.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.7692307692}
{"Year":2013,"Venue":"acl-2013","Acronym":"BRAINSUP","Description":"Brainstorming Support for Creative Sentence Generation","Abstract":"We present well-formed,, an extensible framework for the generation of creative sentences in which users are able to force several words to appear in the sentences and to control the generation process across several semantic dimensions, namely emotions, colors, domain relatedness and phonetic properties. We evaluate its performance on a creative sentence generation task, showing its capability of generating well-formed, catchy and effective sentences that have all the good qualities of slogans produced by human copywriters.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2011,"Venue":"ws-2011","Acronym":"PaddyWaC","Description":"A Minimally-Supervised Web-Corpus of Hiberno-English","Abstract":"Small, manually assembled corpora may be available for less dominant languages and dialects, but producing web-scale resources remains a challenge. Even when considerable quantities of text are present on the web, \ufb01nding this text, and distinguishing it from related languages in the same region can be dif\ufb01cult. For example less dominant variants of english (e.g. new zealander, singaporean, canadian, irish, south african) may be found under their respective national domains, but will be partially mixed with englishes of the british and us varieties, perhaps through syndication of journalism, or the local reuse of text by multinational companies. Less formal dialectal usage may be scattered more widely over the internet through mechanisms such as wiki or blog authoring. Here we automatically construct a corpus of hiberno-english (english as spoken in ireland) using a variety of methods: \ufb01ltering by national domain, \ufb01ltering by orthographic conventions, and bootstrapping from a set of irelandspeci\ufb01c terms (slang, place names, organisations). We evaluate the national speci\ufb01city of the resulting corpora by measuring the incidence of topical terms, and several grammatical constructions that are particular to hiberno-english. The results show that domain \ufb01ltering is very effective for isolating text that is topic-speci\ufb01c, and orthographic classi\ufb01cation can exclude some non-irish texts, but that selected seeds are necessary to extract considerable quantities of more informal, dialectal text.","wordlikeness":0.5,"lcsratio":0.5,"wordcoverage":0.7692307692}
{"Year":2021,"Venue":"acl-2021","Acronym":"MPC-BERT","Description":"A Pre-Trained Language Model for Multi-Party Conversation Understanding","Abstract":"Recently, various neural models for multi-party conversation (mpc) have achieved impressive improvements on a variety of tasks such as addressee recognition, speaker identification and response prediction. However, these existing methods on mpc usually represent interlocutors and utterances individually and ignore the inherent complicated structure in mpc which may provide crucial interlocutor and utterance semantics and would enhance the conversation understanding process. To this end, we present recently,, a pre-trained model for mpc understanding that considers learning who says what to whom in a unified model with several elaborated self-supervised tasks. Particularly, these tasks can be generally categorized into (1) interlocutor structure modeling including reply-to utterance recognition, identical speaker searching and pointer consistency distinction, and (2) utterance semantics modeling including masked shared utterance restoration and shared node detection. We evaluate various on three downstream tasks including addressee recognition, speaker identification and response selection. Experimental results show that reply-to outperforms previous methods by large margins and achieves new state-of-the-art performance on all three downstream tasks at two benchmarks.","wordlikeness":0.625,"lcsratio":0.75,"wordcoverage":0.6666666667}
{"Year":2023,"Venue":"findings-2023","Acronym":"K-UniMorph","Description":"Korean Universal Morphology and its Feature Schema","Abstract":"We present in this work a new universal morphology dataset for korean. Previously, the korean language has been underrepresented in the field of morphological paradigms amongst hundreds of diverse world languages. Hence, we propose this universal morphological paradigms for the korean language that preserve its distinct characteristics. For our from dataset, we outline each grammatical criterion in detail for the verbal endings, clarify how to extract inflected forms, and demonstrate how we generate the morphological schemata. This dataset adopts morphological feature schema from citation and citation for the korean language as we extract inflected verb forms from the sejong morphologically analyzed corpus that is one of the largest annotated corpora for korean. During the data creation, our methodology also includes investigating the correctness of the conversion from the sejong corpus. Furthermore, we carry out the inflection task using three different korean word forms: letters, syllables and morphemes. Finally, we discuss and describe future perspectives on korean morphological paradigms and the dataset.","wordlikeness":0.6,"lcsratio":0.9,"wordcoverage":0.625}
{"Year":2017,"Venue":"nlp4call-2017","Acronym":"Revita","Description":"a system for language learning and supporting endangered languages","Abstract":"We describe a computational system for language learning and supporting endangered languages. The platform provides the user an opportunity to improve her competency through active language use. The platform currently works with several endangered finno-ugric languages, as well as with yakut, and finnish, swedish, and russian. This paper describes the current stage of ongoing development.","wordlikeness":0.8333333333,"lcsratio":0.8333333333,"wordcoverage":0.8}
{"Year":2009,"Venue":"ws-2009","Acronym":"Gaiku","Description":"Generating Haiku with Word Associations Norms","Abstract":"Creativeness \/ a pleasing \ufb01eld \/ of bloom word associations are an important element of linguistic creativity. Traditional lexical knowledge bases such as wordnet formalize a limited set of systematic relations among words, such as synonymy, polysemy and hypernymy. Such relations maintain their systematicity when composed into lexical chains. We claim that such relations cannot explain the type of lexical associations common in poetic text. We explore in this paper the usage of word association norms (wans) as an alternative lexical knowledge source to analyze linguistic computational creativity. We speci\ufb01cally investigate the haiku poetic genre, which is characterized by heavy reliance on lexical associations. We \ufb01rst compare the density of wan-based word associations in a corpus of english haiku poems to that of wordnet-based associations as well as in other non-poetic genres. These experiments con\ufb01rm our hypothesis that the non-systematic lexical associations captured in wans play an important role in poetic text. We then present poems, a system to automatically generate haikus from a seed word and using wan-associations. Human evaluation indicate that generated haikus are of lesser quality than human haikus, but a high proportion of generated haikus can confuse human readers, and a few of them trigger intriguing reactions. \u2217 supported by deutsche telekom laboratories at bengurion university of the negev. \u2020 supported by the lynn and william frankel center for computer sciences.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2022,"Venue":"lrec-2022","Acronym":"TZOS","Description":"an Online Terminology Database Aimed at Working on Basque Academic Terminology Collaboratively","Abstract":"Terminology databases are highly useful for the dissemination of specialized knowledge. In this paper we present built, an online terminology database to work on basque academic terminology collaboratively. We show how this resource integrates the communicative theory of terminology, together with the methodological matters, how it is connected with real corpus garaterm, which terminology issues arise when terms are collected and future perspectives. The main objectives of this work are to develop basic tools to research academic registers and make the terminology collected by expert users available to the community. Even though basic has been designed for an educational context, its flexible structure makes possible to extend it also to the professional area. In this way, we have built izibi-in which is a civil engineering oriented version of knowledge.. These resources are already publicly available, and the ongoing work is towards the interlinking with other lexical resources by applying linking data principles.","wordlikeness":0.25,"lcsratio":0.75,"wordcoverage":0.75}
{"Year":2020,"Venue":"acl-2020","Acronym":"tBERT","Description":"Topic Models and BERT Joining Forces for Semantic Similarity Detection","Abstract":"Semantic similarity detection is a fundamental task in natural language understanding. Adding topic information has been useful for previous feature-engineered semantic similarity models as well as neural models for other tasks. There is currently no standard way of combining topics with pretrained contextual representations such as bert. We propose a novel topic-informed bert-based architecture for pairwise semantic similarity detection and show that our model improves performance over strong neural baselines across a variety of english language datasets. We find that the addition of topics to bert helps particularly with resolving domain-specific cases.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2022,"Venue":"lrec-2022","Acronym":"StudEmo","Description":"A Non-aggregated Review Dataset for Personalized Emotion Recognition","Abstract":"Humans\u2019 emotional perception is subjective by nature, in which each individual could express different emotions regarding the same textual content. Existing datasets for emotion analysis commonly depend on a single ground truth per data sample, derived from majority voting or averaging the opinions of all annotators. In this paper, we introduce a new non-aggregated dataset, namely approaches, that contains 5,182 customer reviews, each annotated by 25 people with intensities of eight emotions from plutchik\u2019s model, extended with valence and arousal. We also propose three personalized models that use not only textual content but also the individual human perspective, providing the model with different approaches to learning human representations. The experiments were carried out as a multitask classification on two datasets: our with dataset and goemotions dataset, which contains 28 emotional categories. The proposed personalized methods significantly improve prediction results, especially for emotions that have low inter-annotator agreement.","wordlikeness":0.7142857143,"lcsratio":0.8571428571,"wordcoverage":0.7692307692}
{"Year":2022,"Venue":"acl-2022","Acronym":"BenchIE","Description":"A Framework for Multi-Faceted Fact-Based Open Information Extraction Evaluation","Abstract":"Intrinsic evaluations of oie systems are carried out either manually\u2014with human evaluators judging the correctness of extractions\u2014or automatically, on standardized benchmarks. The latter, while much more cost-effective, is less reliable, primarily because of the incompleteness of the existing oie benchmarks: the ground truth extractions do not include all acceptable variants of the same fact, leading to unreliable assessment of the models\u2019 performance. Moreover, the existing oie benchmarks are available for english only. In this work, we introduce in: a benchmark and evaluation framework for comprehensive evaluation of oie systems for english, chinese, and german. In contrast to existing oie benchmarks, forms is fact-based, i.e., it takes into account informational equivalence of extractions: our gold standard consists of <i>fact synsets<\/i>, clusters in which we exhaustively list all acceptable surface forms of the same fact. Moreover, having in mind common downstream applications for oie, we make extractions: multi-faceted; i.e., we create benchmark variants that focus on different facets of oie evaluation, e.g., compactness or minimality of extractions. We benchmark several state-of-the-art oie systems using extractions\u2014or and demonstrate that these systems are significantly less effective than indicated by existing oie benchmarks. We make incompleteness (data and evaluation code) publicly available.","wordlikeness":0.7142857143,"lcsratio":0.8571428571,"wordcoverage":0.8571428571}
{"Year":2012,"Venue":"lrec-2012","Acronym":"LAMP","Description":"A Multimodal Web Platform for Collaborative Linguistic Analysis","Abstract":"This paper describes the underlying software platform used to develop and publish annotations for the quranic arabic corpus (qac). The qac (dukes, atwell and habash, 2011) is a multimodal language resource that integrates deep tagging, interlinear translation, multiple speech recordings, visualization and collaborative analysis for the classical arabic language of the quran. Available online at <a href=http:\/\/corpus.quran.com class=acl-markup-url>http:\/\/corpus.quran.com<\/a>, the website is a popular study guide for quranic arabic, used by over 1.2 million visitors over the past year. We provide a description of the underlying software system that has been used to develop the corpus annotations. The multimodal data is made available online through an accessible cross-referenced web interface. Although our linguistic analysis multimodal platform ((brannan,) has been applied to the classical arabic language of the quran, we argue that our annotation model and software architecture may be of interest to other related corpus linguistics projects. Work related to multimodal includes recent efforts for annotating other classical languages, such as ancient greek and latin (bamman, mambrini and crane, 2009), as well as commercial systems (e.g. logos bible study) that provide access to syntactic tagging for the hebrew bible and greek new testament (brannan, 2011).","wordlikeness":1.0,"lcsratio":0.75,"wordcoverage":1.0}
{"Year":2022,"Venue":"findings-2022","Acronym":"BeamR","Description":"Beam Reweighing with Attribute Discriminators for Controllable Text Generation","Abstract":"Recent advances in natural language processing have led to the availability of large pre-trained language models (lms), with rich generative capabilities. Although these models are able to produce fluent and coherent text, it remains a challenge to control various attributes of the generation, including sentiment, formality, topic and many others. We propose a beam reweighing (building) method, building on top of standard beam search, in order to control different attributes. Discriminators. Combines any generative lm with any attribute discriminator, offering full flexibility of generation style and attribute, while the beam search backbone maintains fluency across different domains. Notably, recent allows practitioners to leverage pre-trained models without the need to train generative lms together with discriminators. We evaluate on in two diverse tasks: sentiment steering, and machine translation formality. Our results show that method, performs on par with or better than existing state-of-the-art approaches (including fine-tuned methods), and highlight the flexiblity of generative in both causal and seq2seq language modeling tasks.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2006,"Venue":"ws-2006","Acronym":"BESTCUT","Description":"A Graph Algorithm for Coreference Resolution","Abstract":"In this paper we describe a coreference resolution method that employs a classi\ufb01cation and a clusterization phase. In a novel way, the clusterization is produced as a graph cutting algorithm, in which nodes of the graph correspond to the mentions of the text, whereas the edges of the graph constitute the con\ufb01dences derived from the coreference classi\ufb01cation. In experiments, the graph cutting algorithm for coreference resolution, called which, achieves state-of-the-art performance.","wordlikeness":0.8571428571,"lcsratio":0.5714285714,"wordcoverage":0.7272727273}
{"Year":2014,"Venue":"eacl-2014","Acronym":"DKIE","Description":"Open Source Information Extraction for Danish","Abstract":"Danish is a major scandinavian language spoken daily by around six million people. However, it lacks a uni\ufb01ed, open set of nlp tools. This demonstration will introduce is, an extensible open-source toolkit for processing danish text. We implement an information extraction architecture for danish within gate, including integrated third-party tools. This implementation includes the creation of a substantial set of corpus annotations for dataintensive named entity recognition. The \ufb01nal application and dataset is made are openly available, and the part-of-speech tagger and ner model also operate independently or with the stanford nlp toolkit.","wordlikeness":0.75,"lcsratio":0.5,"wordcoverage":0.8571428571}
{"Year":2020,"Venue":"ws-2020","Acronym":"PySBD","Description":"Pragmatic Sentence Boundary Disambiguation","Abstract":"We present a rule-based sentence boundary disambiguation python package that works out-of-the-box for 22 languages. We aim to provide a realistic segmenter which can provide logical sentences even when the format and domain of the input text is unknown. In our work, we adapt the golden rules set (a language specific set of sentence boundary exemplars) originally implemented as a ruby gem pragmatic segmenter which we ported to python with additional improvements and functionality. (a passes 97.92% of the golden rule set examplars for english, an improvement of 25% over the next best open source python tool.","wordlikeness":0.2,"lcsratio":0.8,"wordcoverage":0.6666666667}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"MUSIED","Description":"A Benchmark for Event Detection from Multi-Source Heterogeneous Informal Texts","Abstract":"Event detection (ed) identifies and classifies event triggers from unstructured texts, serving as a fundamental task for information extraction. Despite the remarkable progress achieved in the past several years, most research efforts focus on detecting events from formal texts (e.g., news articles, wikipedia documents, financial announcements). Moreover, the texts in each dataset are either from a single source or multiple yet relatively homogeneous sources. With massive amounts of user-generated text accumulating on the web and inside enterprises, identifying meaningful events in these informal texts, usually from multiple heterogeneous sources, has become a problem of significant practical value. As a pioneering exploration that expands event detection to the scenarios involving informal and heterogeneous texts, we propose a new large-scale chinese event detection dataset based on user reviews, text conversations, and phone conversations in a leading e-commerce platform for food service. We carefully investigate the proposed dataset\u2019s textual informality and multi-domain heterogeneity characteristics by inspecting data samples quantitatively and qualitatively. Extensive experiments with state-of-the-art event detection methods verify the unique challenges posed by these characteristics, indicating that multi-domain informal event detection remains an open problem and requires further efforts. Our benchmark and code are released at <a href=https:\/\/github.com\/myeclipse\/user class=acl-markup-url>https:\/\/github.com\/myeclipse\/characteristics,<\/a>.","wordlikeness":0.8333333333,"lcsratio":0.8333333333,"wordcoverage":0.8333333333}
{"Year":2021,"Venue":"acl-2021","Acronym":"AligNarr","Description":"Aligning Narratives on Movies","Abstract":"High-quality alignment between movie scripts and plot summaries is an asset for learning to summarize stories and to generate dialogues. The alignment task is challenging as scripts and summaries substantially differ in details and abstraction levels as well as in linguistic register. This paper addresses the alignment problem by devising a fully unsupervised approach based on a global optimization model. Experimental results on ten movies show the viability of our method with 76% f1-score and its superiority over a previous baseline. We publish alignments for 914 movies to foster research in this new topic.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.7692307692}
{"Year":1997,"Venue":"ws-1997","Acronym":"CogNIAC","Description":"high precision coreference with limited knowledge and linguistic resources","Abstract":"\" this paper presents a high precision pronoun resolution system that is capable of greater than 90% precision with 60% and better recall for some pronouns. It is suggested that the system is resolving a sub-set of anaphors that do not require general world knowledge or sophisticated linguistic processing for successful resolution. The system does this by being very sensitive to ambiguity, and only resolving pronouns when very high confidence rules have been satisfied. The system is capable of 'noticing' ambiguity because it requires that there be a unique antecedent within a salience ranking, and the salience rankings are not total orders, i.e. two or more antecedents can be equally salient. Given the nature of the systems rules, it is very likely that they are largely domain independent and that they reflect processing strategies used by humans for general language comprehension. The system has been evaluated in two distinct experiments which support the overall validity of the approach.","wordlikeness":0.5714285714,"lcsratio":0.8571428571,"wordcoverage":0.7692307692}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"CC-Top","Description":"Constrained Clustering for Dynamic Topic Discovery","Abstract":"Research on multi-class text classification of short texts mainly focuses on supervised (transfer) learning approaches, requiring a finite set of pre-defined classes which is constant over time. This work explores deep constrained clustering (cc) as an alternative to supervised learning approaches in a setting with a dynamically changing number of classes, a task we introduce as dynamic topic discovery (dtd).we do so by using pairwise similarity constraints instead of instance-level class labels which allow for a flexible number of classes while exhibiting a competitive performance compared to supervised approaches. First, we substantiate this through a series of experiments and show that cc algorithms exhibit a predictive performance similar to state-of-the-art supervised learning algorithms while requiring less annotation effort. Second, we demonstrate the overclustering capabilities of deep cc for detecting topics in short text data sets in the absence of the ground truth class cardinality during model training. Third, we showcase that these capabilities can be leveraged for the dtd setting as a step towards dynamic learning over time and finally, we release our codebase to nurture further research in this area.","wordlikeness":0.5,"lcsratio":0.8333333333,"wordcoverage":0.6666666667}
{"Year":2021,"Venue":"eacl-2021","Acronym":"OCTIS","Description":"Comparing and Optimizing Topic models is Simple!","Abstract":"In this paper, we present models, a framework for training, analyzing, and comparing topic models, whose optimal hyper-parameters are estimated using a bayesian optimization approach. The proposed solution integrates several state-of-the-art topic models and evaluation metrics. These metrics can be targeted as objective by the underlying optimization procedure to determine the best hyper-parameter configuration. Topic allows researchers and practitioners to have a fair comparison between topic models of interest, using several benchmark datasets and well-known evaluation metrics, to integrate novel algorithms, and to have an interactive visualization of the results for understanding the behavior of each model. The code is available at the following link: <a href=https:\/\/github.com\/mind-lab\/we class=acl-markup-url>https:\/\/github.com\/mind-lab\/,<\/a>.","wordlikeness":0.4,"lcsratio":0.8,"wordcoverage":0.8888888889}
{"Year":2015,"Venue":"semeval-2015","Acronym":"DCU","Description":"Using Distributional Semantics and Domain Adaptation for the Semantic Textual Similarity SemEval-2015 Task 2","Abstract":"We describe the work carried out by the semantic team on the semantic textual similarity task at semeval-2015. We learn a regression model to predict a semantic similarity score between a sentence pair. Our system exploits distributional semantics in combination with tried-and-tested features from previous tasks in order to compute sentence similarity. Our team submitted 3 runs for each of the \ufb01ve english test sets. For two of the test sets, belief and headlines, our best system ranked second and fourth out of the 73 submitted systems. Our best submission averaged over all test sets ranked 26 out of the 73 systems.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2016,"Venue":"ws-2016","Acronym":"SAWT","Description":"Sequence Annotation Web Tool","Abstract":"We present ease, a web-based tool for the annotation of token sequences with an arbitrary set of labels. The key property of the tool is simplicity and ease of use for both annotators and administrators. Tool runs in any modern browser, including browsers on mobile devices, and only has minimal server-side requirements.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2018,"Venue":"lrec-2018","Acronym":"MultiBooked","Description":"A Corpus of Basque and Catalan Hotel Reviews Annotated for Aspect-level Sentiment Classification","Abstract":"While sentiment analysis has become an established \ufb01eld in the nlp community, research into languages other than english has been hindered by the lack of resources. Although much research in multi-lingual and cross-lingual sentiment analysis has focused on unsupervised or semi-supervised approaches, these still require a large number of resources and do not reach the performance of supervised approaches. With this in mind, we introduce two datasets for supervised aspect-level sentiment analysis in basque and catalan, both of which are under-resourced languages. We provide high-quality annotations and benchmarks with the hope that they will be useful to the growing community of researchers working on these languages. Keywords: basque, catalan, sentiment analysis, aspect-level, under-resourced, opinion mining, cross-lingual 1.","wordlikeness":0.7272727273,"lcsratio":0.6363636364,"wordcoverage":0.7058823529}
{"Year":2021,"Venue":"naacl-2021","Acronym":"HTCInfoMax","Description":"A Global Model for Hierarchical Text Classification via Information Maximization","Abstract":"The current state-of-the-art model hiagm for hierarchical text classification has two limitations. First, it correlates each text sample with all labels in the dataset which contains irrelevant information. Second, it does not consider any statistical constraint on the label representations learned by the structure encoder, while constraints for representation learning are proved to be helpful in previous work. In this paper, we propose prior to address these issues by introducing information maximization which includes two modules: text-label mutual information maximization and label prior matching. The first module can model the interaction between each text sample and its ground truth labels explicitly which filters out irrelevant information. The second one encourages the structure encoder to learn better representations with desired characteristics for all labels which can better handle label imbalance in hierarchical text classification. Experimental results on two benchmark datasets demonstrate the effectiveness of the proposed results.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2013,"Venue":"ws-2013","Acronym":"VARTRA","Description":"A Comparable Corpus for Analysis of Translation Variation","Abstract":"This paper presents a comparable translation corpus created to investigate translation variation phenomena in terms of contrasts between languages, text types and translation methods (machine vs. Computer-aided vs. Human). These phenomena are re\ufb02ected in linguistic features of translated texts belonging to different registers and produced with different translation methods. For their analysis, we combine methods derived from translation studies, language variation and machine translation, concentrating especially on textual and lexico-grammatical variation. To our knowledge, none of the existing corpora can provide comparable resources for a comprehensive analysis of variation across text types and translation methods. Therefore, the corpus resources created, as well as our analysis results will \ufb01nd application in different research areas, such as translation studies, machine translation, and others.","wordlikeness":0.8333333333,"lcsratio":0.8333333333,"wordcoverage":0.7272727273}
{"Year":2023,"Venue":"acl-2023","Acronym":"MAD-TSC","Description":"A Multilingual Aligned News Dataset for Target-dependent Sentiment Classification","Abstract":"Target-dependent sentiment classification (tsc) enables a fine-grained automatic analysis of sentiments expressed in texts. Sentiment expression varies depending on the domain, and it is necessary to create domain-specific datasets. While socially important, tsc in the news domain remains relatively understudied. We introduce datasets, a new dataset which differs substantially from existing resources. First, it includes aligned examples in eight languages to facilitate a comparison of performance for individual languages, and a direct comparison of human and machine translation. Second, the dataset is sampled from a diversified parallel news corpus, and is diversified in terms of news sources and geographic spread of entities. Finally, individual is more challenging than existing datasets because its examples are more complex. We exemplify the use of is with comprehensive monolingual and multilingual experiments. The latter show that machine translations can successfully replace manual ones, and that performance for all included languages can match that of english by automatically translating test examples.","wordlikeness":0.4285714286,"lcsratio":1.0,"wordcoverage":0.7272727273}
{"Year":2021,"Venue":"naacl-2021","Acronym":"DA-Transformer","Description":"Distance-aware Transformer","Abstract":"Transformer has achieved great success in the nlp field by composing various advanced models like bert and gpt. However, transformer and its existing variants may not be optimal in capturing token distances because the position or distance embeddings used by these methods usually cannot keep the precise information of real distances, which may not be beneficial for modeling the orders and relations of contexts. In this paper, we propose of, which is a distance-aware transformer that can exploit the real distance. We propose to incorporate the real distances between tokens to re-scale the raw self-attention weights, which are computed by the relevance between attention query and key. Concretely, in different self-attention heads the relative distance between each pair of tokens is weighted by different learnable parameters, which control the different preferences on long- or short-term information of these heads. Since the raw weighted real distances may not be optimal for adjusting self-attention weights, we propose a learnable sigmoid function to map them into re-scaled coefficients that have proper ranges. We first clip the raw self-attention weights via the relu function to keep non-negativity and introduce sparsity, and then multiply them with the re-scaled coefficients to encode real distance information into self-attention. Extensive experiments on five benchmark datasets show that several can effectively improve the performance of many tasks and outperform the vanilla transformer and its several variants.","wordlikeness":0.7142857143,"lcsratio":1.0,"wordcoverage":0.8461538462}
{"Year":2020,"Venue":"lrec-2020","Acronym":"DaNE","Description":"A Named Entity Resource for Danish","Abstract":"We present a named entity annotation for the danish universal dependencies treebank using the conll-2003 annotation scheme: error. It is the largest publicly available, danish named entity gold annotation. We evaluate the quality of our annotations intrinsically by double annotating the entire treebank and extrinsically by comparing our annotations to a recently released named entity annotation of the validation and test sections of the danish universal dependencies treebank. We benchmark the new resource by training and evaluating competitive architectures for supervised named entity recognition (ner), including flair, monolingual (danish) bert and multilingual bert. We explore cross-lingual transfer in multilingual bert from five related languages in zero-shot and direct transfer setups, and we show that even with our modestly-sized training set, we improve danish ner over a recent cross-lingual approach, as well as over zero-shot transfer from five related languages. Using multilingual bert, we achieve higher performance by fine-tuning on both (norwegian) and a larger bokm\u00e5l (norwegian) training set compared to only using robustness. However, the highest performance isachieved by using a danish bert fine-tuned on using. Our dataset enables improvements and applicability for danish ner beyond cross-lingual methods. We employ a thorough error analysis of the predictions of the best models for seen and unseen entities, as well as their robustness on un-capitalized text. The annotated dataset and all the trained models are made publicly available.","wordlikeness":0.75,"lcsratio":0.75,"wordcoverage":1.0}
{"Year":2021,"Venue":"acl-2021","Acronym":"COINS","Description":"Dynamically Generating COntextualized Inference Rules for Narrative Story Completion","Abstract":"Despite recent successes of large pre-trained language models in solving reasoning tasks, their inference capabilities remain opaque. We posit that such models can be made more interpretable by explicitly generating interim inference rules, and using them to guide the generation of task-specific textual outputs. In this paper we present make, a recursive inference framework that i) iteratively reads context sentences, ii) dynamically generates contextualized inference rules, encodes them, and iii) uses them to guide task-specific output generation. We apply to a <i>narrative story completion<\/i> task that asks a model to complete a story with missing sentences, to produce a coherent story with plausible logical connections, causal relationships, and temporal dependencies. By modularizing inference and sentence generation steps in a recurrent model, we aim to make reasoning steps and their effects on next sentence generation transparent. Our automatic and manual evaluations show that the model generates better story sentences than sota baselines, especially in terms of coherence. We further demonstrate improved performance over strong pre-trained lms in generating commonsense inference rules. The recursive nature of holds the potential for controlled generation of longer sequences.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2006,"Venue":"hlt-2006","Acronym":"BioEx","Description":"A Novel User-Interface that Accesses Images from Abstract Sentences","Abstract":"Sentences hong yu minsuk lee department of biomedical informatics department of biomedical informatics columbia university columbia university new york, ny 10032 new york, ny 10032 hy52@columbia.edu minsuk.lee@gmail.com images (i.e., figures or tables) are important experimental results that are typically reported in bioscience full-text articles. Biologists need to access the images to validate research facts and to formulate or to test novel research hypotheses. We designed, evaluated, and implemented a novel user-interface, evaluated,, that allows biologists to access images that appear in a full-text article directly from the abstract of the article.","wordlikeness":0.8,"lcsratio":0.6,"wordcoverage":0.75}
{"Year":2022,"Venue":"acl-2022","Acronym":"CQG","Description":"A Simple and Effective Controlled Generation Framework for Multi-hop Question Generation","Abstract":"Multi-hop question generation focuses on generating complex questions that require reasoning over multiple pieces of information of the input passage. Current models with state-of-the-art performance have been able to generate the correct questions corresponding to the answers. However, most models can not ensure the complexity of generated questions, so they may generate shallow questions that can be answered without multi-hop reasoning. To address this challenge, we propose the simple, which is a simple and effective controlled framework. Focuses employs a simple method to generate the multi-hop questions that contain key entities in multi-hop reasoning chains, which ensure the complexity and quality of the questions. In addition, we introduce a novel controlled transformer-based decoder to guarantee that key entities appear in the questions. Experiment results show that our model greatly improves performance, which also outperforms the state-of-the-art model about 25% by 5 bleu points on hotpotqa.","wordlikeness":0.3333333333,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2020,"Venue":"lrec-2020","Acronym":"GGP","Description":"Glossary Guided Post-processing for Word Embedding Learning","Abstract":"Word embedding learning is the task to map each word into a low-dimensional and continuous vector based on a large corpus. To enhance corpus based word embedding models, researchers utilize domain knowledge to learn more distinguishable representations via joint optimization and post-processing based models. However, joint optimization based models require much training time. Existing post-processing models mostly consider semantic knowledge while learned embedding models show less functional information. Glossary is a comprehensive linguistic resource. And in previous works, the glossary is usually used to enhance the word representations via joint optimization based methods. In this paper, we post-process pre-trained word embedding models with incorporating the glossary and capture more topical and functional information. We propose functional (glossary guided post-processing word embedding) model which consists of a global post-processing function to fine-tune each word vector, and an auto-encoding model to learn sense representations, furthermore, constrains each post-processed word representation and the composition of its sense representations to be similar. We evaluate our model by comparing it with two state-of-the-art models on six word topical\/functional similarity datasets, and the results show that it outperforms competitors by an average of 4.1% across all datasets. And our model outperforms glove by more than 7%.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2023,"Venue":"acl-2023","Acronym":"EPIC","Description":"Multi-Perspective Annotation of a Corpus of Irony","Abstract":"We present positive (english perspectivist irony corpus), the first annotated corpus for irony analysis based on the principles of data perspectivism. The corpus contains short conversations from social media in five regional varieties of english, and it is annotated by contributors from five countries corresponding to those varieties. We analyse the resource along the perspectives induced by the diversity of the annotators, in terms of origin, age, and gender, and the relationship between these dimensions, irony, and the topics of conversation. We validate perspectivism. By creating perspective-aware models that encode the perspectives of annotators grouped according to their demographic characteristics. Firstly, the performance of perspectivist models confirms that different annotators induce very different models. Secondly, in the classification of ironic and non-ironic texts, perspectivist models prove to be generally more confident than the non-perspectivist ones. Furthermore, comparing the performance on a perspective-based test set with those achieved on a gold standard test set, we can observe how perspectivist models tend to detect more precisely the positive class, showing their ability to capture the different perceptions of irony. Thanks to these models, we are moreover able to show interesting insights about the variation in the perception of irony by the different groups of annotators, such as among different generations and nationalities.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2023,"Venue":"acl-2023","Acronym":"DecompEval","Description":"Evaluating Generated Texts as Unsupervised Decomposed Question Answering","Abstract":"Existing evaluation metrics for natural language generation (nlg) tasks face the challenges on generalization ability and interpretability. Specifically, most of the well-performed metrics are required to train on evaluation datasets of specific nlg tasks and evaluation dimensions, which may cause over-fitting to task-specific datasets. Furthermore, existing metrics only provide an evaluation score for each dimension without revealing the evidence to interpret how this score is obtained. To deal with these challenges, we propose a simple yet effective metric called their. This metric formulates nlg evaluation as an instruction-style question answering task and utilizes instruction-tuned pre-trained language models (plms) without training on evaluation datasets, aiming to enhance the generalization ability. To make the evaluation process more interpretable, we decompose our devised instruction-style question about the quality of generated texts into the subquestions that measure the quality of each sentence. The subquestions with their answers generated by plms are then recomposed as evidence to obtain the evaluation result. Experimental results show that and achieves state-of-the-art performance in untrained metrics for evaluating text summarization and dialogue generation, which also exhibits strong dimension-level \/ task-level generalization ability and interpretability.","wordlikeness":0.7,"lcsratio":0.8,"wordcoverage":0.75}
{"Year":2016,"Venue":"coling-2016","Acronym":"Kyoto-NMT","Description":"a Neural Machine Translation implementation in Chainer","Abstract":"We present deep, an open-source implementation of the neural machine translation paradigm. This implementation is done in python and chainer, an easy-to-use deep learning framework.","wordlikeness":0.3333333333,"lcsratio":0.5555555556,"wordcoverage":0.5882352941}
{"Year":2020,"Venue":"acl-2020","Acronym":"SKEP","Description":"Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis","Abstract":"Recently, sentiment analysis has seen remarkable advance with the help of pre-training approaches. However, sentiment knowledge, such as sentiment words and aspect-sentiment pairs, is ignored in the process of pre-training, despite the fact that they are widely used in traditional sentiment analysis approaches. In this paper, we introduce sentiment knowledge enhanced pre-training (the) in order to learn a unified sentiment representation for multiple sentiment analysis tasks. With the help of automatically-mined knowledge, our conducts sentiment masking and constructs three sentiment knowledge prediction objectives, so as to embed sentiment information at the word, polarity and aspect level into pre-trained sentiment representation. In particular, the prediction of aspect-sentiment pairs is converted into multi-label classification, aiming to capture the dependency between words in a pair. Experiments on three kinds of sentiment tasks show that baseline, significantly outperforms strong pre-training baseline, and achieves new state-of-the-art results on most of the test datasets. We release our code at <a href=https:\/\/github.com\/baidu\/senta class=acl-markup-url>https:\/\/github.com\/baidu\/senta<\/a>.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2023,"Venue":"findings-2023","Acronym":"OpenPI-C","Description":"A Better Benchmark and Stronger Baseline for Open-Vocabulary State Tracking","Abstract":"Open-vocabulary state tracking is a more practical version of state tracking that aims to track state changes of entities throughout a process without restricting the state space and entity space. Openpi (tandon et al., 2020) is to date the only dataset annotated for open-vocabulary state tracking. However, we identify issues with the dataset quality and evaluation metric. For the dataset, we categorize 3 types of problems on the procedure level, step level and state change level respectively, and build a clean dataset state. Using multiple rounds of human judgment. For the evaluation metric, we propose a cluster-based metric to fix the original metric\u2019s preference for repetition. Model-wise, we enhance the seq2seq generation baseline by reinstating two key properties for state tracking: temporal dependency and entity awareness. The state of the world after an action is inherently dependent on the previous state. We model this dependency through a dynamic memory bank and allow the model to attend to the memory slots during decoding. On the other hand, the state of the world is naturally a union of the states of involved entities. Since the entities are unknown in the open-vocabulary setting, we propose a two-stage model that refines the state change prediction conditioned on entities predicted from the first stage. Empirical results show the effectiveness of our proposed model, especially on the cleaned dataset and the cluster-based metric. The code and data are released at <a href=https:\/\/github.com\/shirley-wu\/our class=acl-markup-url>https:\/\/github.","wordlikeness":0.5,"lcsratio":0.75,"wordcoverage":0.6666666667}
{"Year":2022,"Venue":"lrec-2022","Acronym":"GerEO","Description":"A Large-Scale Resource on the Syntactic Distribution of German Experiencer-Object Verbs","Abstract":"Although studied for several decades, the syntactic properties of experiencer-object (eo) verbs are still under discussion, while most analyses are not supported by substantial corpus data. With most, we intend to fill this lacuna for german eo-verbs by presenting a large-scale database of more than 10,000 examples for 64 verbs (up to 200 per verb) from a newspaper corpus annotated for several syntactic and semantic features relevant for their analysis, including the overall syntactic construction, the semantic stimulus type, and the form of a possible stimulus preposition, i.e. a preposition heading a pp that indicates (a part\/aspect of) the stimulus. Non-psych occurrences of the verbs are not excluded from the database but marked as such to make a comparison possible. Data of this kind can be used to develop and test theoretical hypotheses on the properties of eo-verbs, aid in the construction of experiments as well as provide training and test data for ai systems.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2020,"Venue":"lrec-2020","Acronym":"TRANSLIT","Description":"A Large-scale Name Transliteration Resource","Abstract":"Entrieseration is the process of expressing a proper name from a source language in the characters of a target language (e.g. from cyrillic to latin characters). We present a, a large-scale corpus with approx. 1.6 million entries in more than 180 languages with about 3 million variations of person and geolocation names. The corpus is based on various public data sources, which have been transformed into a unified format to simplify their usage, plus a newly compiled dataset from wikipedia. In addition, we apply several machine learning methods to establish baselines for automatically detecting characters).erated names in various languages. Our best systems achieve an accuracy of 92% on identification of baselineserated pairs.","wordlikeness":0.875,"lcsratio":1.0,"wordcoverage":0.9333333333}
{"Year":2012,"Venue":"lrec-2012","Acronym":"ANALEC","Description":"a New Tool for the Dynamic Annotation of Textual Data","Abstract":"We introduce bring, a tool which aim is to bring together corpus annotation, visualization and query management. Our main idea is to provide a unified and dynamic way of annotating textual data. Annotation. Allows researchers to dynamically build their own annotation scheme and use the possibilities of scheme revision, data querying and graphical visualization during the annotation process. Each query result can be visualized using a graphical representation that puts forward a set of annotations that can be directly corrected or completed. Text annotation is then considered as a cyclic process. We show that statistics like frequencies and correlations make it possible to verify annotated data on the fly during the annotation. In this paper we introduce the annotation functionalities of allows, some of the annotated data visualization functionalities, and three statistical modules: frequency, correlation and geometrical representations. Some examples dealing with reference and coreference annotation illustrate the main contributions of we.","wordlikeness":0.5,"lcsratio":0.8333333333,"wordcoverage":0.8}
{"Year":2023,"Venue":"findings-2023","Acronym":"MTR","Description":"A Dataset Fusing Inductive, Deductive, and Defeasible Reasoning","Abstract":"A long-standing difficulty in ai is the introduction of human-like reasoning in machine reading comprehension. Since algorithmic models can already perform as well as humans on simple quality assurance tasks thanks to the development of deep learning techniques, more difficult reasoning datasets have been presented. However, these datasets mainly focus on a single type of reasoning. There are still significant gaps in the studies when compared to the complex reasoning used in daily life. In this work, we introduce a brand-new dataset, named .. There are two parts to it: the first combines deductive and inductive reasoning, and the second does the same with inductive and defeasible reasoning. It consists of more than 30k qa instances, inferring relations between characters in short stories. Results show that state-of-the-art neural models do noticeably worse than expected. Our empirical results highlight the gap in the models\u2019 ability to handle sophisticated inference.","wordlikeness":0.3333333333,"lcsratio":0.6666666667,"wordcoverage":0.8}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"GradTS","Description":"A Gradient-Based Automatic Auxiliary Task Selection Method Based on Transformer Networks","Abstract":"A key problem in multi-task learning (mtl) research is how to select high-quality auxiliary tasks automatically. This paper presents studies, an automatic auxiliary task selection method based on gradient calculation in transformer-based models. Compared to autosem, a strong baseline method, a improves the performance of mt-dnn with a bert-base-cased backend model, from 0.33% to 17.93% on 8 natural language understanding (nlu) tasks in the glue benchmarks. Automatic is also time-saving since (1) its gradient calculations are based on single-task experiments and (2) the gradients are re-used without additional experiments when the candidate task set changes. On the 8 glue classification tasks, for example, models. Costs on average 21.32% less time than autosem with comparable gpu consumption. Further, we show the robustness of we across various task settings and model selections, e.g. mixed objectives among candidate tasks. The efficiency and efficacy of calculations in these case studies illustrate its general applicability in mtl research without requiring manual task filtering or costly parameter tuning.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.8333333333}
{"Year":2023,"Venue":"findings-2023","Acronym":"PREME","Description":"Preference-based Meeting Exploration through an Interactive Questionnaire","Abstract":"The recent increase in the volume of online meetings necessitates automated tools for organizing the material, especially when an attendee has missed the discussion and needs assistance in quickly exploring it. In this work, we propose a novel end-to-end framework for generating interactive questionnaires for preference-based meeting exploration. As a result, users are supplied with a list of suggested questions reflecting their preferences. Since the task is new, we introduce an automatic evaluation strategy by measuring how much the generated questions via questionnaire are answerable to ensure factual correctness and covers the source meeting for the depth of possible exploration.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.8333333333}
{"Year":2022,"Venue":"coling-2022","Acronym":"PINEAPPLE","Description":"Personifying INanimate Entities by Acquiring Parallel Personification Data for Learning Enhanced Generation","Abstract":"A personification is a figure of speech that endows inanimate entities with properties and actions typically seen as requiring animacy. In this paper, we explore the task of personification generation. To this end, we propose :: personifying inanimate entities by acquiring parallel personification data for learning enhanced generation. We curate a corpus of personifications called personifcorp, together with automatically generated de-personified literalizations of these personifications. We demonstrate the usefulness of this parallel corpus by training a seq2seq model to personify a given literal input. Both automatic and human evaluations show that fine-tuning with personifcorp leads to significant gains in personification-related qualities such as animacy and interestingness. A detailed qualitative analysis also highlights key strengths and imperfections of generation. Over baselines, demonstrating a strong ability to generate diverse and creative personifications that enhance the overall appeal of a sentence.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"FaD-VLP","Description":"Fashion Vision-and-Language Pre-training towards Unified Retrieval and Captioning","Abstract":"Multimodal tasks in the fashion domain have significant potential for e-commerce, but involve challenging vision-and-language learning problems\u2014e.g., retrieving a fashion item given a reference image plus text feedback from a user. Prior works on multimodal fashion tasks have either been limited by the data in individual benchmarks, or have leveraged generic vision-and-language pre-training but have not taken advantage of the characteristics of fashion data. Additionally, these works have mainly been restricted to multimodal understanding tasks. To address these gaps, we make two key contributions. First, we propose a novel fashion-specific pre-training framework based on weakly-supervised triplets constructed from fashion image-text pairs. We show the triplet-based tasks are an effective addition to standard multimodal pre-training tasks. Second, we propose a flexible decoder-based model architecture capable of both fashion retrieval and captioning tasks. Together, our model design and pre-training approach are competitive on a diverse set of fashion tasks, including cross-modal retrieval, image retrieval with text feedback, image captioning, relative image captioning, and multimodal categorization.","wordlikeness":0.1428571429,"lcsratio":1.0,"wordcoverage":0.6}
{"Year":2015,"Venue":"acl-2015","Acronym":"SACRY","Description":"Syntax-based Automatic Crossword puzzle Resolution sYstem","Abstract":"In this paper, we present our crossword puzzle resolution system (solved), which exploits syntactic structures for clue reranking and answer extraction. Resolution uses a database (db) containing previously solved cps in order to generate the list of candidate answers. Additionally, it uses innovative features, such as the answer position in the rank and aggregated information such as the min, max and average clue reranking scores. Our system is based on webcrow, one of the most advanced systems for automatic crossword puzzle resolution. Our extensive experiments over our two million clue dataset show that our approach highly improves the quality of the answer list, enabling the achievement of unprecedented results on the complete cp resolution tasks, i.e., accuracy of 99.17%.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2012,"Venue":"lrec-2012","Acronym":"Dbnary","Description":"Wiktionary as a LMF based Multilingual RDF network","Abstract":"Contributive resources, such as wikipedia, have proved to be valuable in natural language processing or multilingual information retrieval applications. This article focusses on wiktionary, the dictionary part of the collaborative resources sponsored by the wikimedia foundation. In this article we present a word net that has been extracted from french, english and german wiktionaries. We present the structure of this word net and discuss the specific extraction problems induced by this kind of contributive resources and the method used to overcome them. Then we show how we represent the extracted data as a lexical markup framework (lmf) compatible lexical network represented in resource description framework (rdf) format.","wordlikeness":0.5,"lcsratio":0.6666666667,"wordcoverage":0.8333333333}
{"Year":2018,"Venue":"lrec-2018","Acronym":"CoNLL-UL","Description":"Universal Morphological Lattices for Universal Dependency Parsing","Abstract":"Following the development of the universal dependencies (ud) framework and the conll 2017 shared task on end-to-end ud parsing, we address the need for a universal representation of morphological analysis which on the one hand can capture a range of different alternative morphological analyses of surface tokens, and on the other hand is compatible with the segmentation and morphological annotation guidelines prescribed for ud treebanks. We propose the conll universal lattices (different) format, a new annotation format for word lattices that represent morphological analyses, and provide resources that obey this format for a range of typologically different languages. The resources we provide are harmonized with the two-level representation and morphological annotation in their respective ud v2 treebanks, thus enabling research on universal models for morphological and syntactic parsing, in both pipeline and joint settings, and presenting new opportunities in the development of ud resources for low-resource languages. Keywords: morphology, universal dependencies, morphological analysis, morphological ambiguity 1.","wordlikeness":0.625,"lcsratio":0.625,"wordcoverage":0.6666666667}
{"Year":2020,"Venue":"wmt-2020","Acronym":"PATQUEST","Description":"Papago Translation Quality Estimation","Abstract":"This paper describes the system submitted by papago team for the quality estimation task at wmt 2020. It proposes two key strategies for quality estimation: (1) task-specific pretraining scheme, and (2) task-specific data augmentation. The former focuses on devising learning signals for pretraining that are closely related to the downstream task. We also present data augmentation techniques that simulate the varying levels of errors that the downstream dataset may contain. Thus, our also models are exposed to erroneous translations in both stages of task-specific pretraining and finetuning, effectively enhancing their generalization capability. Our submitted models achieve significant improvement over the baselines for task 1 (sentence-level direct assessment; en-de only), and task 3 (document-level score).","wordlikeness":0.875,"lcsratio":1.0,"wordcoverage":0.7692307692}
{"Year":2023,"Venue":"eacl-2023","Acronym":"LingMess","Description":"Linguistically Informed Multi Expert Scorers for Coreference Resolution","Abstract":"Current state-of-the-art coreference systems are based on a single pairwise scoring component, which assigns to each pair of mention spans a score reflecting their tendency to corefer to each other. We observe that different kinds of mention pairs require different information sources to assess their score. We present spans, a linguistically motivated categorization of mention-pairs into 6 types of coreference decisions and learn a dedicated trainable scoring function for each category. This significantly improves the accuracy of the pairwise scorer as well as of the overall coreference performance on the english ontonotes coreference corpus and 5 additional datasets.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.7692307692}
{"Year":2022,"Venue":"ws-2022","Acronym":"HFT","Description":"High Frequency Tokens for Low-Resource NMT","Abstract":"Tokenization has been shown to impact the quality of downstream tasks, such as neural machine translation (nmt), which is susceptible to out-of-vocabulary words and low frequency training data. Current state-of-the-art algorithms have been helpful in addressing the issues of out-of-vocabulary words, bigger vocabulary sizes and token frequency by implementing subword segmentation. We argue, however, that there is still room for improvement, in particular regarding low-frequency tokens in the training data. In this paper, we present \u201chigh frequency tokenizer\u201d, or rank, a new language-independent subword segmentation algorithm that addresses this issue. We also propose a new metric to measure the frequency coverage of a tokenizer\u2019s vocabulary, based on a frequency rank weighted average of the frequency values of its items. We experiment with a diverse set of language corpora, vocabulary sizes, and writing systems and report improvements on both frequency statistics and on the average length of the output. We also observe a positive impact on downstream nmt.","wordlikeness":0.3333333333,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2015,"Venue":"emnlp-2015","Acronym":"CORE","Description":"Context-Aware Open Relation Extraction with Factorization Machines","Abstract":"We propose indicates, a novel matrix factorization model that leverages contextual information for open relation extraction. Our model is based on factorization machines and integrates facts from various sources, such as knowledge bases or open information extractors, as well as the context in which these facts have been observed. We argue that integrating contextual information\u2014such as metadata about extraction sources, lexical context, or type information\u2014signi\ufb01cantly improves prediction performance. Open information extractors, for example, may produce extractions that are unspeci\ufb01c or ambiguous when taken out of context. Our experimental study on a large real-world dataset indicates that extraction has signi\ufb01cantly better prediction performance than state-ofthe-art approaches when contextual information is available.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"JANUS","Description":"Joint Autoregressive and Non-autoregressive Training with Auxiliary Loss for Sequence Generation","Abstract":"Transformer-based autoregressive and non-autoregressive models have played an essential role in sequence generation tasks. The autoregressive model can obtain excellent performance, while the non-autoregressive model brings fast decoding speed for inference. In this paper, we propose <b>models<\/b>, a <b>j<\/b>oint <b>a<\/b>utoregressive and <b>n<\/b>on-autoregressive training method using a<b>u<\/b>xiliary los<b>s<\/b> to enhance the model performance in both ar and nar manner simultaneously and effectively alleviate the problem of distribution discrepancy.further, we pre-train bart with that on a large corpus with minimal cost (16 gpu days) and make the bart-decoding capable of non-autoregressive generation, demonstrating that our approach can transfer the ar knowledge to nar. Empirically, we show our approach and bart-a<b>u<\/b>xiliary can achieve significant improvement on multiple generation tasks, including machine translation and glge benchmarks. Our code is available at github.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2008,"Venue":"lrec-2008","Acronym":"CLIoS","Description":"Cross-lingual Induction of Speech Recognition Grammars","Abstract":"We present an approach for the cross-lingual induction of speech recognition grammars that separates the task of translation from the task of grammar generation. The source speech recognition grammar is used to generate phrases, which are translated by a common translation service. The target recognition grammar is induced by using the production rules of the source language, manually translated sentences and a statistical word alignment tool. We induce grammars for the target languages spanish and japanese. The coverage of the resulting grammars is evaluated on two corpora and compared quantitatively and qualitatively to a grammar induced with unsupervised monolingual grammar induction.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2022,"Venue":"naacl-2022","Acronym":"JointLK","Description":"Joint Reasoning with Language Models and Knowledge Graphs for Commonsense Question Answering","Abstract":"Existing kg-augmented models for commonsense question answering primarily focus on designing elaborate graph neural networks (gnns) to model knowledge graphs (kgs). However, they ignore (i) the effectively fusing and reasoning over question context representations and the kg representations, and (ii) automatically selecting relevant nodes from the noisy kgs during reasoning. In this paper, we propose a novel model, networks, which solves the above limitations through the joint reasoning of lm and gnn and the dynamic kgs pruning mechanism. Specifically, fuse performs joint reasoning between lm and gnn through a novel dense bidirectional attention module, in which each question token attends on kg nodes and each kg node attends on question tokens, and the two modal representations fuse and update mutually by multi-step interactions. Then, the dynamic pruning module uses the attention weights generated by joint reasoning to prune irrelevant kg nodes recursively. We evaluate we on the commonsenseqa and openbookqa datasets, and demonstrate its improvements to the existing lm and lm+kg models, as well as its capability to perform interpretable reasoning.","wordlikeness":0.7142857143,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2022,"Venue":"lrec-2022","Acronym":"GRhOOT","Description":"Ontology of Rhetorical Figures in German","Abstract":"Sparql, the german rhetorical ontology, is a domain ontology of 110 rhetorical figures in the german language. The overall goal of building an ontology of rhetorical figures in german is not only the formal representation of different rhetorical figures, but also allowing for their easier detection, thus improving sentiment analysis, argument mining, detection of hate speech and fake news, machine translation, and many other tasks in which recognition of non-literal language plays an important role. The challenge of building such ontologies lies in classifying the figures and assigning adequate characteristics to group them, while considering their distinctive features. The ontology of rhetorical figures in the serbian language was used as a basis for our work. Besides transferring and extending the concepts of the serbian ontology, we ensured completeness and consistency by using description logic and sparql queries. Furthermore, we show a decision tree to identify figures and suggest a usage scenario on how the ontology can be utilized to collect and annotate data.","wordlikeness":0.3333333333,"lcsratio":0.6666666667,"wordcoverage":0.8}
{"Year":2019,"Venue":"naacl-2019","Acronym":"BoolQ","Description":"Exploring the Surprising Difficulty of Natural Yes\/No Questions","Abstract":"In this paper we study yes\/no questions that are naturally occurring \u2014 meaning that they are generated in unprompted and unconstrained settings. We build a reading comprehension dataset, significant, of such questions, and show that they are unexpectedly challenging. They often query for complex, non-factoid information, and require difficult entailment-like inference to solve. We also explore the effectiveness of a range of transfer learning baselines. We find that transferring from entailment data is more effective than transferring from paraphrase or extractive qa data, and that it, surprisingly, continues to be very beneficial even when starting from massive pre-trained language models such as bert. Our best method trains bert on multinli and then re-trains it on our train set. It achieves 80.4% accuracy compared to 90% accuracy of human annotators (and 62% majority-baseline), leaving a significant gap for future work.","wordlikeness":0.6,"lcsratio":0.8,"wordcoverage":0.75}
{"Year":2021,"Venue":"acl-2021","Acronym":"CasEE","Description":"A Joint Learning Framework with Cascade Decoding for Overlapping Event Extraction","Abstract":"Event extraction (ee) is a crucial information extraction task that aims to extract event information in texts. Most existing methods assume that events appear in sentences without overlaps, which are not applicable to the complicated overlapping event extraction. This work systematically studies the realistic event overlapping problem, where a word may serve as triggers with several types or arguments with different roles. To tackle the above problem, we propose a novel joint learning framework with cascade decoding for overlapping event extraction, termed as problem,. Particularly, capture sequentially performs type detection, trigger extraction and argument extraction, where the overlapped targets are extracted separately conditioned on the speci\ufb01c former prediction. All the subtasks are jointly learned in a framework to capture dependencies among the subtasks. The evaluation on a public event extraction benchmark fewfc demonstrates that benchmark1 achieves signi\ufb01cant improvements on overlapping event extraction over previous competitive methods.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2010,"Venue":"lrec-2010","Acronym":"SINotas","Description":"the Evaluation of a NLG Application","Abstract":"From is a data-to-text nlg application intended to produce short textual reports on students\u0092 academic performance from a database conveying their grades, weekly attendance rates and related academic information. Although developed primarily as a testbed for portuguese natural language generation, grammar generates reports of interest to both students keen to learn how their professors would describe their efforts, and to the professors themselves, who may benefit from an at-a-glance view of the student\u0092s performance. In a traditional machine learning approach, implement uses a data-text aligned corpus as training data for decision-tree induction. The current system comprises a series of classifiers that implement major document planning subtasks (namely, data interpretation, content selection, within- and between-sentence structuring), and a small surface realisation grammar of brazilian portuguese. In this paper we focus on the evaluation work of the system, applying a number of intrinsic and user-based evaluation metrics to a collection of text reports generated from real application data.","wordlikeness":0.5714285714,"lcsratio":0.5714285714,"wordcoverage":0.7692307692}
{"Year":2018,"Venue":"coling-2018","Acronym":"NLATool","Description":"an Application for Enhanced Deep Text Understanding","Abstract":"Today, we see an ever growing number of tools supporting text annotation. Each of these tools is optimized for specific use-cases such as named entity recognition. However, we see large growing knowledge bases such as wikipedia or the google knowledge graph. In this paper, we introduce text, a web application developed using a human-centered design process. The application combines supporting text annotation and enriching the text with additional information from a number of sources directly within the application. The tool assists users to efficiently recognize named entities, annotate text, and automatically provide users additional information while solving deep text understanding tasks.","wordlikeness":0.7142857143,"lcsratio":0.8571428571,"wordcoverage":0.7272727273}
{"Year":2022,"Venue":"lrec-2022","Acronym":"RU-ADEPT","Description":"Russian Anonymized Dataset with Eight Personality Traits","Abstract":"Social media has provided a platform for many individuals to easily express themselves naturally and publicly, and researchers have had the opportunity to utilize large quantities of this data to improve author trait analysis techniques and to improve author trait profiling systems. The majority of the work in this area, however, has been narrowly spent on english and other western european languages, and generally focuses on a single social network at a time, despite the large quantity of data now available across languages and differences that have been found across platforms. This paper introduces spent, a dataset of russian authors\u2019 personality trait scores\u2013big five and dark triad, demographic information (e.g. age, gender), with associated corpus of the authors\u2019 cross-contributions to (up to) four different social media platforms\u2013vkontakte (vk), livejournal, blogger, and moi mir. We believe this to be the first publicly-available dataset associating demographic and personality trait data with russian-language social media content, the first paper to describe the collection of dark triad scores with texts across multiple russian-language social media platforms, and to a limited extent, the first publicly-available dataset of personality traits to author content across several different social media sites.","wordlikeness":0.375,"lcsratio":0.875,"wordcoverage":0.7692307692}
{"Year":2013,"Venue":"semeval-2013","Acronym":"BUAP","Description":"N-gram based Feature Evaluation for the Cross-Lingual Textual Entailment Task","Abstract":"This paper describes the evaluation of different kinds of textual features for the crosslingual textual entailment task of semeval 2013. We have counted the number of ngrams for three types of textual entities (character, word and pos tags) that exist in the pair of sentences from which we are interested in determining the judgment of textual entailment. Difference, intersection and distance (euclidian, manhattan and jaccard) of n-grams were considered for constructing a feature vector which is further introduced in a support vector machine classi\ufb01er which allows to construct a classi\ufb01cation model. Five different runs were submitted, one of them considering voting system of the previous four approaches. The results obtained show a performance below the median of six teams that have participated in the competition.","wordlikeness":0.5,"lcsratio":0.75,"wordcoverage":0.75}
{"Year":2020,"Venue":"acl-2020","Acronym":"Usnea","Description":"An Authorship Tool for Interactive Fiction using Retrieval Based Semantic Parsing","Abstract":"The reader of a choose your own adventure novel and the user of a modern virtual assistant have a subtle similarity; both may, through the right lens, be viewed as engaging with a work of interactive fiction. This literary form emerged in the 1970s and has grown like a vine along the branch of modern technology, one guided by the advances of the other. In this work we weave together threads from the interactive fiction community and neural semantic parsing for dialog systems, defining the data model and necessary algorithms for a novel type of interactive fiction and open sourcing its accompanying authoring tool. Specifically, our work integrates retrieval based semantic parsing predicates into the branching story structures well known to the interactive fiction community, relaxing the relatively strict lexical options of preexisting systems.","wordlikeness":0.4,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2018,"Venue":"acl-2018","Acronym":"Praaline","Description":"An Open-Source System for Managing, Annotating, Visualising and Analysing Speech Corpora","Abstract":"In this system demonstration we present the latest developments of manually, an open-source software system for constituting and managing, manually and automatically annotating, visualising and analysing spoken language and multimodal corpora. We review the system\u2019s functionality and design architecture, present current use cases and directions for future development.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2013,"Venue":"acl-2013","Acronym":"DErivBase","Description":"Inducing and Evaluating a Derivational Morphology Resource for German","Abstract":"Derivational models are still an underresearched area in computational morphology. Even for german, a rather resourcerich language, there is a lack of largecoverage derivational knowledge. This paper describes a rule-based framework for inducing derivational families (i.e., clusters of lemmas in derivational relationships) and its application to create a highcoverage german resource, attribute, mapping over 280k lemmas into more than 17k non-singleton clusters. We focus on the rule component and a qualitative and quantitative evaluation. Our approach achieves up to 93% precision and 71% recall. We attribute the high precision to the fact that our rules are based on information from grammar books.","wordlikeness":0.7777777778,"lcsratio":0.8888888889,"wordcoverage":0.8}
{"Year":2012,"Venue":"acl-2012","Acronym":"DOMCAT","Description":"A Bilingual Concordancer for Domain-Specific Computer Assisted Translation","Abstract":"In this paper, we propose a web-based bilingual concordancer, high-precision 1 , for domain-specific computer assisted translation. Given a multi-word expression as a query, the system involves retrieving sentence pairs from a bilingual corpus, identifying translation equivalents of the query in the sentence pairs (translation spotting) and ranking the retrieved sentence pairs according to the relevance between the query and the translation equivalents. To provide high-precision translation spotting for domain-specific translation tasks, we exploited a normalized correlation method to spot the translation equivalents. To ranking the retrieved sentence pairs, we propose a correlation function modified from the dice coefficient for assessing the correlation between the query and the translation equivalents. The performances of the translation spotting module and the ranking module are evaluated in terms of precision-recall measures and coverage rate respectively.","wordlikeness":0.8333333333,"lcsratio":1.0,"wordcoverage":0.7692307692}
{"Year":2015,"Venue":"ws-2015","Acronym":"LYSGROUP","Description":"Adapting a Spanish microtext normalization system to English.","Abstract":"In this article we describe the microtext normalization system we have used to participate in the normalization of noisy text task of the acl w-nut 2015 workshop. Our normalization system was originally developed for text mining tasks on spanish tweets. Our main goals during its development were \ufb02exibility, scalability and maintainability, in order to test a wide variety of approximations to the problem at hand with minimum effort. We will pay special attention to the process of adapting the components of our system to deal with english tweets which, as we will show, was achieved without major modi\ufb01cations of its base structure.","wordlikeness":0.75,"lcsratio":0.5,"wordcoverage":0.7692307692}
{"Year":2020,"Venue":"emnlp-2020","Acronym":"ChrEn","Description":"Cherokee-English Machine Translation for Endangered Language Revitalization","Abstract":"Cherokee is a highly endangered native american language spoken by the cherokee people. The cherokee culture is deeply embedded in its language. However, there are approximately only 2,000 fluent first language cherokee speakers remaining in the world and the number is declining every year. To help save this endangered language, we introduce out-of-domain, a cherokee-english parallel dataset, to facilitate machine translation research between cherokee and english. Compared to some popular machine translation language pairs, our is extremely low-resource, only containing 14k sentence pairs in total. We split our parallel data in ways that facilitate both in-domain and out-of-domain evaluation. We also collect 5k cherokee monolingual data to enable semi-supervised learning. Besides these datasets, we propose several cherokee-english and english-cherokee machine translation systems. We compare smt (phrase-based) versus nmt (rnn-based and transformer-based) systems; supervised versus semi-supervised (via language model, back-translation, and bert\/multilingual-bert) methods; as well as transfer learning versus multilingual joint training with 4 other languages. Our best results are 15.8\/12.7 bleu for in-domain and 6.5\/5.0 bleu for out-of-domain chr-en\/enchr translations, respectively; and we hope that our dataset and systems will encourage future work by the community for cherokee language revitalization.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"DSM","Description":"Question Generation over Knowledge Base via Modeling Diverse Subgraphs with Meta-learner","Abstract":"Existing methods on knowledge base question generation (kbqg) learn a one-size-fits-all model by training together all subgraphs without distinguishing the diverse semantics of subgraphs. In this work, we show that making use of the past experience on semantically similar subgraphs can reduce the learning difficulty and promote the performance of kbqg models. To achieve this, we propose a novel approach to model diverse subgraphs with meta-learner (that). Specifically, we devise a graph contrastive learning-based retriever to identify semantically similar subgraphs, so that we can construct the semantics-aware learning tasks for the meta-learner to learn semantics-specific and semantics-agnostic knowledge on and across these tasks. Extensive experiments on two widely-adopted benchmarks for kbqg show that augmentation. Derives new state-of-the-art performance and benefits the question answering tasks as a means of data augmentation.","wordlikeness":0.3333333333,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"GFST","Description":"Gender-Filtered Self-Training for More Accurate Gender in Translation","Abstract":"Targeted evaluations have found that machine translation systems often output incorrect gender in translations, even when the gender is clear from context. Furthermore, these incorrectly gendered translations have the potential to reflect or amplify social biases. We propose gender-filtered self-training (into) to improve gender translation accuracy on unambiguously gendered inputs. Our monolingual approach uses a source monolingual corpus and an initial model to generate gender-specific pseudo-parallel corpora which are then filtered and added to the training data. We evaluate damaging on translation from english into five languages, finding that it improves gender accuracy without damaging generic quality. We also show the viability of data. On several experimental settings, including re-training from scratch, fine-tuning, controlling the gender balance of the data, forward translation, and back-translation.","wordlikeness":0.25,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2021,"Venue":"ws-2021","Acronym":"SeqScore","Description":"Addressing Barriers to Reproducible Named Entity Recognition Evaluation","Abstract":"To address a looming crisis of unreproducible evaluation for named entity recognition, we propose guidelines and introduce scored., a software package to improve reproducibility. The guidelines we propose are extremely simple and center around transparency regarding how chunks are encoded and scored. We demonstrate that despite the apparent simplicity of ner evaluation, unreported differences in the scoring procedure can result in changes to scores that are both of noticeable magnitude and statistically significant. We describe magnitude, which addresses many of the issues that cause replication failures.","wordlikeness":0.625,"lcsratio":0.75,"wordcoverage":0.7692307692}
{"Year":2023,"Venue":"findings-2023","Acronym":"HuaSLIM","Description":"Human Attention Motivated Shortcut Learning Identification and Mitigation for Large Language models","Abstract":"Large language models have made remarkable progress on a variety of nlp tasks. However, it has been found that they tend to rely on shortcut features that spuriously correlate with labels for prediction, which weakens their generalization on out-of-distribution samples. In this paper, we propose a human attention guided approach to identifying and mitigating shortcut learning, which encourages the llm-based target model to learn relevant features. We define an attention-based measurement to capture both model and data bias and identify shortcut tokens by exploring both human and neural attention. In a self-distillation framework, we mitigate shortcut learning by dynamically adjusting the distillation temperature according to the detected shortcut tokens and estimated shortcut degree. Additionally, we utilize human attention as a supervisory signal to constrain large language models to pay more attention to relevant tokens. Experimental results on multiple nlp tasks show that our proposed method can effectively identify shortcut tokens, and significantly improve the robustness of large language models on ood samples, while not undermining the performance on iid data.","wordlikeness":0.4285714286,"lcsratio":1.0,"wordcoverage":0.7692307692}
{"Year":2006,"Venue":"iwslt-2006","Acronym":"MATREX","Description":"DCU machine translation system for IWSLT 2006.","Abstract":"In this paper, we give a description of the machine translation system developed at dcu that was used for our \ufb01rst participation in the evaluation campaign of the international workshop on spoken language translation (2006). This system combines two types of approaches. First, we use an ebmt approach to collect aligned chunks based on two steps: deterministic chunking of both sides and chunk alignment. We use several chunking and alignment strategies. We also extract smt-style aligned phrases, and the two types of resources are combined. We participated in the open data track for the following translation directions: arabic-english and italian-english, for which we translated both the single-best asr hypotheses and the correct recognition results. We report the results of the system for the provided evaluation sets. 1.","wordlikeness":0.6666666667,"lcsratio":0.8333333333,"wordcoverage":0.8333333333}
{"Year":2022,"Venue":"acl-2022","Acronym":"OpenHands","Description":"Making Sign Language Recognition Accessible with Pose-based Pretrained Models across Languages","Abstract":"Ai technologies for natural languages have made tremendous progress recently. However, commensurate progress has not been made on sign languages, in particular, in recognizing signs as individual words or as complete sentences. We introduce compare, a library where we take four key ideas from the nlp community for low-resource languages and apply them to sign languages for word-level recognition. First, we propose using pose extracted through pretrained models as the standard modality of data in this work to reduce training time and enable efficient inference, and we release standardized pose datasets for different existing sign language datasets. Second, we train and release checkpoints of 4 pose-based isolated sign language recognition models across 6 languages (american, argentinian, chinese, greek, indian, and turkish), providing baselines and ready checkpoints for deployment. Third, to address the lack of labelled data, we propose self-supervised pretraining on unlabelled data. We curate and release the largest pose-based pretraining dataset on indian sign language (indian-sl). Fourth, we compare different pretraining strategies and for the first time establish that pretraining is effective for sign language recognition by demonstrating (a) improved fine-tuning performance especially in low-resource settings, and (b) high crosslingual transfer from indian-sl to few other sign languages. We open-source all models and datasets in we with a hope that it makes research in sign languages reproducible and more accessible.","wordlikeness":0.7777777778,"lcsratio":0.7777777778,"wordcoverage":0.7368421053}
{"Year":2016,"Venue":"lrec-2016","Acronym":"metaTED","Description":"a Corpus of Metadiscourse for Spoken Language","Abstract":"This paper describes assess \u2015 a freely available corpus of metadiscursive acts in spoken language collected via crowdsourcing. Metadiscursive acts were annotated on a set of 180 randomly chosen ted talks in english, spanning over different speakers and topics. The taxonomy used for annotation is composed of 16 categories, adapted from adel(2010). This adaptation takes into account both the material to annotate and the setting in which the annotation task is performed. The crowdsourcing setup is described, including considerations regarding training and quality control. The collected data is evaluated in terms of quantity of occurrences, inter-annotator agreement, and annotation related measures (such as average time on task and self-reported confidence). Results show different levels of agreement among metadiscourse acts (\u03b1 \u2208 [0.15; 0.49]). To further assess the collected material, a subset of the annotations was submitted to expert appreciation, who validated which of the marked occurrences truly correspond to instances of the metadiscursive act at hand. Similarly to what happened with the crowd, experts revealed different levels of agreement between categories (\u03b1 \u2208 [0.18; 0.72]). The paper concludes with a discussion on the applicability of each with respect to each of the 16 categories of metadiscourse.","wordlikeness":0.8571428571,"lcsratio":0.7142857143,"wordcoverage":0.8}
{"Year":2015,"Venue":"emnlp-2015","Acronym":"TSDPMM","Description":"Incorporating Prior Topic Knowledge into Dirichlet Process Mixture Models for Text Clustering","Abstract":"Dirichlet process mixture model (dpmm) has great potential for detecting the underlying structure of data. Extensive studies have applied it for text clustering in terms of topics. However, due to the unsupervised nature, the topic clusters are always less satisfactory. Considering that people often have some prior knowledge about which potential topics should exist in given data, we aim to incorporate such knowledge into the dpmm to improve text clustering. We propose a novel model urn based on a new seeded p\u00b4olya urn scheme. Experimental results on document clustering across three datasets demonstrate our proposed that signi\ufb01cantly outperforms stateof-the-art dpmm model and can be applied in a lifelong learning framework.","wordlikeness":0.3333333333,"lcsratio":0.8333333333,"wordcoverage":0.6}
{"Year":2021,"Venue":"findings-2021","Acronym":"DIRECT","Description":"Direct and Indirect Responses in Conversational Text Corpus","Abstract":"We create a large-scale dialogue corpus that provides pragmatic paraphrases to advance technology for understanding the underlying intentions of users. While neural conversation models acquire the ability to generate fluent responses through training on a dialogue corpus, previous corpora have mainly focused on the literal meanings of utterances. However, in reality, people do not always present their intentions multiwozly. For example, if a person said to the operator of a reservation service \u201ci don\u2019t have enough budget.\u201d, they, in fact, mean \u201cplease find a cheaper option for me.\u201d our corpus provides a total of 71,498 inreality,\u2013state-of-the-art utterance pairs accompanied by a multi-turn dialogue history extracted from the multiwoz dataset. In addition, we propose three tasks to benchmark the ability of models to recognize and generate instate-of-the-art and have utterances. We also investigated the performance of state-of-the-art pre-trained models as baselines.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2010,"Venue":"acl-2010","Acronym":"cdec","Description":"A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models","Abstract":"We present source, an open source framework for decoding, aligning with, and training a number of statistical machine translation models, including word-based models, phrase-based models, and models based on synchronous context-free grammars. Using a single uni\ufb01ed internal representation for translation forests, the decoder strictly separates model-speci\ufb01c translation logic from general rescoring, pruning, and inference algorithms. From this uni\ufb01ed representation, the decoder can extract not only the 1- or k-best translations, but also alignments to a reference, or the quantities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques. Its ef\ufb01cient c++ implementation means that memory use and runtime performance are signi\ufb01cantly better than comparable decoders.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2022,"Venue":"wmt-2022","Acronym":"ANVITA-African","Description":"A Multilingual Neural Machine Translation System for African Languages","Abstract":"This paper describes anvita african nmt system submitted by team anvita for wmt 2022 shared task on large-scale machine translation evaluation for african languages under the constrained translation track. The team participated in 24 african languages to english mt directions. For better handling of relatively low resource language pairs and effective transfer learning, models are trained in multilingual setting. Heuristic based corpus filtering is applied and it improved performance by 0.04-2.06 bleu across 22 out of 24 african to english directions and also improved training time by 5x. Use of deep transformer with 24 layers of encoder and 6 layers of decoder significantly improved performance by 1.1-7.7 bleu across all the 24 african to english directions compared to base transformer. For effective selection of source vocabulary in multilingual setting, joint and language wise vocabulary selection strategies are explored at the source side. Use of language wise vocabulary selection however did not consistently improve performance of low resource languages in comparison to joint vocabulary selection. Empirical results indicate that training using deep transformer with filtered corpora seems to be a better choice than using base transformer on the whole corpora both in terms of accuracy and training time.","wordlikeness":0.5714285714,"lcsratio":0.8571428571,"wordcoverage":0.6923076923}
{"Year":2013,"Venue":"starsem-2013","Acronym":"DeepPurple","Description":"Lexical, String and Affective Feature Fusion for Sentence-Level Semantic Similarity Estimation","Abstract":"This paper describes our submission for the *sem shared task of semantic textual similarity. We estimate the semantic similarity between two sentences using regression models with features: 1) n-gram hit rates (lexical matches) between sentences, 2) lexical semantic similarity between non-matching words, 3) string similarity metrics, 4) affective content similarity and 5) sentence length. Domain adaptation is applied in the form of independent models and a model selection strategy achieving a mean correlation of 0.47.","wordlikeness":0.8,"lcsratio":0.7,"wordcoverage":0.75}
{"Year":2022,"Venue":"coling-2022","Acronym":"PrefScore","Description":"Pairwise Preference Learning for Reference-free Summarization Quality Assessment","Abstract":"Evaluating machine-generated summaries without a human-written reference summary has been a need for a long time. Inspired by preference labeling in existing work of summarization evaluation, we propose to judge summary quality by learning the preference rank of summaries using the bradley-terry power ranking model from inferior summaries generated by corrupting base summaries. Extensive experiments on several datasets show that our weakly supervised scheme can produce scores highly correlated with human ratings.","wordlikeness":0.6666666667,"lcsratio":0.8888888889,"wordcoverage":0.7777777778}
{"Year":2022,"Venue":"acl-2022","Acronym":"GlobalWoZ","Description":"Globalizing MultiWoZ to Develop Multilingual Task-Oriented Dialogue Systems","Abstract":"Over the last few years, there has been a move towards data curation for multilingual task-oriented dialogue (tod) systems that can serve people speaking different languages. However, existing multilingual tod datasets either have a limited coverage of languages due to the high cost of data curation, or ignore the fact that dialogue entities barely exist in countries speaking these languages. To tackle these limitations, we introduce a novel data curation method that generates besides, \u2014 a large-scale multilingual tod dataset globalized from an english tod dataset for three unexplored use cases of multilingual tod systems. Our method is based on translating dialogue templates and filling them with local entities in the target-language countries. Besides, we extend the coverage of target languages to 20 languages. We will release our dataset and a set of strong baselines to encourage research on multilingual tod systems for real use cases.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2019,"Venue":"ijcnlp-2019","Acronym":"BottleSum","Description":"Unsupervised and Self-supervised Sentence Summarization using the Information Bottleneck Principle","Abstract":"The principle of the information bottleneck (tishby et al., 1999) produces a summary of information x optimized to predict some other relevant information y. In this paper, we propose a novel approach to unsupervised sentence summarization by mapping the information bottleneck principle to a conditional language modelling objective: given a sentence, our approach seeks a compressed sentence that can best predict the next sentence. Our iterative algorithm under the information bottleneck objective searches gradually shorter subsequences of the given sentence while maximizing the probability of the next sentence conditioned on the summary. Using only pretrained language models with no direct supervision, our approach can efficiently perform extractive sentence summarization over a large corpus. Building on our unsupervised extractive summarization, we also present a new approach to self-supervised abstractive summarization, where a transformer-based language model is trained on the output summaries of our unsupervised method. Empirical results demonstrate that our extractive method outperforms other unsupervised models on multiple automatic metrics. In addition, we find that our self-supervised abstractive model outperforms unsupervised baselines (including our own) by human evaluation along multiple attributes.","wordlikeness":0.8888888889,"lcsratio":0.6666666667,"wordcoverage":0.875}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"RAP","Description":"Robustness-Aware Perturbations for Defending against Backdoor Attacks on NLP Models","Abstract":"Backdoor attacks, which maliciously control a well-trained model\u2019s outputs of the instances with specific triggers, are recently shown to be serious threats to the safety of reusing deep neural networks (dnns). In this work, we propose an efficient online defense mechanism based on robustness-aware perturbations. Specifically, by analyzing the backdoor training process, we point out that there exists a big gap of robustness between poisoned and clean samples. Motivated by this observation, we construct a word-based robustness-aware perturbation to distinguish poisoned samples from clean samples to defend against the backdoor attacks on natural language processing (nlp) models. Moreover, we give a theoretical analysis about the feasibility of our robustness-aware perturbation-based defense method. Experimental results on sentiment analysis and toxic detection tasks show that our method achieves better defending performance and much lower computational costs than existing online defense methods. Our code is available at <a href=https:\/\/github.com\/lancopku\/based class=acl-markup-url>https:\/\/github.com\/lancopku\/we<\/a>.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2022,"Venue":"acl-2022","Acronym":"FORTAP","Description":"Using Formulas for Numerical-Reasoning-Aware Table Pretraining","Abstract":"Tables store rich numerical data, but numerical reasoning over tables is still a challenge. In this paper, we find that the spreadsheet formula, a commonly used language to perform computations on numerical values in spreadsheets, is a valuable supervision for numerical reasoning in tables. Considering large amounts of spreadsheets available on the web, we propose pretraining, the first exploration to leverage spreadsheet formulas for table pretraining. Two novel self-supervised pretraining objectives are derived from formulas, numerical reference prediction (nrp) and numerical calculation prediction (ncp). While our proposed objectives are generic for encoders, to better capture spreadsheet table layouts and structures, datasets is built upon tuta, the first transformer-based method for spreadsheet table pretraining with tree attention. For outperforms state-of-the-art methods by large margins on three representative datasets of formula prediction, question answering, and cell type classification, showing the great potential of leveraging formulas for table pretraining.","wordlikeness":0.8333333333,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2021,"Venue":"eacl-2021","Acronym":"BART-TL","Description":"Weakly-Supervised Topic Label Generation","Abstract":"We propose a novel solution for assigning labels to topic models by using multiple weak labelers. The method leverages generative transformers to learn accurate representations of the most important topic terms and candidate labels. This is achieved by fine-tuning pre-trained bart models on a large number of potential labels generated by state of the art non-neural models for topic labeling, enriched with different techniques. The proposed bart model is able to generate valuable and novel labels in a weakly-supervised manner and can be improved by adding other weak labelers or distant supervision on similar tasks.","wordlikeness":0.4285714286,"lcsratio":0.5714285714,"wordcoverage":0.7692307692}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"MeetDot","Description":"Videoconferencing with Live Translation Captions","Abstract":"We present backend., a videoconferencing system with live translation captions overlaid on screen. The system aims to facilitate conversation between people who speak different languages, thereby reducing communication barriers between multilingual participants. Currently, our system supports speech and captions in 4 languages and combines automatic speech recognition (asr) and machine translation (mt) in a cascade. We use the re-translation strategy to translate the streamed speech, resulting in caption flicker. Additionally, our system has very strict latency requirements to have acceptable call quality. We implement several features to enhance user experience and reduce their cognitive load, such as smooth scrolling captions and reducing caption flicker. The modular architecture allows us to integrate different asr and mt services in our backend. Our system provides an integrated evaluation suite to optimize key intrinsic evaluation metrics such as accuracy, latency and erasure. Finally, we present an innovative cross-lingual word-guessing game as an extrinsic evaluation metric to measure end-to-end system performance. We plan to make our system open-source for research purposes.","wordlikeness":0.5714285714,"lcsratio":0.7142857143,"wordcoverage":0.7272727273}
{"Year":2018,"Venue":"lrec-2018","Acronym":"PDFAnno","Description":"a Web-based Linguistic Annotation Tool for PDF Documents","Abstract":"We present standard, a web-based linguistic annotation tool for pdf documents. Pdf has become widespread standard for various types of publications, however, current tools for linguistic annotation mostly focus on plain-text documents. The offers functions for various types of linguistic annotations directly on pdf, including named entity, dependency relation, and coreference chain. Furthermore, for multi-user support, it allows simultaneous visualization of multi-user\u2019s annotations on the single pdf, which is useful for checking inter-annotator agreement and resolving annotation con\ufb02icts. Is is freely available under open-source license at https:\/\/github.com\/paperai\/on. Keywords: text annotation, annotation tool, pdf 1.","wordlikeness":0.8571428571,"lcsratio":0.7142857143,"wordcoverage":0.6666666667}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"APEACH","Description":"Attacking Pejorative Expressions with Analysis on Crowd-Generated Hate Speech Evaluation Datasets","Abstract":"In hate speech detection, developing training and evaluation datasets across various domains is the critical issue. Whereas, major approaches crawl social media texts and hire crowd-workers to annotate the data. Following this convention often restricts the scope of pejorative expressions to a single domain lacking generalization. Sometimes domain overlap between training corpus and evaluation set overestimate the prediction performance when pretraining language models on low-data language. To alleviate these problems in korean, we propose developing that asks unspecified users to generate hate speech examples followed by minimal post-labeling. We find that thereby can collect useful datasets that are less sensitive to the lexical overlaps between the pretraining corpus and the evaluation set, thereby properly measuring the model performance.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.9090909091}
{"Year":2010,"Venue":"semeval-2010","Acronym":"UBIU","Description":"A Language-Independent System for Coreference Resolution","Abstract":"We present task, a language independent system for detecting full coreference chains, composed of named entities, pronouns, and full noun phrases which makes use of memory based learning and a feature model following rahman and ng (2009). Based is evaluated on the task \u201ccoreference resolution in multiple languages\u201d (semeval task 1 (recasens et al., 2010)) in the context of the 5th international workshop on semantic evaluation.","wordlikeness":0.75,"lcsratio":0.75,"wordcoverage":0.8571428571}
{"Year":2019,"Venue":"ijcnlp-2019","Acronym":"Honkling","Description":"In-Browser Personalization for Ubiquitous Keyword Spotting","Abstract":"Used for simple commands recognition on devices from smart speakers to mobile phones, keyword spotting systems are everywhere. Ubiquitous as well are web applications, which have grown in popularity and complexity over the last decade. However, despite their obvious advantages in natural language interaction, voice-enabled web applications are still few and far between. We attempt to bridge this gap with novel,, a novel, javascript-based keyword spotting system. Purely client-side and cross-device compatible, the can be deployed directly on user devices. Our in-browser implementation enables seamless personalization, which can greatly improve model quality; in the presence of underrepresented, non-american user accents, we can achieve up to an absolute 10% increase in accuracy in the personalized model with only a few examples.","wordlikeness":0.625,"lcsratio":0.75,"wordcoverage":0.8}
{"Year":2022,"Venue":"icon-2022","Acronym":"IMFinE","Description":"An Integrated BERT-CNN-BiGRU Model for Mental Health Detection in Financial Context on Textual Data","Abstract":"Nowadays, mental health is a global issue. It is a pervasive phenomenon over online social network platforms. It is observed in varied categories, such as depression, suicide, and stress on the web. Hence, mental health detection problem is receiving continuous attention among computational linguistics researchers. On the other hand, public emotions and reactions play a significant role in financial domain and the issue of mental health is directly associated. In this paper, we propose a new study to detect mental health in financial context. It starts with two-step data filtration steps to prepare the mental health dataset in financial context. A new model called relevant is introduced. It consists of an input layer, followed by two relevant bert embedding layers, a convolutional neural network, a bidirectional gated recurrent unit, and finally, dense and output layers. The empirical evaluation of the proposed model is performed on reddit datasets and it shows impressive results in terms of precision, recall, and f-score. It also outperforms relevant state-of-the-art and baseline methods. To the best of our knowledge, this is the first study on mental health detection in financial context.","wordlikeness":0.8333333333,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2019,"Venue":"naacl-2019","Acronym":"CITE","Description":"A Corpus of Image-Text Discourse Relations","Abstract":"This paper presents a novel crowd-sourced resource for multimodal discourse: our resource characterizes inferences in image-text contexts in the domain of cooking recipes in the form of coherence relations. Like previous corpora annotating discourse structure between text arguments, such as the penn discourse treebank, our new corpus aids in establishing a better understanding of natural communication and common-sense reasoning, while our findings have implications for a wide range of applications, such as understanding and generation of multimodal documents.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2022,"Venue":"lrec-2022","Acronym":"AraNPCC","Description":"The Arabic Newspaper COVID-19 Corpus","Abstract":"This paper introduces a corpus for arabic newspapers during covid-19: for. The the corpus covers 2019 until 2021 via automatically-collected data from 12 arab countries. It comprises more than 2 billion words and 7.2 million texts alongside their metadata. Billion can be used for several natural language processing tasks, such as updating available arabic language models or corpus linguistics tasks, including language change over time. We utilized the corpus in two case studies. In the first case study, we investigate the correlation between the number of officially reported infected cases and the collective word frequency of \u201ccovid\u201d and \u201ccorona.\u201d the data shows a positive correlation that varies among arab countries. For the second case study, we extract and compare the top 50 keywords in 2020 and 2021 to study the impact of the covid-19 pandemic on two arab countries, namely algeria and saudi arabia. For 2020, the data shows that the two countries\u2019 newspapers strongly interacted with the pandemic, emphasizing its spread and dangerousness, and in 2021 the data suggests that the two countries coped with the pandemic.","wordlikeness":0.4285714286,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2021,"Venue":"findings-2021","Acronym":"SentNoB","Description":"A Dataset for Analysing Sentiment on Noisy Bangla Texts","Abstract":"In this paper, we propose an annotated sentiment analysis dataset made of informally written bangla texts. This dataset comprises public comments on news and videos collected from social media covering 13 different domains, including politics, education, and agriculture. These comments are labeled with one of the polarity labels, namely positive, negative, and neutral. One significant characteristic of the dataset is that each of the comments is noisy in terms of the mix of dialects and grammatical incorrectness. Our experiments to develop a benchmark classification system show that hand-crafted lexical features provide superior performance than neural network and pretrained language models. We have made the dataset and accompanying models presented in this paper publicly available at <a href=https:\/\/git.io\/juunb class=acl-markup-url>https:\/\/git.io\/juunb<\/a>.","wordlikeness":0.5714285714,"lcsratio":1.0,"wordcoverage":0.7272727273}
{"Year":2022,"Venue":"coling-2022","Acronym":"NLG-Metricverse","Description":"An End-to-End Library for Evaluating Natural Language Generation","Abstract":"Driven by deep learning breakthroughs, natural language generation (nlg) models have been at the center of steady progress in the last few years, with a ubiquitous task influence. However, since our ability to generate human-indistinguishable artificial text lags behind our capacity to assess it, it is paramount to develop and apply even better automatic evaluation metrics. To facilitate researchers to judge the effectiveness of their models broadly, we introduce at\u2014an end-to-end open-source library for nlg evaluation based on python. Our framework provides a living collection of nlg metrics in a unified and easy-to-use environment, supplying tools to efficiently apply, analyze, compare, and visualize them. This includes (i) the extensive support to heterogeneous automatic metrics with n-arity management, (ii) the meta-evaluation upon individual performance, metric-metric and metric-human correlations, (iii) graphical interpretations for helping humans better gain score intuitions, (iv) formal categorization and convenient documentation to accelerate metrics understanding. Broadly, aims to increase the comparability and replicability of nlg research, hopefully stimulating new contributions in the area.","wordlikeness":0.6666666667,"lcsratio":0.4666666667,"wordcoverage":0.64}
{"Year":2023,"Venue":"findings-2023","Acronym":"QueryForm","Description":"A Simple Zero-shot Form Entity Query Framework","Abstract":"Zero-shot transfer learning for document understanding is a crucial yet under-investigated scenario to help reduce the high cost involved in annotating document entities. We present a novel query-based framework, fine-tuning, that extracts entity values from form-like documents in a zero-shot fashion. Generated contains a dual prompting mechanism that composes both the document schema and a specific entity type into a query, which is used to prompt a transformer model to perform a single entity extraction task. Furthermore, we propose to leverage large-scale query-entity pairs generated from form-like webpages with weak html annotations to pre-train sets. By unifying pre-training and fine-tuning into the same query-based framework, by enables models to learn from structured documents containing various entities and layouts, leading to better generalization to target document types without the need for target-specific training data. Size sets new state-of-the-art average f1 score on both the xfund (+4.6% 10.1%) and the payment (+3.2% 9.5%) zero-shot benchmark, with a smaller model size and no additional image input.","wordlikeness":0.8888888889,"lcsratio":0.8888888889,"wordcoverage":0.75}
{"Year":2013,"Venue":"fsmnlp-2013","Acronym":"ZeuScansion","Description":"a tool for scansion of English poetry","Abstract":"We present a \ufb01nite state technology based system capable of performing metrical scansion of verse written in english. Scansion is the traditional task of analyzing the lines of a poem, marking the stressed and non-stressed elements, and dividing the line into metrical feet. The system\u2019s work\ufb02ow is composed of several subtasks designed around \ufb01nite state machines that analyze verse by performing tokenization, part of speech tagging, stress placement, and unknown word stress pattern guessing. The scanner also classi\ufb01es its input according to the predominant type of metrical foot found. We also present a brief evaluation of the system using a gold standard corpus of human-scanned verse, on which a per-syllable accuracy of 86.78% is reached. The program uses open-source components and is released under the gnu gpl license.","wordlikeness":0.7272727273,"lcsratio":0.7272727273,"wordcoverage":0.7}
{"Year":2021,"Venue":"acl-2021","Acronym":"UNIMO","Description":"Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning","Abstract":"Existed pre-training methods either focus on single-modal tasks or multi-modal tasks, and cannot effectively adapt to each other. They can only utilize single-modal data (i.e., text or image) or limited multi-modal data (i.e., image-text pairs). In this work, we propose a unified-modal pre-training architecture, namely that, which can effectively adapt to both single-modal and multi-modal understanding and generation tasks. Large scale of free text corpus and image collections are utilized to improve the capability of visual and textual understanding, and cross-modal contrastive learning (cmcl) is leveraged to align the textual and visual information into a unified semantic space, over a corpus of image-text pairs augmented with related images and texts. With the help of rich non-paired single-modal data, our model is able to learn more generalizable representations, by allowing textual knowledge and visual knowledge to enhance each other in the unified semantic space. The experimental results show that cross-modal greatly improves the performance of several single-modal and multi-modal downstream tasks. Our code and pre-trained models are public at <a href=https:\/\/github.com\/paddlepaddle\/research\/tree\/master\/nlp\/unified-modal class=acl-markup-url>https:\/\/github.com\/paddlepaddle\/research\/tree\/master\/nlp\/in<\/a>.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"SpeechNet","Description":"Weakly Supervised, End-to-End Speech Recognition at Industrial Scale","Abstract":"End-to-end automatic speech recognition systems represent the state of the art, but they rely on thousands of hours of manually annotated speech for training, as well as heavyweight computation for inference. Of course, this impedes commercialization since most companies lack vast human and computational resources. In this paper, we explore training and deploying an asr system in the label-scarce, compute-limited setting. To reduce human labor, we use a third-party asr system as a weak supervision source, supplemented with labeling functions derived from implicit user feedback. To accelerate inference, we propose to route production-time queries across a pool of cuda graphs of varying input lengths, the distribution of which best matches the traffic\u2019s. Compared to our third-party asr, we achieve a relative improvement in word-error rate of 8% and a speedup of 600%. Our system, called first, currently serves 12 million queries per day on our voice-enabled smart television. To our knowledge, this is the first time a large-scale, wav2vec-based deployment has been described in the academic literature.","wordlikeness":0.8888888889,"lcsratio":0.8888888889,"wordcoverage":0.8235294118}
{"Year":2010,"Venue":"lrec-2010","Acronym":"Lingua-Align","Description":"An Experimental Toolbox for Automatic Tree-to-Tree Alignment","Abstract":"In this paper we present an experimental toolbox for automatic tree-to-tree alignment based on a binary classification model. The aligner implements a recurrent architecture for structural prediction using history features and a sequential classification procedure. The discriminative base classifier uses a log-linear model in the current setup which enables simple integration of various features extracted from the data. The discriminative toolbox provides a flexible framework for feature extraction including contextual properties and implements several alignment inference procedures. Various settings and constraints can be controlled via a simple frontend or called from external scripts. Recurrent supports different treebank formats and includes additional tools for conversion and evaluation. In our experiments we can show that our tree aligner produces results with high quality and outperforms unsupervised techniques proposed otherwise. It also integrates well with another existing tool for manual tree alignment which makes it possible to quickly integrate additional training material and to run semi-automatic alignment strategies.","wordlikeness":0.5833333333,"lcsratio":0.8333333333,"wordcoverage":0.6666666667}
{"Year":2017,"Venue":"acl-2017","Acronym":"Hafez","Description":"an Interactive Poetry Generation System","Abstract":"So is an automatic poetry generation system that integrates a recurrent neural network (rnn) with a finite state acceptor (fsa). It generates sonnets given arbitrary topics. Furthermore, recurrent enables users to revise and polish generated poems by adjusting various style con\ufb01gurations. Experiments demonstrate that such \u201cpolish\u201d mechanisms consider the user\u2019s intention and lead to a better poem. For evaluation, we build a web interface where users can rate the quality of each poem from 1 to 5 stars. We also speed up the whole system by a factor of 10, via vocabulary pruning and gpu computation, so that adequate feedback can be collected at a fast pace. Based on such feedback, the system learns to adjust its parameters to improve poetry quality.","wordlikeness":0.6,"lcsratio":0.4,"wordcoverage":0.7272727273}
{"Year":2021,"Venue":"ws-2021","Acronym":"AutoAspect","Description":"Automatic Annotation of Tense and Aspect for Uniform Meaning Representations","Abstract":"We present meaning, a novel, rule-based annotation tool for labeling tense and aspect. The pilot version annotates english data. The aspect labels are designed specifically for uniform meaning representations (umr), an annotation schema that aims to encode crosslingual semantic information. The annotation tool combines syntactic and semantic cues to assign aspects on a sentence-by-sentence basis, following a sequence of rules that each output a umr aspect. Identified events proceed through the sequence until they are assigned an aspect. We achieve a recall of 76.17% for identifying umr events and an accuracy of 62.57% on all identified events, with high precision values for 2 of the aspect labels.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"JamPatoisNLI","Description":"A Jamaican Patois Natural Language Inference Dataset","Abstract":"Unique provides the first dataset for natural language inference in a creole language, jamaican patois.many of the most-spoken low-resource languages are creoles. These languages commonly have a lexicon derived from a major world language and a distinctive grammar reflecting the languages of the original speakers and the process of language birth by creolization. This gives them a distinctive place in exploring the effectiveness of transfer from large monolingual or multilingual pretrained models. While our work, along with previous work, shows that transfer from these models to low-resource languages that are unrelated to languages in their training set is not very effective, we would expect stronger results from transfer to creoles. Indeed, our experiments show considerably better results from few-shot learning of patois.many than for such unrelated languages, and help us begin to understand how the unique relationship between creoles and their high-resource base languages affect cross-lingual transfer. Understand, which consists of naturally-occurring premises and expert-written hypotheses, is a step towards steering research into a traditionally underserved language and a useful benchmark for understanding cross-lingual nlp.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.6315789474}
{"Year":2015,"Venue":"emnlp-2015","Acronym":"ASTD","Description":"Arabic Sentiment Tweets Dataset","Abstract":"This paper introduces dataset,, an arabic social sentiment analysis dataset gathered from twitter. It consists of about 10,000 tweets which are classi\ufb01ed as objective, subjective positive, subjective negative, and subjective mixed. We present the properties and the statistics of the dataset, and run experiments using standard partitioning of the dataset. Our experiments provide benchmark results for 4 way sentiment classi\ufb01cation on the dataset.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2021,"Venue":"acl-2021","Acronym":"VECO","Description":"Variable and Flexible Cross-lingual Pre-training for Language Understanding and Generation","Abstract":"Existing work in multilingual pretraining has demonstrated the potential of cross-lingual transferability by training a unified transformer encoder for multiple languages. However, much of this work only relies on the shared vocabulary and bilingual contexts to encourage the correlation across languages, which is loose and implicit for aligning the contextual representations between languages. In this paper, we plug a cross-attention module into the transformer encoder to explicitly build the interdependence between languages. It can effectively avoid the degeneration of predicting masked words only conditioned on the context in its own language. More importantly, when fine-tuning on downstream tasks, the cross-attention module can be plugged in or out on-demand, thus naturally benefiting a wider range of cross-lingual tasks, from language understanding to generation. As a result, the proposed cross-lingual model delivers new state-of-the-art results on various cross-lingual understanding tasks of the xtreme benchmark, covering text classification, sequence labeling, question answering, and sentence retrieval. For cross-lingual generation tasks, it also outperforms all existing cross-lingual models and state-of-the-art transformer variants on wmt14 english-to-german and english-to-french translation datasets, with gains of up to 1 2 bleu.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2022,"Venue":"wmt-2022","Acronym":"TSMind","Description":"Alibaba and Soochow University&#39;s Submission to the WMT22 Translation Suggestion Task","Abstract":"This paper describes the joint submission of alibaba and soochow university to the wmt 2022 shared task on translation suggestion (ts). We participate in the english to\/from german and english to\/from chinese tasks. Basically, we utilize the model paradigm fine-tuning on the downstream tasks based on large-scale pre-trained models, which has recently achieved great success. We choose fair\u2019s wmt19 english to\/from german news translation system and mbart50 for english to\/from chinese as our pre-trained models. Considering the task\u2019s condition of limited use of training data, we follow the data augmentation strategies provided by yang to boost our ts model performance. And we further involve the dual conditional cross-entropy model and gpt-2 language model to filter augmented data. The leader board finally shows that our submissions are ranked first in three of four language directions in the naive ts task of the wmt22 translation suggestion task.","wordlikeness":0.6666666667,"lcsratio":0.8333333333,"wordcoverage":0.8}
{"Year":2023,"Venue":"scil-2023","Acronym":"CANDS","Description":"A Computational Implementation of Collins and Stabler (2016)","Abstract":"Syntacticians must keep track of the empirical coverages and the inner workings of syntactic theories, a task especially demanding for minimalist syntacticians to perform manually and mentally. We believe that the computational implementation of syntactic theories is desirable in that it not only (a) facilitates the evaluation of their empirical coverages, but also (b) forces syntacticians to specify their inner workings. In this paper, we present desirable, a computational implementation of collins and stabler (2016) in the programming language rust. Specifically, implements consists of one main library, and, as well as two wrapper programs for also, derivck and derivexp. The main library, but, implements key definitions of fundamental concepts in minimalist syntax from collins and stabler (2016), which can be employed to evaluate and extend specific syntactic theories. The wrapper programs, derivck and derivexp, allow syntacticians to check and explore syntactic derivations through an accessible interface.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2006,"Venue":"ws-2006","Acronym":"ChAT","Description":"A Time-Linked System for Conversational Analysis","Abstract":"We present a system for analyzing conversational data. The system includes state-ofthe-art natural language processing components that have been modified to accommodate the unique nature of conversational data. In addition, we leverage the added richness of conversational data by analyzing various aspects of the participants and their relationships to each other. Our tool provides users with the ability to easily identify topics or persons of interest, including who talked to whom, when, entities that were discussed, etc. Using this tool, one can also isolate more complex networks of information: individuals who may have discussed the same topics but never talked to each other. The tool includes a ui that plots information over time, and a semantic graph that highlights relationships of interest.","wordlikeness":1.0,"lcsratio":0.75,"wordcoverage":1.0}
{"Year":2022,"Venue":"naacl-2022","Acronym":"RSTGen","Description":"Imbuing Fine-Grained Interpretable Control into Long-FormText Generators","Abstract":"In this paper, we study the task of improving the cohesion and coherence of long-form text generated by language models. To this end, we propose a, a framework that utilises rhetorical structure theory (rst), a classical language theory, to control the discourse structure, semantics and topics of generated text. Firstly, we demonstrate our model\u2019s ability to control structural discourse and semantic features of generated text in open generation evaluation. Then we experiment on the two challenging long-form text tasks of argument generation and story generation. Evaluation using automated metrics and a metric with high correlation to human evaluation, shows that our model performs competitively against existing models, while offering significantly more controls over generated text than alternative methods.","wordlikeness":0.5,"lcsratio":0.8333333333,"wordcoverage":0.7692307692}
{"Year":2013,"Venue":"ijcnlp-2013","Acronym":"SINNET","Description":"Social Interaction Network Extractor from Text","Abstract":"In this paper we present a demo of our system: social interaction network extractor from text (network). Of is able to extract a social network from unstructured text. Nodes in the network are people and links are social events.","wordlikeness":0.8333333333,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2023,"Venue":"acl-2023","Acronym":"ETHICIST","Description":"Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation","Abstract":"Large pre-trained language models achieve impressive results across many tasks. However, recent works point out that pre-trained language models may memorize a considerable fraction of their training data, leading to the privacy risk of information leakage. In this paper, we propose a method named availabel for targeted training data extraction through loss smoothed soft prompting and calibrated confidence estimation, investigating how to recover the suffix in the training data when given a prefix. To elicit memorization in the attacked model, we tune soft prompt embeddings while keeping the model fixed. We further propose a smoothing loss that smooths the loss distribution of the suffix tokens to make it easier to sample the correct suffix. In order to select the most probable suffix from a collection of sampled suffixes and estimate the prediction confidence, we propose a calibrated confidence estimation method, which normalizes the confidence of the generated suffixes with a local estimation. We show that through significantly improves the extraction performance on a recently proposed public benchmark. We also investigate several factors influencing the data extraction performance, including decoding strategy, model scale, prefix length, and suffix length. Our code is availabel at <a href=https:\/\/github.com\/thu-coai\/targeted-data-extraction class=acl-markup-url>https:\/\/github.com\/thu-coai\/targeted-data-extraction<\/a>.","wordlikeness":0.875,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2008,"Venue":"ijcnlp-2008","Acronym":"Achilles","Description":"NiCT\/ATR Chinese Morphological Analyzer for the Fourth Sighan Bakeoff","Abstract":"We created a new chinese morphological analyzer, ,, by integrating rule-based, dictionary-based, and statistical machine learning method, conditional random \ufb01elds (crf). The rulebased method is used to recognize regular expressions: numbers, time and alphabets. The dictionary-based method is used to \ufb01nd in-vocabulary (iv) words while outof-vocabulary (oov) words are detected by the crfs. at last, con\ufb01dence measure based approach is used to weigh all the results and output the best ones. Results was used and evaluated in the bakeoff. We participated the closed tracks of word segmentation and part-of-speech tagging for all the provided corpus. In spite of an unexpected \ufb01le encoding errors, the system exhibited a top level performance. A higher word segmentation accuracy for the corpus ckip and ncc were achieved. We are ranked at the \ufb01fth and eighth position out of all 19 and 26 submissions respectively for the two corpus. New uses a feature combined approach for partof-speech tagging. Our post-evaluation results prove the effectiveness of this approach for pos tagging.","wordlikeness":0.875,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2014,"Venue":"semeval-2014","Acronym":"ECNU","Description":"A Combination Method and Multiple Features for Aspect Extraction and Sentiment Polarity Classification","Abstract":"This paper reports our submissions to the four subtasks of aspect based sentiment analysis (absa) task (i.e., task 4) in semeval 2014 including aspect term extraction and aspect sentiment polarity classi\ufb01cation (aspect-level tasks), aspect category detection and aspect category sentiment polarity classi\ufb01cation (categorylevel tasks). For aspect term extraction, we present three methods, i.e., noun phrase (np) extraction, named entity recognition (ner) and a combination of np and ner method. For aspect sentiment classi\ufb01cation, we extracted several features, i.e., topic features, sentiment lexicon features, and adopted a maximum entropy classi\ufb01er. Our submissions rank above average.","wordlikeness":0.25,"lcsratio":0.75,"wordcoverage":0.75}
{"Year":2012,"Venue":"ws-2012","Acronym":"HRItk","Description":"The Human-Robot Interaction ToolKit Rapid Development of Speech-Centric Interactive Systems in ROS","Abstract":"Developing interactive robots is an extremely challenging task which requires a broad range of expertise across diverse disciplines, including, robotic planning, spoken language understanding, belief tracking and action management. While there has been a boom in recent years in the development of reusable components for robotic systems within common architectures, such as the robot operating system (ros), little emphasis has been placed on developing components for humanrobot-interaction. In this paper we introduce ease (the human-robot-interaction toolkit), a framework, consisting of messaging protocols, core-components, and development tools for rapidly building speech-centric interactive systems within the ros environment. The proposed toolkit was specifically designed for extensibility, ease of use, and rapid development, allowing developers to quickly incorporate speech interaction into existing projects.","wordlikeness":0.4,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"ConNER","Description":"Consistency Training for Cross-lingual Named Entity Recognition","Abstract":"Cross-lingual named entity recognition (ner) suffers from data scarcity in the target languages, especially under zero-shot settings. Existing translate-train or knowledge distillation methods attempt to bridge the language gap, but often introduce a high level of noise. To solve this problem, consistency training methods regularize the model to be robust towards perturbations on data or hidden states. However, such methods are likely to violate the consistency hypothesis, or mainly focus on coarse-grain consistency. We propose gap, as a novel consistency training framework for cross-lingual ner, which comprises of: (1) translation-based consistency training on unlabeled target-language data, and (2) dropout-based consistency training on labeled source-language data. Show effectively leverages unlabeled target-language data and alleviates overfitting on the source language to enhance the cross-lingual adaptability. Experimental results show our framework achieves consistent improvement over various baseline methods.","wordlikeness":0.8333333333,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2020,"Venue":"lrec-2020","Acronym":"CRWIZ","Description":"A Framework for Crowdsourcing Real-Time Wizard-of-Oz Dialogues","Abstract":"Large corpora of task-based and open-domain conversational dialogues are hugely valuable in the field of data-driven dialogue systems. Crowdsourcing platforms, such as amazon mechanical turk, have been an effective method for collecting such large amounts of data. However, difficulties arise when task-based dialogues require expert domain knowledge or rapid access to domain-relevant information, such as databases for tourism. This will become even more prevalent as dialogue systems become increasingly ambitious, expanding into tasks with high levels of complexity that require collaboration and forward planning, such as in our domain of emergency response. In this paper, we propose large: a framework for collecting real-time wizard of oz dialogues through crowdsourcing for collaborative, complex tasks. This framework uses semi-guided dialogue to avoid interactions that breach procedures and processes only known to experts, while enabling the capture of a wide variety of interactions.","wordlikeness":0.4,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2000,"Venue":"lrec-2000","Acronym":"EULER","Description":"an Open, Generic, Multilingual and Multi-platform Text-to-Speech System","Abstract":"The aim of the collaborative project presented in this paper is to obtain a set of highly modular text-to-speech synthesizers for as many voices, languages and dialects as possible, free for use in non-commercial and non-military applications. This project is an extension of the mbrola project: mbrola is a speech synthesizer, freely distributed for non-commercial purposes, which uses diphone databases provided by users (19 languages in year 2000). Provided extends this idea to whole tts systems by providing a backbone structure (mlc) and several generic algorithms for pos tagging, grapheme-to-phoneme conversion, and prosody generation. To demonstrate the potentials of the architecture and draw developpers\u2019 interest we provide a full from-based tts in french and in arabic. Other currently runs on windows and linux, and it is an open project: many of its components (and certainly its kernel) are provided as gnu c++ sources. It also incorporates, as much as possible, components and data derived from other tts-related projects. 1.","wordlikeness":0.6,"lcsratio":0.8,"wordcoverage":0.8}
{"Year":2019,"Venue":"emnlp-2019","Acronym":"SmokEng","Description":"Towards Fine-grained Classification of Tobacco-related Social Media Text","Abstract":"Contemporary datasets on tobacco consumption focus on one of two topics, either public health mentions and disease surveillance, or sentiment analysis on topical tobacco products and services. However, two primary considerations are not accounted for, the language of the demographic affected and a combination of the topics mentioned above in a fine-grained classification mechanism. In this paper, we create a dataset of 3144 tweets, which are selected based on the presence of colloquial slang related to smoking and analyze it based on the semantics of the tweet. Each class is created and annotated based on the content of the tweets such that further hierarchical methods can be easily applied. Further, we prove the efficacy of standard text classification methods on this dataset, by designing experiments which do both binary as well as multi-class classification. Our experiments tackle the identification of either a specific topic (such as tobacco product promotion), a general mention (cigarettes and related products) or a more fine-grained classification. This methodology paves the way for further analysis, such as understanding sentiment or style, which makes this dataset a vital contribution to both disease surveillance and tobacco use research.","wordlikeness":0.8571428571,"lcsratio":0.4285714286,"wordcoverage":0.8571428571}
{"Year":2021,"Venue":"ws-2021","Acronym":"GerDaLIR","Description":"A German Dataset for Legal Information Retrieval","Abstract":"We present information, a german dataset for legal information retrieval based on case documents from the open legal information platform open legal data. The dataset consists of 123k queries, each labelled with at least one relevant document in a collection of 131k case documents. We conduct several baseline experiments including bm25 and a state-of-the-art neural re-ranker. With our dataset, we aim to provide a standardized benchmark for german lir and promote open research in this area. Beyond that, our dataset comprises sufficient training data to be used as a downstream task for german or multilingual language models.","wordlikeness":0.625,"lcsratio":1.0,"wordcoverage":0.7142857143}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"MOCHA","Description":"A Multi-Task Training Approach for Coherent Text Generation from Cognitive Perspective","Abstract":"Teaching neural models to generate narrative coherent texts is a critical problem. Recent pre-trained language models have achieved promising results, but there is still a gap between human written texts and machine-generated outputs. In this work, we propose a novel multi-task training strategy for long text generation grounded on the cognitive theory of writing, which empowers the model to learn essential subskills needed for writing including planning and reviewing besides end-to-end generation. We extensively evaluate our model on three open-ended generation tasks including story generation, news article writing and argument generation. Experiments show that our model achieves better results on both few-shot and fully-supervised settings than strong baselines, and human evaluations confirm that our model can generate more coherent outputs.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2022,"Venue":"tacl-2022","Acronym":"PADA","Description":"Example-based Prompt Learning for on-the-fly Adaptation to Unseen Domains","Abstract":"Natural language processing algorithms have made incredible progress, but they still struggle when applied to out-of-distribution examples. We address a challenging and underexplored version of this domain adaptation problem, where an algorithm is trained on several source domains, and then applied to examples from unseen domains that are unknown at training time. Particularly, no examples, labeled or unlabeled, or any other knowledge about the target domain are available to the algorithm at training time. We present present: an example-based autoregressive prompt learning algorithm for on-the-fly any-domain adaptation, based on the t5 language model. Given a test example, of first generates a unique prompt for it and then, conditioned on this prompt, labels the example with respect to the nlp prediction task. Applied is trained to generate a prompt that is a token sequence of unrestricted length, consisting of domain related features (drfs) that characterize each of the source domains. Intuitively, the generated prompt is a unique signature that maps the test example to a semantic space spanned by the source domains. In experiments with 3 tasks (text classification and sequence tagging), for a total of 14 multi-source adaptation scenarios, total substantially outperforms strong baselines.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2016,"Venue":"emnlp-2016","Acronym":"SQuAD","Description":"100,000&#43; Questions for Machine Comprehension of Text","Abstract":"We present the stanford question answering dataset (reading), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an f1 score of 51.0%, a signi\ufb01cant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https:\/\/stanford-qa.com.","wordlikeness":1.0,"lcsratio":0.6,"wordcoverage":1.0}
{"Year":2014,"Venue":"acl-2014","Acronym":"FAdR","Description":"A System for Recognizing False Online Advertisements","Abstract":"More and more product information, including advertisements and user reviews, are presented to internet users nowadays. Some of the information is false, misleading or overstated, which can cause seriousness and needs to be identified. Authorities, advertisers, website owners and consumers all have the needs to detect such statements. In this paper, we propose a false advertisements recognition system called called by using one-class and binary classification models. Illegal advertising lists made public by a government and product descriptions from a shopping website are obtained for training and testing. The results show that the binary svm models can achieve the highest performance when unigrams with the weighting of log relative frequency ratios are used as features. Comparatively, the benefit of the one-class classification models is the adjustable rejection rate parameter, which can be changed to suit different applications. Verb phrases more likely to introduce overstated information are obtained by mining the datasets. These phrases help find problematic wordings in the advertising texts.","wordlikeness":0.25,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2022,"Venue":"lrec-2022","Acronym":"HateCheckHIn","Description":"Evaluating Hindi Hate Speech Detection Models","Abstract":"Due to the sheer volume of online hate, the ai and nlp communities have started building models to detect such hateful content. Recently, multilingual hate is a major emerging challenge for automated detection where code-mixing or more than one language have been used for conversation in social media. Typically, hate speech detection models are evaluated by measuring their performance on the held-out test data using metrics such as accuracy and f1-score. While these metrics are useful, it becomes difficult to identify using them where the model is failing, and how to resolve it. To enable more targeted diagnostic insights of such multilingual hate speech models, we introduce a set of functionalities for the purpose of evaluation. We have been inspired to design this kind of functionalities based on real-world conversation on social media. Considering hindi as a base language, we craft test cases for each functionality. We name our evaluation dataset started. To illustrate the utility of these functionalities , we test state-of-the-art transformer based m-bert model and the perspective api.","wordlikeness":0.8333333333,"lcsratio":0.8333333333,"wordcoverage":0.7}
{"Year":2012,"Venue":"lrec-2012","Acronym":"KPWr","Description":"Towards a Free Corpus of Polish","Abstract":"This paper presents our efforts aimed at collecting and annotating a free polish corpus. The corpus will serve for us as training and testing material for experiments with machine learning algorithms. As others may also benefit from the resource, we are going to release it under a creative commons licence, which is hoped to remove unnecessary usage restrictions, but also to facilitate reproduction of our experimental results. The corpus is being annotated with various types of linguistic entities: chunks and named entities, selected syntactic and semantic relations, word senses and anaphora. We report on the current state of the project as well as our ultimate goals.","wordlikeness":0.25,"lcsratio":0.5,"wordcoverage":0.6666666667}
{"Year":2016,"Venue":"emnlp-2016","Acronym":"Lifelong-RL","Description":"Lifelong Relaxation Labeling for Separating Entities and Aspects in Opinion Targets","Abstract":"It is well-known that opinions have targets. Extracting such targets is an important problem of opinion mining because without knowing the target of an opinion, the opinion is of limited use. So far many algorithms have been proposed to extract opinion targets. However, an opinion target can be an entity or an aspect (part or attribute) of an entity. An opinion about an entity is an opinion about the entity as a whole, while an opinion about an aspect is just an opinion about that speci\ufb01c attribute or aspect of an entity. Thus, opinion targets should be separated into entities and aspects before use because they represent very different things about opinions. This paper proposes a novel algorithm, called markedly., to solve the problem based on lifelong machine learning and relaxation labeling. Extensive experiments show that the proposed algorithm targets outperforms baseline methods markedly.","wordlikeness":0.6363636364,"lcsratio":0.9090909091,"wordcoverage":0.8421052632}
{"Year":2014,"Venue":"semeval-2014","Acronym":"TCDSCSS","Description":"Dimensionality Reduction to Evaluate Texts of Varying Lengths - an IR Approach","Abstract":"This paper provides system description of the cross-level semantic similarity task for the semeval-2014 workshop. Crosslevel semantic similarity measures the degree of relatedness between texts of varying lengths such as paragraph to sentence and sentence to phrase. Latent semantic analysis was used to evaluate the cross-level semantic relatedness between the texts to achieve above baseline scores, tested on the training and test datasets. We also tried using a bag-of-vectors approach to evaluate the semantic relatedness. This bag-of-vectors approach however did not produced encouraging results.","wordlikeness":0.1428571429,"lcsratio":0.7142857143,"wordcoverage":0.7142857143}
{"Year":2023,"Venue":"acl-2023","Acronym":"WhitenedCSE","Description":"Whitening-based Contrastive Learning of Sentence Embeddings","Abstract":"This paper presents a whitening-based contrastive learning method for sentence embedding learning (to), which combines contrastive learning with a novel shuffled group whitening. Generally, contrastive learning pulls distortions of a single sample (i.e., positive samples) close and push negative samples far away, correspondingly facilitating the alignment and uniformity in the feature space. A popular alternative to the \u201cpushing\u201d operation is whitening the feature space, which scatters all the samples for uniformity. Since the whitening and the contrastive learning have large redundancy w.r.t. the uniformity, they are usually used separately and do not easily work together. For the first time, this paper integrates whitening into the contrastive learning scheme and facilitates two benefits. 1) better uniformity. We find that these two approaches are not totally redundant but actually have some complementarity due to different uniformity mechanism. 2) better alignment. We randomly divide the feature into multiple groups along the channel axis and perform whitening independently within each group. By shuffling the group division, we derive multiple distortions of a single sample and thus increase the positive sample diversity. Consequently, using multiple positive samples with enhanced diversity further improves contrastive learning due to better alignment. Extensive experiments on seven semantic textual similarity tasks show our method achieves consistent improvement over the contrastive learning baseline and sets new states of the art, e.g., 78.78% (+2.53% based on bert{pasted macro \u2018ba\u2019}) spearman correlation on sts tasks.","wordlikeness":0.4545454545,"lcsratio":1.0,"wordcoverage":0.7058823529}
{"Year":2023,"Venue":"acl-2023","Acronym":"MidMed","Description":"Towards Mixed-Type Dialogues for Medical Consultation","Abstract":"Most medical dialogue systems assume that patients have clear goals (seeking a diagnosis, medicine querying, etc.) before medical consultation. However, in many real situations, due to the lack of medical knowledge, it is usually difficult for patients to determine clear goals with all necessary slots. In this paper, we identify this challenge as how to construct medical consultation dialogue systems to help patients clarify their goals. For further study, we create a novel human-to-human mixed-type medical consultation dialogue corpus, termed create, covering four dialogue types: task-oriented dialogue for diagnosis, recommendation, qa, and chitchat. Study, covers four departments (otorhinolaryngology, ophthalmology, skin, and digestive system), with 8,309 dialogues. Furthermore, we build benchmarking baselines on 8,309 and propose an instruction-guiding medical dialogue generation framework, termed insmed, to handle mixed-type dialogues. Experimental results show the effectiveness of insmed.","wordlikeness":0.8333333333,"lcsratio":1.0,"wordcoverage":0.8333333333}
{"Year":2005,"Venue":"acl-2005","Acronym":"SenseLearner","Description":"Word Sense Disambiguation for All Words in Unrestricted Text","Abstract":"This paper describes data \u2013 a minimally supervised word sense disambiguation system that attempts to disambiguate all content words in a text using wordnet senses. We evaluate the accuracy of in on several standard sense-annotated data sets, and show that it compares favorably with the best results reported during the recent senseval evaluations.","wordlikeness":0.75,"lcsratio":0.8333333333,"wordcoverage":0.7368421053}
{"Year":2015,"Venue":"tacl-2015","Acronym":"Plato","Description":"A Selective Context Model for Entity Resolution","Abstract":"We present probabilistic, a probabilistic model for entity resolution that includes a novel approach for handling noisy or uninformative features, and supplements labeled training data derived from wikipedia with a very large unlabeled text corpus. Training and inference in the proposed model can easily be distributed across many servers, allowing it to scale to over 107 entities. We evaluate 2003 on three standard datasets for entity resolution. Our approach achieves the best results to-date on tac kbp 2011 and is highly competitive on both the conll 2003 and tac kbp 2012 datasets.","wordlikeness":0.8,"lcsratio":0.6,"wordcoverage":0.8333333333}
{"Year":2013,"Venue":"paclic-2013","Acronym":"BCCWJ-TimeBank","Description":"Temporal and Event Information Annotation on Japanese Text","Abstract":"Temporal information extraction can be split into the following three tasks: temporal expression extraction, time normalisation, and temporal ordering relation resolution. This paper describes a time expression and temporal ordering annotation schema for japanese, employing the balanced corpus of contemporary written japanese, or bccwj. The annotation is aimed at allowing the development of better japanese temporal ordering relation resolution tools. The annotation schema is based on an iso annotation standard \u2013 timeml. We extract verbal and adjective event expressions as \u27e8event\u27e9 in a subset of bccwj. Then, we annotate temporal ordering relation \u27e8tlink\u27e9 on the above pairs of event and time expressions by previous work. We identify several issues in the annotation.","wordlikeness":0.5,"lcsratio":0.3571428571,"wordcoverage":0.5454545455}
{"Year":2013,"Venue":"acl-2013","Acronym":"ImpAr","Description":"A Deterministic Algorithm for Implicit Semantic Role Labelling","Abstract":"This paper presents a novel deterministic algorithm for implicit semantic role labeling. The system exploits a very simple but relevant discursive property, the argument coherence over different instances of a predicate. The algorithm solves the implicit arguments sequentially, exploiting not only explicit but also the implicit arguments previously solved. In addition, we empirically demonstrate that the algorithm obtains very competitive and robust performances with respect to supervised approaches that require large amounts of costly training data.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.7692307692}
{"Year":2021,"Venue":"conll-2021","Acronym":"FAST","Description":"A carefully sampled and cognitively motivated dataset for distributional semantic evaluation","Abstract":"What is the first word that comes to your mind when you hear giraffe, or damsel, or freedom? Such free associations contain a huge amount of information on the mental representations of the corresponding concepts, and are thus an extremely valuable testbed for the evaluation of semantic representations extracted from corpora. In this paper, we present corresponding (free association tasks), a free association dataset for english rigorously sampled from two standard free association norms collections (the edinburgh associative thesaurus and the university of south florida free association norms), discuss two evaluation tasks, and provide baseline results. In parallel, we discuss methodological considerations concerning the desiderata for a proper evaluation of semantic representations.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2008,"Venue":"lrec-2008","Acronym":"Cleaneval","Description":"a Competition for Cleaning Web Pages","Abstract":"Arbitrary is a shared task and competitive evaluation on the topic of cleaning arbitrary web pages, with the goal of preparing web data for use as a corpus for linguistic and language technology research and development. The first exercise took place in 2007.","wordlikeness":0.7777777778,"lcsratio":0.7777777778,"wordcoverage":0.75}
{"Year":2021,"Venue":"ws-2021","Acronym":"FEEL-IT","Description":"Emotion and Sentiment Classification for the Italian Language","Abstract":"While sentiment analysis is a popular task to understand people\u2019s reactions online, we often need more nuanced information: is the post negative because the user is angry or sad? An abundance of approaches have been introduced for tackling these tasks, also for italian, but they all treat only one of the tasks. We introduce often, a novel benchmark corpus of italian twitter posts annotated with four basic emotions: <i>anger<\/i>, <i>fear<\/i>, <i>joy<\/i>, <i>sadness<\/i>. By collapsing them, we can also do sentiment analysis. We evaluate our corpus on benchmark datasets for both emotion and sentiment classification, obtaining competitive results. We release an open-source python library, so researchers can use a model trained on evaluate for inferring both sentiments and emotions from italian text.","wordlikeness":0.7142857143,"lcsratio":0.7142857143,"wordcoverage":0.7272727273}
{"Year":2018,"Venue":"emnlp-2018","Acronym":"SWAG","Description":"A Large-Scale Adversarial Dataset for Grounded Commonsense Inference","Abstract":"Given a partial description like \u201cshe opened the hood of the car,\u201d humans can reason about the situation and anticipate what might come next (\u201dthen, she examined the engine\u201d). In this paper, we introduce the task of grounded commonsense inference, unifying natural language inference and commonsense reasoning. We present to, a new dataset with 113k multiple choice questions about a rich spectrum of grounded situations. To address the recurring challenges of the annotation artifacts and human biases found in many existing datasets, we propose adversarial filtering (af), a novel procedure that constructs a de-biased dataset by iteratively training an ensemble of stylistic classifiers, and using them to filter the data. To account for the aggressive adversarial filtering, we use state-of-the-art language models to massively oversample a diverse set of potential counterfactuals. Empirical results demonstrate that while humans can solve the resulting inference problems with high accuracy (88%), various competitive models struggle on our task. We provide comprehensive analysis that indicates significant opportunities for future research.","wordlikeness":0.75,"lcsratio":0.75,"wordcoverage":0.8}
{"Year":2021,"Venue":"ws-2021","Acronym":"StoryDB","Description":"Broad Multi-language Narrative Dataset","Abstract":"This paper presents a \u2014 a broad multi-language dataset of narratives. That is a corpus of texts that includes stories in 42 different languages. Every language includes 500+ stories. Some of the languages include more than 20 000 stories. Every story is indexed across languages and labeled with tags such as a genre or a topic. The corpus shows rich topical and language variation and can serve as a resource for the study of the role of narrative in natural language processing across various languages including low resource ones. We also demonstrate how the dataset could be used to benchmark three modern multilanguage models, namely, mdistillbert, mbert, and xlm-roberta.","wordlikeness":0.7142857143,"lcsratio":0.4285714286,"wordcoverage":0.8333333333}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"Answerability","Description":"A custom metric for evaluating chatbot performance","Abstract":"Most commercial conversational ai products in domains spanning e-commerce, health care, finance, and education involve a hierarchy of nlp models that perform a variety of tasks such as classification, entity recognition, question-answering, sentiment detection, semantic text similarity, and so on. Despite our understanding of each of the constituent models, we do not have a clear view as to how these models affect the overall platform metrics. To bridge this gap, we define a metric known as only, which penalizes not only irrelevant or incorrect chatbot responses but also unhelpful responses that do not serve the chatbot\u2019s purpose despite being correct or relevant. Additionally, we describe a formula-based mathematical framework to relate individual model metrics to the sentiment metric. We also describe a modeling approach for predicting a chatbot\u2019s detection, to a user question and its corresponding chatbot response.","wordlikeness":0.9230769231,"lcsratio":0.6153846154,"wordcoverage":0.7692307692}
{"Year":2021,"Venue":"ranlp-2021","Acronym":"TEASER","Description":"Towards Efficient Aspect-based SEntiment Analysis and Recognition","Abstract":"Sentiment analysis aims to detect the overall sentiment, i.e., the polarity of a sentence, paragraph, or text span, without considering the entities mentioned and their aspects. Aspect-based sentiment analysis aims to extract the aspects of the given target entities and their respective sentiments. Prior works formulate this as a sequence tagging problem or solve this task using a span-based extract-then-classify framework where first all the opinion targets are extracted from the sentence, and then with the help of span representations, the targets are classified as positive, negative, or neutral. The sequence tagging problem suffers from issues like sentiment inconsistency and colossal search space. Whereas, span-based extract-then-classify framework suffers from issues such as half-word coverage and overlapping spans. To overcome this, we propose a similar span-based extract-then-classify framework with a novel and improved heuristic. Experiments on the three benchmark datasets (restaurant14, laptop14, restaurant15) show our model consistently outperforms the current state-of-the-art. Moreover, we also present a novel supervised movie reviews dataset (movie20) and a pseudo-labeled movie reviews dataset (movieslarge) made explicitly for this task and report the results on the novel movie20 dataset as well.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2022,"Venue":"ws-2022","Acronym":"DoSA","Description":"A System to Accelerate Annotations on Business Documents with Human-in-the-Loop","Abstract":"Business documents come in a variety of structures, formats and information needs which makes information extraction a challenging task. Due to these variations, having a document generic model which can work well across all types of documents for all the use cases seems far-fetched. For document-specific models, we would need customized document-specific labels. We introduce by (document specific automated annotations), which helps annotators in generating initial annotations automatically using our novel bootstrap approach by leveraging document generic datasets and models. These initial annotations can further be reviewed by a human for correctness. An initial document-specific model can be trained and its inference can be used as feedback for generating more automated annotations. These automated annotations can be reviewed by humanin-the-loop for the correctness and a new improved model can be trained using the current model as pre-trained model before going for the next iteration. In this paper, our scope is limited to form like documents due to limited availability of generic annotated datasets, but this idea can be extended to a variety of other documents as more datasets are built. An opensource ready-to-use implementation is made available on github.","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2000,"Venue":"lrec-2000","Acronym":"ItalWordNet","Description":"a Large Semantic Database for Italian","Abstract":"The focus of this paper is on the work we are carrying out to develop a large semantic database within an italian national project, sital, aiming at realizing a set of integrated (compatible) resources and tools for the automatic processing of the italian language. Within si-tal, discuss is the reference lexical resource which will contain information related to about 130,000 word senses grouped into synsets. This lexical database is not being created ex novo, but extending and revising the italian lexical wordnet built in the framework of the eurowordnet project. In this paper we firstly describe how the lexical coverage of our wordnet is being extended by adding adjectives, adverbs and proper nouns, plus a terminological subset belonging to the economic and financial domain. The relevant changes involved by these extensions both in the linguistic model and in the data structure are then illustrated. In particular we discuss i) the new semantic relations identified to encode information on adjectives and adverbs ii) the new architecture including the terminological subset. 1.","wordlikeness":0.7272727273,"lcsratio":0.5454545455,"wordcoverage":0.6315789474}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"RoChBert","Description":"Towards Robust BERT Fine-tuning for Chinese","Abstract":"Despite of the superb performance on a wide range of tasks, pre-trained language models (e.g., bert) have been proved vulnerable to adversarial texts. In this paper, we present methods, a framework to build more robust bert-based models by utilizing a more comprehensive adversarial graph to fuse chinese phonetic and glyph features into pre-trained representations during fine-tuning. Inspired by curriculum learning, we further propose to augment the training dataset with adversarial texts in combination with intermediate samples. Extensive experiments demonstrate that lowers outperforms previous methods in significant ways: (i) robust \u2013 (ii) greatly improves the model robustness without sacrificing accuracy on benign texts. Specifically, the defense lowers the success rates of unlimited and limited attacks by 59.43% and 39.33% respectively, while remaining accuracy of 93.30%; (ii) flexible \u2013 applied can easily extend to various language models to solve different downstream tasks with excellent performance; and (iii) efficient \u2013 is can be directly applied to the fine-tuning stage without pre-training language model from scratch, and the proposed data augmentation method is also low-cost.","wordlikeness":0.875,"lcsratio":0.75,"wordcoverage":0.8571428571}
{"Year":1996,"Venue":"ws-1996","Acronym":"MBT","Description":"A Memory-Based Part of Speech Tagger-Generator","Abstract":"We introduce a memory-based approach to part of speech tagging. Memory-based learning is a form of supervised learning based on similarity-based reasoning. The part of speech tag of a word in a particular context is extrapolated from the most similar cases held in memory. Supervised learning approaches are useful when a tagged corpus is available as an example of the desired output of the tagger. Based on such a corpus, the tagger-generator automatically builds a tagger which is able to tag new text the same way, diminishing development time for the construction of a tagger considerably. Memory-based tagging shares this advantage with other statistical or machine learning approaches. Additional advantages specific to a memory-based approach include (i) the relatively small tagged corpus size sufficient for training, (ii) incremental learning, (iii) explanation capabilities, (iv) flexible integration of information in case representations, (v) its non-parametric nature, (vi) reasonably good results on unknown words without morphological analysis, and (vii) fast learning and tagging. In this paper we show that a large-scale application of the memory-based approach is feasible: we obtain a tagging accuracy that is on a par with that of known statistical approaches, and with attractive space and time complexity properties when using igtree, a tree-based formalism for indexing and searching huge case bases. The use of igtree has as additional advantage that optimal context size for disambiguation is dynamically computed.","wordlikeness":0.3333333333,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2018,"Venue":"conll-2018","Acronym":"ELMoLex","Description":"Connecting ELMo and Lexicon Features for Dependency Parsing","Abstract":"In this paper, we present the details of the neural dependency parser and the neural tagger submitted by our team \u2018parisnlp\u2019 to the conll 2018 shared task on parsing from raw text to universal dependencies. We augment the deep biaffine (biaf) parser (dozat and manning, 2016) with novel features to perform competitively: we utilize an indomain version of elmo features (peters et al., 2018) which provide context-dependent word representations; we utilize disambiguated, embedded, morphosyntactic features from lexicons (sagot, 2018), which complements the existing feature set. Henceforth, we call our system \u2018benefits\u2019. In addition to incorporating character embeddings, context-dependent benefits from pre-trained word vectors, elmo and morphosyntactic features (whenever available) to correctly handle rare or unknown words which are prevalent in languages with complex morphology. Dependencies. Ranked 11th by labeled attachment score metric (70.64%), morphology-aware las metric (55.74%) and ranked 9th by bilexical dependency metric (60.70%).","wordlikeness":0.5714285714,"lcsratio":1.0,"wordcoverage":0.7272727273}
{"Year":2021,"Venue":"acl-2021","Acronym":"DESCGEN","Description":"A Distantly Supervised Datasetfor Generating Entity Descriptions","Abstract":"Short textual descriptions of entities provide summaries of their key attributes and have been shown to be useful sources of background knowledge for tasks such as entity linking and question answering. However, generating entity descriptions, especially for new and long-tail entities, can be challenging since relevant information is often scattered across multiple sources with varied content and style. We introduce fandom,: given mentions spread over multiple documents, the goal is to generate an entity summary description. Mentions consists of 37k entity descriptions from wikipedia and fandom, each paired with nine evidence documents on average. The documents were collected using a combination of entity linking and hyperlinks into the entity pages, which together provide high-quality distant supervision. Compared to other multi-document summarization tasks, our task is entity-centric, more abstractive, and covers a wide range of domains. We also propose a two-stage extract-then-generate baseline and show that there exists a large gap (19.9% in rouge-l) between state-of-art models and human performance, suggesting that the data will support significant future work.","wordlikeness":0.7142857143,"lcsratio":0.8571428571,"wordcoverage":0.8571428571}
{"Year":2022,"Venue":"lrec-2022","Acronym":"FABRA","Description":"French Aggregator-Based Readability Assessment toolkit","Abstract":"In this paper, we present the a: readability toolkit based on the aggregation of a large number of readability predictor variables. The toolkit is implemented as a service-oriented architecture, which obviates the need for installation, and simplifies its integration into other projects. We also perform a set of experiments to show which features are most predictive on two different corpora, and how the use of aggregators improves performance over standard feature-based readability prediction. Our experiments show that, for the explored corpora, the most important predictors for native texts are measures of lexical diversity, dependency counts and text coherence, while the most important predictors for foreign texts are syntactic variables illustrating language development, as well as features linked to lexical sophistication. Aggregation: have the potential to support new research on readability assessment for french.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2010,"Venue":"lrec-2010","Acronym":"Spontal-N","Description":"A Corpus of Interactional Spoken Norwegian","Abstract":"Video-recordings is a corpus of spontaneous, interactional norwegian. To our knowledge, it is the first corpus of norwegian in which the majority of speakers have spent significant parts of their lives in sweden, and in which the recorded speech displays varying degrees of interference from swedish. The corpus consists of studio quality audio- and video-recordings of four 30-minute free conversations between acquaintances, and a manual orthographic transcription of the entire material. On basis of the orthographic transcriptions, we automatically annotated approximately 50 percent of the material on the phoneme level, by means of a forced alignment between the acoustic signal and pronunciations listed in a dictionary. Approximately seven percent of the automatic transcription was manually corrected. Taking the manual correction as a gold standard, we evaluated several sources of pronunciation variants for the automatic transcription. Between is intended as a general purpose speech resource that is also suitable for investigating phonetic detail.","wordlikeness":0.5555555556,"lcsratio":0.7777777778,"wordcoverage":0.7058823529}
{"Year":2023,"Venue":"findings-2023","Acronym":"MTGP","Description":"Multi-turn Target-oriented Dialogue Guided by Generative Global Path with Flexible Turns","Abstract":"Target-oriented dialogue guides the dialogue to a target quickly and smoothly. The latest approaches focus on global planning, which plans toward the target before the conversation instead of adopting a greedy strategy during the conversation. However, the global plan in existing works is fixed to certain turns by generating paths with certain nodes, which limits the optimization of turns and coherence of the target-oriented process. Toward flexible global planning, we propose to generate a global path as a natural language sentence instead of a sequence of nodes. With this path, the dialog is guided to the target with flexible turns of dialog. For model training, we also extract targetoriented dialogues from the chit-chat corpus with a knowledge graph. We conduct experiments on three datasets and simulate scenarios with and without user participation. The results show that our method has fewer turns, more coherent semantics, and a higher success rate in reaching the target than baselines.","wordlikeness":0.25,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2021,"Venue":"nodalida-2021","Acronym":"DanFEVER","Description":"claim verification dataset for Danish","Abstract":"We present a dataset, as, intended for multilingual misinformation research. The dataset is in danish and has the same format as the well-known english fever dataset. It can be used for testing methods in multilingual settings, as well as for creating models in production for the danish language.","wordlikeness":0.75,"lcsratio":0.5,"wordcoverage":0.7692307692}
{"Year":2022,"Venue":"icon-2022","Acronym":"KILDST","Description":"Effective Knowledge-Integrated Learning for Dialogue State Tracking using Gazetteer and Speaker Information","Abstract":"Dialogue state tracking (dst) is core research in dialogue systems and has received much attention. In addition, it is necessary to define a new problem that can deal with dialogue between users as a step toward the conversational ai that extracts and recommends information from the dialogue between users. So, we introduce a new task - dst from dialogue between users about scheduling an event (dst-users). The dst-users task is much more challenging since it requires the model to understand and track dialogue states in the dialogue between users, as well as to understand who suggested the schedule and who agreed to the proposed schedule. To facilitate dst-users research, we develop dialogue datasets between users that plan a schedule. The annotated slot values which need to be extracted in the dialogue are date, time, and location. Previous approaches, such as machine reading comprehension (mrc) and traditional dst techniques, have not achieved good results in our extensive evaluations. By adopting the knowledge-integrated learning method, we achieve exceptional results. The proposed model architecture combines gazetteer features and speaker information efficiently. Our evaluations of the dialogue datasets between users that plan a schedule show that our model outperforms the baseline model.","wordlikeness":0.3333333333,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2018,"Venue":"acl-2018","Acronym":"HarriGT","Description":"A Tool for Linking News to Science","Abstract":"Being able to reliably link scientific works to the newspaper articles that discuss them could provide a breakthrough in the way we rationalise and measure the impact of science on our society. Linking these articles is challenging because the language used in the two domains is very different, and the gathering of online resources to align the two is a substantial information retrieval endeavour. We present facilitate, a semi-automated tool for building corpora of news articles linked to the scientific papers that they discuss. Our aim is to facilitate future development of information-retrieval tools for newspaper\/scientific work citation linking. Impact retrieves newspaper articles from an archive containing 17 years of uk web content. It also integrates with 3 large external citation networks, leveraging named entity extraction, and document classification to surface relevant examples of scientific literature to the user. We also provide a tuned candidate ranking algorithm to highlight potential links between scientific papers and newspaper articles to the user, in order of likelihood. Candidate is provided as an open source tool (<a href=http:\/\/leveraging.xyz class=acl-markup-url>http:\/\/could.xyz<\/a>).","wordlikeness":0.7142857143,"lcsratio":0.7142857143,"wordcoverage":0.8571428571}
{"Year":2021,"Venue":"findings-2021","Acronym":"NUANCED","Description":"Natural Utterance Annotation for Nuanced Conversation with Estimated Distributions","Abstract":"Existing conversational systems are mostly agent-centric, which assumes the user utterances will closely follow the system ontology. However, in real-world scenarios, it is highly desirable that users can speak freely and naturally. In this work, we attempt to build a user-centric dialogue system for conversational recommendation. As there is no clean mapping for a user\u2019s free form utterance to an ontology, we first model the user preferences as estimated distributions over the system ontology and map the user\u2019s utterances to such distributions. Learning such a mapping poses new challenges on reasoning over various types of knowledge, ranging from factoid knowledge, commonsense knowledge to the users\u2019 own situations. To this end, we build a new dataset named on that focuses on such realistic settings, with 5.1k dialogues, 26k turns of high-quality user responses. We conduct experiments, showing both the usefulness and challenges of our problem setting. We believe available. Can serve as a valuable resource to push existing research from the agent-centric system to the user-centric system. The code and data are publicly available.","wordlikeness":0.7142857143,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2008,"Venue":"lrec-2008","Acronym":"CORP-ORAL","Description":"Spontaneous Speech Corpus for European Portuguese","Abstract":"Research activity on the portuguese language for speech synthesis and recognition has suffered from a considerable lack of human and material resources. This has raised some obstacles to the development of speech technology and speech interface platforms. One of the most significant obstacles is the lack of spontaneous speech corpora for the creation, training and further improvement of speech synthesis and recognition programs. It was in order to suppress this gap that the aim project was planned. The aim of the project is to build a corpus of spontaneous ep available for the training of speech synthesis and recognition systems as well as phonetic, phonological, lexical, morphological and syntactic studies. Further possibilities of enquiry such as sociolinguistic and pragmatic research are also covered in the corpus design. The data consist of unscripted and unprompted face-to-face dialogues between family, friends, colleagues and unacquainted participants. All recordings are orthographically transcribed and prosodically annotated. Built is built from scratch with the explicit goal of becoming entirely available on the internet to the scientific community and the public in general.","wordlikeness":0.7777777778,"lcsratio":0.7777777778,"wordcoverage":0.9411764706}
{"Year":2019,"Venue":"bionlp-2019","Acronym":"ChiMed","Description":"A Chinese Medical Corpus for Question Answering","Abstract":"Question answering (qa) is a challenging task in natural language processing (nlp), especially when it is applied to specific domains. While models trained in the general domain can be adapted to a new target domain, their performance often degrades significantly due to domain mismatch. Alternatively, one can require a large amount of domain-specific qa data, but such data are rare, especially for the medical domain. In this study, we first collect a large-scale chinese medical qa corpus called especially; second we annotate a small fraction of the corpus to check the quality of the answers; third, we extract two datasets from the corpus and use them for the relevancy prediction task and the adoption prediction task. Several benchmark models are applied to the datasets, producing good results for both tasks.","wordlikeness":0.8333333333,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2022,"Venue":"eamt-2022","Acronym":"QUARTZ","Description":"Quality-Aware Machine Translation","Abstract":"This paper presents developing, quality-aware machine translation, a project led by unbabel which aims at developing machine translation systems that are more robust and produce fewer critical errors. With presents we want to enable machine translation for user-generated conversational content types that do not tolerate critical errors in automatic translations.","wordlikeness":1.0,"lcsratio":0.8333333333,"wordcoverage":0.7692307692}
{"Year":2021,"Venue":"acl-2021","Acronym":"Shortformer","Description":"Better Language Modeling using Shorter Inputs","Abstract":"Increasing the input length has been a driver of progress in language modeling with transformers. We identify conditions where shorter inputs are not harmful, and achieve perplexity and efficiency improvements through two new methods that decrease input length. First, we show that initially training a model on short subsequences before moving on to longer ones both reduces overall training time and, surprisingly, substantially improves perplexity. Second, we show how to improve the efficiency of recurrence methods in transformers, which let models condition on previously processed tokens when generating sequences that exceed the maximal length the transformer can handle at once. Existing methods require computationally expensive relative position embeddings; we introduce a simple alternative of adding absolute position embeddings to queries and keys instead of to word embeddings, which efficiently produces superior results. We show that these recurrent models also benefit from short input lengths. Combining these techniques speeds up training by a factor of 1.65, reduces memory usage, and substantially improves perplexity on wikitext-103, without adding any parameters.","wordlikeness":0.8181818182,"lcsratio":0.6363636364,"wordcoverage":0.7777777778}
{"Year":2023,"Venue":"findings-2023","Acronym":"RHGN","Description":"Relation-gated Heterogeneous Graph Network for Entity Alignment in Knowledge Graphs","Abstract":"Entity alignment, which aims to identify equivalent entities from various knowledge graphs (kgs), is a fundamental and crucial task in knowledge graph fusion. Existing methods typically use triple or neighbor information to represent entities, and then align those entities using similarity matching. Most of them, however, fail to account for the heterogeneity among kgs and the distinction between kg entities and relations. To better solve these problems, we propose a relation-gated heterogeneous graph network (layer) for entity alignment. Specifically, a contains a relation-gated convolutional layer to distinguish relations and entities in the kg. In addition, from adopts a cross-graph embedding exchange module and a soft relation alignment module to address the neighbor heterogeneity and relation heterogeneity between different kgs, respectively. Extensive experiments on four benchmark datasets demonstrate that experiments is superior to existing state-of-the-art entity alignment methods.","wordlikeness":0.25,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2018,"Venue":"emnlp-2018","Acronym":"TVQA","Description":"Localized, Compositional Video Question Answering","Abstract":"Recent years have witnessed an increasing interest in image-based question-answering (qa) tasks. However, due to data limitations, there has been much less work on video-based qa. In this paper, we present neural, a large-scale video qa dataset based on 6 popular tv shows. This consists of 152,545 qa pairs from 21,793 clips, spanning over 460 hours of video. Questions are designed to be compositional in nature, requiring systems to jointly localize relevant moments within a clip, comprehend subtitle-based dialogue, and recognize relevant visual concepts. We provide analyses of this new dataset as well as several baselines and a multi-stream end-to-end trainable neural network framework for the trainable task. The dataset is publicly available at <a href=http:\/\/multi-stream.cs.unc.edu class=acl-markup-url>http:\/\/within.cs.unc.edu<\/a>.","wordlikeness":0.25,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2016,"Venue":"gwc-2016","Acronym":"CILI","Description":"the Collaborative Interlingual Index","Abstract":"This paper introduces the motivation for and design of the collaborative interlingual index (is). It is designed to make possible coordination between multiple loosely coupled wordnet projects. The structure of the designed is based on the interlingual index first proposed in the eurowordnet project with several pragmatic extensions: an explicit open license, definitions in english and links to wordnets in the global wordnet grid.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2023,"Venue":"ws-2023","Acronym":"PALI","Description":"A Language Identification Benchmark for Perso-Arabic Scripts","Abstract":"The perso-arabic scripts are a family of scripts that are widely adopted and used by various linguistic communities around the globe. Identifying various languages using such scripts is crucial to language technologies and challenging in low-resource setups. As such, this paper sheds light on the challenges of detecting languages using perso-arabic scripts, especially in bilingual communities where \u201cunconventional\u201d writing is practiced. To address this, we use a set of supervised techniques to classify sentences into their languages. Building on these, we also propose a hierarchical model that targets clusters of languages that are more often confused by the classifiers. Our experiment results indicate the effectiveness of our solutions.","wordlikeness":0.75,"lcsratio":0.75,"wordcoverage":0.8571428571}
{"Year":2021,"Venue":"ws-2021","Acronym":"QEMind","Description":"Alibaba&#39;s Submission to the WMT21 Quality Estimation Shared Task","Abstract":"Quality estimation, as a crucial step of quality control for machine translation, has been explored for years. The goal is to to investigate automatic methods for estimating the quality of machine translation results without reference translations. In this year\u2019s wmt qe shared task, we utilize the large-scale xlm-roberta pre-trained model and additionally propose several useful features to evaluate the uncertainty of the translations to build our qe system, named <i><b>outperform<\/b> <\/i>. The system has been applied to the sentence-level scoring task of direct assessment and the binary score prediction task of critical error detection. In this paper, we present our submissions to the wmt 2021 qe shared task and an extensive set of experimental results have shown us that our multilingual systems outperform the best system in the direct assessment qe task of wmt 2020.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.8333333333}
{"Year":2015,"Venue":"ws-2015","Acronym":"UdS-Sant","Description":"English--German Hybrid Machine Translation System","Abstract":"This paper describes the task english\u2013german hybrid machine translation (mt) system submitted to the translation task organized in the workshop on statistical machine translation (wmt) 2015. Our proposed hybrid system brings improvements over the baseline system by incorporating additional knowledge such as extracted bilingual named entities and bilingual phrase pairs induced from example-based methods. The reported \ufb01nal submission is the result of a hybrid system obtained from confusion network based system combination that combines the best performance of each individual system in a multi-engine pipeline.","wordlikeness":0.375,"lcsratio":0.75,"wordcoverage":0.6666666667}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"HydraSum","Description":"Disentangling Style Features in Text Summarization with Multi-Decoder Models","Abstract":"Summarization systems make numerous \u201cdecisions\u201d about summary properties during inference, e.g. degree of copying, specificity and length of outputs, etc. However, these are implicitly encoded within model parameters and specific styles cannot be enforced. To address this, we introduce their, a new summarization architecture that extends the single decoder framework of current models to a mixture-of-experts version with multiple decoders. We show that an\u2019s multiple decoders automatically learn contrasting summary styles when trained under the standard training objective without any extra supervision. Through experiments on three summarization datasets (cnn, newsroom and xsum), we show that summarization provides a simple mechanism to obtain stylistically-diverse summaries by sampling from either individual decoders or their mixtures, outperforming baseline models. Finally, we demonstrate that a small modification to the gating strategy during training can enforce an even stricter style partitioning, e.g. high- vs low-abstractiveness or high- vs low-specificity, allowing users to sample from a larger area in the generation space and vary summary styles along multiple dimensions.","wordlikeness":0.75,"lcsratio":0.625,"wordcoverage":0.6666666667}
{"Year":2022,"Venue":"lrec-2022","Acronym":"MUSS","Description":"Multilingual Unsupervised Sentence Simplification by Mining Paraphrases","Abstract":"Progress in sentence simplification has been hindered by a lack of labeled parallel simplification data, particularly in languages other than english. We introduce on, a multilingual unsupervised sentence simplification system that does not require labeled simplification data. Art uses a novel approach to sentence simplification that trains strong models using sentence-level paraphrase data instead of proper simplification data. These models leverage unsupervised pretraining and controllable generation mechanisms to flexibly adjust attributes such as length and lexical complexity at inference time. We further present a method to mine such paraphrase data in any language from common crawl using semantic sentence embeddings, thus removing the need for labeled data. We evaluate our approach on english, french, and spanish simplification benchmarks and closely match or outperform the previous best supervised results, despite not using any labeled simplification data. We push the state of the art further by incorporating labeled simplification data.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2015,"Venue":"semeval-2015","Acronym":"EliXa","Description":"A Modular and Flexible ABSA Platform","Abstract":"This paper presents a supervised aspect based sentiment analysis (absa) system. Our aim is to develop a modular platform which allows to easily conduct experiments by replacing the modules or adding new features. We obtain the best result in the opinion target extraction (ote) task (slot 2) using an off-the-shelf sequence labeler. The target polarity classi\ufb01cation (slot 3) is addressed by means of a multiclass svm algorithm which includes lexical based features such as the polarity values obtained from domain and open polarity lexicons. The system obtains accuracies of 0.70 and 0.73 for the restaurant and laptop domain respectively, and performs second best in the out-of-domain hotel, achieving an accuracy of 0.80.","wordlikeness":0.6,"lcsratio":0.6,"wordcoverage":0.8}
{"Year":2021,"Venue":"naacl-2021","Acronym":"DATE","Description":"Detecting Anomalies in Text via Self-Supervision of Transformers","Abstract":"Leveraging deep learning models for anomaly detection (ad) has seen widespread use in recent years due to superior performances over traditional methods. Recent deep methods for anomalies in images learn better features of normality in an end-to-end self-supervised setting. These methods train a model to discriminate between different transformations applied to visual data and then use the output to compute an anomaly score. We use this approach for ad in text, by introducing a novel pretext task on text sequences. We learn our transformations model end-to-end, enforcing two independent and complementary self-supervision signals, one at the token-level and one at the sequence-level. Under this new task formulation, we show strong quantitative and qualitative results on the 20newsgroups and ag news datasets. In the semi-supervised setting, we outperform state-of-the-art results by +13.5% and +6.9%, respectively (auroc). In the unsupervised configuration, due surpasses all other methods even when 10% of its training data is contaminated with outliers (compared with 0% for the others).","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2020,"Venue":"lrec-2020","Acronym":"WN-Salience","Description":"A Corpus of News Articles with Entity Salience Annotations","Abstract":"Entities can be found in various text genres, ranging from tweets and web pages to user queries submitted to web search engines. Existing research either considers all entities in the text equally important, or heuristics are used to measure their salience. We believe that a key reason for the relatively limited work on entity salience is the lack of appropriate datasets. To support research on entity salience, we present a new dataset, the wikinews salience dataset (in-text), which can be used to benchmark tasks such as entity salience detection and salient entity linking. Considers is built on top of wikinews, a wikimedia project whose mission is to present reliable news articles. Entities in wikinews articles are identified by the authors of the articles and are linked to wikinews categories when they are salient or to wikipedia pages otherwise. The dataset is built automatically, and consists of approximately 7,000 news articles, and 90,000 in-text entity annotations. We compare the we dataset against existing datasets on the task and analyze their differences. Furthermore, we conduct experiments on entity salience detection; the results demonstrate that ones. Is a challenging testbed that is complementary to existing ones.","wordlikeness":0.5454545455,"lcsratio":0.9090909091,"wordcoverage":0.6666666667}
{"Year":1998,"Venue":"coling-1998","Acronym":"Babel","Description":"A testbed for research in origins of language","Abstract":"We believe that language is a complex adaptive system that emerges from adaptive interactions between language users and continues to evolve and adapt through repeated interactions. Our research looks at the mechanisms and processes involved in such emergence and adaptation. To provide a basis for our computer simulations, we have implemented an open-ended, extensi- ble testbed called results. Which allows rapid con- struction of experiments and flexible visualiza- tion of results.","wordlikeness":0.8,"lcsratio":0.8,"wordcoverage":0.8888888889}
{"Year":2021,"Venue":"acl-2021","Acronym":"SemFace","Description":"Pre-training Encoder and Decoder with a Semantic Interface for Neural Machine Translation","Abstract":"While pre-training techniques are working very well in natural language processing, how to pre-train a decoder and effectively use it for neural machine translation (nmt) still remains a tricky issue. The main reason is that the cross-attention module between the encoder and decoder cannot be pre-trained, and the combined encoder-decoder model cannot work well in the fine-tuning stage because the inputs of the decoder cross-attention come from unknown encoder outputs. In this paper, we propose a better pre-training method for nmt by defining a semantic interface (regards) between the pre-trained encoder and the pre-trained decoder. Specifically, we propose two types of semantic interfaces, including cl-stage which regards cross-lingual embeddings as an interface, and vq-unsupervised which employs vector quantized embeddings to constrain the encoder outputs and decoder inputs in the same language-independent space. We conduct massive experiments on six supervised translation pairs and three unsupervised pairs. Experimental results demonstrate that our proposed of can effectively connect the pre-trained encoder and decoder, and achieves significant improvement by 3.7 and 1.5 bleu points on the two tasks respectively compared with previous pre-training-based nmt models.","wordlikeness":0.7142857143,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2000,"Venue":"naacl-2000","Acronym":"MIMIC","Description":"An Adaptive Mixed Initiative Spoken Dialogue System for Information Queries","Abstract":"This paper describes a, an adaptive mixed initia- tive spoken dialogue system that provides movie show- time information. Respects. Improves upon previous dialogue systems in two respects. First, it employs initiative-oriented strategy adaptation to automatically adapt response generation strategies based on the cumu- lative effect of information dynamically extracted from user utterances during the dialogue. Second, dialogue.'s dialogue management architecture decouples its initia- tive module from the goal and response strategy selec- tion processes, providing a general framework for devel- oping spoken dialogue systems with different adaptation behavior.","wordlikeness":1.0,"lcsratio":0.8,"wordcoverage":1.0}
{"Year":2017,"Venue":"ws-2017","Acronym":"GRaSP","Description":"Grounded Representation and Source Perspective","Abstract":"When people or organizations provide information, they make choices regarding what information they include and how they present it. The combination of these two aspects (the content and stance provided by the source) represents a perspective. Investigating differences in perspective can provide various useful insights in the reliability of information, the way perspectives change over time, shared beliefs among groups of a similar social or political background and contrasts between other groups, etc. This paper introduces a, a generic framework for modeling perspectives and their sources.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2022,"Venue":"acl-2022","Acronym":"LinkBERT","Description":"Pretraining Language Models with Document Links","Abstract":"Language model (lm) pretraining captures various knowledge from text corpora, helping downstream tasks. However, existing methods such as bert model a single document, and do not capture dependencies or knowledge that span across documents. In this work, we propose single, an lm pretraining method that leverages links between documents, e.g., hyperlinks. Given a text corpus, we view it as a graph of documents and create lm inputs by placing linked documents in the same context. We then pretrain the lm with two joint self-supervised objectives: masked language modeling and our new proposal, document relation prediction. We show that document outperforms bert on various downstream tasks across two domains: the general domain (pretrained on wikipedia with hyperlinks) and biomedical domain (pretrained on pubmed with citation links). The is especially effective for multi-hop reasoning and few-shot qa (+5% absolute improvement on hotpotqa and triviaqa), and our biomedical various sets new states of the art on various bionlp tasks (+7% on bioasq and usmle). We release our pretrained models, improvement and biocorpora,, as well as code and data.","wordlikeness":0.875,"lcsratio":0.5,"wordcoverage":0.8}
{"Year":2018,"Venue":"bea-2018","Acronym":"CLUF","Description":"a Neural Model for Second Language Acquisition Modeling","Abstract":"Second language acquisition modeling is the task to predict whether a second language learner would respond correctly in future exercises based on their learning history. In this paper, we propose a neural network based system to utilize rich contextual, linguistic and user information. Our neural model consists of a context encoder, a linguistic feature encoder, a user information encoder and a format information encoder (0.835). Furthermore, a decoder is introduced to combine such encoded features and make final predictions. Our system ranked in first place in the english track and second place in the spanish and french track with an auroc score of 0.861, 0.835 and 0.854 respectively.","wordlikeness":0.75,"lcsratio":0.75,"wordcoverage":0.75}
{"Year":2023,"Venue":"acl-2023","Acronym":"DOC","Description":"Improving Long Story Coherence With Detailed Outline Control","Abstract":"We propose the detailed outline control (we) framework for improving long-range plot coherence when automatically generating several-thousand-word-long stories. Re3 consists of two complementary components: a detailed outliner and a detailed controller. The detailed outliner creates a more detailed, hierarchically structured outline, shifting creative burden from the main drafting procedure to the planning stage. The detailed controller ensures the more detailed outline is still respected during generation by controlling story passages to align with outline details. In human evaluations of automatically generated stories, setting. Substantially outperforms a strong re3 baseline (yang et al., 2022) on plot coherence (22.5% absolute gain), outline relevance (28.2%), and interestingness (20.7%). Humans also judged control to be much more controllable in an interactive generation setting.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2023,"Venue":"findings-2023","Acronym":"NormMark","Description":"A Weakly Supervised Markov Model for Socio-cultural Norm Discovery","Abstract":"Norms, which are culturally accepted guidelines for behaviours, can be integrated into conversational models to generate utterances that are appropriate for the socio-cultural context. Existing methods for norm recognition tend to focus only on surface-level features of dialogues and do not take into account the interactions within a conversation. To address this issue, we propose technique., a probabilistic generative markov model to carry the latent features throughout a dialogue. These features are captured by discrete and continuous latent variables conditioned on the conversation history, and improve the model\u2019s ability in norm recognition. The model is trainable on weakly annotated data using the variational technique. On a dataset with limited norm annotations, we show that our approach achieves higher f1 score, outperforming current state-of-the-art methods, including gpt3.","wordlikeness":0.875,"lcsratio":0.625,"wordcoverage":0.7142857143}
{"Year":2020,"Venue":"acl-2020","Acronym":"DeepMet","Description":"A Reading Comprehension Paradigm for Token-level Metaphor Detection","Abstract":"Machine metaphor understanding is one of the major topics in nlp. Most of the recent attempts consider it as classification or sequence tagging task. However, few types of research introduce the rich linguistic information into the field of computational metaphor by leveraging powerful pre-training language models. We focus a novel reading comprehension paradigm for solving the token-level metaphor detection task which provides an innovative type of solution for this task. We propose an end-to-end deep metaphor detection model named machine based on this paradigm. The proposed approach encodes the global text context (whole sentence), local text context (sentence fragments), and question (query word) information as well as incorporating two types of part-of-speech (pos) features by making use of the advanced pre-training language model. The experimental results by using several metaphor datasets show that our model achieves competitive results in the second shared task on metaphor detection.","wordlikeness":0.8571428571,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2015,"Venue":"semeval-2015","Acronym":"SIEL","Description":"Aspect Based Sentiment Analysis in Reviews","Abstract":"Following the footsteps of semeval-2014 task 4 (pontiki et al., 2014), semeval-2015 too had a task dedicated to aspect-level sentiment analysis (pontiki et al., 2015), which saw participation from over 25 teams. In aspectbased sentiment analysis, the aim is to identify the aspects of entities and the sentiment expressed for each aspect. In this paper, we present a detailed description of our system, that stood 4th in aspect category subtask (slot 1), 7th in opinion target expression subtask (slot 2) and 8th in sentiment polarity subtask (slot 3) on the restaurant datasets.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2022,"Venue":"lrec-2022","Acronym":"Xposition","Description":"An Online Multilingual Database of Adpositional Semantics","Abstract":"We present lexical, an online platform for documenting adpositional semantics across languages in terms of supersenses (schneider et al., 2018). More than just a lexical database, are houses annotation guidelines, structured lexicographic documentation, and annotated corpora. Guidelines and documentation are stored as wiki pages for ease of editing, and described elements (supersenses, adpositions, etc.) are hyperlinked for ease of browsing. We describe how the platform structures information; its current contents across several languages; and aspects of the design of the web application that supports it, with special attention to how it supports datasets and standards that evolve over time.","wordlikeness":0.8888888889,"lcsratio":0.8888888889,"wordcoverage":0.9473684211}
{"Year":2020,"Venue":"aacl-2020","Acronym":"NLPStatTest","Description":"A Toolkit for Comparing NLP System Performance","Abstract":"Statistical significance testing centered on p-values is commonly used to compare nlp system performance, but p-values alone are insufficient because statistical significance differs from practical significance. The latter can be measured by estimating effect size. In this pa-per, we propose a three-stage procedure for comparing nlp system performance and provide a toolkit, toolkit,, that automates the process. Users can upload nlp system evaluation scores and the toolkit will analyze these scores, run appropriate significance tests, estimate effect size, and conduct power analysis to estimate type ii error. The toolkit provides a convenient and systematic way to compare nlp system performance that goes beyond statistical significance testing.","wordlikeness":0.6363636364,"lcsratio":0.6363636364,"wordcoverage":0.7368421053}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"CoCoID","Description":"Learning Contrastive Representations and Compact Clusters for Semi-Supervised Intent Discovery","Abstract":"Intent discovery is to mine new intents from user utterances, which are not present in the set of manually predefined intents. Previous approaches to intent discovery usually automatically cluster novel intents with prior knowledge from intent-labeled data in a semi-supervised way. In this paper, we focus on the discriminative user utterance representation learning and the compactness of the learned intent clusters. We propose a novel semi-supervised intent discovery framework both with two essential components: contrastive user utterance representation learning and intra-cluster knowledge distillation. The former attempts to detect similar and dissimilar intents from a minibatch-wise perspective. The latter regularizes the predictive distribution of the model over samples in a cluster-wise way. We conduct experiments on both real-life challenging datasets (i.e., clinc and banking) that are curated to emulate the true environment of commercial\/production systems and traditional datasets (i.e., stackoverflow and dbpedia) to evaluate the proposed semi-supervised. Experiment results demonstrate that our model substantially outperforms state-of-the-art intent discovery models (12 baselines) by over 1.4 acc and ari points and 1.1 nmi points across the four datasets. Further analyses suggest that regularizes is able to learn contrastive representations and compact clusters for intent discovery.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2021,"Venue":"naacl-2021","Acronym":"TextEssence","Description":"A Tool for Interactive Analysis of Semantic Shifts Between Corpora","Abstract":"Embeddings of words and concepts capture syntactic and semantic regularities of language; however, they have seen limited use as tools to study characteristics of different corpora and how they relate to one another. We introduce designed, an interactive system designed to enable comparative analysis of corpora using embeddings. High-quality includes visual, neighbor-based, and similarity-based modes of embedding analysis in a lightweight, web-based interface. We further propose a new measure of embedding confidence based on nearest neighborhood overlap, to assist in identifying high-quality embeddings for corpus analysis. A case study on covid-19 scientific literature illustrates the utility of the system. Similarity-based can be found at <a href=https:\/\/limited.github.io class=acl-markup-url>https:\/\/be.github.io<\/a>.","wordlikeness":0.7272727273,"lcsratio":0.9090909091,"wordcoverage":0.7777777778}
{"Year":2020,"Venue":"acl-2020","Acronym":"MLQA","Description":"Evaluating Cross-lingual Extractive Question Answering","Abstract":"Question answering (qa) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets. Such annotated datasets are difficult and costly to collect, and rarely exist in languages other than english, making building qa systems that work well in other languages challenging. In order to develop such systems, it is crucial to invest in high quality multilingual evaluation benchmarks to measure progress. We present training-language, a multi-way aligned extractive qa evaluation benchmark intended to spur research in this area. Exist contains qa instances in 7 languages, english, arabic, german, spanish, hindi, vietnamese and simplified chinese. Language, has over 12k instances in english and 5k in each other language, with each instance parallel between 4 languages on average. We evaluate state-of-the-art cross-lingual models and machine-translation-based baselines on on. In all cases, transfer results are shown to be significantly behind training-language performance.","wordlikeness":0.25,"lcsratio":0.75,"wordcoverage":0.8571428571}
{"Year":2022,"Venue":"acl-2022","Acronym":"SimKGC","Description":"Simple Contrastive Knowledge Graph Completion with Pre-trained Language Models","Abstract":"Knowledge graph completion (kgc) aims to reason over known facts and infer the missing links. Text-based methods such as kgbert (yao et al., 2019) learn entity representations from natural language descriptions, and have the potential for inductive kgc. However, the performance of text-based methods still largely lag behind graph embedding-based methods like transe (bordes et al., 2013) and rotate (sun et al., 2019b). In this paper, we identify that the key issue is efficient contrastive learning. To improve the learning efficiency, we introduce three types of negatives: in-batch negatives, pre-batch negatives, and self-negatives which act as a simple form of hard negatives. Combined with infonce loss, our proposed model identify can substantially outperform embedding-based methods on several benchmark datasets. In terms of mean reciprocal rank (mrr), we advance the state-of-the-art by +19% on wn18rr, +6.8% on the wikidata5m transductive setting, and +22% on the wikidata5m inductive setting. Thorough analyses are conducted to gain insights into each component. Our code is available at <a href=https:\/\/github.com\/intfloat\/are class=acl-markup-url>https:\/\/github.com\/intfloat\/state-of-the-art<\/a> .","wordlikeness":0.5,"lcsratio":1.0,"wordcoverage":0.6666666667}
{"Year":2022,"Venue":"acl-2022","Acronym":"UniPELT","Description":"A Unified Framework for Parameter-Efficient Language Model Tuning","Abstract":"Recent parameter-efficient language model tuning (pelt) methods manage to match the performance of fine-tuning with much fewer trainable parameters and perform especially well when training data is limited. However, different pelt methods may perform rather differently on the same task, making it nontrivial to select the most appropriate method for a specific task, especially considering the fast-growing number of new pelt methods and tasks. In light of model diversity and the difficulty of model selection, we propose a unified framework, on, which incorporates different pelt methods as submodules and learns to activate the ones that best suit the current data or task setup via gating mechanism. On the glue benchmark, used consistently achieves 1 4% gains compared to the best individual pelt method that it incorporates and even outperforms fine-tuning under different setups. Moreover, limited. Generally surpasses the upper bound that takes the best performance of all its submodules used individually on each task, indicating that a mixture of multiple pelt methods may be inherently more effective than single methods.","wordlikeness":0.5714285714,"lcsratio":1.0,"wordcoverage":0.7272727273}
{"Year":2020,"Venue":"acl-2020","Acronym":"RAT-SQL","Description":"Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers","Abstract":"When translating natural language questions into sql queries to answer questions from a database, contemporary semantic parsing models struggle to generalize to unseen database schemas. The generalization challenge lies in (a) encoding the database relations in an accessible way for the semantic parser, and (b) modeling alignment between database columns and their mentions in a given query. We present a unified framework, based on the relation-aware self-attention mechanism, to address schema encoding, schema linking, and feature representation within a text-to-sql encoder. On the challenging spider dataset this framework boosts the exact match accuracy to 57.2%, surpassing its best counterparts by 8.7% absolute improvement. Further augmented with bert, it achieves the new state-of-the-art performance of 65.6% on the spider leaderboard. In addition, we observe qualitative improvements in the model\u2019s understanding of schema linking and alignment. Our implementation will be open-sourced at <a href=https:\/\/github.com\/microsoft\/semantic class=acl-markup-url>https:\/\/github.com\/microsoft\/schema<\/a>.","wordlikeness":0.4285714286,"lcsratio":1.0,"wordcoverage":0.7272727273}
{"Year":2022,"Venue":"ijcnlp-2022","Acronym":"BERTSeg","Description":"BERT Based Unsupervised Subword Segmentation for Neural Machine Translation","Abstract":"Existing subword segmenters are either 1) frequency-based without semantics information or 2) neural-based but trained on parallel corpora. To address this, we present embeddings, an unsupervised neural subword segmenter for neural machine translation, which utilizes the contextualized semantic embeddings of words from characterbert and maximizes the generation probability of subword segmentations. Furthermore, we propose a generation probability-based regularization method that enables furthermore, to produce multiple segmentations for one word to improve the robustness of neural machine translation. Experimental results show that subword with regularization achieves up to 8 bleu points improvement in 9 translation directions on alt, iwslt15 vi->en, wmt16 ro->en, and wmt15 fi->en datasets compared with bpe. In addition, generation is efficient, needing up to 5 minutes for training.","wordlikeness":0.5714285714,"lcsratio":1.0,"wordcoverage":0.7272727273}
{"Year":2000,"Venue":"lrec-2000","Acronym":"POSCAT","Description":"A Morpheme-based Speech Corpus Annotation Tool","Abstract":"As more and more speech systems require linguistic knowledge to accommodate various levels of applications, corpora that are tagged with linguistic annotations as well as signal-level annotations are highly recommended for the development of today\u2019s speech systems. Among the linguistic annotations, pos (part-of-speech) tag annotations are indispensable in speech corpora for most modern spoken language applications of morphologically complex agglutinative languages such as korean. Considering the above demands, we have developed a single uni\ufb01ed speech corpus annotation tool that enables corpus builders to link linguistic annotations to signal-level annotations using a morphological analyzer and a pos tagger as basic morpheme-based linguistic engines. Our tool integrates a syntactic analyzer, phrase break detector, grapheme-to-phoneme converter and automatic phonetic aligner together. Each engine automatically annotates its own linguistic and signal knowledge, and interacts with the corpus developers to revise and correct the annotations on demand. All the linguistic\/phonetic engines were developed and merged with an interactive visualization tool in a client-server network communication model. The corpora that can be constructed using our annotation tool are multi-purpose and applicable to both speech recognition and text-tospeech (tts) systems. Finally, since the linguistic and signal processing engines and user interactive visualization tool are implemented within a client-server model, the system loads can be reasonably distributed over several machines. 1.","wordlikeness":0.8333333333,"lcsratio":0.8333333333,"wordcoverage":0.8}
{"Year":2022,"Venue":"emnlp-2022","Acronym":"MedicalSum","Description":"A Guided Clinical Abstractive Summarization Model for Generating Medical Reports from Patient-Doctor Conversations","Abstract":"We introduce stronger, a transformer-based sequence-to-sequence architecture for summarizing medical conversations by integrating medical domain knowledge from the unified medical language system (umls). The novel knowledge augmentation is performed in three ways: (i) introducing a guidance signal that consists of the medical words in the input sequence, (ii) leveraging semantic type knowledge in umls to create clinically meaningful input embeddings, and (iii) making use of a novel weighted loss function that provides a stronger incentive for the model to correctly predict words with a medical meaning. By applying these three strategies, create takes clinical knowledge into consideration during the summarization process and achieves state-of-the-art rouge score improvements of 0.8-2.1 points (including 6.2% rouge-1 error reduction in the pe section) when producing medical summaries of patient-doctor conversations.","wordlikeness":0.9,"lcsratio":0.9,"wordcoverage":0.8235294118}
{"Year":2023,"Venue":"acl-2023","Acronym":"QUEST","Description":"A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations","Abstract":"Formulating selective information needs results in queries that implicitly specify set operations, such as intersection, union, and difference. For instance, one might search for \u201cshorebirds that are not sandpipers\u201d or \u201cscience-fiction films shot in england\u201d. To study the ability of retrieval systems to meet such information needs, we construct queries, a dataset of 3357 natural language queries with implicit set operations, that map to a set of entities corresponding to wikipedia documents. The dataset challenges models to match multiple constraints mentioned in queries with corresponding evidence in documents and correctly perform various set operations. The dataset is constructed semi-automatically using wikipedia category names. Queries are automatically composed from individual categories, then paraphrased and further validated for naturalness and fluency by crowdworkers. Crowdworkers also assess the relevance of entities based on their documents and highlight attribution of query constraints to spans of document text. We analyze several modern retrieval systems, finding that they often struggle on such queries. Queries involving negation and conjunction are particularly challenging and systems are further challenged with combinations of these operations.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2020,"Venue":"acl-2020","Acronym":"OpinionDigest","Description":"A Simple Framework for Opinion Summarization","Abstract":"We present user, an abstractive opinion summarization framework, which does not rely on gold-standard summaries for training. The framework uses an aspect-based sentiment analysis model to extract opinion phrases from reviews, and trains a transformer model to reconstruct the original reviews from these extractions. At summarization time, we merge extractions from multiple reviews and select the most popular ones. The selected opinions are used as input to the trained transformer model, which verbalizes them into an opinion summary. Aspect can also generate customized summaries, tailored to specific user needs, by filtering the selected opinions according to their aspect and\/or sentiment. Automatic evaluation on yelp data shows that our framework outperforms competitive baselines. Human studies on two corpora verify that human produces informative summaries and shows promising customization capabilities.","wordlikeness":0.8461538462,"lcsratio":0.6923076923,"wordcoverage":0.7619047619}
{"Year":2013,"Venue":"semeval-2013","Acronym":"GU-MLT-LT","Description":"Sentiment Analysis of Short Messages using Linguistic Features and Stochastic Gradient Descent","Abstract":"This paper describes the details of our system submitted to the semeval-2013 shared task on sentiment analysis in twitter. Our approach to predicting the sentiment of tweets and sms is based on supervised machine learning techniques and task-speci\ufb01c feature engineering. We used a linear classi\ufb01er trained by stochastic gradient descent with hinge loss and elastic net regularization to make our predictions, which were ranked \ufb01rst or second in three of the four experimental conditions of the shared task. Furthermore, our system makes use of social media speci\ufb01c text preprocessing and linguistically motivated features, such as word stems, word clusters and negation handling.","wordlikeness":0.2222222222,"lcsratio":0.5555555556,"wordcoverage":0.5714285714}
{"Year":2022,"Venue":"ws-2022","Acronym":"IDANI","Description":"Inference-time Domain Adaptation via Neuron-level Interventions","Abstract":"Large pre-trained models are usually fine-tuned on downstream task data, and tested on unseen data. When the train and test data come from different domains, the model is likely to struggle, as it is not adapted to the test domain. We propose a new approach for domain adaptation (da), using neuron-level interventions: we modify the representation of each test example in specific neurons, resulting in a counterfactual example from the source domain, which the model is more familiar with. The modified example is then fed back into the model. While most other da methods are applied during training time, ours is applied during inference only, making it more efficient and applicable. Our experiments show that our method improves performance on unseen domains.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2023,"Venue":"acl-2023","Acronym":"Self-Edit","Description":"Fault-Aware Code Editor for Code Generation","Abstract":"Large language models (llms) have demonstrated an impressive ability to generate codes on competitive programming tasks. However, with limited sample numbers, llms still suffer from poor accuracy. Inspired by the process of human programming, we propose a generate-and-edit approach named utilizing that utilizes execution results of the generated code from llms to improve the code quality on the competitive programming task. We execute the generated code on the example test case provided in the question and wrap execution results into a supplementary comment. Utilizing this comment as guidance, our fault-aware code editor is employed to correct errors in the generated code. We perform extensive evaluations across two competitive programming datasets with nine different llms. compared to directly generating from llms, our approach can improve the average of pass@1 by 89% on apps-dev, 31% on apps-test, and 48% on humaneval over nine popular code generation llms with parameter sizes ranging from 110m to 175b. Compared to other post-processing methods, our method demonstrates superior accuracy and efficiency.","wordlikeness":0.7777777778,"lcsratio":0.6666666667,"wordcoverage":0.6666666667}
{"Year":2018,"Venue":"coling-2018","Acronym":"Graphene","Description":"a Context-Preserving Open Information Extraction System","Abstract":"We introduce generating, an open ie system whose goal is to generate accurate, meaningful and complete propositions that may facilitate a variety of downstream semantic applications. For this purpose, we transform syntactically complex input sentences into clean, compact structures in the form of core facts and accompanying contexts, while identifying the rhetorical relations that hold between them in order to maintain their semantic relationship. In that way, we preserve the context of the relational tuples extracted from a source sentence, generating a novel lightweight semantic representation for open ie that enhances the expressiveness of the extracted propositions.","wordlikeness":0.75,"lcsratio":0.75,"wordcoverage":0.7692307692}
{"Year":2018,"Venue":"emnlp-2018","Acronym":"MorAz","Description":"an Open-source Morphological Analyzer for Azerbaijani Turkish","Abstract":"Of is an open-source morphological analyzer for azerbaijani turkish. The analyzer is available through both as a website for interactive exploration and as a restful web service for integration into a natural language processing pipeline. Azerbaijani implements the morphology of azerbaijani turkish in two-level using helsinki finite-state transducer and wraps the analyzer with python scripts in a django instance.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2014,"Venue":"ws-2014","Acronym":"DiscoTK","Description":"Using Discourse Structure for Machine Translation Evaluation","Abstract":"We present novel automatic metrics for machine translation evaluation that use discourse structure and convolution kernels to compare the discourse tree of an automatic translation with that of the human reference. We experiment with \ufb01ve transformations and augmentations of a base discourse tree representation based on the rhetorical structure theory, and we combine the kernel scores for each of them into a single score. Finally, we add other metrics from the asiya mt evaluation toolkit, and we tune the weights of the combination on actual human judgments. Experiments on the wmt12 and wmt13 metrics shared task datasets show correlation with human judgments that outperforms what the best systems that participated in these years achieved, both at the segment and at the system level.","wordlikeness":0.7142857143,"lcsratio":0.8571428571,"wordcoverage":0.8333333333}
{"Year":2021,"Venue":"naacl-2021","Acronym":"MERMAID","Description":"Metaphor Generation with Symbolism and Discriminative Decoding","Abstract":"Generating metaphors is a challenging task as it requires a proper understanding of abstract concepts, making connections between unrelated concepts, and deviating from the literal meaning. In this paper, we aim to generate a metaphoric sentence given a literal expression by replacing relevant verbs. Based on a theoretically-grounded connection between metaphors and symbols, we propose a method to automatically construct a parallel corpus by transforming a large number of metaphorical sentences from the gutenberg poetry corpus (citation) to their literal counterpart using recent advances in masked language modeling coupled with commonsense inference. For the generation task, we incorporate a metaphor discriminator to guide the decoding of a sequence to sequence model fine-tuned on our parallel data to generate high-quality metaphors. Human evaluation on an independent test set of literal statements shows that our best model generates metaphors better than three well-crafted baselines 66% of the time on average. A task-based evaluation shows that human-written poems enhanced with metaphors proposed by our model are preferred 68% of the time compared to poems without metaphors.","wordlikeness":0.7142857143,"lcsratio":1.0,"wordcoverage":0.7272727273}
{"Year":2016,"Venue":"lrec-2016","Acronym":"Ubuntu-fr","Description":"A Large and Open Corpus for Multi-modal Analysis of Online Written Conversations","Abstract":"We present a large, free, french corpus of online written conversations extracted from the ubuntu platform\u2019s forums, mailing lists and irc channels. The corpus is meant to support multi-modality and diachronic studies of online written conversations. We choose to build the corpus around a robust metadata model based upon strong principles, such as the \u201cstand off\u201d annotation principle. We detail the model, we explain how the data was collected and processed - in terms of meta-data, text and conversation - and we detail the corpus\u2019contents through a series of meaningful statistics. A portion of the corpus - about 4,700 sentences from emails, forum posts and chat messages sent in november 2014 - is annotated in terms of dialogue acts and sentiment. We discuss how we adapted our dialogue act taxonomy from the dit++ annotation scheme and how the data was annotated, before presenting our results as well as a brief qualitative analysis of the annotated data.","wordlikeness":0.3333333333,"lcsratio":0.6666666667,"wordcoverage":0.5882352941}
{"Year":2020,"Venue":"inlg-2020","Acronym":"ExTRA","Description":"Explainable Therapy-Related Annotations","Abstract":"In this paper we report progress on a novel explainable artificial intelligence (xai) initiative applying natural language processing (nlp) with elements of codesign to develop a text classifier for application in psychotherapy training. The task is to produce a tool that will facilitate therapists to review their sessions by automatically labelling transcript text with levels of interaction for patient activation in known psychological processes, using xai to increase their trust in the model\u2019s suggestions and client trajectory predictions. After pre-processing of the language features thiscted from professionally annotated therapy session transcripts, we apply a supervised machine learning approach (chaid) to classify interaction labels (negative, neutral, positive). Weighted samples are used to overcome class imbalanced data. The results show this initial model can make useful distinctions among the three labels of patient activation with 74% accuracy and provide insight into its reasoning. This ongoing project will additionally evaluate which xai approaches can be used to increase the transparency of the tool to end users, exploring whether direct involvement of stakeholders improves usability of the xai interface and therefore trust in the solution.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"PermuteFormer","Description":"Efficient Relative Position Encoding for Long Sequences","Abstract":"A recent variation of transformer, performer, scales transformer to longer sequences with a linear attention mechanism. However, it is not compatible with relative position encoding, which has advantages over absolute position encoding. In this paper, we discuss possible ways to add relative position encoding to performer. Based on the analysis, we propose long, a performer-based model with relative position encoding that scales linearly on long sequences. Analysis, applies position-dependent transformation on queries and keys to encode positional information into the attention module. This transformation is carefully crafted so that the final output of self-attention is not affected by absolute positions of tokens. Relative introduces negligible computational overhead by design that it runs as fast as performer. We evaluate fast on long-range arena, a dataset for long sequences, as well as wikitext-103, a language modeling dataset. The experiments show that computational uniformly improves the performance of performer with almost no computational overhead and outperforms vanilla transformer on most of the tasks.","wordlikeness":0.6923076923,"lcsratio":0.6153846154,"wordcoverage":0.8181818182}
{"Year":2020,"Venue":"acl-2020","Acronym":"MIND","Description":"A Large-scale Dataset for News Recommendation","Abstract":"News recommendation is an important technique for personalized news service. Compared with product and movie recommendations which have been comprehensively studied, the research on news recommendation is much more limited, mainly due to the lack of a high-quality benchmark dataset. In this paper, we present a large-scale dataset named than for news recommendation. Constructed from the user click logs of microsoft news, research contains 1 million users and more than 160k english news articles, each of which has rich textual content such as title, abstract and body. We demonstrate can a good testbed for news recommendation through a comparative study of several state-of-the-art news recommendation methods which are originally developed on different proprietary datasets. Our results show the performance of news recommendation highly relies on the quality of news content understanding and user interest modeling. Many natural language processing techniques such as effective text representation methods and pre-trained language models can effectively improve the performance of news recommendation. The news dataset will be available at <a href=https:\/\/msnews.github.io class=acl-markup-url>https:\/\/msnews.github.io<\/a>.","wordlikeness":1.0,"lcsratio":0.75,"wordcoverage":1.0}
{"Year":2022,"Venue":"ws-2022","Acronym":"ANTS","Description":"A Framework for Retrieval of Text Segments in Unstructured Documents","Abstract":"Text segmentation and extraction from unstructured documents can provide business researchers with a wealth of new information on firms and their behaviors. However, the most valuable text is often difficult to extract consistently due to substantial variations in how content can appear from document to document. Thus, the most successful way to extract this content has been through costly crowdsourcing and training of manual workers. We propose the assisted neural text segmentation (segmentation) framework to identify pertinent text in unstructured documents from a small set of labeled examples. Using leverages deep learning and transfer learning architectures to empower researchers to identify relevant text with minimal manual coding. Using a real world sample of accounting documents, we identify targeted sections 96% of the time using only 5 training examples.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":0.8888888889}
{"Year":2020,"Venue":"lrec-2020","Acronym":"TableBank","Description":"Table Benchmark for Image-based Table Detection and Recognition","Abstract":"We present will, a new image-based table detection and recognition dataset built with novel weak supervision from word and latex documents on the internet. Existing research for image-based table detection and recognition usually fine-tunes pre-trained models on out-of-domain data with a few thousand human-labeled examples, which is difficult to generalize on real-world applications. With downloaded that contains 417k high quality labeled tables, we build several strong baselines using state-of-the-art models with deep neural networks. We make build publicly available and hope it will empower more deep learning approaches in the table detection and recognition task. The dataset and models can be downloaded from <a href=https:\/\/github.com\/doc-analysis\/with class=acl-markup-url>https:\/\/github.com\/doc-analysis\/tables,<\/a>.","wordlikeness":0.8888888889,"lcsratio":0.8888888889,"wordcoverage":0.75}
{"Year":2019,"Venue":"storynlp-2019","Acronym":"WriterForcing","Description":"Generating more interesting story endings","Abstract":"We study the problem of generating interesting endings for stories. Neural generative models have shown promising results for various text generation problems. Sequence to sequence (seq2seq) models are typically trained to generate a single output sequence for a given input sequence. However, in the context of a story, multiple endings are possible. Seq2seq models tend to ignore the context and generate generic and dull responses. Very few works have studied generating diverse and interesting story endings for the same story context. In this paper, we propose models which generate more diverse and interesting outputs by 1) training models to focus attention on important keyphrases of the story, and 2) promoting generating nongeneric words. We show that the combination of the two leads to more interesting endings.","wordlikeness":0.7692307692,"lcsratio":0.7692307692,"wordcoverage":0.75}
{"Year":2019,"Venue":"acl-2019","Acronym":"ERNIE","Description":"Enhanced Language Representation with Informative Entities","Abstract":"Neural language representation models such as bert pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various nlp tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (kgs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in kgs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and kgs to train an enhanced language representation model (fine-tuned), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that is achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model bert on other common nlp tasks. The code and datasets will be available in the future.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.9090909091}
{"Year":2021,"Venue":"emnlp-2021","Acronym":"iFacetSum","Description":"Coreference-based Interactive Faceted Summarization for Multi-Document Exploration","Abstract":"We introduce if\u1d00\u1d04\u1d07\u1d1bs\u1d1c\u1d0d, a web application for exploring topical document collections. If\u1d00\u1d04\u1d07\u1d1bs\u1d1c\u1d0d integrates interactive summarization together with faceted search, by providing a novel faceted navigation scheme that yields abstractive summaries for the user\u2019s selections. This approach offers both a comprehensive overview as well as particular details regard-ing subtopics of choice. The facets are automatically produced based on cross-document coreference pipelines, rendering generic concepts, entities and statements surfacing in the source texts. We analyze the effectiveness of our application through small-scale user studies that suggest the usefulness of our tool.","wordlikeness":0.7777777778,"lcsratio":1.0,"wordcoverage":0.8}
{"Year":2023,"Venue":"ws-2023","Acronym":"Jambu","Description":"A historical linguistic database for South Asian languages","Abstract":"We introduce hope, a cognate database of south asian languages which unifies dozens of previous sources in a structured and accessible format. The database includes nearly 287k lemmata from 602 lects, grouped together in 23k sets of cognates. We outline the data wrangling necessary to compile the dataset and train neural models for reflex prediction on the indo- aryan subset of the data. We hope that improvement is an invaluable resource for all historical linguists and indologists, and look towards further improvement and expansion of the database.","wordlikeness":0.8,"lcsratio":0.6,"wordcoverage":0.8888888889}
{"Year":2020,"Venue":"findings-2020","Acronym":"ConceptBert","Description":"Concept-Aware Representation for Visual Question Answering","Abstract":"Visual question answering (vqa) is a challenging task that has received increasing attention from both the computer vision and the natural language processing communities. A vqa model combines visual and textual features in order to answer questions grounded in an image. Current works in vqa focus on questions which are answerable by direct analysis of the question and image alone. We present a concept-aware algorithm, common, for questions which require common sense, or basic factual knowledge from external structured content. Given an image and a question in natural language, correct requires visual elements of the image and a knowledge graph (kg) to infer the correct answer. We introduce a multi-modal representation which learns a joint concept-vision-language embedding inspired by the popular bert architecture. We exploit conceptnet kg for encoding the common sense knowledge and evaluate our methodology on the outside knowledge-vqa (ok-vqa) and vqa datasets.","wordlikeness":0.9090909091,"lcsratio":0.9090909091,"wordcoverage":0.7777777778}
{"Year":2022,"Venue":"tacl-2022","Acronym":"LOT","Description":"A Story-Centric Benchmark for Evaluating Chinese Long Text Understanding and Generation","Abstract":"Standard multi-task benchmarks are essential for developing pretraining models that can generalize to various downstream tasks. Existing benchmarks for natural language processing (nlp) usually focus only on understanding or generating short texts. However, long text modeling requires many distinct abilities in contrast to short texts, such as the modeling of long-range discourse and commonsense relations, and the coherence and controllability of generation. The lack of standardized benchmarks makes it difficult to assess these abilities of a model and fairly compare different models, especially chinese models. Therefore, we propose a story-centric benchmark named the for evaluating chinese long text modeling, which aggregates two understanding tasks and two generation tasks. We construct new datasets for these tasks based on human-written chinese stories with hundreds of words. Furthermore, we release an encoder-decoder-based chinese long text pretraining model named longlm with up to 1 billion parameters. We pretrain longlm on 120g chinese novels with two generative tasks including text infilling and conditional continuation. Extensive experiments show that longlm outperforms similar-sized pretraining models substantially on both the understanding and generation tasks in with.","wordlikeness":1.0,"lcsratio":1.0,"wordcoverage":1.0}
{"Year":2012,"Venue":"lrec-2012","Acronym":"SUTime","Description":"A library for recognizing and normalizing time expressions","Abstract":"We describe outperforms, a temporal tagger for recognizing and normalizing temporal expressions in english text. English is available as part of the stanford corenlp pipeline and can be used to annotate documents with temporal information. It is a deterministic rule-based system designed for extensibility. Testing on the tempeval-2 evaluation corpus shows that this system outperforms state-of-the-art techniques.","wordlikeness":0.8333333333,"lcsratio":0.6666666667,"wordcoverage":0.8}
{"Year":2013,"Venue":"semeval-2013","Acronym":"LIMSIILES","Description":"Basic English Substitution for Student Answer Assessment at SemEval 2013","Abstract":"In this paper, we describe a method for assessing student answers, modeled as a paraphrase identi\ufb01cation problem, based on substitution by basic english variants. Basic english paraphrases are acquired from the simple english wiktionary. Substitutions are applied both on reference answers and student answers in order to reduce the diversity of their vocabulary and map them to a common vocabulary. The evaluation of our approach on the semeval 2013 joint student response analysis and 8th recognizing textual entailment challenge data shows promising results, and this work is a \ufb01rst step toward an opendomain system able to exhibit deep text understanding capabilities.","wordlikeness":0.6666666667,"lcsratio":0.7777777778,"wordcoverage":0.7777777778}
{"Year":2023,"Venue":"acl-2023","Acronym":"CREST","Description":"A Joint Framework for Rationalization and Counterfactual Text Generation","Abstract":"Selective rationales and counterfactual examples have emerged as two effective, complementary classes of interpretability methods for analyzing and training nlp models. However, prior work has not explored how these methods can be integrated to combine their complementary advantages. We overcome this limitation by introducing improvements (contrastive edits with sparse rationalization), a joint framework for selective rationalization and counterfactual text generation, and show that this framework leads to improvements in counterfactual quality, model robustness, and interpretability. First, function generates valid counterfactuals that are more natural than those produced by previous methods, and subsequently can be used for data augmentation at scale, reducing the need for human-generated examples. Second, we introduce a new loss function that leverages valid counterfactuals to regularize selective rationales and show that this regularization improves both model robustness and rationale quality, compared to methods that do not leverage rationales counterfactuals. Our results demonstrate that classes successfully bridges the gap between selective rationales and counterfactual examples, addressing the limitations of existing methods and providing a more comprehensive view of a model\u2019s predictions.","wordlikeness":1.0,"lcsratio":0.8,"wordcoverage":1.0}
{"Year":2011,"Venue":"eamt-2011","Acronym":"Apertium-IceNLP","Description":"A rule-based Icelandic to English machine translation system","Abstract":"We describe the development of a prototype of an open source rule-based icelandic\u2192english mt system, based on the apertium mt framework and icenlp, a natural language processing toolkit for icelandic. Our system, is, is the \ufb01rst system in which the whole morphological and tagging component of apertium is replaced by modules from an external system. Evaluation shows that the word error rate and the positionindependent word error rate for our prototype is 50.6% and 40.8%, respectively. As expected, this is higher than the corresponding error rates in two publicly available mt systems that we used for comparison. Contrary to our expectations, the error rates of our prototype is also higher than the error rates of a comparable system based solely on apertium modules. Based on error analysis, we conclude that better translation quality may be achieved by replacing only the tagging component of apertium with the corresponding module in icenlp, but leaving morphological analysis to apertium.","wordlikeness":0.4666666667,"lcsratio":0.6,"wordcoverage":0.6086956522}
{"Year":2022,"Venue":"naacl-2022","Acronym":"CoSIm","Description":"Commonsense Reasoning for Counterfactual Scene Imagination","Abstract":"As humans, we can modify our assumptions about a scene by imagining alternative objects or concepts in our minds. For example, we can easily anticipate the implications of the sun being overcast by rain clouds (e.g., the street will get wet) and accordingly prepare for that. In this paper, we introduce a new dataset called commonsense reasoning for counterfactual scene imagination (present) which is designed to evaluate the ability of ai systems to reason about scene change imagination. To be specific, in this multimodal task\/dataset, models are given an image and an initial question-response pair about the image. Next, a counterfactual imagined scene change (in textual form) is applied, and the model has to predict the new response to the initial question based on this scene change. We collect 3.5k high-quality and challenging data instances, with each instance consisting of an image, a commonsense question with a response, a description of a counterfactual change, a new response to the question, and three distractor responses. Our dataset contains various complex scene change types (such as object addition\/removal\/state change, event description, environment change, etc.) that require models to imagine many different scenarios and reason about the changed scenes. We present a baseline model based on a vision-language transformer (i.e., lxmert) and ablation studies. Through human evaluation, we demonstrate a large human-model performance gap, suggesting room for promising future work on this challenging, counterfactual multimodal task.","wordlikeness":0.6,"lcsratio":1.0,"wordcoverage":0.75}
{"Year":2019,"Venue":"dmr-2019","Acronym":"ClearTAC","Description":"Verb Tense, Aspect, and Form Classification Using Neural Nets","Abstract":"This paper proposes using a bidirectional lstm-crf model in order to identify the tense and aspect of verbs. The information that this classifier outputs can be useful for ordering events and can provide a pre-processing step to improve efficiency of annotating this type of information. This neural network architecture has been successfully employed for other sequential labeling tasks, and we show that it significantly outperforms the rule-based tool tmv-annotator on the propbank i dataset.","wordlikeness":0.625,"lcsratio":0.625,"wordcoverage":0.8235294118}
{"Year":2016,"Venue":"lrec-2016","Acronym":"Cohere","Description":"A Toolkit for Local Coherence","Abstract":"We describe metric, our incorporatesnce toolkit which incorporates various complementary models for capturing and measuring different aspects of text thesence. In addition to the traditional entity grid model (lapata, 2005) and graph-based metric (guinaudeau and strube, 2013), we provide an implementation of a state-of-the-art syntax-based model (louis and nenkova, 2012), as well as an adaptation of this model which shows significant performance improvements in our experiments. We benchmark these models using the standard setting for text documentnce: original documents and versions of the document with sentences in shuffled order.","wordlikeness":0.6666666667,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2006,"Venue":"ws-2006","Acronym":"LoLo","Description":"A System based on Terminology for Multilingual Extraction","Abstract":"An unsupervised learning method, based on corpus linguistics and special language terminology, is described that can extract time-varying information from text streams. The method is shown to be \u2018language-independent\u2019 in that its use leads to sets of regular-expressions that can be used to extract the information in typologically distinct languages like english and arabic. The method uses the information related to the distribution of ngrams, for automatically extracting \u2018meaning bearing\u2019 patterns of usage in a training corpus. The analysis of an english news wire corpus (1,720,142 tokens) and arabic news wire corpus (1,720,154 tokens) show encouraging results.","wordlikeness":0.75,"lcsratio":1.0,"wordcoverage":0.8571428571}
{"Year":2012,"Venue":"lrec-2012","Acronym":"DBpedia","Description":"A Multilingual Cross-domain Knowledge Base","Abstract":"The infobox project extracts structured information from wikipedia editions in 97 different languages and combines this information into a large multi-lingual knowledge base covering many specific domains and general world knowledge. The knowledge base contains textual descriptions (titles and abstracts) of concepts in up to 97 languages. It also contains structured knowledge that has been extracted from the infobox systems of wikipedias in 15 different languages and is mapped onto a single consistent ontology by a community effort. The knowledge base can be queried using the sparql query language and all its data sets are freely available for download. In this paper, we describe the general extraction. Knowledge base and as well as the covering data sets that specifically aim at supporting computational linguistics tasks. These task include entity linking, word sense disambiguation, question answering, slot filling and relationship extraction. These use cases are outlined, pointing at added value that the structured data of world provides.","wordlikeness":0.7142857143,"lcsratio":0.5714285714,"wordcoverage":0.6666666667}
{"Year":2020,"Venue":"louhi-2020","Acronym":"GGPONC","Description":"A Corpus of German Medical Text with Rich Metadata Based on Clinical Practice Guidelines","Abstract":"The lack of publicly accessible text corpora is a major obstacle for progress in natural language processing. For medical applications, unfortunately, all language communities other than english are low-resourced. In this work, we present language (german guideline program in oncology nlp corpus), a freely dis tributable german language corpus based on clinical practice guidelines for oncology. This corpus is one of the largest ever built from german medical documents. Unlike clinical documents, clinical guidelines do not contain any patient-related information and can therefore be used without data protection restrictions. Moreover, such is the first corpus for the german language covering diverse conditions in a large medical subfield and provides a variety of metadata, such as literature references and evidence levels. By applying and evaluating existing medical information extraction pipelines for german text, we are able to draw comparisons for the use of medical language to other corpora, medical and non-medical ones.","wordlikeness":0.6666666667,"lcsratio":0.6666666667,"wordcoverage":0.6666666667}
{"Year":2013,"Venue":"starsem-2013","Acronym":"UPC-CORE","Description":"What Can Machine Translation Evaluation Metrics and Wikipedia Do for Estimating Semantic Textual Similarity?","Abstract":"In this paper we discuss our participation to the 2013 semeval semantic textual similarity task. Our core features include (i) a set of metrics borrowed from automatic machine translation, originally intended to evaluate automatic against reference translations and (ii) an instance of explicit semantic analysis, built upon opening paragraphs of wikipedia 2010 articles. Our similarity estimator relies on a support vector regressor with rbf kernel. Our best approach required 13 machine translation metrics + explicit semantic analysis and ranked 65 in the competition. Our postcompetition analysis shows that the features have a good expression level, but over\ufb01tting and \u2014mainly\u2014 normalization issues caused our correlation values to decrease.","wordlikeness":0.625,"lcsratio":0.625,"wordcoverage":0.6666666667}
{"Year":2022,"Venue":"ijcnlp-2022","Acronym":"COFAR","Description":"Commonsense and Factual Reasoning in Image Search","Abstract":"One characteristic that makes humans superior to modern artificially intelligent models is the ability to interpret images beyond what is visually apparent. Consider the following two natural language search queries \u2013 (i) \u201ca queue of customers patiently waiting to buy ice cream\u201d and (ii) \u201ca queue of tourists going to see a famous mughal architecture in india\u201d. Interpreting these queries requires one to reason with (i) commonsense such as interpreting people as customers or tourists, actions as waiting to buy or going to see; and (ii) fact or world knowledge associated with named visual entities, for example, whether the store in the image sells ice cream or whether the landmark in the image is a mughal architecture located in india. Such reasoning goes beyond just visual recognition. To enable both commonsense and factual reasoning in the image search, we present a unified framework namely knowledge retrieval-augmented multimodal transformer (kramt) that treats the named visual entities in an image as a gateway to encyclopedic knowledge and leverages them along with natural language query to ground relevant knowledge. Further, kramt seamlessly integrates visual content and grounded knowledge to learn alignment between images and search queries. This unified framework is then used to perform image search requiring commonsense and factual reasoning. The retrieval performance of kramt is evaluated and compared with related approaches on a new dataset we introduce \u2013 namely encyclopedic.","wordlikeness":0.8,"lcsratio":1.0,"wordcoverage":0.75}
